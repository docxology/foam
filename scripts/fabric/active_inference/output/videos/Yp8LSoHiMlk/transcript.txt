all right welcome everyone from Co cohort 6 and 2/3 um we're in chapter 7 discussion and then also in cohort 7 were in Chapter 2 so we'll maybe explore a little bit of both of them and also I'll just note that the octopus math sessions are in Full Effect they're Tuesdays one hour earlier than this um the zoom link is here and they're they're doing really fun the recordings are available and they're doing fun math education in octopus is super knowledgeable so if anyone has questions on from from very basic background math to more speculative maths I recommend they look at the recordings or go to those sessions um all right so on seven or or to if if uh it's relevant what's something that anyone would like to begin with so for the discretization of time I feel like there were a few things that I wanted to be a little bit more defined could you say a little bit about how you actually take this thing that the discretization of time or States seems like it could be broken up into smaller and smaller pieces which kind of leads me to believe that there's some bundling that's going on behind the scenes and it makes me feel like I want this discretization as well as the definition of time to be brought into in a little bit more depth could you say a little bit about your thoughts about that yes great question so there's a few different settings where you might be applying a discrete time model first let's let's just go to figure 4.3 kind of the Rosetta Stone with the disree and the continuous time so this image highlights that with the same structural relationship amongst different variables we can have a discreet time or a continuous time active inference setting so that's kind of handled chapter seven and eight that's what a big part of the second part of the book is so H how is time discretized is this a claim about real time is this like an ontological Claim about time or how does it come into play instrumentally so sometimes you may be interested in making an example where there actually is a discrete time counter like it's a metronome or a clock or you're using minutes or hours so again whether or not you took a ontological position that time underlying the minutes was continuous if you were measuring by the minute you whether you're doing trading or or some other kind of measurement you could still make a discrete time just by having um some kind of click um in the mouse te's example it's even less problematic I think because there's no clock time wall time it's just action time so it's like action State step one and then an action is taken so that's kind of like Chess Time where it's disree and there's not even a reference to a wall clock time so that's kind of the simplest setting is where there's no clock it's just like a chess game then the sort of intermediate setting is real time is happening but your modeling discreet units of time like daily or hourly and then the sort of most open and challenging of the settings relatively is where you have a degree of Freedom around how you discretize the time like what should the time discretization be and so there that is basically getting into a parameter sweep over possible Delta T's and then that gets into okay so now we have multiple possible models some are taking at a finer scale some are taking at a slower scale and then do we get like more accuracy or interpretability or are the benefits in accuracy and interpretability worth the increased cost for a faster ticking model so do you mind if I just try and reiterate this again so the question was what uh how do we make the choice about the uh the sort of time that we're going to be modeling modeling in the discrete sense so we're gonna have either some absolute clock time or some purely ordinal sense of events um you know following one another and the question is like how do we choose between those because I think if that is the question uh it would be very use case dependent so you know in in something where there's a lot of interacting components you're going to have to be aware of conflicts between events that can happen but if if you're just modeling a mouse and a maze maybe you can just have a global clock time and not have to worry about the um the clashes of events that's that's the only reason I I bring up that yeah good good and yeah here there's no clock at all it's just it's time point one and then an action is taken and then it's time point two right sorry that's what I meant you don't you don't need a clock because you don't have to worry about clashes so I suppose I suppose it would be use case dependent um the choice let's say like there's nothing a priori in active inference that would perhaps necessitate one choice or the other I I at least I don't see how um in the chapter 6 recipe I mean this is a yeah go ahead well I just me uh let's say you're you're trying to just decide which of those two you know kinds of of times you want to have without the specific use case that is to say this specific example like there there's there I'm trying to think like there's nothing that would mitigate in favor of one or the other like it's very use case dependent I I think I could be wrong though yeah I mean in the particulars every single aspect of the generative model is system or use case specific there's there's nothing that you could reduce your uncertainty about further without reference to a particular system yeah yeah like even in the in the chapter 6 recipe that's one of the questions is you know what form should the model have should we use discrete time discrete State space continuous time continu spad base or or hybrids of them so at the at the system independent scale we're just learning the motifs and the possible moves and then they're all then then they're selected from and composed in making generative model for a specific situation so like in chapter 7 it starts off familiarizing with the discrete time partially observable downstairs sense making part of hidden State temperature in the room observation thermometer readings or in the in the chapter 7 case it's the true note that was meant to be played and then this is the observed note that's listened to D is your beginning prior on hidden state it just kicks off this Markoff chain and then after that it the the prior for the following hidden state is just the preceding hidden State a maps from hidden states to observations so it can be run forward in the so-called generative direction again it's a little confusing with the namespace because the whole thing is a generative model but it's generating data synthetic data like generating observations conditioned upon a temperature of the room or in the recognition Direction taking in observations from the thermometer and then making inference about what the likely hidden state of the room is and then B is a transition operator so s is hidden State at a given time B is the discrete time State update function then you get hidden State at the next time point so this is kind of the the downward facing e that ends up being the downstairs of the PDP so this highlights just the part partially observable component and then action is brought in later and the example in chapter 7 is listening to music so we can look more at that example or we can go any anywhere or any question that someone prefers I I have a question I suppose maybe following on from that PDP with with action so I think that's figure 7.3 if I'm not mistaken but um if anyone else has something they want to jump in with please don't let me stop you so yeah so 7.3 here um there's a bit of a discussion about this you know we've introduced action now uh well we've introduced policies now you know very nice and this this is um we we need the EF to score these policies the question that that I have is about and this is not really I don't really see here in this chapter where it's kind of finessed but on the next page I don't know if that's relatively easy to do equation 7.4 they actually give expect free energy um and of course the you know within the various terms you've got Pi your policy um just kind of um you know interpreting this Pi Pi is a sequence of actions so there is in my mind let's say you know I'm trying to think how do I model some particular um event or you know system I don't it's going to be kind of bad to have to try and form uh approximate posterior Bel over sequences of actions that's going to get quite massive very quickly you know uh you're going to grow you're going to have to form exponentially many approximate posterior beliefs for you know one more uh action that you add to your policy so there there arises pretty quickly this necessity to deal with that problem um and I'm just wondering if in in the text uh or in general if there is a relatively uh let's say kind of first pass you know way we can deal with this problem um the one that comes to mind immediately is to just assume that the um the approximate posterior over policies is you can just do a mean field thing so instead of having to do you know beliefs over things that are 16 times into the future you just say okay I'm going to assume that I can multiply the probability of action one versus action two versus action three and form posterior beliefs over that over you know individual actions and sort of just you know take the product um but nevertheless this this is something that is a little bit more of a general uh query that I've had for a long time about the EF you know you have to to kind of you're sort of forced to deal with this problem of how to uh represent beliefs over policies because those policies can get so enormous very quickly so I'm sorry that's maybe not too uh well formed but alas that is my question if anyone wants to jump in yeah does anyone want to give any thoughts or or is is it related to anything someone's thought or wondered and then let's explore it um yeah so I I've actually seen something somewhat similar to this question recently um where you know like the naive way is to do Monte Carlos sampling right and you just you looking over like you said like every possibility in this really deep tree um and then there's marov chain uh Monte Carlo sampling where you you're doing it very intelligently and there's a lot I'm just starting to read on these things but there's a lot of depth to that and it it serves as a basis to uh sort of the alternative approach to probabilistic programming from ARX and fur um yeah it's quite quite different yeah it's quite different but but it can I guess just one use case I I've seen it on is where you condition on each past time step and and like with RX Ander you can um kind of go you can go back you can trace every single step through it so just like a high level of this algorithm you sample you know one step and you're like okay this is pretty good and then based on you condition off that you sample another and if you run into a problem you go back to like the last good State and then you try a different Direction um that's the high level there's a lot of other really smart optimizations on it um but it it can allow for example like very effective path finding um with minimal tree exploration um yeah yeah so oh yeah go ahead I was just gonna say um that yeah that that's that sounds really cool I I actually did my honors project on this problem last year and I have like a heuristic research thing but the pain associated with that is ah you still have to enumerate some amount of St you know actions into the future for some amount of policies and that gets very painful very quickly um so you know things like heuristic Tre search things like I suppose some kind of Monte Carlo something um they're sort of the only things that I'm aware of uh for for doing this kind of sampling based thing if we're not going to do RX and fur type stuff maybe we're getting a little bit far field now but um this this is let's just let's connect it back to to the the basic problem and there's a variety of methods to to solve it so the fundamental issue is that if we have a given number of affordances per time step let's just say there's two like left or right it it'ss a branching tree then for a function of growing time Horizon that you're considering there's an exponentially growing number of total policies because it's it's dou so it's just like that's the problem of Chess it's a problem of of getting specific about planning any other kind of control method so then there's some some uh structural methods that can amarate this so one thing is like you might do a parameter sweeper search like what time Horizon do I really need to plan for like do you need to plan 50 moves in chess or what fraction of the the value can you get get with five moves of planning or something there's especially an active inference there's nested modeling so let's just say we were doing a move every minute um we could and we wanted to model 120 moves in the future well that might be like a huge number of exponentially exploding minutes or maybe it's only two hours okay so then we have computational methods for approximating these comp um computations because we're going to think of it as this kind of inference problem and just because you can write out the inference problem it's like okay but but now the observations um it's a 4K video 60 frames per second and the hidden state is going to be like the you know so just because you can write the equation simply doesn't mean like that there's enough computational resources on Earth to calculate it but the equation still might be very simple so then when that is happening there are as kind of mentioned there's several approaches ranging from different sampling based approaches which attempt to empirically approximate complex or large or high-dimensional distributions by speckling all over them or taking smart paths through them and then looking to reconstruct the entire posterior so in this case we have like an action prior is what we're taking into this whole inference question we want to update that action prior and we want to update the probabilities of taking different actions according to their expected free energy which is to say we want to like UPR rank actions policies um that contribute pragmatic value observing future observations with our preferences and and those that contribute epistemic value okay so that's this question a few few different ways to deal with it you can structurally deal with it with changing the structure generative model you can do sampling based or variational inference based approaches um let let let's also get specific here's in pmdp um tutorial one this is what it looks like to to get the expected free energy so this is like operationalizing an equation like this and this is um programmed in the PDP um imperative style so it's going to give a sort of procedural code for how you calculate the expected free energy for each policy what expected free energy does is it just takes in the policy vector and for each policy considered separately it calculates this value so it's shown here and it's there's other ways to compute it there's other ways to write out the procedure to compute this but this is that sort of uh imperative style of pmdp where calculations are described that take in certain inputs combine them in certain ways and result in certain outputs in this case like the expected free energy of moving left or right um then there's the just more again general questions for machine learning about like uh policy rollouts branching time active inference this was described in several papers this is a a method of heris tree search like rolling out certain areas of the tree um that's oh leave there but yes expected free energy calculations can be um having growing computational complexity because if you want to be dealing with something that's a large policy space over long times then that's a a pretty large challenge yeah it's just it's frustrating to have such a nice beautiful equation that that quote unquote sols it and you know it's it's it's theoretically beautiful but it's um it's very very difficult in practice for I suppose non-toy problems to actually do um analytic anyway that that was that yeah that was my um main concern yeah I mean um look at the computational resources it and and uh compare it with other methods because that's true yeah it's it's it isn't that each of these expected free energy calculations are necessarily that resource intensive also they can be paralyzed in the dispatch because Computing the G for policy one and policy 2 those are separate so that's a highly paralyzable step and and again it just depends on um the overall like if you had a simulation with a thousand nestmates making a single step of action then the computational bottleneck in that simulation might not be G if you were doing a single agent doing a thousand time steps of planning maybe it would be yeah that's probably how we actually get around it is by by having many agents kind of work together with with relatively low time scales yeah interesting to to think about pretty pretty cool yeah Nest I mean and multi-agents Andor nested models reduce the need for explicit planning because they have fewer explicitly planned state or or like in live stream 42 with the robot slam the simultaneous localization mapping the lower level model was the postural model so extensive planning wasn't needed there because it was like body posture then the higher level model was essentially the location of the warehouse so then again planning was rarely more than one or two nodes deep so yeah that's because you had that extra extra structure of of this multi-layer uh model I suppose yeah yeah yeah whereas trying to plan posture to get across the warehouse would have seemed intractable oh awesome thank you Harris yes exactly yeah then also this is sort of this would be the the deeper the deeper Tech angle would be like um as discussed in some of the message passing in Magnus cool dolls work in um live stream 55 generalized free energy encompasses variational and expected free energy and it enables another kind of nonplanning based way to talk about reducing uncertainty about Pol policy in the future okay stockfish uses Alpha Beta pruning search algorithm Alpha Beta pruning improves Mini Max search by avoiding variations that will never be reached an optimal play because either player will re rect the game yeah that's That's a classic chess algorithm and you can Implement in your in your script um any any you could say only evaluate G for the even policies or um any policy with three of the same of um action selected in row just remove it so there's there it's not like we're procrustes and we're being forced to do a calculation these are just ways that you can choose what you want to calculate but it it also may be a lot clear in a specific case and each of these are just Matrix operations so again if you're dealing with massive matrices that you can't core grain or or um discretize so like you want you have a large Matrix and you want to have continuous um approximations and Etc it's like well then you're setting up for a larger problem but if your matrices are discreet or sparse then these are super fast calculations and that's kind of the amazing thing is there isn't any need this is again the contrast with with reward learning there's no secondary like well let's calculate the reward associated with the policy question mark question mark question mark and then use that to update policy it's like know just directly calculate these exact conditional probabilities that are written here reweight your policy prior into the policy posterior and then select from it or like shown in uh policy 5 here's a more of a biological inspiration on that um challenge with the dopamine balance so here it's it's saying well this is sort of habit on the left here policy is being determined just by The Habit so e is your is the policy prior and so this is the kind of like Thinking Fast system one and then here's thinking slow system two where there's a deliberative reweighting of policy so then it then there's this second level question which is when should I deliberate and if maybe only deliberation has to be engaged in a tiny fraction of cases and habits can be carried through or updated and then just reused okay chapter 7 so starts off again raise your hand or or write if you have a question but it starts off familiarizing with the partially observable Markov model or just the hidden marov model just introducing okay we're we're we're putting some space with the a matrix between the temperature in the room and the thermometer reading so if it if it were fully observable like here's the location of the chest piece on the board and this is the observation of the chest piece on the board then it's a fully observable marov process there's an example with music playing then action is brought in action is not intervening in The Hidden directly it's not it's not reaching into the hidden state of the room's temperature and modifying it it's more like selecting an index card from the B tensor and then selecting which index card of B is going to get applied to update the state so then it's like there's one Matrix transition Matrix that describes air conditioner on and there's one that describes heater on and then the air conditioner Maps like 25 to 24 Maps 24 to 23 so it just drops all the temperatures down by one and the heater one maps all the temperatures up by one so what action does is Select which slice of B is going to be used to multiply with the prior state of s to reach the current state of s so then then the question arises well how do you choose which policy to take and and here you could just use habit like use just a fixed uh decision rule or probabilistic Draw from a fixed uh policy prior or there could be a another way to select and that's G one important figure for G is 2.6 so here's G again pragmatic value is now here on the first term that it it it it doesn't matter whether it's first or second term pragmatic value epistemic value so an important relationship to remember is if you only have pragmatic value then you have utility maximization so in a situation where there either is no epistemic value to gain or it's just not considered the special case of expected free energy without epistemic value is basing decision Theory expected utility Theory conversely if you have no pragmatic value again it's a situation that doesn't have it or you're choosing not to model it or you have like a uniform preference over outcomes measurements then you just get 2 three four five and you get Infomax so then having both it then beets this question well how do you balance both and that's where we see some of the slightly [Music] um potentially inflated um language around like dissolving the explore exploit dilemma does it does it dissolve it or does it articulate it so that we have a degree of freedom to balance it and and adaptively explore Solutions but is that dissolving it so just just writing it out this way like doesn't solve action selection in general or for a specific case but it's um a setting that allows modeling a a tremendous range of cases which isn't an answer itself but that's kind of the reason why there's so much parameterization to do and why fine-tuning the balance between these two is an interesting topic okay back to seven o interesting it just like reloaded differently didn't it okay then action comes into play so saying we took that music listening example but this was a passive music listening example so now we're considering action inference unified inference problem sense making downstairs action selection upstairs little bit more detail revisiting equation 2.6 saying here's the functional that's used to update the policy distribution it's an energy based functional that has a pragmatic and an epistemic component sometimes it gets little tricky with like are we talking about the negative free energy or the the the negative of the free energy suffice to say just check the script and see how the values are being stored and then just confirm it and verify it like with what you know is like a good policy in a limited situation what you know is going to be a inferior one relatively and just calibrate it and reflect it and then add it in the documentation um so then we get to the Tas a classic decision example um here we only have like one a matrix but it turns out that you can actually have multiple A's that can have the same or different shapes so it's kind of like parallel A's A1 is mapping the sense making relationship between location and the observation of the location the reason why there's four columns is because there's four locations 1 2 3 four but the reason why there's five rows is because there's like five sense making outcomes because the bottom location can either give this l or the R so when we're in context one which is to say that the bottom Q is going to reveal an R then deterministically being there will reveal the r again this just begets the question this is just all modeling well how does the mouse come to know that locations reflect their location or how does it come to know the semantics of R how does it come to associate r with the right side like that that is the question that's what then but then you focus the map it's like saying well um it's a subway map of the city but then where where are the power lines it's like there's a lot of ways to make models of the situation the second a uh component is mapping from location to either the absence of either food or a verse of stimuli or mapping the probability of of measuring the either um food or aversive stimuli here's context one where the Q tells you it's on the right and if you go to the right 98% of the time there's the food here here's context two so you see a swap now the one is here if you go to the Q you get the L and now the 98 is there but the 98 was here and the one was there so that's the kind of like Matrix surgery that in the nitty-gritty a lot of this gets down to it's like what are the rows and The Columns of a now we into B here these are um as PDP goes into in a lot of depth you have transition for um controllable Transitions and uncontrollable transitions uncontrollable transitions are one where the agent's policy selection has no efficacy so it's not really a control setting it's more just like a Time series unfolding it's kind of like a passive inference element and then the B Matrix describes the agent's beliefs about its efficacy of action again this is another question well what if the um driver believes that pushing on the gas will will have this B Matrix effect on their acceleration but actually the car has a different acceleration if that were the question you were modeling then you might look at a situation where there's a misat match between the believed efficacy and the actual efficacy but the simpler setting is one where like what the agent believes an action will do is what it actually does but that's a major degree of freedom in modeling is how do agents have accurate representations of sense making and action then so it's just it's the ABCs it's like Sesame Street then it goes into describing preferences just like reflecting the one and the two of the a A1 is location A2 is basically food so C1 corresponds to location there's a negative one kicking it off the starting blocks and then it has a flat locational preference this gets into interpretability and AI alignment and safety in all these topics because here we can say it's not learning a proxy measure there is no pragmatic value being contributed by moving up in the teamas OR moving to the right we can interpret this model and say it it does have an e to the 6th pragmatic preference for getting food that is contributing to pragmatic policy these ways you could even do Trace back and you could look at how the expected free energy values were contributed to by specific pragmatic and epistemic value components and in the simple case it's all just Matrix math so that's the preferences here again the thing to kind of look out for is are the preferences being uh encoded in exponent form already in which case a six is e to the 6 fold preference or you might choose to in your program or in the program that you're using six might reflect a six-fold preference and then and then under the hood it gets exponentiated and just dealt with a different way a b c d here D1 location it begins with perfect knowledge that it's 100% in the starting location D2 that's about the food so D2 is saying so it's about the food context it has an equal probability here this is also alluding a little bit to the learning by counting so this one one is like it's one half of one one so it's 0.5 0.5 but learning by counting what that would look like would be every time that you um so this is trial by trial learning so let's just say that it was on the right side then you could change this to one3 1 comma 2 so then that that would be equivalent to saying 3 and 67 and then now let's just say it went to write again it' be 1/4 1 comma 3 then it' be 0. 25.75 so that's the learning by counting because you basically just increment what you've seen and then if it's 1 million 1 million then it's like a higher Precision belief because seeing one more is only going to move your update a tiny bit the Taz is again mobilized to focus a little bit on exploration exploitation setting and information foraging continuing on the epistemic value theme they move into an is Paradigm but it's separated by from where it's brought up by several Pages there's a box that talks about Precision just in a general sense here's a seccade model there are icade models with discrete and continuous elements then a discussion on learning if there's no hyper prior there's no hyper parameter for a given parameter of Interest the base case is that it's fixed and not learnable so when we're looking at the teamas we're looking at like this a matrix this a matrix is being used to run forward inference like to run sense making and um to the extent that it has values that contribute to it it also contributes epistemic values to different policy but the a itself is not learnable it is not an object of inference it's not updated the values are not updated in the base case but this describes how you could make a basian model where there is updating of the a the B Etc this is the learning by Counting more unpacking of equation 2.6 more different decompositions of expected free energy and demonstrating that in this maze paper it's bringing in a lot of topics structure learning viewing alternative structural models including with different numbers of parameters as essentially a policy choice of the modeler so then that allows you to have the metab basian approach to basian model selection and then here hierarchical nested modeling and a nested model so seven is kind of a lot but it covers starting from a a passive listening example all the way on through action and then some of these uh doing a little bit of showing not telling on unifying cognitive modeling and treat in different cognitive phenomena in terms of free energy does anyone have a question or we can look at some of the written questions or please please add many questions to these staples nothing for me right now sorry this was some I think this was Eric sounds from several years ago it's like even taking another level of meta there is no dilemma explore exploits already sted in the clearest way yeah I mean there's there's trade-offs of that kind everywhere efficiency and robustness and so on so it's not like you have to choose one of the other or that if you choose one you can't choose the other yeah I suppose you know if he gives you a principled way perhaps of deciding which one you should trade off at any given time maybe that's what it's meant by it solves it but yeah it's it's a question for interpretation of what it means to solve it's like saying because we have a volume knob you don't have to worry about the volume it's like no that means the sound engineer is worried about the volume it's like he has a principled way of controlling it yeah and and there's an interpretable simple way so for example you could have a uh an agent with a fixed epistemic value so kind of certain uncertainties known unknowns about the world and then there's a policy on pragmatic value policy on the preferences like I scale up and down how much I am excited about food but studying is always the same amount so then when I want to focus on studying I just kind of Turn Down how much I care about food and or turn it up if I want food or you could say I don't have control over C in this case and then you could explore well but could we tune the epistemic side to modify essentially the Curiosity from lower to higher levels to affect the same concept but whether you're modifying the the scale of the pragmatic value or the scale of the epistemic value like you still need to then have this question well what what would make an Adaptive approach to navigating that tradeoff let's just see what else we have what I cannot create I do not understand some people feel again more funny comments that people have written the the mild answer okay the mild answer you need to implement the algorithms and see how they perform well the cool thing is there's notebooks like in RX Ander and pmdp where even if you don't have any program um language installed locally like just in the browser you can hit play and get the active inference agent that may help certain people if you'd like to contribute to those efforts and improve the notebooks that's a great way because a lot of people will benefit from it stronger answer here okay let's see math and theory and hand waving how the brain might do thus and so are all weak te the only way to really prove the idea is to build artifacts that do interesting things especially things that other methods cannot accomplish that's sort of the embodied robotics JF cler moral comp moral computation method rows and columns these are great understanding checks like if if you look at the rows and The Columns of all the matrices um that are reflected in figure 7.3 you basically understand the discrete time active inference model because that's what it is each of these are matrices or tensors and then the operations are essentially manipulations or multiplications on these matrices so that's a very deflationary approach to active inference but it's super informative because it's like there's not like some other magic layer that then is is applied here similarities and differences between prediction error and free energy gradients that's a good one does that does that show up in uh chapter seven like um prediction error specifically it looks like it's in the caption some people use them including sometimes even possibly authors who who know better slash differently but and and there's so it's you know words words words etc etc etc but a prediction error is denominated in the units of what the prediction is about like if I predict that it's going to be 30 degrees and then it's 32 degrees the prediction error was two degrees surprise is denominated in units of information Theory like the knots or the bits depending on base e or base 2 and then for free energy is is a unitless quality a quantity that that's that's just a calculation so you could have a free energy relative free just like you could have a maximum likelihood or a likelihood value for 30 degrees and 32 degrees you could have a free energy value for 30 and 32 degrees as part of the model fitting with respect to a generative model but once you get into talking about free energy or surprise you're talking about with reference to a given generative model whereas pred ition error in the kind of narrowest sense also it it does entail the model making the prediction but it is denominated in the units of what the prediction is about h i I like that the talking about what the prediction is about because I think the two aren't inherently so related for example like if I'm trying to improve my pragmatics or something like that in the sense that maybe I'm playing a fighting game and the uh if I'm understanding this correctly the actual loss what the model is reflecting back is that I'm practicing my combos I'm practicing some inside of that that is going to be some instrumental thing that I want in order to improve at the game but in me doing that I actually start to lose more often so the actual expected loss the actual thing that I expect to see is me improving my model of how the game works while the actual reality of it is that I losing at the game more I think that's a interesting thing oh okay so it's like a game where um you're or it's like I wonder what will happen if I play this differently and then you might be learning more even though the so kind of leaning into the epistemic side like yeah what if I what if I try to play with what if with just this weird chess strategy with a longer term pragmatic value or something that's an interesting point about the um prediction ER is being denominated in specific units because it's it's called you know it's about some qualitative phenomena out there whereas the VF is or you know for it's just more about the fit of your beliefs that's that something I hadn't considered yeah free energy value is a lot like a model likelihood value for several reasons first off model likelihood saying maximum likelihood ml is minimum surprise so when we say that free energy is kind of bounding surprise like in the special case or or in the complete case the free energy is one and the same as the surprise minimization which would make it the the the evidence maximization or in the sort of more General approximation case it's it's heading closer and closer towards minimizing surprise so it's heading closer and closer to maximizing model evidence but key similarity with likelihood estimates uh or free energy estimates or like sum of squares is they're not comparable across different settings like the sum of squares the L2 Norm that's used in linear regression that depends on the number of data points you add another data point and even if it doesn't change the regression line it's going to like it it it changes the sum of the squares of the residuals but that so that's why the sum of squares are not compared for like data sets with different numbers of value so free energy will also C EG sum of squares likelihoods they're they're like a statistical diagnostic I mean these are summary variables on maps again this is part of the the deflationary educational Elements which is like once free energy is seen as the calculation that it's described as in the text ttbook it puts some of the discussions around real systems and their free energy into a different light because it'd be like well would would this person also argue that the sum of squares of the regression between height and weight is a real thing in the world okay would they argue that the free energy value of the relationship between height and weight is a real thing in the world so then what is the claim any last uh thoughts or or or what people plan to like add for questions or work on for the coming few weeks I guess the only thing is um the this this the thing that I find is natural to follow on from this discussion is the kind of stuff I brought up earlier about um particular ways to deal with the explosion in the policy space that's that's kind of where I feel like uh this this whole chapter like where you would want to go after this let's say so that's that's one thing I I'm very interested in is uh trying to trying to further those huris means of you know navigating those very large spaces yep that's important topic um I mean also when uh build things out and whether timing them like just using logging and timing functions in the script or or using other ways like just explore and be like yeah what what is what does the runtime look when I plot the runtime for time for 1 2 3 4 five step time Horizon like what does that look like and then this model stream 6.1 this is really interesting work with the branching time active inference and this is some of the only still actually some of the only computational complexity like Big O notation I don't know why it's not loading but for active so so look at that if you're interested in in computational complexity estimates but that's another great advantage that with our X infer and everything we will um realize which is like we let's just say that we knew okay temperature it's a number between one and 100 observations number between one and 100 this has this defined dimensionality this has this defined dimensionality in the discreet time we could say exactly the RAM and the CPU and all these other attributes about the model statically so then you could anticipate resourcing deterministically even for a probabilistic model there's a there's a lot of things like that and then you can say well now we have three ways to implement this we could we could do the full policy roll out you know that's going to be 1999 or here's the you know half off but we only we only sample half the policies uh Daniel would you be able to link that that Big O paper talk you'd mentioned it's actually something curious about ranching model stream there's actually two papers I believe there's the there is the original paper and then there's a followup which is the kind of Big O stuff I think yes L link that or just add it you know any and then anywhere we search we'll find it but yeah that that that's cool and uh yeah that was like sort of our I think just last week in ARX and F we were discussing like how much other computation is there like what is the overhead what and then what like if the payload is this sort of like hypothetical minimum computational complexity of just storing the matrices and doing the operations and then it's like how much is the computational burden of the message passing and like what are the situations where the Matrix math has a given computational complexity and then the message passing has like this or that right yeah that's a big open question to me um hope read more about where it's covered just because there's so many different message passing scenarios yes okay so um all right paper two that's the the the sort of theory and then paper one is the uh complexity time complexity class analysis I believe cool thank you f cool well um see you all either uh at this time like next week or later today Andrew will be uh facilitating I believe so enjoy um talk to you soon and not anything till then all right bye bye by