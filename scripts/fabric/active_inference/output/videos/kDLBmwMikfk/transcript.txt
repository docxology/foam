Hello, welcome everyone. It is May 30th, 2025. We're in Active Inference Math Stream 14.1 and here with Jesse Venomstrom discussing a concise mathematical description of active inference in discrete time. This will be a very useful and epistemic discussion. So Jesse, thank you for writing this paper and coming on to you to introduce it and start walking us through it. Yeah, thank you Daniel. Thank you for the opportunity of uh being here and allowing me to uh present my paper. Yeah, first of all, I wrote this paper together with uh my colleague Carlo Langanger and my supervisor Niad Iay. We are all in Hamburg at the Institute of Data Science Foundations and um yeah about three years ago I um started getting interested in active inference and um wanted to know really exactly um how it works so to say but this uh I had quite a hard time figuring this out and that's a bit why this paper came about and um maybe I can I also have some notes here I can so maybe I can make very specific my question so what I wanted to know was okay we have an agent who has observations 01 up to OT and has performed actions A1 up to 80 t minus one and then what I wanted to know was an uh a agent acting according to active inference what will its action at how will it choose that and this question for me was so with the literature at the time I found it hard to really pin it down so to say so that was the first uh main objective of making that more clear and especially also making it clear in a way um that is with in notation that is very accessible to people who have studied probability theory or any other type of standard mathematics and um yeah and then throughout the process um a second question came up so that was the following so um so I will go into more detail on this later but [Music] um an agent has something that's called a generative model and uh how to understand that is as follows. So uh the world we can first look at the real world and that is uh generating observations. For example, observation one the world is in world state one then uh observation two. So this is all happening in the world but then an agent. So this is what we could call a generative process. and uh a generative model is as follows. So this this is some kind of where an agent has um maybe some kind of simplified version of what's happening in the world. It is representing in its brain and um from that world from that some kind of simplified world state it can predict okay what kind of observations can I expect um from this state and this is some kind of a very similar uh model and in both cases we can also actually include actions. So if it acts on the world it will influence its next world some kind of simplified world state and the same also here it will also influence the real world state. So this is called a generative model and um this generative model it uses it to um to choose its next action. But then the second question is how does it learn uh uh how does it learn this model? So um this paper is basically we can basically divide it into two parts. One part is called inference and the other part so this is we could say also action selection and another part is called learning generative model. So um this presentation will also be um somehow divided into these two parts. So I'll first go further into this first question. How does the agent select its actions and then also uh this second later on I will go on to in the second part how does it learning the generative model and uh please if anything is unclear please feel free to ask questions. If you're now in the live chat or if you're watching this later, you're also always welcome to send me an email or get in touch in another way. Um, if you have any questions, I'm really happy. So I wrote this paper for people in my situation three years ago who really want to know the exact mathematical details of how active inference is formulated and uh want some kind of an accessible easy overview and um if you have any suggestions for improvement I'm also very open to hear them. So uh yeah so that's the basic overview and what I have to say as well is that this is all as you saw also from the symbols and stuff this is all happening in a discrete space uh discrete observation space action space and discrete time so there's a whole other literature on the continuous side of active inference I'm not going into that right now so this that's also not what this paper is about it's only about the discrete time um formulation. Um yeah, I'm not sure if we already should stop here for some questions or that I just uh continue going deeper into the Yep. continue. Thank you. Okay. Yeah. So maybe so one point maybe I should spend a little bit more time on it is this idea of the generative model. So um maybe I write a new one here. So what I think is important to emphasize that this these states s are really uh inside the brain. So we could say uh this is really in the brain of the agent and um from these states it's predicting observations and usually this is some kind of simplified uh state of the world and it needs is to learn these states without anyone telling it. Okay, so maybe in machine learning or something you're familiar with supervised learning where you get an observation and uh then it might learn which state do I have to associate to it. That's not the case here. So the agent really needs to learn itself. Okay, what is a good uh state to somehow encode this observation in so that I can do a useful planning. So this is you could call unsupervised learning and there is no way of saying okay the real world for example is generating observations and therefore this state should be somehow related to that uh the real world state. Um yeah and to also uh maybe uh make a bit more clear how it can be simplified. So for example, we could have an agent. So we could have a world in color for example that uh like there's all different colors in this world. But for example, an uh agent that can only see uh light intensity but no color will have also a simplified um simplified uh state of the world so to say. So that this will only be black. We don't have gray I see but this is white and this is [Music] gray. So that's just to um just to emphasize this point. Okay, this uh generative model is really something that's inside the brain of the agent and it can use it to for example plan ahead. Um yeah, then I'll go back to the paper. So um I will start with some notation. So [Music] um yeah so what we are considering is an agent which um which is going through capital t time step. So it starts at time one and um has also some kind of finite uh time t and it's performing actions the whole time and um um receiving observations so that we can also see here. So we have observations and I'm using toao the Greek letter toao for any time point and t for the current time point. So you'll see that appearing more often this distinction in the in the paper. And we can use this subscript uh with the colon to uh have like a sequence of observations for example. And then very important there's this word policy. And actually if you come from reinforcement learning background this has a little bit different meaning but um here in this context a policy is a sequence of actions and more specifically a sequence of future actions. So that is sometimes I find in the other literature not so clear but here I want to make that very specific. So we have a policy pi t and that is actions small t. So there is also small t. So actions small t up to big t. So a sequence of actions and pi will be then the whole from time point one. And so usually I write a for actions that are already performed in the past and pi for the future actions that it somehow needs to do planning on. Um yeah and uh we have this generative model which we uh write like this and um so it's a probability distribution over observations and states given actions and this uh theta is the parameter of the uh of the generative model. And in this first part we uh leave this uh theta we leave it out in the notation because we are assuming okay it has learned this generative model and we're just interested in how it's using it and then in the second part we will actually look at we will include it again to see how it's learning this. Um yeah and then also a big part of active inference is of course that it needs to infer in what kind of state it is. So what kind? So it has an observation and it has this generative model and then it somehow needs to infer okay what kind of state could I be in and um we use this that's also in general in the literature we use this letter Q. So for example an uh an agent receives this observation 01 and then wants to find out okay or express its belief in which state it thinks it is and that we write for example like this. So this is the belief. So this is a probability distribution over states and it has as information this first observation and uh in general that's always or we hope that it's approximately equal to the true posterior um of the generative model. We will go into this later some more and how it's sometimes exact and sometimes not. But for now it's good to uh remember that this Q is expressing a belief about the state that the agent is in and we sometimes so for example if we uh I can also show that here in the paper. So uh so this is the the actual generative model here and here we have this belief over states and what we are doing to make the notation a little bit more concise is to um write QT for example QT uh of ST let's say [Music] Um then this T is implying all the information that it has already from the past. So this is we can also write it QST given all the observations it has made and all the actions that it has performed. So this T is just saying okay we include in this belief all the information all the observations all the actions to form this belief so to say. [Music] Um yeah so I think I have discussed now the most important things from the notation then we can I can have a break here also if there's any questions [Music] or otherwise I just continue. Sounds good. Continue. Thank you. Okay. Yeah. So then we immediately so that was also one of my goals of this paper is to immediately get down to this main question. Okay, which action is the agent going to perform? And we immediately arrive there already. So um the action selection procedure can be described as follows. So we have here a probability distribution over policies. So policies were sequences of future actions. And um we say according to active inference an agent selects its next action by sampling a policy pi t from this distribution and selecting the action a corresponding to that policy. So I can write it here again. So we have this distribution for now. We can uh uh ignore a little bit um how exactly what exactly this um how this distribution comes about. I will get to that in a minute. But we can just say okay there is some kind of distribution over policies over future action sequences and we sample some kind of uh pi t star we sample it from this distribution. So we have this pi t star which is a sequence of a t star up to a t star and then we are performing this action at star and yeah you could say forget about the rest. So this is all right I'll do it like this. this is somehow uh we don't are not interested anymore. So we are only performing this action 80 star and then we go to the next time step get the next observation and then we'll do this whole process again. So this for example this uh a this action a t + one is somehow discarded. So for the next time step we'll do the process again and then take the first action from that uh sampled policy and that in a very basic way is already some kind of answer to this first question. Okay, how is it selecting its next action? And what we see is so it has received uh these observations and performed these actions and they also flow into this this probability distribution is dependent on this um on these observations and actions. And then the next question is okay how does it compute this probability distribution? So [Music] um that would be the second question to answer if we really want to get down to how is it selecting its actions. And then we see the main ingredient of this distribution is this function G which is the expected free energy and I have written one uh way of writing it written it here. Um, one one question there, Jesse. Um, yeah. Why is the policy conditioned on actions from the past when I don't see the A1 through T minus one explicitly entering into the unpacking and policy inference is based upon the observation coming in and how that affects the belief inference. But how does or why do you have a conditioning on a as well? So the function g is dependent on a. So in order to compute this function g, we need to know which actions have we done in the past. Do we though? Yeah. Because it's but it's a bit hidden in the notation here. So we use these past observations to perform inference uh about for example here our belief in future observations. So in this T here in the Q it's it's captured. Okay, this is a belief about future observations but with the information from observations from the past and actions from the past. It's using those to form the beliefs about the future or here for example the same for future states or here. So in order to make this distribution, what state will I be in in the next time step? Um it is using uh some kind of belief about what state am I in in the current time step which then again depends on what actions and what observations we have in the past. Okay. So in in practice the B transition matrix embodies the expected consequences of different actions but here it's just written in terms of being conditioned upon the whole past sequence of observations and actions that like would have led to that transition matrix being learned. Yeah. So here uh we are assuming that the whole um that whole B matrix is already learned. So that we can just take as given and we only use this O and this A for performing state inference so to say. So we need to know somehow in what state am I now and then you can go back and think okay what actions did I perform in the past and then that gives you more information about what state you could be in now. And I wrote this very explicitly because often in other uh literature this dependence is somehow completely uh not made clear and then uh I find that sometimes a bit confusing. So that's why I wrote it very explicitly. Okay, in order to compute this distribution we need this information so to say. So that's why I wrote it in this way. Uh does that answer your question? I think so. I just still I just still don't I know it's sort of a small thing but and I I see that that's it's that you've thought about it. It's just I don't see any of the past time steps in the right hand side. So why would the action have that you took 27 time steps ago other than how it influences how you believe the action to work now? Why would the specific memory of the action a given number of time steps ago bear into the calculation? Now, so for example, if we take this distribution, right, which is uh our belief in what state we will be in in the next time step then it I think there's many uh situations where that we can think of that it's relevant what I did to steps two time steps before. So for example, if two hours ago I decided to fly to Germany, then that will still have influence on where I will be maybe the next hour. But like in a grid world example, if the transition matrix is the up, down, left, right, the fact that you moved a certain way 25 time steps ago doesn't need to specifically come into play when you're thinking about the next consequences of your action. So let's say we have a grid world uh which is just 1D. let's say 1D grid world and you can go left or left or right. Those are the two actions. Um, and let's say this agent knows I'm always starting in uh location zero. And um let's say it uh or let's say differently here. Let's say here zero. That's a bit easier. And then [Music] um every action you did in the past will have an influence on in what location you're in now. Right? If I for example we are now at t = 3 then your location will be different if a1 is left compared to if a1 is right. But is it conditioned upon your current location and the consequences of action from your current location only or is it specifically conditioned upon an action that you took in a past moment other than how that action resulted in you getting to where you are now? Ah maybe I sorry that took maybe long. So these actions are really the actions we performed in the past. So not any random actions but really only the one that we performed in the past. Is that your question or the past? Yeah, I we we can continue. I just I see the past actions matter for changing where you are and if you're able and and you could do other learnings and so on but the past actions themselves don't influence where you are or your beliefs about the consequences of action which are all that come into play in the operational calculation of G which doesn't specifically invoke a later okay yeah we can maybe uh we can maybe postpone it to later to see if we can uh yeah make it uh more clear. So um yeah so maybe to get back to the to the uh so we wanted to know okay what is this what will be the next action then we saw okay there's this distribution uh that we sample policies from and uh from there we take the first action and that will be our action and then we are Now looking at okay what is this function g that is used for uh for uh computing this um probability distribution and that function g uh looks a little bit complicated. So we can just now uh see what is all coming in to compute this uh this probability distribution. So we see um there is this uh this distributions Q which are our beliefs about future states and future observations. So those are and we will discuss later exactly how it can uh derive these uh distributions and they are all dependent on our policies or the next possible actions. So for each different policy we'll also have different beliefs about what observations we could uh get in the future. So those are the main ingredients here. And then we have this distribution PC which is a preference distribution of the agent. And this preference distribution I also wrote it here. It's completely distinct from the generative model P. So it has nothing to do so P is a certain distribution and then PC is a very different distribution. And so they have nothing to do with each other in that sense but it is a an ingredient for computing this uh expected free energy. Um yeah maybe also nice to see. So for example this uh distribu this distribution also in the paper of uh lens costa and others uh we see it here in equation 10. So he also describes it here and actually we don't see it it's just a black bar. Oh, that's But you do see the notes, right? Now it's just the tabs. And now, yeah, still just the tabs. Um, this weird. Yeah. Maybe reshare the screen. Yeah. Did something change or not? Yep. Okay, now we see the paper. Okay, great. Sorry about that. Oh, good. And now you see the notes. Yep. And now you see the other paper. There we go. There we go. Yeah. So, this is the paper of Lens D Costa and others. and he has here the same um distribution uh over policies and then actually in Thomas Par's book and others um there the distribution is a bit more complicated they also have an E and an F term and I'm writing in the paper uh I'm was planning not to go into that now but I'm writing in the paper here also a note on why um they have those terms. But then you can at least see okay um this uh you can find this distribution in these I use those two as main sources for writing this paper. Um yeah so let's so maybe people who have already studied some active inference are more familiar with what comes now but I thought just to briefly discuss uh this expected free energy and how you can think of it. So um we have the equation here and we see that there are two terms and this first term is an uh KL divergence between two distributions and this first distribution is distribution over our beliefs in what state we will be in in the future given that we have some observations that which observations are again appearing here and we compare this distribution. So the beliefs about states given that we have already the observations with this distribution where we don't have the observations and um so what you could say a little bit is if these observations are very informative then our belief about the state will be very different from when we don't have the uh observations. S so then this difference will be large. But if these observations are not so informative then these two distributions will be pretty similar and then this difference will be small. So the size of this difference says okay how informative will these future observations be? And these future observations depend again on the policy. So we somehow take an average over what future observations do we expect given that we have this as next actions and given that we already have some observations and actions done in the past. Um so this first term is called epistemic value of information or information gain. And then uh this second term we see this preference distribution here come up. So this is the observations that the u agent would really like to make and the distribution here is um the observations that it thinks it will make if it is going to do these actions. And so um the higher this is basically the more likely this uh policy will give observations that it actually wants to have. So we want this um and that we see also if we go back to this equation. So we see the lower the expected free energy of a policy the more likely it is that we will sample that policy. And we also see that uh the higher these two terms are because of this minus sign the lower the expected free energy will be. So we want both this utility and the information gain. We want it to be high for a specific policy or those are policies that we are likely the agent is likely going to sample. Um so that is one way to think about the expected free energy and then another equivalent way of writing it is the following. So here we actually don't have this minus sign. So these terms we both want them to be small or the uh the smaller they are the more likely that policy is going to be sampled. And this term here is an entropy. And it's basically um looking at the entropy or you could also say uncertainty about the observation if the agent knows the state it's in. So if it's in a certain state but um even though it knows what state it's in it is very hard for the uh agent to predict what the observation will be. then this entropy will be high and then so therefore also this ambiguity will be high and therefore the agent prefers to be in um states that have low ambiguity. So that's this first term and then the second term is again a KL divergence but this time between the uh belief uh that the agent has about its future observations given a certain policy compared to the observations it actually wants to have. So if this difference is big then it believes it will get different observations from the ones that it wants to have. So that you could say is bad. So this term which is called the expected complexity or risk those uh um we we want it to be small. So that's a little bit of interpretation on this um on this term uh on this G that is used for calc for computing the distribution for sampling the policies and therefore the actions. So there is [Music] um one more thing is that we can that is for the computation nice. So we can make some kind of approximations. We will come back to that later as well in some more detail. And then this G actually we can write it as a sum of different uh of like single time step G ts and that makes all the computations a bit easier and um I know that there's much more to write or to say about this expected free energy and where it comes from but I think that was not the aim really of this paper. So then I refer you to these uh other sources. Um yeah. So now we have discussed this distribution where the policies are sampled from. We have discussed this G. But then an important ingredient of this G of computing this G are these uh distributions Q QT. So this belief about future or about states and about observations and also in the actual often we are actually interested in these so not over a sequence of states but just over one specific time point towel and um I was actually thinking for this podcast not to go into this too much because I think it's relatively straightforward. So, um at least what I write here because I think my uh I think uh the philosophy of this whole approach is that we want to use some kind of basian um basian approach as much as possible. And what I have done here is just describe exact basian inference which would be somehow the simplest way of forming beliefs about states and observations. And then actually in more advanced literature so to say you could say that is some kind of an approximation of this uh exact basian inference. So I would say you could read through this. I can maybe uh show one small example. So we have this um generative model over observations and states given actions. And you could also write this model uh in this way. [Music] And there we see that this model factorizes. So maybe I can just use what I already wrote down. So we see that we can really somehow decompose it into very simple elements where we have a prior over the first observation. This one um we have a distribution that is telling us okay if we are in a certain state and we perform a certain action what state will be the next one and we have a distribution that is saying okay if we are in a certain state what observation do we expect to get. So those are the three ingredients which are also often referred to as the um maybe I write on this also. So this is called the usually called the B matrix. This is called the A matrix and this I think usually a D vector or something this prior. So those are the three ingredients for this generative model. And then for example, so how you would perform um exact patient inference is for example we say okay we so we first can express our belief in what state we are in we can just use okay that's the prior which we assumed we already had and then we then get an observation. So that's why this that one is referring to and that we want in this case to be the exact posterior S1 given O1 which is proportional to uh O1 given S1 and our so this is this uh A matrix and the prime higher or our belief, let's say that's maybe more general, our belief about what state we were in before receiving this observation. So this is a very simple example of belief updating so to say which I describe in more detail here in the paper. And I think if you're just starting out with active inference, I think it's quite nice to just think about this whole state inference in this way and think of these other approaches like uh minimizing some kind of variation of free energy as some kind of approximations to just doing this exact patient inference. Um yeah and that basically concludes all the ingredients we need for describing how an agent is uh performing its next actions or how it's choosing which next action it's taking. And u so I'm skipping now over this learning part. We will get into that a little bit. And then here in section four there is a completely worked out example. So I really recommend so you've might also seen this uh in for example Thomas Par's book. Um I've tried to make it very explicit here and very precise that you can really follow exactly what is happening and I invite you to just go through this example. So this up till here is just some kind of definition of the model and then um here we say okay how it's performing uh the inferences how it's making these cues and then here we see we compute the expected free energy for all different policies. So this is the first action this is the second action. So we walk somehow through a an example episode of this mouse in this T-mase um uh which is acting according to active inference and then second time step it's updating its beliefs and then again computing expected free energy and then again uh selecting sampling a policy and its action. So um yeah so I invite you to go through that as well and um what I found hard so um I looked at this um there is also so in order to compute make these computations I thought okay I really want to uh uh make the computations so I looked into this um library py mdp and also a a little bit at the other one SPM I think um which is written in MTLOP but what I found hard was that in both code bases the actual computations of what's really going on are very deep uh uh down in the code so to say so are not so accessible and what I did was actually um to uh to write my own code to do the op to do the these calculations which is also uh available on GitHub. So here on the first page is the GitHub link and I can and what I think is relatively nice from this code is that it's very so you see the code right? Yep. Yeah. Um so that it's relatively easy to read. So for example there's a function sample action which is exactly and I have also tried to refer to all the equations in the paper. So for example here this is the first equation we've been talking about this uh distribution over policies and here you see exactly how it's computed. So we have to compute for every policy the expected free energy and in order to have the expected free energy we need these beliefs over states and then we have to take this soft max function. Oh, I did not actually say anything about so this this zigma here that's maybe good to just clarify this is the soft max function which makes um just these values. So we get for every pi we get a different value makes these values into a probability distribution. Um so for every pi we have a different uh expected free energy. We make it into a probability distribution and then we sample from it and then we take the first element basically of the policy which is our action and then we return the action. So I think this code so if you're just interested you can play around with it. you can take some parts of it. You can do with it whatever you want. But if maybe if the maths are not so clear for you, but it's more clearer in code. Then um you can also have a look at this. So it's has the same kind of sequence as the paper. So we sample an action. How's the expected energy computed? We need these two things info gain and utility. And here we have a part about how it's um updating its beliefs and that's exactly the same as what we uh what is described here in the state inference. So this is a little bit different for example from the PMDP and um and SPM where they use more advanced methods for doing this state inference. But for easy examples, I think this is really more than enough just to get an very precise idea of what is really happening. And then there's also uh functionality for learning which we will go into now. So I think that's everything I wanted to say at least from my side about the inference. So maybe uh if there's any questions I'm very happy to go into those now. Okay, I could uh maybe maybe finish covering all the sections of the paper and then there's a few questions in the live chat and people can add some more. Okay. Yeah. So for the second part learning um I was thinking to actually go a little bit more or to make it a bit more difficult so to say and really go into depth of uh where the equations come from because uh for a long time that was actually also unclear to me and actually quite late in the process. It became exactly clear of how um where the the equations for learning come from. And um I think that's also a quite a relevant or significant contribution of this paper of this paper to uh present that and I thought it might be nice for the people who are interested also in where they come from to go in to a little bit more depth now. So I would actually say that we go then to the appendix. So there's here appendix B um and we actually first look at a little bit simpler setting to somehow get familiar with the basics and then we go back to this setting of an agent operating in a world. [Music] Um yeah maybe I introduce it a little bit here in the notes. though I'm slightly changing notation now to uh avoid confusion but maybe it also makes it a bit more complex. I don't know. So we have now um X and Z and you could basically think of X as an observation and Z as a [Music] uh latent state I call it. So um and maybe I just I think in most cases you can also think about this as the O and this about the S so to say of the of the model. Um but that doesn't always hold. So that's why I chose to uh call it X and Z. And we first for now forget completely about this uh about this Z. And we uh imagine some kind of very simplistic setting where we want to where we are receiving observations X and we want to somehow make a model of these received observations. So we want to make some kind of probability distribution X. I'm checking which notation exactly I'm using. Yeah. So there's some different observations we can make. uh one up to n. And we want to model um the observations we make with a probability distribution which is dependent on theta and then very specifically. So it is just the probability that we see an observation I given theta is theta I low. Um so theta is a vector from uh theta 1 up to theta n. And um the learning and then you could say learning is finding this theta and um what uh the active inference uh literature is suggesting is to think of this theta also. So that is a very basian kind of approach that we think of this theta also as something that we can have a belief about. So we uh on its so we can have also a probability distribution over the theta which is somehow expressing okay I believe my belief about the theta uh that theta i is 0.1 for example is uh is this number and that is again so I'm going to use the bar here Um that is again dependent on some kind of hyperparameter alpha and um this distribution is called the dirishle distribution which is a product over the theta i that we raise to the power alpha minus one and so alpha is also an vector from uh alpha 1 up to alpha n. So this distribution and by the way there's another paper from Ryan Smith I think it's also in the references which also describes this gives a little bit more intuition uh about this whole process which I think is really nice. So I can also recommend uh looking at that. [Music] Um yeah, but so this is basically describing our beliefs about our parameters and then just so we could say the prior belief and then um we can for example we make a specific observation XAR and then we can ask ourselves How is this uh belief going to change if we incorporate this new information that we made a certain observation and that I'm not going to all write down but we can u look at here let's here so what I just described was this so Uh we have this um distribution uh parameterized and this belief about our theta and then we are interested in this posterior belief so to say after having this made this observation XAR and um what we can if you work that out you see that that's again a dish distribution ution but then uh with a different alpha. So we call the updated uh alpha we call it alpha prime. And uh this [Music] um this thing here is the indicator function which is basically um which is one when xar is equal to x i and zero otherwise. So what is happening is that we increase uh the alpha i by one which corresponds exactly to the observation to the i of the observation and the rest of the alpha stays the same. And if we look again at this definition of the dishlay distribution. So we see the higher the alpha the more weight um that theta i is going to get in the probability distribution. So this is a very I would say elegant way of updating your beliefs about the theta uh when you get new observations. And this is the general idea of how the theta is also learned in the setting um that we are looking at in this uh pom dp this partially observed mark of decision process setting where we have states so hidden states uh observations and actions. [Music] Um yeah, but so what we were looking at here was just a setting where we had just observations but we didn't have any latent states and now the question is what happens when we do include latent states. So we can do a similar derivation. So we now have two distributions uh depending parameterized by two different thetas. So we have a prior which is a vector over the latent states and then we have some kind of matrix which is saying okay given we have a certain latent state what's the probability of a certain observation and [Music] um then we can again ask ourselves this question um what happens with our belief over theta Yeah, if we get an extra observation and what is actually uh the case here is that we get some kind of very complicated distribution which is not uh a dearish distribution. Um so this exact method is not so useful for updating. So this is more like the setting we uh we are looking at right where we have observations and latent states. Um so we have to do something slightly different and uh that is uh that we do by the mean field approximation and um so what we do instead is we um uh let's see what how I can best [Music] continue. So maybe I go very brief briefly to appendix A. So in appendix A I uh describe some kind of very general. So I think most of you have already seen this but just for the completeness uh a variational free energy minimization approach. So [Music] um it's described here. So we have some kind of uh joint distribution and we want to uh um compute a posterior which is also the setting we had before. Um but often this uh posterior is hard to compute directly. So what we do then is we instead uh choose a family of distributions Q and um find so this Q tildas are all element of this Q and we try to find uh that Q tilda that is minimizing this expression and that Q Q we call qx because it's actually depend dependent on this X on the observation that we made and this is some kind of very general approach which is called the variational approach and that is somehow used to approximate a posterior that we can't uh compute directly. So that is described in more detail here in this appendix and also how it plays a role in active inference. Um I think for time reasons I won't go into that much deeper now but just to describe this method at least. So we use that same method also here. So right now we were dealing with latent states. So these zetss that we don't know and we are dealing with the parameters that we don't know. So we somehow uh but what we do know is the observation and the hyperparameters and we want to find the distribution over the latence and the parameters. And then we use the same uh variational free energy minimization approach where we are trying to find the Q that is minimizing this distribution. And then what we do what I was also saying before often we restrict the cues that we can choose from which is uh and in this case we uh are only looking at Q's which can be written as a product of these individual distributions. So that's some kind of an approximation that we are making here which is called a mean field approximation. And um if we do that then um this uh this equation simplifies a bit and what we also do so this is um very important insight is that it's quite hard to minim ize both of these cues at the same time because they are both influencing each other. And there is a specific algorithm called the Kafi algorithm coordinate ascent variational inference algorithm which is saying okay instead of trying to find both at the same time we start by fixing one and then optimizing the other and then use that. Uh so for example we start by fixing this q theta and then find qz and then use this newly found qz to find an q d theta and then you can do that iteratively and hopefully get uh to the optimizer and that is also exactly the approach that is used for learning in the active inference. So very specifically we first have some kind of prior belief about our generative model about the parameters of the generative model. So we fix those uh that belief so to say or those parameters and with those parameters with the generative model we can uh do inference like uh we looked at before like this basian uh basian inference and that's also what's done here. So we uh get some kind of qz. So that is so this is a different way of describing inference in terms of variation of free energy but what's basically what's done here is we are just trying to infer what kind of latent state are we and we have fixed the parameters Q theta so and then we have this uh qz so this belief about the state we are in and then we are somehow where we want to be We want to to find this distribution uh Q theta and then we use this this fixed QZ to then optimize this distribution which is only dependent somehow on Q theta and the QZ is fixed. And if you would really want to find the minimizer, you would continuously be doing this alternating these two steps. [Music] Um and um what is very important to note here is that uh the active inference literature is saying our update is only doing the first step. So we only do the first step. We do one time state inference and one time uh we optimize uh this parameter believe on the parameter given this state inference and then we stop. So that is so it's not exactly the same as the kafi algorithm but you could say it's only the first step of the kafi algorithm and um so here I'm describing what does that minimizer then look like and that is given by this um basically the exponent of the term that's here. So you can also see that if we take the exponent and we take the log we just get what's there and then it's minus what's there. So that is the minimizer so to say and I'm using these terms C with the backslash to say okay these are all terms independent of the argument so we don't have to worry about them for the minimization. So here we have a very specific form of the updated belief in our parameter. And um then we can use the fact that we are actually working with this this uh de slay prior and this categorical model and then we can so that p here that was defined here. So in terms of these thetas and in terms of these durishlays and what you see then is that this uh updated Q after the first step in this algorithm uh looks like this and this is exactly again a deishlay distribution. So that's a really nice result. So I didn't make any assumption that it has to be a deishlay distribution. I only said okay we do this mean field approximation but it comes out that it is if you only do the mean field approximation that this first step of the cafe algorithm is again a dear slay distribution. So we can um formulate the update again in a very elegant way like this. So we just add something to these two updated alphas and what we add exactly is the following. So for example this uh alpha d is related to the theta d which is [Music] um um which is the belief about the the prior belief in the latent state and we add um we add the belief after uh doing the inference that we are in state. So this is in some sense a very similar update to this update here where we are just adding one for the observation that we did but now we are not adding one for the observation but we adding only the belief that we are in that state to to the uh to the original alpha and something similar here where we add the belief in the transition more or less So, um, yeah, I can imagine that this was a little bit fast and maybe a little bit abstract because there were not not so many examples. But I felt this for the people who really want to know, okay, where do these equations come from? I think this is quite a quite a relevant contribution. And um so what I was talking about now was only this X's and Zs. The notes are still there, right? Yeah. Great. Um and now we want to translate this to this setting where we are actually uh having this S and the A's and the O's. So then the equations get even a little bit more complicated. So I thought it was good to have this build up to first see okay what is the general principle this mean field approximation and the dishlay coming out and then you can apply exactly the same procedure to this more complicated setting where we have this full generative model and then you get this derivation here which I don't uh I'm not going into now but there we see again at the end that we have a dirishlay distribution and here you see okay alpha we have to add this to the alpha we have to add this to the alpha a we have have to add this to the alpha b and then we get so this is the the main part of the paper uh we have these uh updates here for the learning So um yeah so for the people interested in uh knowing where do these learning rules come from um there is this appendix B which I went through now a little bit quickly and maybe little bit high level but um yeah you can look at that in more detail or ask me questions about it if you uh are interested in this and maybe to just link it to the other literature. So here there's this is the book of Thomas Far. There's his appendix B23 learning and here he is coming to the same uh equations but using a bit different method that I found hard to follow. So uh yeah but these these update rules they are also here and uh the same also in this paper. Let's see if the reference. Yeah. So here we also have these uh updates but also using a different method than the one I'm using and actually this method that I am presenting is referred to me by Connor Hines. So thanks very much for uh pointing me to this derivation because I think this for me at least is the most clear one I have seen so far. [Music] Um yeah so so far about uh learning so there's one I would be able to if there are time allows could go briefly over the appendix A but maybe we also first take some time to uh for questions about this learning part and I'm also fine with not discussing uh this appendix Hey, awesome. Okay, I will uh ask some questions that were submitted by Arun and then if anyone else has any questions, they can go for it. Okay, first question. Do you intend the accompanying repository to be a static accompaniment to the paper or do you see it evolving? for example, adding habitual policy selection to the agent via e tensor. [Music] Um I would say from my side uh I'm not really planning on uh developing it further but uh if people want to I am very happy for example if they fork the repository copy it in any way would be nice if you reference me if you still use it but um yeah so from my side I don't see any further developments ments uh but I would be very happy if other people want to actively start using it. So please get in touch if you want to. Cool. Okay. Would it be possible to further document the flattening class at first with comments but ideally with unit tests? Yeah. So that is indeed so uh maybe a good point to make about the the code. So um in PIMDP uh there is some I have that also here in appendix C I think. Uh some yeah there is some extra um kind of structure being put on for example these uh um these conditional probability distribution. So for example we say the location in the N next time step is independent of the reward condition in the previous time step and that is for the computations it's very nice because the computations get a bit uh easier but um the code gets a lot more complicated and you don't see especially because for example here. Um, if we look at all these, uh, for example, get believe next states. Yeah, I think that's a good example. Update belief current state. Something like this. Um, for example, this one. Um, this is very simple and elegant. I would say and that is because I am ignoring that so to say in the code that that there exist these independencies and I did that uh uh on purpose just to keep the code as simple as possible. But what is then a bit harder is that so um so what uh PIM MDP is using is um having some kind of state which is somehow uh multi-dimensional and then also for example uh the A matrix let's say which um is also has is I think some kind of uh list so and the observation state so there's observation modality state factor so these are both have can have multiple uh elements so this is a list of matrices and these matrices so for each modalities Let's say it's modality m and um this is then again um some kind of tensor where we have here the uh observations for that specific modality. But then we still have here uh also for every state factor uh a different um dimension so to say and in my case what I am doing is just saying okay we just say that we the state space is let's say onedimensional and we just have uh s1 one one s uh one two let's say there's two state vectors and I'm just putting them all into a long list so to say so that's what I call flattening and then this a matrix is just one matrix where there's all the the flattened states here let's say S flat and here O flat and that makes the the code very easy. But there there I do need something to translate between the pi and dp states which are somehow have these different state factors and the flat states that I am using and that is indeed I agree is not uh uh the best documented code. So, I I wrote that and then I tucked it away in this flattening uh class and was hoping that everyone would believe me that uh it works well. But we could talk about it because I do think it's uh a good point that it would be nice if it has like uh if you don't just have to believe me that it works, but that you can actually check it yourself. So maybe we can you can send me an email and we can discuss how to best uh to best document it. Yeah, lots of uh little challenges with matrix format and yeah, small things that uh yeah, definitely. Okay, I'll ask Erin's last last uh question here and this is kind of a good uh final discussion piece which is what do you see as the next key area to tackle in terms of increasing the clarity of how active inference works? Hm. Yeah, that's an interesting question. So, I felt that with this paper, most of the questions I had were answered. Um I'm now looking a bit uh together with uh Lance Tak Costa also at how well for example these learning rules in practice are actually performing. Um but that is maybe different from uh from so I feel at least in my opinion in the paper it's clear how they are formulated so I would say yeah I would be open for suggestions but I so I have answered with this paper most of the questions at least I at about how active inference works and then then of course about this very specific domain of the discrete time uh domain. Yeah. What what did you uh think or have any conversations about how this relates to the continuous and the path-based versions? So I have actually only spent time with the discrete setting. So um I was I think in the beginning very optimistic. I thought okay first understand the discrete and then the continuous but then I first spent time on understanding it and then I thought okay let me write this paper on it which also took quite some time. So I actually have no experience with the continuous case. I'm not sure L Lens is not here right or no otherwise we could ask him but he is more familiar with so I think that's more a question for him maybe he is at some way able to answer that at some point cool what are your next steps or how are you going to move forward having satisfyingly reduced your uncertainty about discrete states yeah so I am very interested in general in this uh we could maybe call it AB learning uh domain. So how is an agent actually learning these uh matrices? So uh together with Carlotta with my colleague we were also applying this framework to study uh consciousness or we're still busy with that and a paper about it hopefully coming out soon where we uh use an active inference agent and then look at how it's um integrated information is uh uh developing and uh Um but we saw actually in the environment that we were using that it was very hard for the agent to learn the A and B matrices or learn an A and B matrices that allows it to uh operate well in the environment. So for me that's the next big question is how what is actually are these learning rules from active inference the right ones or are there other rules that are maybe more effective and maybe more probable. So that is for me a big question that I completely don't have an answer to right now but this really I find really fascinating. Awesome. Any other comments you'd like to make? Um so I think it's good to finish it here but just to to say that it's there is there's also this appendix A. So maybe a bit weirdly but the uh I think all of free energy and active inference usually is formulated as a minimization of this variation of free energy. But starting the story from there I find then it gets very confusing uh most of the time. But I do write here in appendix A how everything that's written in this paper can also be related to uh to this bigger idea of minimizing an variational free energy function. So you can just read through it here. perception, learning and action selection, how they are all related to minimizing dysfunction. Just so you know that it's there. Awesome. Well, again, thank you for the important and pedagogical work. I I look forward to many in this genre. Thank you so much for the opportunity. And uh again if anyone is has has any questions or wants to reach out please uh feel free to do so. Awesome. Thank you. See you. Thank you.