hello and welcome it's September 18th 2023 and it's active guest stream 57.1 with Andy Keller we're going to be talking about natural neural structure for artificial intelligence there will be a presentation followed by a discussion so if you're watching live please feel free to write questions in the live chat otherwise thank you Andy for this really looking forward to it and to you for the presentation yeah thanks so much uh thanks for having me I'm super excited to be able to present this stuff with the active inference group I'm a a fan and very interested so hopefully uh yeah get to have a good discussion and see what you guys think about it um so my name's Andy I'm finishing up my PhD supervised by Max Welling at the University of Amsterdam um and starting up St pert after this so I'll start out um just talking about the goal of my work in general is to try to bring modern artificial intelligence closer to more humanlike generalization and so what we mean by this is maybe some sort of structured generalization um or maybe more familiar to the active inference Community like a structured World model which we believe that humans have and the way that we propose to do this is by integrating natural neural structure into to artificial intelligence so first let's define what we mean by structure generalization so I think it's fairly uncontroversial to say that modern machine learning generalizes Beyond its training set in the traditional sense so for example even the earliest artificial neural networks multi-layer perceptrons could be trained on data sets of images like this uh and Achieve high accuracy then when they're presented with a heldout test set of images that they've never seen before they can still classify them relatively easily with the same level of accuracy and this is what we typically call generalization however even fairly early on it was noticed that these systems really struggle with small shifts or deformations applied to the images for example if model so we think why is this surprising um and I argue it's really due to our innate ability to perform this type of structure generalization that this example is a failure so uh for example this shift is nearly imperceptible to us and we handle it automatically whereas in the systems it's very clearly a major problem so in words we can say that structure generalization is a generalization to some symmetry transformations of the input or in this case the Symmetry transformation is a small shift that leaves the digit class unchanged so the obvious question then is what precisely do we mean by this natural structure and why do we think that this would help us with the settings so first let's talk about what we mean by natural neural structure um one way to talk about structure or or any type of uh bias in a system is an inductive bias and so an inductive bias can Loosely be defined as an opior restriction of a set of realizable hypotheses when you're doing model selection um more colloquially we can call this something like the forseeing any data it's a restriction of what and how you can learn so very broadly this can include anything from model class to optimization procedures or even hyperparameters um and in some sense they really Define what is uh what is possible to learn and it defines generalization and that you actually can't generalize Beyond a training set without having some inductive biases and this is explained more thoroughly in this paper by David Wolford so what we mean by natural inductive biases then is biases that stem from the restrictions and limitations that are faced by natural systems uh by the nature of having to live in the real world for example the brain has many efficiency constraints and physical constraints ping sure of its construction uh and following this logic then these constraints are really playing some role in our generalization abilities which currently exceed modern artificial intelligence as we go into next so in this talk I'll be focusing specifically on two types of structure which my work has studied uh these are are topographic organization and spao temporal Dynamics and before I go into my work I'll give a short example for why I believe that natural structure may be useful to achieve the structure generalization that I was talking about before so the first example comes from uh fukushima's neocognitron architecture from the 1980s which was actually which was actually built to directly address the problem of robustness to these small shifts and deformations so in the paper he it's about inspiration from pbil and weasel's measurements of hierarchy and pooling in order to achieve robustness to these distortions and so if you look at the figure he writes U sub S1 U sub C1 and these stand for simple and complex cells um and so this was a fairly radical Approach at the time but it really served to improve robustness and shifts that were plaguing these early artificial neural networks and over time these ideas were simplified and abstracted and obviously yielded the convolutional neuron networks that we know today which ultimately drove the success of the deep learning Revolution so this is really an example of a natural inductive bias which achieved structured generalization so for our research it's really of utmost interest to try and understand what makes these models work so well um and see if this principle can potentially be generalized to cover more AB more abstract Transformations and symmetries so what makes a convolution a achieve this structure generalization intuitively you can see this is done by applying the same filter at or or feature extractor at various spatial locations so here we see a single convolutional filter being applied at all locations of an image and this means that no matter where your input is whether it's kind of in the middle of the image or on the right you'll have the exact same features with one exception they'll be equivalently shifted so mathematically this type of a mapping is called a homomorphism it preserves the algebra structure of the input space and the output space in this case is with respect to translation and at a simple a simple level something like that will be important to remember for the rest of this talk is that we can verify homomorphisms of our feature extractor uh if we can see that there's this Comm commutation with the the transformation this communative diagram and so we can write this also algebraically by showing that the feature extractor f commutes with the transformation operator T and basically what we want is there to be no difference between first extracting the features and then performing the transformation or performing the transformation and then extracting the features so the challenges to date is we don't really know how to construct homomorphisms with respect to more complex Transformations that we see in the real world for example our brain is able to handle changes in lighting and season naturally um so here we see Lighting on a person's face or the change of seasons we can tell it's the same face or the the same road um but we don't know how how to build models which respect these Transformations and so it makes us hard to build systems which handle them in a robust and predictable way to give an even more abstract example of what I mean by this and the potential negative repercussions of models which don't handle symmetry Transformations uh consider modern text to image generation programs so in this example I asked Dolly 2 to generate a image of a teddy bear on the moon and it does this incredibly well right probably better than I could it has texture for uh incredibly detailed however if I ask it to do something which I see is conceptually simpler such as draw a blue cube on top of a Red Cube it fails to do this and to me this seems unintuitive since the second task seems significantly easier um but what I'm arguing is that the reason that this is surprising is precisely the same reason that the mnus translation example was surprising there's this symmetry transformation Happening Here namely the transformation between these complex objects of a teddy bear in the moon and these simple objects of Cubes which we intuitively expect the network to be able to handle and respect and we see that it doesn't so just like how fukushima's work showed that these natural structure of hierarchy and pooling of our visual system are effective for making generalizations to small Transformations uh I argue that potentially higher level structure may be necessary to fix these abstract generalization problems and so the the question then that I'm studying and that I'm asking is uh what might this structure be and how do we implement this in a artificial neural network architecture that can actually be used for performing computation so to begin to answer that I'll jump into my first line of work on topographic organization so topographic organization uh is observed widely throughout the brain from primary visual cortex to higher level areas and it can very Loosely be described as this property that neurons which are close to one another tend to respond to similar things for example on the left we show the colorcoded preprints of each neuron in the Mako primary visual cortex uh as a response to oriented lines and we see this smoothly varying set of selectivities another type of organization is known as retinotopic organization where nearby neurons in the visual cortex tend to respond to nearby receptive Fields however this organization isn't limited to these lowlevel field features it extends up to more complex features such as those present in faces or objects or places and this relates to the so-called functionally specific areas of the brain such as the fusiform face area FFA and the parip temple face area PPA so in in this work the main idea again is that perhaps this topographic organization in some sense which is intimately related to the convolution operation and fush architecture uh we can maybe generalize the benefits of this to more abstract transformations in other words learn how to build more complex homomorphisms that we can't do that we can't do analytically right now so just to show that we're not completely insane with this idea uh there is some prior work in this domain from people such as uh honen y aparin in the early 90s and and 2000s and they studied how topographic organization may be useful for learning invariances mostly in linear models so the question for us when we entered the space is what is the most scalable abstract mechanism that can be leveraged from these approaches which we can integrate into modern deep neural network architectures and ultimately we settled on a generative model modeling approach which I think might be interesting to the people in this community um which then allows us to relate it more closely to topographic independent component analysis with the basic idea being that we can learn a topographic feature Space by proposing a topographic prior distribution over our latent variables so just to give a brief background I assume most people are already familiar with this um but the kind of General assumption is that the brain is a generative model and this idea in some sense can be attributed to helmholtz from the 19th century uh where he said that what we see is the solution to a computational problem our brains compute the most likely causes from Photon absorptions within our eyes and so as an examp example if I show you this image you immediately recognize it as a globe with some curvature however it could just as equally be a dis with a distorted perspective on it so this is how we get optical illusions or or images so like this one your brain infers that there is a cube here because of the structure but really it's just a flat piece of paper so you can think of this generative model aspect that's kind of like an inverse Graphics program in the program the abstract properties of the sphere are known the position the size the lighting and these are used to project the sphere to create the 2D image that is rendered so in effect what Holtz and others are saying is that as a generative model the brain is actually trying to invert this generative process and doing inference and infer the underlying causes of our Sensations so the reason I'm kind of belaboring this point is that there's a lot of talk of generative models today um and I'm not necessarily just talking about generating images or pretty pictures um I really want to mean a framework for unsupervised learning so then to get a little bit more into the details what do I mean by a topographic prior so generative models are typically described as a joint distribution over observations X and latent variables which we'll call Z uh and this is typically factorized or one way that this is done is factorized in terms of a prior P of Z and this true generative model conditional generative model P of x given Z and so one way that we can think about this is that the prior can be seen to encode relative penalties for each type of code that is produced when we invert our generative model this is called Computing the posterior e ofz given x uh and so to develop a topographic latent space we want to introduce some sort of a topographic prior which has been or which this topographic ICA work showed is equivalent to something like a group sparsity penalty so people might be familiar with typical sparsity penalties from independent component analysis you want your activations to be sparse meaning many of them are zero so that could look something like this you have a bunch of blue squares that are active but most of them are not active but specifically with a group sparsity penalty we want these priors to assign lower probability to these distributed sparse activations and higher probability to these grouped densely packed representations you can also think of this like a higher penalty when things are spread out a lower penalty when things are closer together so again uh this can be written abstractly like this but I want to make clear that these neuron it each one of these squares here represents kind of a neuron in our model and they're organized in this 2D grid so when we're talking about grouping we really mean grouping in that 2D topology um so one thing that's really interesting and kind of important uh is that these priors don't just give us topographic organization but they've also been noted by people like or studied by people like Ros Manelli and Bruno Olen uh to to actually fit the statistics of natural data better uh specifically natural images uh they've shown that using this type of a prior you actually get a sparer set of activations meaning that the prior fits the true generative process a little bit better and as we're aware the brain has a high degree of sparsity uh and this is believed to be very relevant for efficiency so so to get a little bit more into the details to implement this type of a group sparse prior we use a hierarchical generative model and this is uh basically introduced by some of the topographic ICA work um the idea is that you have a higher level latent variable U which simultaneously regulates the variance of multiple lower level variables T and this is how we get group sparsity then to get topographic organization you can have multiple of these latent variables used slightly over overlapping with their fields of influence so their uh their neighborhoods we can call them and this will give you this smooth correlation structure you're act you're after so get the intuition for this you see that this variable T over down on the bottom here is not getting any input from this U on the top but it is sharing a u variable with this T in the middle so it's like they're sharing variants they're sharing some components with their neighbors but not all components and that's really due to this local connectivity of these higher level variables you so to keep it simple about how we use a generative model let's go back to a single U variable and the challenge in this type of an architecture which made it difficult for for many years is how do you infer the approximate posterior over these intermediate latent variables in this hierarchical architecture um and this is not super straightforward so prior Works have use heuristics developed for linear models and in our work we found that these really didn't extend to Modern Network architectures so really our Insight is to leverage a factorization a specific reparameterization of this distribution and so this reparameterization specifically is achieved by defining the prior to be what's known as a a gaan scale mixture meaning that our conditional distribution of T given U is actually a normal distribution where the the variance is defined by uh this variable U and for certain choices of U this distribution is indeed spar and encompasses a range of distributions such as laosan and Su T distributions one way of defining it is a gaoui scale mixture admits a particular repar reparameterization in terms of independent gausian random variables Z and U so specifically then we we see that this T variable which was originally fairly complex is actually just a product of a bunch of gausin Rand of variables which we now know how to work with much more efficiently uh in in generative models and specifically what we're going to do is so that we can actually get approximate posteriors for U and Z separately and then do a deterministic combination of them in order to compute our topographic variable T and this is much easier to do so without going into too many details um the method that we decide to use is what's known as a variational auto encoder which leverages techniques from variational inference uh to derrive a lower bound on the likelihood allowing us to parameterize these approximate posteriors with powerful linear deep neural networks and optimize them with gradient descent uh this is going to be familiar to the active inference Community but really what we've done is instead of having a single encoder and decoder as is typical vaes we now have two encoders one for U and one for Z separately and then we combine them in this deterministic manner to construct our topographic T variable uh if you see that this is a this actually the construction of a a student te distribution from gaussians uh and then we can plug this we do this before decoding and uh and then maximize the likelihood of the data altogether so this is the elbow the evidence lower bound abound on the likelihood of the data and is actually uh very similar to the variational free energy that uh is used in active inference community so with with these details out of the way what's really interesting is what happens when we train this generative model which has relatively simple group sparsity penalty in its Laten space uh and we want to look at kind of what it's learning in terms of its organization of features and first we start with the simplest possible data set we have a black background with white squares at random XY locations and if we train our autoencoder with this group sparcity penalty on it and then we look at the weight vectors of our decoder uh which we're plotting in blue here again organized in this 2D grid uh we see that indeed they learn to be organized according to spatial location so this can be seen as similar to convolutional receptive Fields where the receptive field of each neuron is really given by the the kind of input at its location and this makes sense intuitively from the group sparsity perspective since for any given region which you highlight like in yellow here the filters in a given group are much more highly correlated they have these overlapping receptive Fields than other random locations so essentially we see that our model is learning to Cluster activ activities together uh in sort of a simulated cortical sheet uh according to the correlations in the data set so instead of in convolution where you're actually doing weight tying and you're manually specifying I want to copy this weight everywhere you can maybe think of this as like approximate weight tying um and really we're learning this from the correlation structure of the data set itself and just to give a little bit more of a biological inspiration for this and we know that retinotopy is present in the brain this is an example of reat Toopy in the MCO visual cortex uh and you can see if you show the Maco an image like this it gets projected into this uh topology preserving uh space actually on the surface of the cortex so the idea is that topographic organization and even learn topographic organization is preserving the input correlations of our data set uh and and potentially uh this may be beneficial for generalizing these ideas a little bit further so like I said at the beginning uh it would be even better if we could just learn something more than just convolution maybe more complicated equiv variances so how do we do that one thing that's clear in natural intelligence is that we don't exist in this world of IID frames right we exist in a world with continuous sequences of Transformations so maybe we can extend our model to this setting to learn observe Transformations this is the idea of temporal coherence so what would happen if we just simply extended our previous framework over the time Dimension right so instead of just grouping saying we want our neurons to be group sparse in terms of spatial extent on the cortex we actually want them to be group sparse over time meaning that if one set of neurons is active now we want that same set of neurons to be active into the future as well if we look at if we kind of intuitively think about this we see that this is actually more encouraging invariance than equivariance a way to understand this is we're saying we want the same neurons to be active constantly but the input transformation is changing right the the feet of this little fox are moving so if the same neurons are coding for the same thing over and over again but the feet are moving those neurons are going to learn to be invariant to the motion of that leg of this dog for example so instead is that oops I went the wrong way here uh so instead uh our Insight was that this group starcity could instead be shifted with respect to time so this would mean that sequentially shifted sets of activations would be encouraged to activate together and then our latent space would really be structured with respect to the observed Transformations so you can see here that rather than the same set of neurons being active at all time steps it's really a sequentially permuted set of neurons that were grouping together in this sparse way uh and and then this allows us to model different observations over time but they're still connected in terms of learning a transformation and preserving this correlation structure of the input data set so if we put this together into our topographic vae architecture you can get something that looks like this you see that we have an input sequence we're again encoding a a z variable and then multiple U variables in the denominator here um and then each one of these U variables is shifted uh kind of like we were showing before in order to achieve this this shift equivariant structure that we're looking for when we combine these in this student T product distribution we get a single latent variable this is now our topographic latent variable T and now that we have this known structure in our latent space you can think of it like a structured World model we know how to transform this latent space in this case it's by permuting these activations around these circles during like a cyclic roll a cyclic shift we know that this is going to correspond to our learned input Transformations and we can verify that by saying okay what if I continue this input transformation the true transformation in the data set which is a rotation and then I compare that with how I've done my role in my lat Space by moving my activations around in my brain and then we decode and we see that we get the exact same thing and so this is demonstrating this commuity property that I was talking about before for verifying homomorphism and so to measure this little bit more qual quantitatively uh we can measure what's called an equivariance loss so this is really the quantification of this difference between our rolled capsule activation our rolling in our head versus watching the rolling of unfold in for they're watching the transformation unfold before us so we see that topographic Bae achieves significantly lower uh equivariance error this bubble vae is what I was talking about before where it's learning invariance so it doesn't have this shift operation and the traditional VA kind of has no notion of organization or temporal component so it performs very poorly um in addition to this we see that the model is a better generative model of sequences it just gets a lower or lower like negative log likelihood uh on the data set so it's it's better able to model this data set because it has a notion of the structure of the Transformations uh we can test this on multiple different transformation Types on the top row we're showing the true transformation we hold out these grade out images and then on the bottom row we encode and then we just kind of roll our activations around and we keep decoding to see what the what the model uh has learned as the current transformation that's being observed and we see that it can basically perfectly reconstruct these elements of the sequence that it's never seen before additionally with images that are from the test set that it's never seen before simply because it knows what the transformation is that it's currently encoding and it can generalize that to new examples so the takeaway from this part is really topographic organization we show that it preserved input structure and now we're showing it can potentially improve efficiency and generalization as we would hope uh that a structured world world model would be able to do so finally something that surprised us and I thought was potentially the most interesting is that these these Transformations that are learned by our model actually generalize the combinations of transform that were not seen during training so for example despite only training on color and rotation transformations in isolation if the model is presented with a combined color rotation transformation at test time uh we we see that it's able to completely model and and complete these Transformations perfectly through the capsule role uh implying that it's learned a factorized representation with respect to these different Transformations and it can flexibly combine them at inference time so again maybe we also don't just get effic efficiency in generalization we also get some basic compositionality so let's talk about the limitations and and what we could do next uh the main limitation is that there's a predefined transformation that we're imposing uh in both space and time so although we freed ourselves from group Transformations and specifically like uh translation or rotation as is currently done in the machine learning world um we still have uh this hardcoded latent role in our heads for everything we see and to make this a little bit more flexible so hopefully we can model a greater diversity of Transformations um we we think maybe we can take inspiration from more structured spatiotemporal dynamics that are observed in the brain and so that takes us to the the second part of this talk which is uh spao temporal dynamics that we're going to try to integrate into artificial neural networks one one example of that is traveling waves like I showed here so what do we mean by that uh here's a very recent paper where they used a nine Tesla fmri operating at 36 millisecond resolution to image a single slice of a rat brain under anesthesia and what we see is this very clearly structured spatio temporal activity and correlations and these authors of the paper go on to analyze this activity in terms of the principal modes as depicted on the right so our Hy hypthesis is that perhaps some sort of a correlation structure like this may be beneficial for structuring the representations of our model with respect to observe Transformations but in a much more flexible way than simply just a cck shift like we were doing before um and let me say that this is not just observed in anesthesized rats uh this you can see these traveling waves happen in the Mt cortex of awake behaving primates uh so for example on the left here they show traveling waves that actually uh change How likely a primate is to see a low contrast stimuli based on the the phase of the wave uh furthermore they show that the like a high contrast stimulus on the right uh can induce a traveling wave of activity that propagates outwards even in primary visual C cortex so these are really ubiquitous throughout the brain at multiple levels and it would be interesting to study what their implications are for uh structure representation learning in our case or or generally there is PRI work which has studied these types of uh Dynamics and they built models so on the top these are the equations which describe a spiking neural network which they show if you implement uh time delays actually Exon time delays between neurons you do get these structur dynamics of uh traveling waves as long as your network size is large enough um however as many people probably know it's relatively challenging to train spiking neural networks of the same size and performance as as teep neural networks um similarly on the bottom another system which is significantly simpler but perhaps too simple uh is a network of coupled oscillators these are known to exhibit synchrony and uh spatial temporal Dynamics and complex patterns but uh this is called like a phase reduced system and does quite capture the full complexity that we're interested in so we're looking at something that's potentially in between these two and what we settled on as this work in this work is uh to parameterize a network of coupled oscillators slightly more flexibly than a camoto model so this is really built on this couple dilatory recurrent neural network of Constantine R and nishra um where they basically took the equation which describes a simple harmonic oscillator it's a second order differential equation the acceleration on a ball on a spring is proportional to its displacement uh you can add additional terms such as damping so that the oscillations slowly die out over time you can drive this oscillator with an external input to kind of counteract this damping or to give slightly more complexity to the Dynamics and then furthermore if you have many of these oscillators you can couple them together with these coupling matrices W uh as we demonst kind of in this picture here so you can really think of this network as a bunch of balls on Springs and they're may be connected to each other also by Springs or elastic bands whatever you want um and this is the couple dilatory recurrent neural network of of rushan Mishra uh with these various terms and this has been shown to be very powerful for modeling long sequences they also mentioned they were inspired by the brain building this and there's a lot of good analysis in that paper uh for example they show that this has really beneficial properties with respect to uh Vanishing gradient problems that typically happen in recurrent networks um but if we want to look at spatio temporal Dynamics in this type of a model uh it's slightly challenging because these coupling matrices here the W's uh that that connect each neur or each oscillator positioned to one another um these are densely connected matrices like I've tried to depict on the left here so if you try to visualize the Dynamics of this network uh you don't see any spatial organization there's no in topology to the latent space of this model um so you can think of this like in our previous example a neuron is connected to a potentially arbitrary set of other neurons those neurons are connected to another arbitrary set of neurons and you'll just get a cator Dynamics certainly but kind of fluctuations that don't make a lot of structured sense so in our work then we thought okay how can we convert this more to the the types of dynamics that we're interested in this structured propagation of it uh and one clear way to do that is to have a more structured connectivity Matrix W uh which we found is easily implemented and efficiently implemented through a convolution operation which you can think of like a local a locally connected layer so instead of having every neuron connected to every neuron neurons are just connected to their nearby neighbors and then you after training you'll end up getting something that looks like a smooth spacial temporal Dynamics so to be a little bit more clear to train this model we take this second order differential equation that we were describing before you discretize it into two first order equations you can think of this as like numerically integrating the OD uh we now have a velocity and then we update the positions with this velocity uh and and we can train this model as something like an autoencoder or an auto regressive model so we take an input we encode it to our latent space really the input is D is this F ofx term which acts as the driving term so it's like driving these oscillators from from the bottom uh and then they have their own Dynamics which are defined by the coupling terms these local couplings and then at each time step we take this latent State this wave state and we decode to try and reconstruct the input be at the current time step or a future time step we can do some analysis of of these models uh during training uh to see what happens before training and after training we can compute the phase and the velocity of the Dynamics in the latent space basically we see at the beginning of training there's no waves in our model but after training after 50 epochs we see that there's these smooth structured activity propagating downwards uh in service of this sequence modeling task that we're doing like rotating objects um so what's what's the benefit of this I mean the whole reason I motivated this was to say we wanted to have more flexibly learned structure are we actually doing that or are we just getting pretty waves um so what we showed in our paper is that we really are learning some sort of useful structure and the way we showed that is again with something like this communitive diagram if you take an input and you encode it and you get a wave state and then you propagate waves artificially in that wave state and then decode you can observe that it's actually exactly the same as if you had just performed that transformation in the input space originally so what we're in effect showing here is the commuity of our feature extractor and the transformation and in our case the transformation the operator in the latent space is now a traveling wave of activity and so in a sense it is structuring our latent space with respect to observe Transformations that structure comes in the form of waves so natural spao temporal structure yields preserved input structure again as we were looking for um and and one of the benefits of this as opposed to previously is that now this is slightly more flexible and we can see this just by showing a bunch of different images of different Transformations so of different uh digits different features and we see that we get different types of wave activity in each case in order to model that different transformation if we train it on different data sets as well we similarly see more complex Dynamics in this case maybe not even traveling waves or standing waves which can be thought of as uh traveling waves in opposite directions so we see if we're modeling these orbital Dynamics we get these kind of smoothly moving Blobs of activity in lat space if we're modeling a pendulum we simp Lally get kind of complex hillory activity so it's preserved input structure but additionally more flexibility than we had before which is kind of our ultimate goal so finally I I want to talk a bit about how I think the outcome of This research may not only improve artificial intelligence but also how it helps us understand why our measurements of the brain look the way they do so to give a brief example of what I mean by this uh I talked about bit about before about these localized areas that respond to faces and places uh so in this fantastic work with chinga we studied if our simple topographic prior as we discussed may be able to reproduce these same effects so specifically we plot the value of this Co andd selectivity metric for each of our neurons with respect to a different data set of images potentially containing just faces or just objects or bodies and so we measure for every neuron is it more likely to respond to faces or the rest of the images and we see that we get these spatially localized clusters uh that that share many of the properties that we actually see in the visual system of of humans and primates in many animals so uh one one of these properties that's shared Beyond just the fact that we have these spatially localized clusters is like the the relative placement of faces and bodies we see that the face cluster is overlapping with the body cluster which is uh makes a lot of sense and is also seen in humans and faces and bodies are most often seen together um and this isn't just a single uh a single fluke or a cherry-picked example if we rerun this model many times you virtually always get placed in body clusters which are overlapping or right next to one each other one another so to be clear I'm not suggesting that this is exactly how the brain works or how topographic organization emerges in the brain uh but I do think that it tells us that the relative organization of selectivity May at least be partially attributable to correlation statistics in the data after be P after being passed through a highly nonlinear feature extractor such as a deep neural network so in a similar vein something that's interesting there's a known what's called tripartite organization uh the visual stream so uh images of uh or objects are selectivity with respect to objects is organized by more abstract properties such as animacy is this thing alive or animate uh versus also real world object size like what is the size of a teapot versus a car um and what we see is that in the in humans this this selectivity is organized in this tripartite structure you typically have small objects that are in between the animate and inanimate objects in terms of their selectivity and we see the same thing kind of happens here so these are measuring the selectivity of the same set of neurons but with respect to these different sets of stimuli we see the the small cluster is in between anim and an an cluster again this happens for multiple different initializations so this is something I hope we can explore a bit further for this community I think it's interesting because it's it's really a way of showing that we built a a structured World model and potentially this world model is beneficial for better representing real world data in a structured way and you get lower free energy in that sense so um yeah I think by developing these model models like like we showed here we may get insights into new mechanisms for how this structure emerges uh including topographic organization that we never thought of before so as an example in in developing this neural wave Machine model I was looking at the orientation selectivity uh of neurons I wasn't particularly expecting something to happen but uh you're looking at kind of these waves Pro propagate over this simulated cortical surface and I thought okay maybe I'm showing rotated images maybe this has some effect on the orientation selectivity and actually if you go in and you measure the selectivity of each neuron with respect to these differently oriented lines what you see is that it's surprisingly reminiscent of the orientation columns and Hyper columns that are seen in primary visual cortex this is stuff going back to hugle and weasel and this is something that just kind of came out of this model and the fact that it has the spatial temporal structure with respect to transformation so uh of course this is really course analogy but I think this is an example of how building these types of models can help us think about how the brain builds representational structure and the why the way it's organized in a way that maybe we haven't thought about before um I think I'm not the only one who's doing this type of work and and so I want to talk a little bit about some other people who are doing this uh so I've been talking about like this equivariant structure um people such as James Whittington and Tim Barons and sir ganguli uh have shown recently that by introducing algebraic constraints into uh into a learning process in this case it was like the motion of a an an agent in an environment by saying you need to preserve kind of this algebraic structure of if I move in a a circle west north east south I end up back at the same Point again um by introducing these types of constraints you get the emergence of grid cell like representations um so I'd be interested to see how this idea of representational structure can help us explain maybe more the neuroscientific findings we're finding as well um and and how this relates to generative models as a whole um and then finally I think there's something to be said about cognitive plausibility of these models as well maybe we're not just going to be testing them from a neuroscience perspective but also from a cognitive science perspective for example there's these uh Ravens Progressive matrices on the left where you have this like say which one of these images is more likely to fit in this pattern um or for example How likely is it that this Jenga Tower Falls over when you pull over a pull a specific block or or or with a given structure um and I think these things are these types of tests are really testing if our world models that we're building are similar to the types of models that we innately have our own common sense as as humans or as beings living in a a natural world um and I've done some preliminary work in this direction I think very uh preliminary and not nearly this complicated but um trying to model visual Illusions so if you take a really simple data set of a moving bar stimuli or a static bar and you flash you you take out a frame and you move it a little bit you can see that the model will actually infer that missing frame and then actually also infer continued motion so it's like overshooting the trajectory of what the actual stimuli is providing it um before correcting again so I think modeling Illusions is certainly an interesting way to study if our world models are similar to the types of models that we have ourselves so in conclusion uh yeah I think topographic priors we could show that they effectively learn structured representations or structured World models this learn structure is flexible and adaptable to arbitrary Transformations unlike traditional equivariant and topograph Riders can be induced statistically as in the topographic vae or through Dynamics like we were showing in these neural wave machine type models so to conclude I'll I'll leave you with this quote that I found in fukushima's paper from 1980 I thought was uh pretty far ahead of its time where he says if we could make a neural network model which has the same capability for pattern recognition as a human being it would give us a powerful clue to the mechanism of the neural mechan or understanding the neural mechanism in the brain uh so that's kind of I think some of the goals that we're going for here so I'll say thanks to my adviser Max my co-authors Patrick UA Emil jingen and Y and interested in discussion thanks all right all right thank you great very uh interesting presentation a lot of places to start maybe just uh what what brought you to this work a little context on how you came into this work for your PhD Direction yeah um I mean my group has been studying not my the group that I'm in and the university has been studying structured representations from a mathematical point of view for a while and we're some of the people to introduce equivariant neural networks as well as generative models or for like the variational a coder um and I guess what something that had always been somewhat challenging to me uh is that these structured representations were fairly rigidly defined in terms of group structure mathematical group structure um so for example we can build a model that respects rotations 2D rotations perfectly well but if we want to do 3D rotations we can't do that because that's not a group in terms of a projection onto a 2d plane there's you're losing information when this thing rotates around for example um or just any sort of natural Transformations like I was trying to plan out at the beginning I think it was trying to think about how the brain models natural Transformations is something that this these current Frameworks couldn't really explain um and yeah my adviser Max was had worked on this topographic stuff uh a long time ago during his postto and so he kind of had this intuition that maybe topographic organization has a relationship here um then Co happened and I got really deep in Neuroscience literature and uh got into this stuff and yeah haven't left over cool where do you see action playing a role in terms of variational autoencoder models that include not just external patterns but also the consequences of action or World model structure structure with action right yeah no that's a good question I think active inference is uh is effectively the the answer to that I mean I think it's a good answer to that um I know there are reinforcement learning Frameworks that do use use kind of externally trained World models so you train a VA or something and then you use that representation in in your reinforcement learning system um but I think having a fully kind of a system that is a single objective with uh action as part of the likelihood of the data and uh yeah I think that's much more elegant and and so I'm a big proponent of that um I have not gotten so far as to study how these structured World models in a vae or I haven't worked on that at all but I think it would certainly be very interesting to see if having a more structured World model uh in a variational auto encoder would be beneficial in in an active setting as well I think that would be awesome I mean I think some of these examples like uh showing before like emergence of grid cells and things like this maybe Point towards that direction of hey maybe the brain is doing something it's really obviously has a lot of structure um this clearly has to be useful for performing actions in some way oh yeah I thought a really nice parallel that you brought in with a talk was the locally connected units enabled your models to structurally embody the convolutional constraint and pattern and that led to these arising patterns and then analogously there was the uh Doral at all where they had the path exploration constraint right and so then it's interesting to to um you know think about these action or policy heris STS or sparsities like a joint through babbling and motor exploration eventually it becomes understood that there's like two mutually opposing ways to move a joint and then the compositionality across joints can can be learned to the higher levels once it's locked in at lower levels so it's a very appealing and uh uh Niche relevant way to generalize because it's both based upon the actual constraints of the world but then especially through action potentially embedding something that's quite simple right yeah no I think that's definitely true that's a really good point if uh if you do have constraints coming from your actions and themselves then that's that would be hugely beneficial for helping to to structure your your lat and space and I think yeah I guess one thing I wanted to mention there's um something made me think of like Stefano Fu's work on kind of the representational geometry uh and how that determines how we how how generalizable a given understanding of a system is uh and I think if you can understand your representations in terms of geometry of like these uh sets of activities are separable or highly parallel separable with a linear classifier essentially uh then you're going to be able to do generalization and I think by imposing these types of biases or potentially through constraints that are imposed by action something like this you are yielding or kind of inducing a better representational geometry and this has all sorts of benefits for like compositionality or yeah or generalization so it's a great Point cool yeah very interesting area all right I'll read some questions from the live chat love evolve wrote any practical or observed limitations on modeling Illusions ah um yeah it's really hard um one of the challenges is most models that we use that I use that like deep Learning Community uses they're not fobi you don't have a a center of gaze and you also don't have um like a time I mean most convolutional neural networks I'm using these kind of recurrent neural networks but time is not as big clearly defined in these models as it is in a continuous time setting for for human undergoing an illusion trial um and I think the combination of these two of the fact that as a human for most things uh your gaze you're shifting locations and your gaze and a lot of these Illusions are dependent on like you looking to a particular area a lot of cognitive science tests and so I think it would be really helpful if we had models that yeah I mean learn you can think of this as a type of action right of learning where to move your gaze one of the simplest possible actions and uh that would help a lot for being able to model Illusions and just I mean for me it's like I read a paper of some cognitive science experiment or about some illusion and it's I I think of okay can I put this data set into my model and test it and most of the time the answer is no because I don't have a model that looks around or has a restricted field of view something like that um so yeah I think that's one of the limitations another one is um the models that we're training you have to think about what you train your model on before you test it on the illusion because that has a huge impact so it's like do I train this on amness digits or do I train it on imag net do I train it on natural video the the ideal thing would be to train it on a huge data set of natural video to say it's like now it's kind of like learning what a human sees and then you test it on these illusion data sets but obviously then you're going to need a huge model and uh yeah makes it experiment much more complicated so that's one of the Practical limitations wow great answer makes me think of a paper with letters rotating on a table that's the digit rotation task and then great points about the foliation and the Dynamics of the illusion I think you actually did mention an illusion which is however you mentioned in the generalization context which is rotating on the two-dimensional screen doesn't generalize to three dimensions and that dimensional collapse or reduction is the basis of the cube projection Illusions and Cube and figure rotation Illusions it's on your screen and there's a silhouette or there's some ambiguous stimuli that a generative it's near criticality or bifurcation in the generative model so it could represent it one way or another way and so a lot of the switching Illusions are just based upon the flatness of images and the limitations in generalization that are revealed by that right yeah yeah I think there's even some work sorry there's some work where they you can argue people have a kind of three-dimensional image in their heads like even Nancy kwish had a or her lab had a paper on this recently but and showing yeah I don't know do do our models have that it's not super anyway yeah that's pretty interesting um all right from upcycle Club in the chat they wrote Kudos is it true that by inducing sparsity Beyond a certain threshold runaway behavior in artificial neural networks is triggered depending on the task architecture and spar I ification method um H I'm not sure what they mean by runaway Behavior but um yeah I think too much sparsity is is a problem and so there could be a point uh where the model is no longer able to learn nearly as effectively if you imagine you only want a single neuron to be active for every example uh your model is going to be trying to memorize the data set to some extent or something like this um and you're not going to have enough capacity so yeah I think tuning that level of sparsity is certainly uh an important factor um and yeah when you look at the likelihood if you're talking if you're doing a generative modeling framework typically this is balanced automatically with the likelihood itself um if you're you're not doing gener modeling you just have a sparcity penalty you're going to want to tune that parameter okay they added just just to clarify runaway behavior in artificial neural networks can refer to a phenomena where the network becomes unstable or chaotic due to various factors such as feedback loops noise or adversarial inputs yeah I guess I haven't looked at this in in like a recurrent setting where you would get feedback loops um but I could yeah I could see as serial examples being potentially affected by your level of sparcity um the interesting point is would would you be more susceptible or less susceptible to examples I don't know well sparsification projecting from a fully connected higher dimensional model just into progressively smaller it's pretty well understood in general what the trade-offs are it's easier computations a smaller model sparer the basy graph is going to be clearer to represent and then also it will have all of the other trade-offs with false positive and negatives of generalizing but that's why it's an iterative fit process so I guess how does your sparsification approach balance does it use AIC or Bic or some other model fitting approach to determine the relevant sparsification for a given input how do you determine H how like like in lasso regression like how how do you know how how much how do you threshold how many how sparse you want it to be right yeah I think there's a lot of good literature on this and even so some people like D at at Harvard and some people I'm working with now uh have done these kind of unrolled um iterative sparsification networks where it's like a recurrent roll Network and it iteratively sparsify and you can show that this yields something like Rel or uh group act like group AC group sparse activations like we're using here um in this setting uh it's really just by having this um this construction of this T variable where we have Z on top and uh and then it's in some effect gated by these the sum of U variables in the bottom so w maybe I wasn't super clear about this is a matrix that is connecting that's what defines the groups when I'm defining the group sparcity uh that connects all these Ed together and so the idea is uh like here if all of go to one of the other examples if all of your 's uh are not active for a given t or if all of your us are active for a given t uh that t variable is going to be very small right because your denominator is going to be very big and that induces sparcity so it's uh it's a constraint satisfaction if you have a if you have a set of U's that are all small uh then that that constraint is satisfied and now Z is allowed to kind of Express itself and that's what then uh kind of yeah achieves the sparse Activation so this is induced by these two uh K Divergence terms here these are saying like how far is the hu and HZ from a gausian and then through this construction of the student T variable we're effectively constructing a sparse prior distribution just from these gaussin but in terms of the act the actual objective uh the terms and the objective that we're optimizing are just these two KL terms that are pushing it towards sparity to some extent and this is balanced automatically with the likelihood term here through the decoder so we don't have terms that we're tuning but we're learning the parameters of these different encoders and then analyzing the pale divergences cool all right another question from Dave Douglas who wrote speaking of gaze and illusion can the studies on constancies in and infants be separated into lower level illusion relevant neural activities versus perhaps higher level conceptual constancy uh can you read it again sorry I don't know yeah speaking of gaze and illusion the two features that you highlighted were absent from the current kind of architecture might the studies on constancies in ill in infants cognitive constancies be separated into lower level illusion relevant neural activities versus perhaps higher level conceptual constancy ah interesting um yeah probably I'm not I'm not uh an expert or actually even very familiar with like object permanency studies and infants and and constancy stuff so but I think that would be incredibly interesting to study in in neural network architectures and that was kind of some of the idea with this uh illusion that I was trying to model down here with this line I don't know if I was super clear about this but the top row is the input and we're effectively like blocking the input for a single frame and I wanted to see does the network kind of encode that that that the thing is still there when that frame is gone can I still decode the presence of the object from the neural activity uh and then what is it also inferring about the motion because of the fact that it saw the bars at a slightly different location than from before when the after the frame is gone um so yeah I think there's definitely multiple levels to it um where some would probably be much lower level and uh maybe long-term object permanency I would guess would be significantly higher level um it just makes me think of those experiments with cats back in the day where it's like they raised them in darkness except for an hour a day they put them in Vertical World or horizontal world where they only saw horizontal lines or vertical lines uh and you can see the the organization of their cortex changes like they have less receptivity to horizontal lines if they've never seen horizontal lines before and then you take a stick and you wave it in front of their face and if the stick is horizontal they just they do nothing if it's vertical they're swatting at it they're trying to hit it it's like they just literally don't see it at all if there's a horizontal bar in front of their face so I think in that case then this is evidence of a low-level deficiency and vision uh contributing to some sort of an illusion so I I think yeah there could certainly be some aspect to that in infant as well one very curious point you brought up was the animate and inanimate manifold with small things being intermediate right what does that represent or or is it because they're handleable or it might be an insect or it might be something that might move away just with wind or what does that say right yeah uh so this is work by like conl I think was the one who discovered this um organization and they they tried to figure it out and they think I I don't I might be getting this wrong so I recommend people to read her work on that if they call it tripartite organization but if I remember correctly they did a lot of follow-up work on why it's there's this organization and some evidence points to kind of mid-level statistics of curvature of these objects and kind of like the distance that you see objects from or like um animate objects or maybe more curvy or there's there regardless of what the actual answer is there were a lot of different hypotheses that were stemming from like properties of these objects um maybe midlevel or lowlevel properties more so than higher level properties I still don't know if it's exactly been solved of whether it's like interaction like you said with the object causes the separation or um or yeah the general shapes of these objects I would bet as with most things it's like some combination of all of the above uh but I think the interesting thing from this modeling point of view is that um this is only trained on correlation statistics from the image data sets itself this has no interaction this has no notion of animacy uh I mean this is really just training a model on imag net just images of dogs cats boats whatever and yet it still achieves this type of organization so there's some sort of it could be semantic characteristics right we have IM we have a network that can classify boats versus dogs versus 20 other breeds of dogs uh but it it might also have some correspondant with lower level statistics as well so yeah I don't know I guess he my answer yeah and also very provocative analogy was the translational shift in the mnist in the handwriting recognition setting what are the translational shifts that exist today what's the three pixel example is that some prompt engineered attack on an L or something or something a special character being inserted or or a um uh some overlay on an image that we can't even detect that so what do you think those challenges are and what are ways that we can pursue that yeah absolutely I mean I think kind of the way I was thinking about it is like these symmetry Transformations um if you're thinking about language models you can imagine a symmetry transformation it's just like replacing a word with a synonym or something uh you have the sentence to us means the exact same thing but now suddenly the model is going to respond very differently um or or or like translation between languages right uh this can be seen as a type of transformation it preserves the underlying meaning of the input to us but so the model it looks completely different and we would like to have models which behave in a predictable way with respect to the types of Transformations because I think humans behave very predictably with the type of these Transformations and when we're dealing with AI systems we expect them to also behave that way and I think that's part of what causes a lot of challenges interacting with these systems and I kind of tried to do a a rough cheeky demonstration of that with with this bear and and squares and stuff like um we expect it to be able to do something simple like this because we think most humans could and yet it doesn't and if you imagine this is a critical scenario where you expect this and that's a big problem um how do we handle that that's I think that's kind of the what I'm searching for I think my direction I'm taking it is looking for more simple and like bottom up building blocks of neural network architectures or algorithms that kind of yield these emergent structural properties and I think that's a much more generalizable way rather than building something on top of what we already have um I think that's something that will scale much better and also matches more with the brain does very cool one kind of implementational question what are the computational requirements of just running this or what's the data like of being a student or researcher running variants of these like do they use terabytes of data and you're using large computation or is this something that people can run on their own laptops I think almost everything I presented today can be run locally so like this stuff is super simple you can run I mean you're G you're I think you can run it on your laptop if you want to like train and experiment with different things it's going to be pretty slow so I'd recommend some commercial GPU like a I I run pretty much everything on like Nvidia 1080s pretty old pretty cheap but they have 12 gigs of RAM or whatever and it's kind of more than enough for these models most of these are only a couple gigabytes of RAM I think one thing that some people think is weird is I do most of my experiments on stuff like mest so it's 32 by 32 pixel images because I can train it small and locally um if you want to do stuff like uh like yeah most my experiments are on Mists if you want to do stuff like this it's are much more complicated this hamiltonian Dynamic Suite here you're getting into bigger models that are running across multiple gpus and so here I was using a cluster to run these types of models um but I'd say most of these types of ideas you can start to play with on a a single machine with the GPU is is more than enough U or even just like in a collab notebook um something like that if you want to train something on image net it gets more complicated uh and you need at least one GPU ideally more but yeah I don't do a whole lot of big scale stuff yet I think it's certainly interesting and there's definitely a lot more you can do there but for some of these kind of simpler or more fundamental questions I don't know what you want to call it um a smaller machine is nice and fast so cool useful all right I'll read a comment from Dave recalling Bert dev's comment during the applied active inference Symposium about the desirability of spending less effort or ATP on foraging or control situations where we don't need much Precision I don't know if you listen to this but Professor devise mentioned about variable Precision models and how they could be used to enable different features of generalization and actual structural course training as well as like reduced computational requirements does he have any suggestions on how to introduce this distinction into active inference Theory what kinds of experiments could Winkle this out oh wow yeah that's something I don't I don't think I have too much intelligent to say about to be completely honest um h it's super interesting question because I think the intuition makes a lot of sense to me that uh you're talking about if I understand correctly variable rates of precision when you're encoding and or or in your model in general doing computation um that somehow has an impact on your your future performance as a relation to some energy store I think yeah and if you wanted to build this into an active inference system you would need to have really an embodied system where the agent has some notion of energy like an internal energy store and yeah some something that is trying to conserve while it's performing its actions uh and running out of energy would need to mean something bad for the agent um and then maybe you could observe kind of an emergence uh reduction and en coding Precision or something like this as as the agent is trying to learn to act more effectively you might have to give it an ability to control its Precision um yeah like I said definitely out of my expertise but it's thoughts cool okay on this slide right here first very cool image it's kind of like a digital a and poic um if it were a simpler input or reduced data size or just reduced complexity of patterns or if it were an increased complexity how would this image look different yeah so I did some experiments trying to change these orientation columns and um you can yeah basically changing the parameters of the model you can get these columns to be bigger you can get them to not have very similar structure to what we see in the humans where we have you you can get them to have more bands of activity um and it also like you said it depends on the data set that you're using if I use like really simple sinusoidal gradings as input I get something like this if I use rotating nness digits I get something that's a little bit more uh rotational curvy higher entropy um so I think these are all interesting things if you want to study the emergence of the type of organization in an natural system uh if you have a model that now yields different Organization for different settings that's a great way to see okay then what settings best match our observed data um so so yeah I can I can send those around if you're interested but um yeah I think one also other sorry one one other interesting point there is that the uh different animals have different types of uh orientation selectivity and different numbers of pin Wheels some animals don't have it at all I think maybe mice if I'm correct have this kind of uh they call Salt and Pepper selectivity so it's basically random you don't have any sort of like topographic orientation stivity um so there is evidence that yeah different systems do this differently and and it's interesting to figure out why yeah this very cool it reminds me of first the reaction diffusion visual space where there's different SK skin patterns and fur and bands and Speckles and then also um these islands of activity enable the multiplexing which you described with encapsulation through through space and time so it's actually um possible that a region might have no activity from a given granularity like if it was being looked at at fmri spatial and temporal time scale if the pockets of activity first off if they're even reflected by what is being measured but but if the pockets of activity are slow or faster then that measurement is going to not be different than noise it'll all have been averaged out so then there might be some yeah interesting th like data sets that do actually have a lot of richness but then for one reason or another it just was averaged out over because it wasn't being connected to the right scale absolutely I think that is one of the main main reason that uh reasons that that traveling waves in the brain have been difficult to study to this point and there's a fantastic review by thi and vile Miller nature reviews where um they basically go through all the evidence of traveling waves and there and they go and show like uh if you're averaging over trials uh you're going to complet miss this traveling wave activity it's going to look like some mean value or something like this you really need to go at the single trial level and need to have high enough spatial Revolution such that it's you know it satisfies nous frequencies uh and and this just is something that people didn't do for a long time especially if you're doing single electric recordings you're not going to see a traveling wave you're going to see oscillations um so you need like multi- electrode arrays and basically they're saying okay yeah now that we have the technology to do this is one we're seeing this structure that that exists that we didn't see before and potentially this is an explanation for a lot of the noise that we were seeing before maybe it really is just traveling waves um so yeah I think there's a lot to be done in the future with increased abilities for recording that's very cool well any final thoughts or questions or where are you going to take this work yeah no thanks for having me um hopefully in the active infrast Direction that's that's I would love to I think it'll be super fun so yeah I'm not really sure I'm looking at Maybe music uh right now um looking at uh other kind of crazy directions I don't want to sound too crazy uh but I I'll go down yeah a lot of things so one one thing that's coming up something we submitted to nurs is studying memory with traveling waves um so that paper just came out on archive today of how waves are really good at encoding long-term memories which I think is super interesting so I might go a little more in that direction sounds good and yes would be very exciting to see action come into play when there was the neurons that stay Act even as the dogs feet were moving there's a lot of like action sequences like throwing a baseball and then it goes and it's like there's something about that action that's continuing to influence and so having like a deep temporal representation of alternative actions and then the variational autoencoder is already basically the right Basi and statistical architecture [Music] to discuss the variational and the expected free energy so I think it's a very promising area yeah I'm I'm super excited about that so I really hope to to get over there and happy that if people have ideas I would love to hear from you via email or anything like that so really appreciate it all right thank you till next time thanks so much bye by e