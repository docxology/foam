hello everyone we're in our first discussion of chapter seven in cohort tree so let's head over to chapter seven and uh we will come to the questions but let's just start with anyone who wants to do you want to just give a general comment or any reflection or thought on seven or anything up to this point Ali and then anyone else uh yes so uh chapter seven is basically the application of uh the first half of chapter four in terms of uh modeling um discrete time situations uh and uh it it goes through a number of case studies uh in order to show how the matrices a b c and d can be constructed in various situations and the the challenges or uh rather the concerns uh we may have when constructing such matrices in order to have a viable discrete time model uh for each situation uh so yeah that's basically uh the premise of chapter seven awesome thank you totally agree just like chapters two and three with low row to the high road we're kind of like a pair chapters seven and eight are kind of like a pair because seven is going to go into discrete time modeling chapter 8 is on continuous time and hybrid modeling does anyone else want to make any just overall reflection or a comment on chapter seven all right I'm checking if there's any okay does anyone have any question at all otherwise we'll turn first to these questions start with the written questions we have and then just approach it from there does anyone have anything else that they want to consider first though okay okay I'm putting in the chat the um oop that's the um social sciences link but here's the chapter 7 Link in the chat and so you can follow along there um also please upvote the questions that are interesting to you because the the questions that have a bunch of upvoted points we're going to make the short videos for um okay so we'll just go to the I'm going to unvote all the questions from my you can still vote for them but I'm just going to unvote them so that once we get to them all all vote for them okay all right first question here's chapter seven chapter seven begins with the quotation what I cannot create I do not understand Richard feudman what do you think this means in the context of active inference specifically I can't remember exactly who added this in a in a previous uh cohort but um they they gave a mild and a stronger answer the mild answer is you have to learn by doing you actually have to build the generative model to understand active inference um and then the stronger answer it goes even beyond that and it's not just that you have to build the generative model you actually have to have something that escapes your hands and does something interesting to really test the method and understand it foreign anyone else want to add a a comment on this okay kind of a warm-up question but anyone else want to add any anything on this what I cannot create I do not understand otherwise we'll come into the more specifics of the chapter seven now okay so in that case I'm just gonna um go with the most upvoted questions all right this is on page 130 the textbook they wrote this Choice which we're going to look to the textbook in a second speaks to the classical exploration exploitation dilemma and psychology a dilemma that is resolved under active inference what do you think about this okay um so the um setting that's being considered is decision making under uncertain payoff or reward structure so you think there's two slot machines one of them pays out fifty percent of the time you don't know how much the other one pays out so you could continue to get 50 percent that would be exploit or you could explore and it might be better than 50 it might be worse than fifty percent and so that is sometimes called the exploration exploitation dilemma um does anyone want to give a comment or or a related question on how is active inference going to be approaching explore exploit Michael yeah I just um uh wanted to both uh affirm that I found the mathematical representation of these two ideas that this trade-off very interesting but to describe them as resolved is um is something that I find might be a little more um uh a little more over described if you will that it um that it it it that maybe the word is this this formula describes the trade-offs or as a way of representing it but uh resolution is a much more open open topic so just thought I would give a response give feedback and say hey well well yeah that's it yes maybe resolve in the sense of like a high resolution image like we can see it we've resolved the Dilemma by perceiving it not um we have explained it away or said that there is no trade-off I believe this was Eric sounds previously who kind of contested the idea that explore exploit is a dilemma um it's not a dilemma there's not two options that are intrinsically opposed there's just one integrated behavioral Choice um all of this being said let's look at how explore exploit our approach differently in um inactive infants one more remarkable in a different um sense making context um where where the explore versus exploit issue of Rose can continuously the word um sharpening visibility was a way of describing um as uh the space that sometimes just your resolution come and helps remind me of that that it was it's like boosting the visibility of the space does not resolve the problems that you're trying to address but it does it does improve your ability to act on the space and I guess that's um uh maybe a suggestion for language boosting visibility versus resolving that's all yes awesome yeah yeah just to briefly catch us up to where how we get to this claim chapter seven like Aldi mentioned is going to be concerned with a series of examples of building discrete time generative models so these are models where time clicks like one step at a time figure 7.1 is showing just the perceptual or the downstairs component of a generative model so D is the prior s are the hidden States B are how the hidden States change Through Time a is the Matrix that Maps hidden states to observations so this is just the temperature in the room changing Through Time outputting a thermometer reading at every time point this is a hidden Markov model not a not a Markov decision process just a hidden Markov model hmm they then write the A and the B Matrix as well as the D for a musical setting so there's four notes the a matrix is basically saying you hear the correct note 70 percent um of of the time um and then the B is the transition Matrix so this is just playing four notes that transition to each other 97 of the time in this order and it always starts on the first note so with just those three matrices defined a b and d you have fully defined the generative model for this hmm there's no decision making yet but that's all you need to Define this hmm and then in figure 7.2 there's an example where actually um there turns out in this simulation that there that the third observation it should be here but it's actually here um so that's what's heard because there's such a prior that that's what should be heard okay now we get into action which is where the um explore and exploit are going to come back into play so if it's just a purely passive inference then the kind of um dilemma or or um uh unavoidable trade-off is like over and under fitting perceptually making something that's over compressed or undercompressed with your beliefs but action has a difference set of challenges because it's related to a selection of of causal intervention in the world and it changes sequences of future observations and all of this and as we know from the expected free energy the imperative for Action is to have both epistemic value like to gain information and to have pragmatic value to have alignment between your observations your preferences here's figure 7.3 this is kind of the classic generative model in discrete time so we see that same downstairs component it's the exact same plus there's the upstairs decision making component by policy intervening into how States change Through Time expected free nrgg updating policy decisions up waiting them based upon their expected free energy sorry yeah please it's a jumping pack uh uh 60 seconds but right before you describe that diagram you said how you compress the um I can't remember if you use the word compression and I wondered why is he using compression to use to just as the verb to describe what's happening and are you suggesting that in the same way that let's say when you when when you compress data onto a um you know just save more storage space you do an abstraction reduce it and it's a compression of the of what is the what the data is that that's that that is what's happening there is a I'm sorry to be wordsmithing here I'm just trying to make sure I'm tracking what you were saying when you said okay it actually is it is very closely related to this um like there's two ways to think about it one is like we're like we're fitting some gaussian let's just say some bell curve over some uncertainty so we could think about like do we want over compressing it like in terms of tightening the variance that would be like overfitting versus like if it's loose then it's like under so that's a little bit like a loose sense of compression really it's just a variance estimation or you can think about complexity minus accuracy as being like Optimal dot zip compression like obviously if you wanted a lossless compression you want a perfect accuracy then you're opening the door to an infinitely complex model on the other hand if you're willing to find a trade-off between complexity and accuracy which is summarized by variational free energy in the moment then you would end up with um at the point of diminishing returns like the kind of the Pareto optimal compression point so sorry yeah sorry no no it's all good so now to study this action setting which is where explore exploit comes into play we're going to switch from the music um listening example which was just kind of familiarizing us with the a b and the D and now we're going to bring in action which is going to require talking about um Pi policy and C are preferences okay so it's a t maze task there's an averse of stimulus in one arm an attractive stimulus in the other arm and a cue that indicates the location of the two stimuli in the bottom of the T so the organism can either pick one of the arms left or right maybe quickly get to the reward maybe get the aversive stimulus or it could choose to seek out this informative cue and then having reduces uncertainty about the location of the um reward then like just walk over to the reward so that one that's that's going to be at least you know one two three steps instead of just going straight to one of the arms um but you then have higher accuracy with getting to one of the Arps um so how do you get flexible and rapid and adaptive switching between pragmatic oriented behavior and epistemic oriented behavior especially how can that happen as new information are rolling in without necessarily like retraining the model that um is what is going to be explored here foreign how they see it at this point um yeah have a general question in uh you know in terms of if you do an experiment with uh you know different uh animals for example or different observers uh and you want to model uh you know different balance between this epistemic and pragmatic uh Behavioral no where in uh you know in the ABC D matrices will that be yeah very good question one place that this plays out is in the amplitude of C so um if if C let's just say the um left and right um are the two directions on the teammates um and um we don't have a habit for either one of them um if we prefer the sugar over the aversive so now we're going to talk about the C Matrix if it's one and zero so adversive we don't really care about then it would give a value of one for the reward versus if the C was a million and negative a million then the pragmatic value term would basically just be like scaled larger so the amplitude on C is commonly used um to to weight the pragmatic value relative to the epistemic value because just framing things like this does not guarantee that your parameterization is going to be at this adaptive point that actually does flexibly trade off between epistemic and pragmatic Behavior because if you have C if C is very small and washed out so utility seeking has you know shapes Behavior a little bit then you're going to end up with a mainly epistemic agent and if C is vast relative to the epistemic value then you're going to end up with mainly reward seeking um just like we see in the um in figure 2 6 where if we totally discard the pragmatic term we end up with purely epistemic Behavior and if we totally discard the epistemic term we end up with purely pragmatic Behavior but also if this term is just really small you can imagine it doesn't play that large of a role and and vice versa so in an empirical setting you have to parameterize um the the value for example of C so that you do get um goal reward oriented behavior that that but not um just like one track minds um Ali and then anyone else are you well actually this uh C parameter C Matrix here in the formulation of expected free energy is a relatively recent addition to active inference formulation uh because in some earlier literature or even uh some recent literature uh this parameter C is not explicitly expressed in the formulation they just express it as a p of observation because you see uh this is a kind of this is a crucial step actually in active inference because if we want to derive derive this expected free energy as a parallel to variation on free energy then here instead of C we should have only PI right but replacing uh Pi with C denotes the kind of inaccuracy uh in in terms of our policy selection in other words it doesn't necessarily entail I mean undertaking the policy we've selected we have a preference Matrix for undertaking some actions but it doesn't necessarily imply that we we only take those specific actions per se so we somehow approximate our preferences uh without restricting ourselves to uh to the actual policies that's been undertaken um in this situation so that allows us to um even formulate or at least describe some counterfactual examples as well as as well as I mean the actual Behavior we observe for the agent so that's I think one of the um I mean crucial steps uh in in terms of allowing for a broader behavior of the agents and and somehow broadening the possibility spaces of the agents and not just the actualized trajectories of the internal States versus the external States yeah oh yeah it allows us to articulate what is the action possibility space and separate that from the um outcome preference so we don't have our finger on the scale directly can't control observations directly that's the kind of perceptual control theory Insight you want your preferences to be about observations but you don't want to be able to directly control observations so what are the two channels that we have to produce expected free energy change our mind change the world how much to balance those two that was Olivia's question um and that is that is the question is how do you balance those two but those are the two things that you want to have um from a first principles being balanced off in terms of your imperative for Action selection you want your action selection to be in epistemically informative path and a pragmatically useful path epistemic value expected Information Gain under a policy pragmatic value how well it aligns with your preferences expectations um so this is how exploitative and explorative behavior um are again going to be resolved not say addressed away but how it's approached if we were in a reward setting then exploitative Behavior needs no secondary explanation it's like there's money on the ground if it's about getting money then just you know pick it up so exploitative Behavior under reward Centric Paradigm does not need a secondary explanation exploratory behavior on the other hand eventually needs to navigate into the currency of utilitarian value in a reward-centric path and so in exploratory path might be highly valued because for example there might be a reward so if there's a 10 chance of a uh a thousand dollars then that um policy would have that expected reward and so that might um be more preferable than um for sure one dollar or ten percent of a thousand dollars in the reward Centric approach so then that's how you get explore and exploit balancing you have exploratory trajectories being evaluated in terms of their expected returns and reward and then you can have everything compared on the common currency of expected pragmatic value in reward learning okay so how are we going to approach it differently in active inference different trajectories are going to of action are going to be compared again in a common currency but rather than that common currency being expected reward the common currency is going to be expected free energy expected free energy is going to have two components expected Information Gain and expected pragmatic value as stated before balancing those two terms into an Adaptive Zone is a fine-tuning question but this is how we approach the question in a first principle's way by making a unified imperative that one includes perception and action that's the integration of perception action Loop we see like observations and we see actions they're just all there in the same equation and it unifies epistemic and pragmatic value instead of coercing epistemic actions into their expected reward and then comparing everything on the reward meter stick we just have a single unified imperative expected free energy that contains an epistemic and a pragmatic loading Michael great stuff that's all yep equation 2.6 gets Revisited again and again and just different ways of seeing expected free energy so now we're going to continue seeing um the teammates so here the matrices are are shown for this teammate so yeah there's the mouse aversive and beneficial outcome Olivia yeah it was just I was a bit like behind on the last question uh 7.4 uh for you I mean I I find your last explanation really great and very you know enlightening um context of reinforcement learning you will have uh somehow the first term within the second is that what you're saying um it wouldn't be nested within the um second strictly but yes basically policies um you know you have you have a um one slot machine is um you know giving you the 50 reward and then you don't know the other one but a policy might be entertained and even selected because it has a high expected value that is expected value based decision making because you say well um I think slot machine this slot machine might have a high value so I'm going to take an exploratory action to to kind of explore that but not because of the Information Gain it gives me but because of the expected reward so it's kind of like converting everything to a pragmatic currency and then comparing the expected utility or the expected reward and so you get utility discounting you get time preference all this stuff but that's all like um those are all correction factors within pragmatic value yeah okay yeah thank you sorry for the introduction no it's it's and I mean this is one of the key topics and and this is this is um how policies are selected and and that is the empirical challenge is being able to parameterize everything so that it doesn't just behave trivially like a novelty seeking agent over here or like a utility seeking agent over here we want to have some training you know in in the middle um here's uh where the matrices are shown for these two settings here the the block is on the right and here the white is on the right um and it just shows what the matrices look like so it's helpful just just um to see what the actual matrices look like in the generative model because remember this is the generative model so if you define a b and d just like we saw in the music example they're just specific matrices of whatever dimensionality it is that you're studying if there was four notes then D had four and a vector a was a four by four B was a four by four Matrix in the music example now when you add in policy policy has a dimensionality of however many policies you can take so if there's four options policy has there's four options in the pi Vector um and then there's going to be a slice of B for each of the dimensionality of Pi so if there were four options for what you could do then there would be four slices in b and it's kind of like okay if you choose auction three then B three third slice of B is going to be the one that we use to to to to um update s so once you define this very limited very interpretable set of parameters you will have stated the generative model and then you're ready to engage with standard methods for training and updating this generative model and so that's why in active inference we spend a lot of time think about chapter six setting up our understanding of the problem because once you can describe the dimensionality and and the parameters of the generative model that was the work then you just update this as a statistical model using standard methods so there's a lot of work in understanding the problem and then framing it and understanding what is the system of interest and all these other things what form of the generative model is appropriate but then you don't need to engage um hopefully but but actually in like secondary engineering concerns for reasonable projects I'm sure at some very large scale these things do come back into into play but that's kind of the cool thing about active infants we understand what all these variables are and then when we want to model our own setting we just specify the generative model and kind of hit play more matrices being shown um here's the C vector and so plus six for the attractive stimulus and negative 6 for the averse of stimulus now if this were point zero zero zero zero zero zero six and negative 0.0006 so the amplitude of C were just smaller so the the relative preferences would would stay the same but the pragmatic term would just be smaller so then the agent would behave more epistemically whereas if this were 6 trillion and negative 6 trillion then that term the pragmatic term would come to dominate um so again what is the approach of the resolution it's putting different trajectories of behavior action selection on a common footing expected free energy which unifies the epistemic and the pragmatic value of different actions we don't say epistemic and pragmatic value are the same thing in fact they're they're different but they're complementary and so having a unified imperative helps us adaptively approach this question of the actual trade-offs between courses of action that have known consequence and those that are providing new information I don't even want to have a thought yeah go for it this is you know I'm I'm uh a babe out of water in so many levels but in my original field of training in economics we we had this phrase we used Austin called ceteris paribus which was a way of kind of uh saying that's an accidentality to our models and we're just going to keep going with the way we do things which is close enough and the consequences of this kind of um or treatment of dilemmas was that things like pollution and other externalities were they might not fit in the representational model and therefore we're kind of underweighted and trivialized and what I'm hearing you make a case for in a different discipline in a different way is saying changing the math to to handle the what is otherwise trivialized is external you know too inconsequential to be attended to is embracing the the tension and and thus making it a part of what then can be the sense making model of the discipline overall which seems um something we can learn from from from y'all cool so yeah it's only not even one one um layer and connecting that to like kind of good Hearts Paradox like a metric becomes gamified do we care about the um the levels of lead in the measurement or do we care about the hidden unobserved levels of lead in the soil and so in our systems model that hopefully everyone will be able to come to the table on we could say well really you know there's both they're just different things there's the true you know underlying distribution which we're never going to measure every little grain of sand and then there's the Sensor Fusion that we have to do with observations so let's not try to game the observations but like rather come to the table with a more holistic understanding of the relationship between hidden States and observations and how hidden states are changed and how our actions influence the hidden State distribution but how that emits observations just articulate the problem break it down into the components and then the amazing thing is like these natural components of the situation again prior beliefs about hidden State underlying hidden states that are unobserved in mission of observations how the world changes their time actions that influence how the world changes Through Time our strategy of deciding on action those are are naturally separable aspects of thinking about a situation and in a way it's kind of amazing that all you have to do is just like smash the matrices together and the math does kind of work out again in a way that we would want it to brilliant yeah it's chill um uh 7.4 is going to go into a little bit more detail on this epistemic value of policies so there's a few different breakdowns here they're all equal but this is a few different representations of just how we can think about the informational gain associated with a given policy so whereas the pragmatic value for a policy is is fairly straightforwardly identified with how close the expected observations are to the preferences so the most pragmatically valuable trajectory is the one where the body temperature is right in the middle of the temperature distribution the least pragmatic one would be the further away but here's where we see this I of Pi so there's a few more descriptions on I of Pi and so it turns out in this case with this teammates initially the action that the mouse selects is to um it takes an epistemic um action to reduce uncertainty because it's like okay there there might be some pragmatic value but also it's a risky decision up here but going down is gonna be a policy selection that that gives me a lot of information now this isn't the top this isn't the whole iguana as as they brought up in the beginning of the book like how does the mouse come to know that the aboutness of the queue is this setting so this isn't like um you know from The Big Bang to the mouse making the decision the total model there's always stuff that's structurally encoded in the model how does it Mouse come to know that the aversive stimulus is negative and so on so these are levels that you can add in but this is just showing basically again in this simple example it's just showing what it looks like when the Mouse um undertakes an epistemic action and in the um pi mdp model stream 7.2 where there's like a very similar example to this it's not in a t maze but it is with a mouse that um gets some information and then makes a decision so it's like kind of like it it's very similar to this um you could play around in the script and you can see what happens when you change the preference Vector to like a super high or super low value um okay little bit of uh boxed discussion on precision and entropy um the H is associated with entropy and um and it's really but but let's just continue on um that's just related to the over under fitting or the expected um the expected sharpness or blurriness of a distribution they move on to another of the settings that's been very very um well repeatedly studied in active inference which is the I cicade uh setting the motion of the eyes and so one reason why this has been studied a lot is that the the fovea of the vision the area that we have high resolution color vision is actually very very small relative to the visual field and that's why multiple times per second the eye is making these policy selections to move around and those policy selections although we might look at something that's um beautiful or rewarding and have that fix our gaze it's a first approximation [Music] the imperative of the icicade is informational it's gaining information about um the visual field and so that is why the um epistemic visual search is a really um useful and tangible setting again it's not like gambling if there was if there's uh so you you're you're kind of setting up a situation towards pragmatic value but rather this is one where there's going to be an emphasis on the epistemic and so this is like based upon the ambiguity of stimuli um and about how how um epistemic attraction occurs when there's information to learn where something where there's no information to learn doesn't stop moving there does not have epistemic value um um kind of this is a little bit of a formalization of the street light story from every culture where did where you know where are you looking for your keys under the street light oh did you lose them there no this is just where I could look clearly so that is about like looking where there's light and searching where you can not necessarily because you have a high prior that the information is there but just because it's easily resolvable under the street light best place to find high quality unambiguous uncertainty resolving information so the ambiguous poorly lit square is ignored reproducing in silico street light effect if there's if you're not going to get resolvable information there's it's not of epistemic value to go there but if there if it is going to be high quality uncertainty reducing information that is like epistemically valuable seven five learning and uh novelty so in figure four three in this or or in the um in seven where we where we revisit it in um 7-3 there's no learning in this model as written there's updating of the Hidden States you know by moving it through the B Matrix but none of the but the a b and d themselves do not change so now they're gonna talk about how learning can come into play so in other words like the a matrix is fixed in this setting but you might be interested in a situation where the a matrix can change the time like maybe you know the mapping between hidden States and observations changes the way that learning is generally approached in Bayesian inference settings is you make a prior distribution on the variable of interest so for example lowercase a is going to be a hyper hyper prior or just a prior Distribution on a so instead of just saying a is seven one one um like we saw in the music example you can have a distribution over what a looks like that is gonna help with learning in the extreme case where like learning rate converges to zero that's like a super sharp distribution around what you think a is going to be like so that you're very strong prior on a so new information is only going to move that sharp distribution a tiny amount conversely if you were having um a lot of learning on a you'd have a very flat prior on what a could be so that new observations would update your beliefs about a a lot but this is just an example of the composability of this framework in that the base model doesn't have learning attention all these different relatively sophisticated cognitive phenomena but this is like the little firmware nugget or kernel which by understanding the composition compositionality relationships of then we can start to bring in learning on different parameters and of course learning is awesome and it sounds awesome it is but if you also think about it from a statistical perspective that doesn't mean that every parameter should be learnable because sometimes you're just increasing the computational challenge of your problem vastly thanks more information on learning here we see another um difference with uh with learning between active inference and many other machine learning approaches and this is the the um embedding of action in the inference process we actually we actively think about how much information are we gonna get like when people are training image recognition it just here's a thousand images in the training set just plug and chug but what if you had an agent that was like which image should I look in of these thousands then it would become of primary importance which sequence of learning observations the agents took within those thousand labeled data points so that's what G helps us understand some new um terms salience novelty different decompositions of G uh one final example again more just like referenced because it's in other papers but not really explained with the matrices here we have a um a maze exploration agent which is like another kind of classic setting so before it has gray in the likelihood which is like 50 50 between white and black but it starts off you know it takes these three steps and it's like okay it's it's updating now it moves here updates that okay that's a white square that's a white square that's a block Square that's a block Square so this is like somebody looking over a maze visually seeking a path um to solve the mace 7.3 is a little bit of a um summary of a pretty um Advanced topic which is Bayesian model reduction and structure learning which is treating the question of what structure should the generative model have treating that question as a parametric inference problem so um should the model be one two or three time steps deep and then say okay well we're going to have some time Horizon parameter and then we're going to test whether one two or three is the better model so like we're gonna learn the structure of a model as part of reducing down from a portfolio of models but they just sort of briefly note it and there's um more citations and then as then any other just thoughts or questions seven kind of has a lot of of little short vignettes okay just just briefly just so that we touch it all then um next time we can um have voted and added a lot of questions and everything 712. is a two level model it's a nested or a hierarchical model both the higher and the lower level one and two are discrete time you can tell because they all have the S T minus one t t plus one so later we'll see a hierarchical model where the lower level is continuous time and the top levels discrete time but in this setting both nested levels are discrete time so this could be every hour and this is 60 Minutes within an hour every day 24 hours within a day here's a multi-scale updating uh generative model that's related to um sentence reading and they've done a lot of simulation studies of of multi-level inference in Reading like where the eyes are moving to resolve uncertainty about the letter order of letters is sought out to reduce uncertainty about words order of words in a sentence is sought out to reduce uncertainty about semantics the semantics are operated on for pragmatic value so that's chapter seven it's a fairly um Loosely connected sequence of discrete time models several examples starting from the musical example which is just a passive inference hidden Markov model bringing us back to the um discrete time generative model that we saw in figure 4.3 that's shown again in 7.3 and then several decompositions of expected free energy focusing on the epistemic value especially in terms of how getting epistemic and pragmatic value unified under this G functional allows us to approach the Adaptive finessing of exploratory inex and exploitative behavior and again the work is in understanding and designing the generative models that's not work about work that's really the task itself is the system's modeling and systems understanding because essentially by saying what these variables are what shape these variables are you will have prepared your model to run and until these models are defined in terms of their dimensionality it's not ready to run any um kind of closing thoughts um Michael it looks like my dad had to do the math class after all there's so many I mean as we lie there would be so many and again like let's try to uh let's explore how the um let's see how the the AI explanations did for chapter seven math explanations why would a high schooler be interested let's look at it for one of the epistemic ones people can evaluate and add comments or or change the um the text but for those who are learning and and ultimately we all are I I doubt there are a few if any people for whom this equation doesn't require contextualization and clarification and deeper breathing yeah I mean and these lines are not he's not it's not like divided by two and then you know you know you know do this it's like these are multi-step derivations and so going back to the accuracy minus compression that's the challenge of writing the textbook accuracy minus compression of what they had to share and um so our work is to kind of uh refined that balance for ourselves some of these look helpful some of them don't look helpful in terms of AI answers I would use the word scary I gotta run by the way great thank you yeah any other comments all right great well next time second discussion on chapter seven so we'll return and look at what um questions have been upvoted or asked so thank you all um farewell bye thank you