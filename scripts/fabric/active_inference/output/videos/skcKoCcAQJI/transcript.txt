[Music] [Music] all right hello and welcome everyone it's active lab live stream number 46.0 and it's june 10th 2022. welcome to the active lab everyone we're a participatory online lab that is communicating learning and practicing applied active inference you can find us at links here on the slide this is a recorded in an archived live stream so please provide us with feedback so we can improve our work all backgrounds and perspectives are welcome and we'll be following video etiquette for live streams head over to activeinference.org if you want to learn more about live streams at the lab or other projects that are ongoing all right we are here in the first discussion the dot zero for background and context in stream number 46.0 we are learning and discussing the paper active inference models do not contradict folk psychology by ryan smith maxwell ramstead and alex kiefer and the video just like all the videos are is an introduction and a contextualizer and an appetizer for some of these ideas it's not a review or a final word we're going to say hi and give introductions then we'll be covering the roadmap and abstract claims and aims and then we'll head through some of the core sections of the paper and just get some of the key arguments down and during the dot one and the dot two in the coming two weeks we'll have uh open space for asking a lot of questions for those who wanna participate live and ask questions as well so on we go we'll start just by introducing ourself as much or as little as we'd like and if we want to mentioning something that we're excited about in the paper like what brought us to want to contribute to this dot zero and all start i'm daniel i'm a researcher in california and i think the title says it all though there's still more to add and it speaks to many of the hottest and brightest debates in active inference which is the rubber hitting the road with mind and brain and body and psychology and previous and different conceptions i'll pass it to dean thanks danielle dean i'm up here in calgary um i think well a couple of things first of all one of the first active infrastructure that i was on was a paper that ryan had written and so i was kind of impressed with that paper and i knew that if i sort of invested some time in this one stuff that i would become previous and the primary thing i came where i was was looking at active inference from the decisioning and uh and the movement standpoint sort of seeing that's uh looked at from a higher order and the lower order type of um processings and that when i bought the time i got the end of the paper and started looking at the mind's world and world's mind piece and and and seeing where the the potential connectors were between sort of coming into it not necessarily knowing that there would be different things of active inference much like i didn't realize there was an extended active inference until i read axel constance paper that was kind of helpful and i'll pass it to jacob hi everyone i'm jakob i'm a student from the czech republic and i'm excited to uh reduce my uncertainty about the new kind of formulation that that um was introduced in this paper at least it was the first time i came across it in the active inference literature um as you mentioned dean about the different higher order and lower order descriptions and how they conceptually map um to the different mathematical mathematical formulation and also to the psychological ontology and um overall just discuss uh what that what that means and um what it might mean for um other applications as well and i'll pass on to ryan i'm yeah thanks so i'm i'm ryan smith i'm a research associate professor at the uh laureate institute for brain research um you know so i'm the first author of this uh this paper so you know obviously i was especially excited about the ideas to write about it um or sufficiently motivated at least to try to clarify some things that i am you know from my perspective or sort of common misconceptions um so um yeah i mean you know i'm i'm probably more gonna sit in the background here i'll just be here to uh you know answer questions or clarify anything um you know if something comes up where um something in the paper wasn't uh wasn't sufficiently clear because i didn't i didn't do my job well enough so um yeah so i appreciate you guys being with us willing to uh to talk about it thanks well an overall comment was we really appreciate the clarity of the argument and the writing and the weaving together of the the math and the formalisms with the argument the rhetoric and the ontology that's that's prime time so we'll just jump right in and of course anyone is welcome to share their comments so dean would you like to help us contextualize with a big question yeah so i think that there's some questions surrounding and i'm just going to take some of the important quotes that i i lifted from the paper the concern that active infants spoke psychology because they do not explicitly include terms for desires or other cognitive constructs at the mathematical level of description so given that distinction uh that there are active inference models of motor control which need not have desires under folk psychology and active inference models you have desires within folk psychology then worry is that it those can be constructed could you um i think when you're talking you like turn away a little bit or move back it kind of cuts the audio so maybe yeah we'll try that okay continue from the given thank you okay so given the there's a distinction in active inference models of motor control uh which do not have desires under fox ecology and active interest models of decision processes which do have desires with psychology or so as argued here then the worry is that if active inference models can explain cognition without appealing to constructs it can be mapped onto the common sense notion of desires and that's how to explain what that is then this could be seen as threat or intuitive folks i call like oh of ourselves as agents such a situation would also pressure the traditional belief desire intent model of folk psychology that is prominent in philosophy the bdi model is a model of human agency that explains what it means act intentionally so for me i was introduced to the bdi model as yes great and and one way that i kind of saw this big question even pulling back a layer is like is active inference recontextualizing reframing augmenting building on what is already familiar in a sense though there are many folk psychologies and i think that could be something we go into or is this the displacement of some cherished framework for some other construct and the way that people speak like i want the cup of coffee or something like that are we gonna need like a different word for an active compatible folk psychology and linguistics so jacob any thoughts on that well um i guess one question i uh perhaps had was like um this even this formulation of the of the initial of the initial statement like given this distinction of motor control active inference and decision processes active inference then we have this then we have this issue well one thing that i was a bit uncertain about was whether this distinction was made to fit the distinction that's within folk psychology like there is some kind of ontology for motor control and um an ontology for decision processes so therefore we try to um split active inference into these two parts or whether there is actually uh whether these this distinction follows directly from the the same mathematical formulation of um active great well i think i think i think one thing one thing to um you know to clarify i think i think the way that this kind of given the distinction followed by then here um i think could be leading to a little bit of a misunderstanding you know so the the idea that there's a concern about about a threat to full psychology um comes not from that distinction it comes from the the fact that there's at the mathematical level of description it doesn't look like there's anything in there that that's that's desire-ish right i mean that's where the worry comes from um it doesn't really the worry doesn't come from the distinction between motor control and decision processes right the the distinction between motor control and decision process is actually one thing that by making that more explicit and it actually helps to resolve uh the concern or it shows why it's not really a concern or that part of the concern stems from a failure to make that distinction explicitly um so so just to kind of clarify the you know so i think the order of the of the thought process here is a little bit different than the way um the way it is in the paper thanks helpful times and we'll clarify and go through the argument in order to so uh briefly just the aims of the paper as they presented are to provide a brief review of the historical progression from predictive coding to current active inference models and show that despite a superficial tension when viewed at the mathematical level the active inference formalism contains terms that are readily identifiable as desires and related cognitive constructs at the psychological level which is downstream of that clarification of the distinction that ryan just mentioned and then they discuss the additional insights offered by active inference and the implications it has for current debates about active inference any other aims you'd want to add ryan um i think this is fine at the moment i mean i'm sure things will come up great and then um claims um jakub could you read the claims yeah so uh firstly that the apparent problem posed by purely doctastic looking constructs simply is not a problem there do not appear to be cases where the phenotype consistent prior expectation in dai often called prior preferences will ever conflict with or make distinct predictions than a traditional folks i call psychological account in which beliefs and desires are integrated to intentions the second claim is that what we have referred to as dai the partially observable markov decision process formulation of actin is a corollary of the fep and it can be implemented using prediction error minimization but there are many other aspects of the fep and many other theories that fall under the umbrella of predictive processing and in some there are beliefs and desires in the active inference framework thanks and those are just a few claims that we pulled out but many other declarative sentences will also be claims um i wouldn't i wouldn't necessarily call those the primary claims of the paper i mean there's certainly statements that we make but um i think i think the idea that there isn't uh i think the idea that the apparent problem isn't a problem i think that's a claim um and uh that it doesn't that there won't be a conflict with with an account where you're combining beliefs and desires to form intentions that's true um this idea about about what falls under the predictive processing umbrella would um you know immediately i wouldn't necessarily say that's something we're arguing for in the paper it's just it's just um something that that matters when you're trying to correctly frame this kind of this kind of debate because um you know predictive processing is just a very generic term right like like it doesn't refer to any particular mathematical formalism it just refers to a really broad idea that in some way the brain is doing some sort of predicting um either with respect to resolving problems in perception or problems in decision making and motor control so so there really isn't um there's not enough uh there's really just not enough mathematical specificity associated with predictive processing as a term to really even test any predictions that it would make right so i mean i mean what you what you need to do is pick you know whatever specific mathematical formalism what actual hypothesis you're talking about um and you evaluate and test claims with respect to that right so there's there just aren't there's aren't clear uh like what even uh it's just you're predictive processing is just too general in bank is the point so you know we can only really say look under under the current models used that are called active inference right so these you know partially observable markup decision process models um you know that that minimize expected free energy as a way of making decisions right those those arguably count as one particular right theory or or class of models under the predictive processing umbrella and we can evaluate claims with respect to that model but you know what's true of that doesn't need to be true of you know the other you know 20 things out there that might um you know might also fall under a predictive processing umbrella so that i mean the main point is just we need to evaluate um you know claims and predictions and things like that with respect to a specific model not with respect to a kind of vague general category of models thank you okay um so just rapidly through the abstract active inference offers a unified theory of perception learning and decision making at computational and neural levels of description in this article we address the worry that active inference may be in tension with the belief desire intention bdi model within folk psychology because it does not include terms for desires or other cognitive constructs at the mathematical level of description to resolve this concern we first provide a brief review of the historical progression from predictive coding to active inference enabling us to distinguish between active inference formulations of motor control mai which need not have desires under folk psychology and active inference formulations of decision processes dai which do have desires under or sorry within folk psychology we then show that despite a superficial tension when viewed at the mathematical level of description the active inference formalism contains terms that are readily identifiable as encoding both the objects of desire and the strength of desire at the psychological level of description we demonstrate this with simple simulations of an active inference agent motivated to leave a dark room for different reasons despite their consistency we further show how active inference may increase the granularity of folk psychological descriptions by highlighting distinctions between drives to seek information versus reward and how it may also offer more precise quantitative folk psychological predictions finally we consider how the implicitly cognitive components of active inference may have partial analogues i.e as if desires in other systems describable by the broader free energy principle to which it conforms here's the roadmap so the dot zeros all of them in the world wouldn't be enough to hit every stop so we will go through um roughly in order of these sections um and the section titles are listed here but ryan what was the thought going into the ordering or the structuring and why there was such a comprehensive historical and preliminary consideration section i mean i mean a lot of you know i mean the vast majority of these sections are just kind of building up background right i mean sufficient background to kind of see right where where the um you know both both i think historically where you know potential misunderstandings might come from um you know and also and also just enough of the you know providing enough of the formalism you know one part of the formalism just kind of build on another right to see why the um you know to see why the the apparent problem just isn't there right so i mean the vast majority of these sections are just preliminary just kind of building up so that the reader has the information necessary to understand the argument i mean only only the last couple of sections they're really um really the meat of the argument itself i mean so so you know for example just section two you know the are what we just said from the predictive coding active inference is just kind of highlighting how you know these initial brain as as implemented through predictive coding was you know purely a model of perception right so then you know so then when people but a lot of times you'll see especially in the um you know older literature in some philosophy literature um you know predictive processing somehow both refers to predictive coding but then also is talked about as though it's um as though it's the same thing as active inference um you know or there'll be these kinds of ideas where um if you just extend predictive coding as kind of a the way it's talked about as a kind of way of thinking about what the whole brain does you know then all of a sudden um you know controlling the body ends up also just being something about predicting what the body will do um but but and especially when described that way right it sounds like something like desire and motivation and goals and things like that are just kind of completely out the window and it's very hard to make sense of what how a system like that would even work right um and so it's and and part of that i think is because there's you know some um you know some uh the way that certain things are written in some of the literature can be a little bit um confusing or it's not too it's not too uh hard to understand why you know these sorts of um you know these sorts of misunderstandings could uh you know could happen um but but uh but the you know section two there was really really is to show how you know initially right when you're trying to move from predictive coding again which is purely a model of perception it's not about decision making it's not about action selection you know it's not even about motor control right it's just it's just a bayesian just a theory of how the brain can do approximate bayesian inference and perception um you know how that was initially just extended to say hey like if the brain works this way then you know how do you how do you get a system that actually controls the body using the same generic architecture right and so then you know the story that carl fristan you know and and you know some other people started trying to put together was the story about how you can use predictions you know descending predictions um if weighted appropriately right as a way of essentially setting the target states of the body right like like is this kind of you know the way you'll see it talked about is you know i predict that my arm will be here but really it's down here but if i wait that correctly then it'll kind of like you know you'll kind of control the set point in a reflex arc to you know so your arm kind of moves up with a position right but but that's just a theory about how you can use a descending prediction signal to essentially act as a motor command signal right there's nothing in that at all that that's deciding what that prediction ought to be right there's nothing deciding what that what which motor command should should be the one right that's getting sent down in in a in in the with the form of a prediction um right so so again so even there it's just a theory about how you can use predictions as motor commands right like there's no there's a very big distinction between that and whatever the system uh is on top of that right that's deciding okay what motor command what what motor prediction right do i send um and so just making you know making making that clear right because around 2015 there was this big shift right from these from these act where from active inference being talked about as a just kind of a motor control process right like a like a story about how you can extend predictive coding to uh you know also do motor commands right like from that to you know these larger more comprehensive decision making models which are the current right pump bps and those you know which was what we aim to show um very explicitly you know have something that um you know in the mathematics it just does right it specifies the goals of the system um it's pretty hard to get around the fact that you need something a system needs something like goals right because it has to evaluate somehow why one decision is going to be better than another right the only way you can really do that is with respect to how likely it is to get whatever the system wants um so so it's just i mean in my yeah in my view it's it's very difficult to see how um how you could ever get um a system to be able to evaluate one action is better than another without having some sort of target state um and um and so you can call right like the the target dates in active inference um a type of prior belief but at the end of the day its functional role is to specify what what observations are better than what right so i mean you can really think of active inferences as using this kind of trick in a certain sense right of specifying desired outcomes in the form of a probability distribution um because that helped keep everything belief believe me fully bayesian right um but it's not it's not it's not playing functional world belief right it's encoding you know higher probability just means right like the better more rewarding outcome um so so a lot of it is just um you know kind of working step by step through both kind of the historical progression so you can see where the misunderstanding could come from and then also through the current formalism so that you can see exactly where those where the you know the goal states are encoded in terms of something with the form of a probability distribution thank you the keywords are active inference folk psychology predictive processing bayesian beliefs and desires so now we're going to jump in and some of the slides have a lot of text topically arranged so we won't need to read all the text on many of the slides but especially if somebody wants to pull out like one of the highlighted sections and like bring it to our attention that would be awesome so um and also ryan thanks a lot for that great historical overview and i think there's there's so much more to be going into about what happened before 2015 and what happened in the last seven years um in fact i think you covered many of the key points in what you just described here which had to do with the development of predictive coding framework into understanding brain and body is there anything that anyone else wants to add about these points great then um all discussions and um dot zeros are like a two-way street where some people with active inference familiarity are learning about new ideas and frameworks and also people who might have familiarity with a broadly used area outside of actin are learning about active so for both directions on that freeway it's important to understand what is the target non-actin framework that's being juxtaposed and found concordances with active inference so the article is addressing the worry that active models may be in tension with the belief desire intention model and there are some consequences to that um worry in other words um given some priors it's a founded worry um would anyone like to summarize what the belief desire intention model is thank you daniel because you i think you put this slide together ryan what led you to select the bdi model and i wondered about um almost this tension or paradox with like it's folk psychology it's about what the people think but we're gonna do a citation and one specific academic acronym for what people think so is this like the main game in town for folk psychology from an academic perspective or what is the bdi and what led you to select that rather than like a small portfolio of alternate folk psychologies um i think i think that the you know belief desire intention model is just um a very kind of you know generic button but also um you know widely known right like account of you know trying to capture full psychology and you know it's kind of the it's a it's a it's a major um sort of target of uh a lot of discussion within you know like philosophy of mind um but but i mean it's just very generic right i mean just this idea that you know what what are the what are the necessary ingredients um to make a decision well you know i have to have some beliefs about how the world is and i have to have some desires about how the world i would like it to be and my intention involves integrating those two things right and then and then if all goes if all goes well in terms of translating intentions into controlling the body then i'll act out you know something according to my intention right i mean simple you know like the dumb example that you'll often hear just you know something like you know i desire some ice cream you know i believe that there's an ice cream truck you know down the road you know given those two things and and necessarily those two things right i can inform and touch it and then form an intention to go you know walk over the ice cream truck and buy some ice cream you know so i mean the question is how else you know are you going to explain why i went over to the truck to buy the ice cream it literally just is you know that is the general sort of thing that when we're reasoning about other people's you know decisions other people's um you know behavior and we're trying to figure out okay why did this person do what they did you know if we're assuming it's a voluntary action right if we're assuming it's not because like their arms spasmed or you know because they have some like really highly ingrained habit or compulsion or something like that right when we're talking about voluntary behavior um that is just the way right that that we tend to reason about how people do what they do you know either once when somebody does something that's we think is weird right then usually we have to explain that in terms of okay well they just believed something that was incorrect right or or they had some really funny desire right that i don't relate um you know it tends to be you know one of those sorts of things right i mean i mean i don't know a lot of other um you know separate uh folk psychological models that would uh entail anything different than this um you know at a very generic level um and it's also exactly the sort of thing right that that people try to contrast active inference with where where instead of you know belief desires beliefs and desires coming together to form intentions instead it'd be something like one kind of belief and another kind of belief come together to form an intention um which again for a lot of reasons um is you know confusing and rightfully so when it's presented that way thanks yes so ryan when we've had some conversations with other authors and other papers where we've talked about the scale three formalism and we've also talked a little bit about the sort of scale friendly uh times when you have to find that history and that context what you just mentioned was sort of the generic generalized sense that the bdi umbrella provides you're not saying that scale-free quantification and a generic uh way of modeling something are the same thing but what i think i'm like without putting words in your mouth are you saying that the scale freeness as it moves to something more scale friendly sweeps up some of that generic big tent idea or model and now moves it along it allows you to sort of parse and be specific and precise precise but also be able to generalize as you move through those you know quantifications of the just distribution and density and such because that's kind of what i was reading into it i just want to make sure that i got it i was kind of maybe you can help me clean that up a bit um i mean i really don't know that anything that we're saying uh really depends on or even gets into a lot of those sorts of specifics i mean really and really all we're saying is is that um you know if you want to take um the model uh like the the current active inference formulas and the current kind of like vanilla formalism right so in terms of an interested standard one level palmdp or you know if you want to scale it up hierarchically it doesn't ultimately matter it is if you want you use um the vanilla active inference framework to actually model some sort of voluntary decision process in a human right so i mean a lot of my work has to do with people you know modeling behavior on decision making tasks right often have to do with you know the person the person has the goal of maximizing you know how much money they win or you know maximizing some sort of social reward something like that the idea is just that when you're trying to if you're gonna take an active inference model and you're gonna use it to model uh successfully model actual human behavior when they're making decisions then um there's always gonna be a mapping between the different elements in the active inference model of voluntary behavior that or the whatever the prior you know prior preference distribution is you just check that so that it just builds the level of reward you know so so there's really nothing about any scale or anything like that this is much more generic it's just if you're going to use active inference to model voluntary choice then um it will always have something analogous to a belief and a desire because that's just what you need right like to model to model voluntary behavior um it's true that you know that's probably if you're if you actually were going to try to capture something uh kind of like more of the actual architecture in the brain as opposed to just this kind of like voluntary decision process aspect i mean i'm sure there's a lot of kind of hierarchy below that under the hood right that's translating more abstract policy selection processes into whatever the dynamic signals are that end up controlling you know moment by moment muscle movements and things like that but the point is is the below below this kind of top level where you're doing policy selection um you know at that point you don't really need the desires anymore right you just need the policy to specify whatever the motor commands are going down that can take the form of predictions um so so i don't i don't know if that helps but it's just it's just that there's a certain level in any kind of hierarchy of a model that's going to actually be applicable successfully to human behavior and at the level of policy selection um prior preferences will just include the the desired outcomes they'll just encode the goals that the agent's trying to reach and the policy that's chosen will be evaluated as being the most likely policy because it's the thing that's predicted to have the highest probability of um of getting the desired you know generating the desired uh observation um if i may also comment on that and i'm not sure if this is um answering your question dean but the way i thought about it when i was reading the paper uh was that part of the reason why uh the belief desire intention model kind of uh works uh works well for for mapping the active inference ontology to the folk psychology ontology is because it's a discrete model in that it separates these these different these different parts and and i think that also might have motivated the choice to consider mainly discrete pomdps uh and describe the uh decision this decision active inference in that um in that way as well um because then this discrete model can be more easily mapped to say the discrete model of uh the pomdp that's this that's described like a like a factory graph i'm i'm not sure i i was wondering how this would map to continuous tasks as well it well i mean i mean i think the i mean part of the i mean uh you know part of the part of the reason why you need something discreet right is is because actions is discrete right i mean there's either one or there's policy two or this policy right so those those just are sort of necessarily right i mean you get you know continuous state spaces work well or are appropriate um you know at lower levels in a hierarchy you know when you're talking about kind of dynamics and motor movement you know dynamics in the setpoint of some reflex arcore or when you're trying to estimate um something in perception that you know just just as a continuous quantity right like brightness or you know orientation or things like that but but the level of um at the level of decision making um models are necessarily right discreet um i mean i mean there is you know there are there are kind of hybrid models right that are out there in the literature you know like mama's part for example you know has published liberal papers using these sorts of models where the the kind of policy at the higher level which is which is in a discrete model uh generates right some observation that then sets that point right for some continuous or some continuous lower level model that then you know can end up in a in a way that's kind of like this kind of story you know like move the eyes around the cod to different locations and things like that um so um so i i i kind of think that that the discreteness is or the the the reason why um things move to a discrete state space architecture is just because of like the necessary um what's necessary with respect to a model of policy collection um because it is kind of all or none um just to keep the dot zero zero-ish we're gonna move a lot faster through the following slides and there's many important questions um in the chat and and also arising so we're just gonna carry on more rapidly so people can pick up on these key points in the xero and we'll have a lot of time to explore soon section two from predictive coding to active inference traces the uh history and the development from various fields and just one sentence here that that dean and i both highlighted on and then dino let you describe the button was that um crucially for the purposes of this paper the first generation active inference was not a theory of decision making it did not explain how we decide or plan where to move our body it only explained how body movements can be executed using the predictive coding apparatus once a decision has been made so how did you connect that to the bottom right yeah so if you're on a if you're on a some sort of a website and you have to pass through to something my question was based on that statement when the expectation is to know that you've actually crossed a threshold you've moved beyond one page and there's an expectation that there's something that you validated or confirmed or whatever is the feedback that you received does that have how how does the person who's actually expecting that they're on the other side what kind of feedback mechanism do they need to for that confirmation is tactile feedback enough do they need some sort of a visual confirmation as well like what is the what's the it is there because we're talking about thresholds is that different for every single person or is there sort of expectations built in depending upon the kind of situation that you're dealing with um i do know that with with the idea that like changes are saved or links are copied sometimes the click isn't enough to give people confidence that in fact that process is carried through i mean i would i would say that i mean a lot of those questions are really empirical empirical questions as opposed to modeling questions um you know i mean i mean in terms of in terms of how you would model that sort of thing right you just specify what observations count as tactile you specify what observations count as visual and and uh you know what ends up being thresholds first efficient evidence and things like that just just have to do with the way that it processes just how the dynamics naturally you know enroll um in the model under whatever the model parameterization is you know so so um you'll hit a threshold faster if the mapping between observing whatever observations and states is more precise um for example um i mean there's uh you know in relation in relation to that when we're doing policy selection right we also have to check and see whether things are actually going as we expected them to go you know under a choice of policy right so it's possible that you choose policy one and you start to get the subsequent observations and they don't actually match what you expected you know given that you had chosen policy one um you know in which case um you'll update your beliefs through perceptual inference and that might influence how you act going forward um so so but uh the specifics about you know what's enough and whether it's visual or tactile or anything like that i mean those are those are really just empirical questions um that would have to be answered in studies as opposed to something about model choices okay so there was really not a good enough thing when you did the dark room aspect of it like the rhythm that you were prepared to take on right like somebody was prepared to take on the risk because they really urgently wanted the ice cream versus the person who was like i'm i'm agnostic right so that's maybe that's kind of what i was this was on the second reading of your paper right so i was kind of now back filling from further down in the paper so well i'm into that yeah we'll go in the order because we're mentioning things that we haven't brought up yet but let's continue on and we'll return to that and ryan i i also agree that in any specific case it's going to be a model parameterization some of these are very difficult to answer in the abstract model selection setting here the distinction between motor active inference and decision active inference is introduced and i'll allow jakob to just convey one pass what is the difference between motor active inference and decision active inference um so i'm not sure whether i can uh say much that hasn't been said yet but in a [Music] briefly the motor active inference is does well as already mentioned it does not consider design desires in the in the active inference and folk psychological sense uh as um was then connected in the in the paper but uh also it is my understanding that the motor control version of active inference is modeled uh continuously like the models of uh of the movement of individual muscle muscles which does not entail decision making so it's modeled with um in continuous time rather than in discrete time and the decision-making uh active as uh ryan already said as well uh is all about decision making so it describes the discrete process of decision making with uh prior beliefs and preferences and it can i and it can be modeled at different different levels of cognition as well which is one thing that i'm still a bit uncertain about how we can move through these different layers while still keeping the same mathematical formulation uh but i think we'll probably get to that in the other slides yep we're gonna continue with this um distinction of m a i for motor active inference and d a i for the decision active inference here we're showing some key sections from the paper and some citations i think we will continue on without going into this in depth but it describes some of the specific model quantities that are being described in active inference formalism and we're going to come to them when we look at some equations in the coming slides in section 3 preliminary considerations two broad points are uh introduced and we might show um a second one on a later slide um how can we think ryan about what is said here that decision making ai models are largely taken to describe sub-personal non-conscious processes um and also it was asked in the chat not that we have to address it now by david um has active inference produced a single reasonable model of a qualia so when we're thinking about this first broad point that was made here where is experience and what is the distinction between the personal and the sub-personal in dai um well i mean so there's a couple different questions you asked there and i don't necessarily think they're synonymous so you know when you said i think the first question you asked was something about um uh has active inference provided some sort of uh explanation or or model of um of qualia that would be satisfactory um in the strong sense so in the sense of the sense of quality as an explanation for like the hard problem of consciousness then i think active inference is in the same place as anyone else and that no one has a good explanation for for you know how to deal with the hard problem of consciousness right i mean like massive literature out there but um i think everyone thinks it's just as mysterious as ever um so so no right i mean on the other hand um i mean you know myself and others you know have published multiple papers showing how you can successfully use active inference models to um to model conscious access processes right so the distinction between when the brain is representing something unconsciously versus when it's representing something consciously um you know in that it can you know self-report right what it's representing um can use the information to make voluntary choices um and we've shown that those sorts of models are able to um you know reproduce uh like empirical results in like eeg studies and fmri studies and things like that um that an and that also can make novel neurophysiological predictions um some of which seem like they um you know in some subsequent work uh you know have um like empirical support for those predictions um so so in terms of providing a um you know uh what seems like a useful account of the the processes associated with what people self-report that they consciously experience um and uh and the brain basis for that i think active inference has the at least the starting you know the starting um starting point for for being useful and explaining those sorts of things but you know why one little posterior inside a model versus another posterior in a model actually corresponds to like the experience of red versus blue or something i mean no i don't i think active inferences is um not any better than anyone else you know any other model for that question um the um the i guess the second thing you know that you asked had to do with um it's kind of like a related question of you know why it's the case that one level has to do with a conscious process versus unconscious process and um um the best i really got there and you know what what falls out of the models that we've um you know shown really just has to do with temporal scale um you know so there's though there will be a certain level of temporal representations in a hierarchical model that um that integrates enough and represents regularities over a long enough time scale that it can contribute to you know sort of prospective um um it'll be sufficiently compact to generate things like self-reports right so i mean think about how complicated and temporally extended um reporting and choosing to report something like you know i see something green with me right i mean it's a very complicated temporally deep um you know type of policy to select and requires integrating a bunch of information from stuff at lower levels that's happening over faster time scales um so at a minimum right you just need to be at a level of representation where the regularities are sufficiently uh sufficiently long and um especially temporarily deep and then have access to all the relevant information you need to integrate to be able to generate um reports like that um whereas whereas much of the other kind of little pieces in the model right like uh you know the generated um say like expected or for like variational for energy gradients and things like that that you know people might call like surprise right i mean no one's claiming that and often they will not relate in any way to people's conscious reports about feeling surprised because conscious reports about feeling surprised will have to do with representations of surprises in states right as opposed to um you know some sort of prediction error like uh you know gradient minimization process thanks um also in section three there are several clarifications um and some seeds that are planted uh namely if computational and folks psychological predictions converge and no other available theory can account for behavior equally well this could entail the mathematical structure of dai as more than a convenient tool instead that it corresponds to the true information processing structure underlying and enabling folk psychology and related abilities and this is really nicely put here that the crucial point remains that one should not conflate mathematical and psychological levels of description however dai models might nonetheless offer more detailed information about the true form of folk psychological categories and processes and then they add that their aims are to demonstrate that there's a clear isomorphism between the elements of dai models and those of the bdi model and then show how provided one does not assume probability distributions in computational models must be identified with beliefs at the psychological level there will be no tension between dai and bdi and that comparison or juxtaposition is clarified by separating mai from dai like has been mentioned anything else before we start to jump into some of the formalisms and keeping all of this in mind i guess i um i had one question on um on the on which um ontology we're working with when we say there's a isomorphism between the elements of dai models and those of the bdi model um because i think there are multiple ways to interpret this uh one one thing that i'm wondering about is whether this means that we can basically construct a mathematical formulation of the of the bdi model that is like a subset of active inference or maybe um maybe there are certain certain elements of the bdi model that aren't necessarily encapsulated within actin that weren't discussed in this paper and what that would um mean in terms of formulating this mathematical mathematical formulation of of the bdi model so i mean what the isomorphism just means is tell me the description at one level and i'll be able to translate it for you very quickly and directly into whatever the description is at the other level in both directions right i mean so it just means you know tell me tell me what you know tell me what it is that you want you know if i say do you want ice cream or do you want a pizza right tell me tell me that you like if you tell me that you like you want pizza twice as much as you like ice cream then it's very easy to just put you know a four you know in the you know in the in the distribution you know in the preference distribution over the observation of pizza and the two for ice cream and then softmax it and log it right and then like you know there's that's the description right so just in a very direct way just encodes relative desires for one thing versus another you know tell me what your beliefs are right i mean do you believe that pizza is to the left or to the right i can put that in as the different categories of hidden states that you're getting for a distribution over right so like it's just the point is that in either direction right give me the full psychological description i can translate it very straightforwardly um using the same elements every time as being desires or beliefs um you know give it to me at the uh at the mathematical level i can translate that into a description in terms of beliefs and desires right i mean it's just it's just one-to-one um i can't think of an example of hand um where um there would be any kind of like mismatch or an ability for one to um account for the other but um you know it's kind of hard to prove a negative right so i mean maybe you can give me an example but i can't think of one awesome jacob yeah i was just gonna say that i can't really think of an example uh right now um i i just found i found it really interesting with the usage of uh of just the word isomorphism because that like as you as you said it's just a bi-directional subjective map so it's basically equivalent it's just set in a different way so i can imagine that uh meaning that we can basically instead of the wikipedia page for the bdi model uh using all of these terms uh um like believe desire intention it could just be a set of active inference equations and that's the bdi model because it's an isomorphism right literally your your beliefs are just your cues right and your uh and your desires are just your uh you know pfos right i mean and i mean in the you know the way that we've uh you know we and others have tended to write it now right is to literally just explicitly to say like that desires are um or the preference distribution is p of o given c right where c is just a matrix that defines your preferences right so i mean it's a lot clearer to do that um than to just just leave it just p of o right i think that's part of the confusion is that uh it's not clear where the preferences come from but they're just it's just because their condition on this other thing c and c can be fixed or learnt but that's the basis of the calculation there yeah here we get into section four variational and expected free energy considerations and in large script we see the variational free energy equation dean what is the skydiver doing and during the one and the two we'll unpack more about the exact terms and and what is um coming into and coming out of this equation but just as an appetizer here what's the skydiver doing well now i've got the author here i i just put through that in there because i like that i don't think it's a metaphor but the analogy of of how so how do you respond when you jump out of the airplane and you know your parachute's not going to work do you uh do you go head first and really attack it or like what now what are you doing in terms of what your expectations are so maybe maybe that that that was a good analogy ryan maybe what like so where were you where were you going with that because yeah well i mean i think the point the point of those sorts of examples are just uh just to um sorry to put this to to show why it's not um it's not really correct to think about the um like prior uh you know this prior preference distribution even though it's you know it's again it's cast as a prior belief right um it's not really appropriate to think about that as a as a um a belief per say right i mean the um or an expectation at any at a psychological level of description um so you can and the way to contrast that right is you could say look like the person in a model where they don't want right to fall to the ground and die right that would be your your yeah that would be your pia though right it's your preference not to have the observation of hitting the ground and dying but what you act right is is your q of o given pi right it's what do i expect to observe given that i choose this policy versus that policy right so that's the that's the you know the leafy part right it's what i expect given that i do this or that and you know how close is that to how much does that diverge from what i what i want right which would be your pfl um so it's just it's just a clear example of how your you know your belief the expectations and your desire the expectations come apart and that and it just doesn't work conceptually i think one is think of one as like the same kind of uh uh kind of thing as the other yeah i i just thought as if i'm not going to get a soft landing how big of a crater can i can i create well that's that then is going to be packed into your preference distribution right right you would have some desire for you know to make a big you know conditional on you're going to die right like like you know you'd prefer right to make a big crater than a small crater and that would still just go in your pfo right so yeah yeah um jakub um i was just wondering um and this might be me uh thinking thinking or not too deeply about the equation uh rather but in this in this formulation where it's a variational free energy for each policy f of pi um does that does that imply when in the last sentence on this slide that uh free energy is a function of beliefs and a function of observations that doesn't mean that the free energy is essentially a function of three variables uh but in this case we are only writing it f of pi because uh it's always imp it's it's clear that it's a functional of beliefs and observations um or should we think of it more as just a function of each policy um i mean the way it's written here i mean i think that the most preferred way to think about it is i mean it literally just means that um using this equation you will calculate a different f value for each policy right i mean that's really it like you just there will be you're going to be computing some some variational for energy value for each policy and that's going to make up a distribution over policies um and um and uh i mean the the the tricky thing i guess here is is that um you know this isn't about this isn't you know f of f of pi here isn't about making decisions right this is just about you know something like evaluating evidence for um you know for various policies one because you you can't you calculate f when you already have the observation right so you can't really do this perspective decision making thing with f um so this is just basically a way of describing how um how under each policy your beliefs will change uh or wouldn't change right under when you know when you get conditional in each new observation that you get um and um and so it's not uh yeah so all it denotes really is is just you know you have a value of f for each policy and that makes it a distribution and that takes us to g expected free energy the letter after f and as shown here not sure if there's another reason for why it's g um this is equation 2.6 from the active inference textbook just for reference with some other versions of how g is framed g is a prospective value and they write decision making does not only require beliefs about past and present states it also requires making predictions about future states and future observations this requires taking the average i.e expected free energy g of pi given anticipated outcomes under each policy that one might choose and there's um a lot to say on expected free energy what is just one short note that somebody could add or ryan why was it placed here um i'm not sure i understand the question like why what why is why is expected free energy required why is variational free energy not enough i mean expected free energy is just is just what the system's trying to minimize when it's selecting a policy right so i mean it's just trying to i mean in this i mean this decomposition right here is often called like the risk risk with ambiguity decomposition but um i guess uh i like i think that i think that showing the um you know the risk term the risk term here provides a nice kind of intuition for um for again the separation between beliefs and desires and active inference because um you know you're you're uh so the the term underlined in in red there right so the kale divergence between q o given pi and p of l right i mean that's just saying that um you know how different are the observations i expect given that i'd use a given policy and you know how different how how different is that how different do i expect that to be from my uh my preferences are my desires right pfl um so the the closer those are together right the more uh the more similar i expect the observations to be under a policy to my preferred observations the smaller that's going to be and so if i'm trying to select the policy that minimizes uh g right then that means i'm going to select the policy that's going to get me as close that's going to get me as close to my preferred observations as possible right so literally just is that kl divergence literally just is trying to choose the policy that brings beliefs closest to desires very nice and then what does the blue term mean uh that's just an entropy term right so i mean it's just basically saying that the is what drives information seeking or part of what drives information seeking it's just uh it's just yeah basically the agents choose you know driven to choose policies that also are going to generate more precise expected observations observations that are going to disambiguate states more more easily great um here we'll explore it more in the dot one but there's a a description of the posterior probability distribution over policies p of pi and an expression involving the soft max symbol with the sigma we'll return to that later but this is just so that we can cover like the key experiments and the simulation results today this is also one other area of formalism we can explore in the coming discussions about precision and the relationship of beta and gamma and what the convergence towards means but we won't go there there is precision parameter in this model let's go to the dark room problem and many lines of pdf screen and ink has been written on the dark room in the active world since um perhaps first in at all 2012 and also outside of active and um they write in a nutshell the concern is that if agents only act to minimize prediction error as opposed to acting under the impetus of a cognitive desire-like state then they ought to simply seek out very stable predictable environments such as a dark room and stay there would anybody like to add anything about this dark room problem or what is the relationship between doxastic and cognitive ontology and active i mean are you just asking what like what those words mean or what like how do you how do you use these words i mean i mean detoxastic is just a term that refers to you know the leafy you know like epistemic sorts of things right so so a doxastic ontology would be an ontology that includes beliefy things right whereas whereas uh you know conative just refers to yeah being like targeted towards something right like something about you know desires or um um you know something things you want versus don't want things you like versus don't like right so um it's just uh you know when we say it's apparently purely a duck tastic ontologist just saying it might look as though superficially like the um the ontology proposed an act of inference like the things that exist in active inference are purely beliefy yeah that's all that means great and it's applied in the setting of this dark room um for the following two slides dean help us understand what concerns are addressed and what the child on the top right of the slide is doing yeah well so i'm not going to go through all all of the text but essentially this was my introduction to the darkroom problem because it was kind of the opposite of all my experience which is that i tend to find people that were curious and we're trying to move away from information um leveling offers or being static to information gain so first of all i had to learn what what the what the argument was about why people would actually in it who are normally social why they would move away from that social realm and into this sort of dark space under the con under the idea that active inference actually implies that's what people would do um and then secondly it was i was kind of trying to tie it into the idea of the refrigerator and where it is and and until you actually and then danny you and i talked about this a bit the pragmatic piece of until you actually open the refrigerator door which is on the kind of the next slide you don't really i mean you can have a belief that there's something of value in there but it can turn out that when you open the door or monty raises the curtain the thing that you were expecting uh turns out to be a value but not necessarily of that thing that you were you were setting yourself up to believe and so that that young individual spitting the coin out um maybe i guess i was just trying to be a little bit facetious there that it holds value but it doesn't hold the same kind of um tasty value of say having a lick of an ice cream cone so again it was it was trying to move away from the idea that the beliefy stuff alone is satisf satisfactory or necessary i i always get those two mixed up but i think i i was just trying to reinforce the idea that you kind of need to have both so that you can compare and contrast so that you get a sense of what a person or what an agent is doing as they're getting past some of those shrouds or some of those um non-observables into the observable state thanks we'll keep going um this is unpacking some of the concerns about the apparent pure doxastic belief-oriented ontology of actin so ryan described earlier like is it about beliefs colliding and if so if it's purely doxastic then where is this um desire and um [Music] is there anything else that anybody wants to add here i would maybe um comment that i think this also relates to uh your um to the earlier thing that was discussed how this confusion might stem from people um misinterpreting or um mis uh confusing the terms predictive processing and active inference um and i feel like this addresses it as well where if we just if it's only beliefs uh then there is obviously a concern that once we're in a dark room we uh can't because uh if we're only in the regime of predictive processing there is no way for us to move to a different state or there isn't the formalism that that describes planning and action um but as i guess we'll get to in the other slides when we do include this planning we suddenly have desire like terms appearing within the mathematical formulation as well great yeah i want to be clear it's not like they're thrown in at hawk right they emerge naturally as like necessary components of any kind of you know like using this sort of framework or building it out to involve planning so it's not a like that they emerge naturally and necessarily it's not a it's not because someone's just kind of tacking them on great um and this is referring to that minimization of g the minimization of g drives the agent to seek out observations that will reduce uncertainty about the best way to subsequently bring about phenotype congruent or preferred outcomes so that's the information seeking another way to put this is that under dai an agent doesn't simply seek to minimize prediction error with respect to its current sensory input it seeks instead to minimize prediction error with respect to its global beliefs about the environment which entails seeking out observations that are expected to generate prediction errors such that uncertainty is minimized for the generative model as a whole and another aspect of this type of global prediction error minimization process is that it pertains not only to beliefs about states in the present but also to beliefs about the past and in the future and so this is leading to one of the claims which is that in addition to desires dai captures folk psychological experiences associated with the drive to both know about one's current states and learn what will happen when choosing to move to other states among other parameters in a generative model let's go into this generative model and come to the dark room simulation itself um and we'll unpack it more in future times but the prior over policies e of pi they write that um when the agent repeatedly chooses a policy this term increases the probability that the agent will continue to select that policy in the future at the level of the formalism this corresponds to an agent coming to expect that it will choose a policy simply because it has chosen that policy many times in the past this can be thought of as a type of habitization process but it doesn't have any direct connection to preferred outcomes because e of pi is not informed by other beliefs in the agent's model so here's the partially observable markov decision process just one representation of it and e is sort of floating there influencing but not being influenced by and we had some interesting discussions which we can return to another time about how we might be able to assess whether somebody is engaging in a behavior because they simply have habitually engaged in that behavior but they understand the actual mapping of how policies um relate to outcomes or in another case where somebody might be engaging in a behavior not because of habitualization but rather because they have um some sort of deviation from the appropriate mapping between policies and outcomes and maybe there's other ways so i thought that was very interesting how even within the same sort of sparse and first principles model similar behavioral outcomes might be observed on like a behavioral manifold and then different kinds of perturbations or evaluations might reduce uncertainty about what is giving rise to those conversion behaviors in in that case um okay dean with the ice cream truck and the eye no i'm gonna i'm gonna wait because i think that's i think this one and that and the next one are better to take up in a one two part because because if if ryan can come back there's some good questions i want to ask him about those two sections because this slide and the next one are kind of anti uh dark room and so i want to kind of pick his brain about what you know how we can sort of convince people that that that dark room problem is for such a small subset of the population that majority of the stuff that we're talking about here is done in in sort of social settings and social context and there's a third party sometimes it can observe an observer and what role that plays in trying to figure out what's what's going on here as we move from the from the idea of decisioning and and so forth so i'll wait great so here's part one of that and here's part two looking at uh social learning theory a little bit but let's go to the figure and to the simulation uh this is figure one simulation of an active inference agent deciding whether to eat some ice cream and um there's various ways to go about describing it but one of the key pieces that anyone can give a remark about what is occurring here is that there's three cases that are being compared there's a no desire case a weak desire reflected by this p of o and a strong desire case where the ratio between the two alternatives between um wanting the ice cream and or between the ice cream state and the no ice cream state is a sharper distinction in this second row so there's a case in which ice cream is in the fridge but where the kitchen is currently dark and so the agent doesn't know whether the fridge is to the left or right so it's like a tea maze setup with three possible cases one where there's no preference for observing ice cream or not a weak or a strong preference for that and then under these situations we can ask about what happens in terms of the action as well as the um beliefs and valence updates what else would you add about that ryan well i mean i think the point you know the point of this is just to show well i guess makes a couple you know i mean so dean mentioned that um you know the issue with the dark room right i mean as he said i mean as you said i mean this is essentially just a a particular very similar to to like a team is right i mean just putting a putting a different sort of semantics on top of it but the you know but the idea is just that um it's just set up to show that you know even without any kind of desire right just just resolving uncertainty um the agent will be driven to leave a dark room or do something to get rid of the dark room um so you don't need a desire right but but in fact any realistic agent is also going to have desires so um so in the case where there are desires then there's two different reasons why um no active inference agent will ever stay in a dark room or be motivated to stand a dark room like in all cases it will have a motivation to to leave so i mean the point is just to show right in some explicit simulations that um generically right like the this kind of dark room problem um just will it will just never apply um to active inference um again the stark room thing only applies if you assume the thing is just doing something like predictive coding but then somehow also assume that it's making decisions um but that's not active inference um because it's not a theory of decision making awesome um here is some description about the strength of the desire could have been shown before the figure but we wanted to introduce the the simulation setting and as described there is um a desire as well as an information seeking and i believe that's the two reasons why the agent will escape the dark room and there's there's other escape hatches depending on how one constructs their model um potentially like a hierarchical model might have a slightly different um explanation for why an agent does or doesn't stay in the room but even just within this single level model there's an information seeking as well as a preference realizing reason um any other comments here okay dean just a quick thing baby in the island i'll put you in the island go for it yeah uh so maybe ryan just just a just a quick this last last couple of senses however preference distributions are set to zero oh i'm sorry i'm not still on the previous slide daniel sorry can you go back one yeah go for it okay so however if preference distributions are set to zero as in our no desire simulation such that no outcome is desired over any other net and active inferences will nonetheless be driven to choose behaviors that will maximize information gain okay that all makes sense that's just the background this is the part i was curious about while this might reasonably be considered a motivational influence it is basically less plausible that it should be considered cognitive and may therefore be better seen as a type of doctastic drive here the formalism may therefore help us to recover and potentially nuance the folk psychological distinction between desire and curiosity these types of drives seem to differ fundamentally i agree with you but i was wondering if you could maybe explain why that should be something that we should keep separate the idea of desire and curiosity i know you go into explaining it a little more into the paper but why did you want to point that out to readers well i mean i think there's a couple things i mean i mean one i think that you know these things are both driving you know decisions about what to do right so in some way they motivate they both motivate behavior right but but at the same time you know and i think uh this is something you know that alex keeper and you know pointed out when we were writing this as opposed to just you know so it's not just me um but you know pointed out that um you know there is this kind of um fairly clear difference um between the um this uh you know curiosity or epistemically driven um sort of behavior um from the kind of desire-driven behavior and the the the kind of clear differences is the vm this epistemically driven motivation isn't um it isn't driven toward anything right it doesn't have a target it's not trying to get one thing versus another um all it's trying to do is just um you know just become clearer about what's out there um essentially right so so the fact that it lacks a target um is really i think what makes the what makes the distinction clear um but the i mean beyond that i think motivation was just a was just to show that um you know like just a you know generically right like the belief desire intention principle doesn't really um doesn't really say anything about uh you know types of types of desires there's nothing really explicitly just in that very very simple um way of um describing the framework that that separates out uh information seeking from from rewards to gang right um so you know so you could right if all you're doing is trying to you know compare active inference to um do you know the bdi model very simply stated then um then this is able to kind of nuance a little more right like different different things that that um might look like you know promotive motivational kinds of things [Music] but there are separated those um on you know why decisions are made for one thing versus another more kind of tied to beliefs in a different way um you know that being said i mean at the same time i think that um you know like full psychology more more broadly i think i think uh very intuitively and naturally um does you know recognize the difference between things like curiosity right and things like and things like reward-driven behavior right those are concepts that we have just in our natural kind of folk psychology um you know so active inference just captures those right i mean i um yeah i talked about this um just as an example of you know how intuitive i think and like natural the sort of aspect is you know and i just like think of like most of the behaviors that say like my little dog does um you know if i look at if i look at what my dog does 90 of the time it's way more information seeking than it is reward seeking you know she searches out for some food a couple times a day but i take her in the car she's pulling left and right to see what's out the window and how that changed all the time you know any little sound perks up looks right none of those things are reward driven they're all just information seeking um so i mean very very clearly that's a big part of you know what drives us and other animals to do um so so the point was just to just to kind of show that active inference captures that in a certain sense it adds some granularity to um to at least what the bdi model says but but in that case the bdi model is kind of too uh too coarse grained and i think normal hulk psychology doesn't include those things already great um also to be returned to dean yeah because ryan just basically answered it all these two images are basically doing is separating out the sort of goal directed from the curiosity which is you know the the image on the right is basically a documentary about the franklin expedition where they were going out to the event horizon but they never returned so did they have a goal probably but was that their curiosity being carried out i think we i think we've got to be able to make sure that we appreciate both so that's all i wanted this makes me think like um are you driven to watch the sunset over the horizon or are you driven to find out what's at the end of the rainbow and what's over the horizon and that's the sort of like infinite open-ended curiosity drive beyond the horizon versus the preferred specific state that one can desire in terms of their um observations and reduce the divergence there but cool um wishful thinking we'll come back to we're just leaving notes so that we can um have more to discuss later because these are all like there's such vital threads um ryan and maxwell and alex because they um touch to our day-to-day experience in a way that few other um frameworks and and even papers within active do so affect and the role of affect and precision curiosity these are all just like terms that touch humans um one thing that i you know that i think is probably worth just touching animating is something maybe you guys would want to talk about in the future you know maybe when i'm not around the other any other sections not sure but um you know uh so there is a probably a distinction that's worth keeping in mind between uh curiosity per se and um and goal-directed information seeking um i think some of the you know examples you guys have mentioned have been more kind of along the lines of one versus the other so i think it might be good to just just keep that distinction clear that um you know in in vanilla active inference um it is really something more just kind of like curiosity just kind of like independently driven just kind of you know look where you're going to get the most information um whereas you know the construct of directed exploration um you know and reinforcement learning and um and what also you know often something that can emerge i think a little more clearly and like sophisticated active inference um is um you know as information seeking specifically or you know in the service of knowing you know how to get your goal um you know the the the curiosity version that we're showing here it has the effect of helping the agent get what it wants but but the drive to seek out information isn't actually itself due to the fact that the agent thinks it will help it get its goal you know it's just driven to seek the information independently whereas um whereas uh in in other yeah like in sophisticated inference um the agent is actually doing something more like i'm going to look over here because i think looking over here is actually going to help me get to what i want better right so this is kind of strategic information seeking and that's that's a little different than just like intrinsic curiosity so um just something to keep clear that's quite interesting it kind of ties a braid back but let's explore that later um here we have a nice uh clean representation of the summary of the main argument their proposed solution is somewhat deflationary in the sense that it simply argues that the functional role of desire not the experience of desire is straightforward straightforwardly present in the dai formalism and then in more detail they argue that and they provide four points which it'll be great to go over with with the authors based on those considerations the apparent problem posed by purely doxastic looking constructs is simply not a problem there are beliefs and desires in the active inference framework um perhaps we could have uh explored this more with the formalism so we'll bring it up to uh talking more about f but won't go into it now we'll return to the letters later and then the appendix is uh very informative there's a description of all the states and the factors that are used in the simulation and in the model stream one which is by the way ryan it's um our our most popular series was model stream one it was it was uh it was a fan favorite but um it's some similar concepts and matlab scripts so the appendix describes how the figure one results were generated and maybe we'll see if anybody can run that and execute it we can play with a few different things and take some of these qualitative linguistics and even mutate the simulation a little bit see where that takes us we'll close out with our usual closing slide um so who would like to take the first last word yes dean then yeah then ryan first of all thanks for uh shepherding us these three cats through this uh paper because there's an awful lot of stuff to try to cover in a pretty short period of time and it was and it's easy i think to sort of go i don't want to say tangentially but go deeper into some of the parts of the paper because um uh for me it was one of those ones where you i would go back and and have to reflect on something after i read a section and then try to fit it into the larger picture that was ryan i think the paper did a very good job of sort of ordering through what you were trying to say in terms of the reassurance of how the the quantitative uh and the the modeling of the psychosocial could be seen as working with each other as opposed to though this doesn't answer what this other thing questions but it does cause this is this was a paper that for me i won't speak for the others but i had to read the section and then try to plug it back into the overall narrative that was being told and again i think part of that is because the the translation from the stuff that's typically seen as the quantitative part is hard to move into the qualitative part i think that's that's a challenge for anybody that tries to move back and forth between those two things but uh yeah i mean in terms of in terms of making a case and and providing what i think people need to sort of see the two in the same light thank you appreciate it sure i'm happy i'm happy if it's helpful yeah i think um [Music] i guess there are two points that i like to touch on firstly i think i thought it was very helpful to map these folks these concepts that everyone even though everyone uh i guess that's the nature of psychology that everyone will have a slightly slightly different um probably distribution on of what they these terms actually mean but i think it's in just learning about active inference it's helpful to link the math mathematical formulation and even just the active inference ontology which itself can be very cumbersome at times to these very um intuitive concepts and i think as we get we're starting to explore modeling within the actin flap as well i think it's it will be really helpful to use this isomorphism between active inference and folk psychology to explain the the behavior of agents within these models and not beyond a dark room problem when there is some unexpected behavior of an agent we can directly look at okay how how did um this tensor change its values and describe it with this uh psycho psychological ontology so i i think personally that's uh the most exciting thing about this paper for me um [Music] yeah well here i mean if that's i mean if that's the kind of thing that you're interested in then i mean that's kind of the whole um it's kind of the whole motivation of you know for instance like computational psychiatry right which is like the area that i currently work in right you you take for instance clinical populations where people may behave in unexpected or ways or ways that don't necessarily seem like they're all that adaptive right then you can just fit these models to their behavior and you can figure out okay well what is making their behavior abnormal right is it is it something about an overly precise preference distribution or is it something about um you know the belief that states uh states transition to uh to um into volatile or uncertain away or you know things like that so i mean it is the yeah i mean the kind of uh you know the major point is you can use these novels in empirical studies to figure out what the mechanisms are that are leading to healthy and unhealthy behavior um and that can give you kind of guiding information toward you know designing better treatments or you know trying to measure these things in a more quantitative way things like that and um and the uh you know and just as i've said i mean this is all um using using tasks that uh you know that involve some kind of goal right like seeking some kind of reward or social approval or you know whatever it is that you know humans seek out you could do that right unless you have a straightforward way to map um the formalism each element of the formalism to you know the reason that we think people behave in the way that they do right so so on so i guess i'm just saying if if that's an interest of yours then i would think that you would um you'd probably be a fan of a lot of the a lot of the computational psychiatry literature more broadly both you know both the active inference part which is a lot of what my lab does but but also just the broader um computational psychiatric community that uses reinforcement learning and drift fusion models and all the other from you know all the other classics that are out there cool and it opens it up to what organizations want and what do cells want and all these other um transpositions well ryan really appreciate that last minute um belief or desire combination thereof to join us it certainly helped resolve our uncertainty a lot and it's a great conversation in the coming two weeks we're gonna be with hopefully some more authors and more lab participants and just i'm looking forward to taking some notes thank you see you all soon and bye you