hello everyone thank you very much for joining this is the active inference lab and you are listening to the active inference live stream welcome to the active inference lab everyone we are an experiment in online team communication learning and practice related to active inference you can find us on our website twitter email youtube discord or key base team this is a recorded and an archived live stream so please provide us with feedback so that we can improve on our work all backgrounds and perspectives are welcome here and we will follow good conversational live stream etiquette today we're really excited to be discussing the neural correlates of consciousness under the free energy principle with one of the two authors juan juan jolisa and um today we're going to be having our first discussion 16.1 and then next week we'll be having a follow-up discussion on the same paper so if you have more thoughts or questions arising in the next week that will be a great time to bring them up today in act stream 16.1 our goal is to be learning discussing and hearing from everyone and that is going to be facilitated by this awesome presentation that we are about to hear from wanja so for 16.1 we're gonna have a presentation um and then we're gonna just pick up with the participatory discussion so laundry thanks again for joining us and please take it away thank you for having me it's a real pleasure to have the opportunity to present this paper here and discuss it with you i'm just gonna share my slides okay i think you can see the looks great um first i'd like to make a short um advertisement for my journal so together with sasha fing and jennifer wind i founded philosophy in the mind sciences an open access journal for work um at the intersection of philosophy neuroscience and psychology and we recently published a special issue on the neural correlates of consciousness which you can find at philosophymindscience.org so if you're interested in this topic you might want to check this special issue out now in this short presentation i'm going to mention just some some key points from the preprint and this is the structure of the paper these are just section titles after some more sections after the introduction and aren't we revising the paper so we received some very useful reviewer comments and actually one of the rarities pointed out that if you're not already committed to or interested in the free energy principle the results that we present in this paper may not be that interesting or not may not see that seeing that relevant and in fact and in part this is i think a fair comment because in the first section after the introduction we review the standard notion of a neural correlate of consciousness which has been provided by david chalmers in a seminal paper in 2000 and we then mentioned some challenges for this concept and this motivates moving from the neural corners of consciousness to the computational correlates and um actually then in the next discussion we also relate this to the free energy principle and then it turns out that there are some problems if you want to apply the free energy principle to this kind of research because the main reason is that the free energy principle is not itself a theory of consciousness and um it's it's not a theory of computational correlation of consciousness so um there are first some obstacles that have to be overcome if you want to apply the free energy principle to research on consciousness or on neural and computational correlates of consciousness in particular and if you're not already committed to the free energy principle or not not really interested in it um you may wonder well why should i even bother thinking about this if it only creates new problems that you don't get into if you're not dealing with the free energy principle in the first place so um this is i think a fair question and um we we're trying to highlight the um genuine contributions that we're making or that we're trying to make in this paper and the revisions and these mainly relate to the final two sections before the conclusion in which we talk about computational explanations of consciousness in general and what the free energy principle can provide or can contribute to this this project and um then we try to try to explain what the specific role of the free energy principle can be here and we suggest that it may provide unification so i will um take a few minutes to talk about these things in a bit more detail um let's see this is a more detailed um overview of the the paper so in the section in which we talk about general currents of consciousness we mention that according to the standard definition a neural correlate of consciousness is a minimally sufficient neural structure and then there are three challenges that we mentioned one is that actually what is interesting about the neural activity associated with consciousness may not be that it happens in a particular place in the brain but more than it's a that it displaces a certain dynamics and this can even be a global dynamics so um it's it's not what's happening at the place but what's happening over time and the or maybe in the entire brain or in large parts at least of the brain then the question another question is even if you can map certain types of neural activity to conscious experiences this mapping can be seemingly arbitrary so we need to understand why or would like to understand why that particular particular type of activity is associated with consciousness and then a third challenge is that the notion of a neural correlate of consciousness as defined by chalmers and as used in much research on ncc's only applies to usual conditions and does not apply to unusual conditions so after brain damage for instance and another very unusual condition is provided by potential cases of islands of awareness so this is from a recent paper by timbane and yosef and marcelo masimini in which they ask the question whether there can be systems that are causally isolated from the body and on the environment and still be conscious so um can there be conscious states in systems that neither receive sensory input nor produce motor output and there are some potential candidates for such systems which include ex-cranial brains hemospherotomy and cerebral organoids so an ex-cranial brain is a brain that has been extracted from the cranium but is kept alive and which can still display some non-trivial activity [Music] and in hemispherotomy a large part of the brain is more or less disconnected from from other parts or the rest of the brain so neuronal connections to other parts of the brain are cut and in well organoids such as neural structures grown in the lab and in all cases one may wonder well could such structures such systems give rise to activity patterns that are sufficient for consciousness and what we suggest is that computational correlates could provide at least some progress towards an answer because computational correlates can be applied to different kinds of system and um and in fact the authors of this paper mention complexity measures so um there's some evidence that neural activity associated with consciousness displays a large or high complexity high algorithmic complexity and in principle one could measure the complexity of activity generated in these these isolated systems and that could provide some evidence for inferring the presence of consciousness in such systems now um actually um complexity is also an issue when it comes to applying the free energy principle um i will talk about that in a minute um and one of the things we do in the next section section is that we relate the free energy principle to work on computational correlation of consciousness and computational principles that um were or that have been associated with consciousness and um we in in particular we also discussed the challenge of islands of awareness and this is not just a challenge for neural corals of consciousness but also especially a challenge for the free energy principle one of the problems that is created by trying to apply the free energy principle and the reason is this the free energy principle is mainly applicable to systems that are in causal interaction with their environment so systems that are part of an action perception loop in which internal states interact with external states mediated by blanket states which contain sensory and active states but if you have an isolated system with which is cut off from sensory input and motor output it's not clear that you can apply the free energy principle to describe what's going on in the system but actually there's there's some um existing work on the free energy principle and dreaming by alan hobson and carl fristen and in that work they suggest that well um i just go a bit forward um you can as is well known i guess you can um you you can formulate or describe the free energy functional as a as involving a accuracy and a complexity term so you can write variation of free energy as complexity minus accuracy or complexity plus inaccuracy and as it turns out the accuracy term contains um a term denoting particular states which involve blanket states and the complexity term does not so the complexity term can be minimized by changing internal states and so in previous work it was suggested that during dreaming the causal interaction with the environment is attenuated and so the this free energy gradient is mainly driven by the complexity term so actually so basically what the brain's doing during dreaming is minimizing complexity and we suggest that the same holds our foturi for cases of islands of awareness so such systems should at least for some time minimize free energy by minimizing complexity and now this brings up complexity again and the question how this relates to empirical research according to which neural activity associated with consciousness maximizes complexity or at least displays um high complexity and um this um well tension can be resolved by by noting that the um free energy functional contains a statistical form of complexity and complexity measures and consciousness signs are measures of dynamical complexity that are often based on an approximation to algorithmic complexity so they're actually two different notions of complexity and play here and according to the free energy principle islands of awareness should minimize one type of complexity but it leaves open the question whether they maximize a different type of complexity okay and we suggest that it would be useful to formulate or develop a notion of dynamical complexity within the framework provided by the free energy principle so it would be useful to define dynamic complexity or measure of dynamical complexity in terms of the intrinsic information geometry of a system as opposed to a definition in terms of the extrinsic information geometry um but this is actually not something i want to dwell on here so um let's go to the next point um computational explanations of consciousness um we well that's a fundamental problem associated with trying to develop a computational explanation of consciousness because computations can be performed by vastly different types of system so um you can perform computations in the game of life and you you can compute mathematical functions by playing magic the gathering so the game of life is during complete and as i recently learned magic the gathering is turing complete as well but i wouldn't suppose that anything that happens in the game of life gives rise to consciousness or that you can instantiate consciousness by playing magic the gathering in a particular way so um just computing the right mathematical functions in a physical system is not sufficient for consciousness or in other words there's a difference between simulating consciousness in a system and actually instantiating consciousness and the question is okay how can we make this difference of how can we understand the difference between simulating and instantiating consciousness and here um we think that the free energy principle can make a perhaps tiny but we think relevant contribution and the main idea is this as i already mentioned um the this is the causal flow in an action perception loop um according to the free energy principle so here we have a physical system with internal states um and it's coupled to external states via blanket states which comprise sensory and active states and so the main causal flow we have here is in in this physical system is between internal states and external states and they so internal states act on external states via active states and external states act on internal states via sensory states okay now what happens if we simulate this such a system in a digital computer now this is a caricature or a simple visual model of a computer with a von neumann architecture and one of the one of the characteristic features of such a computing device is that it has a distinct memory unit which is separate from the cpu of the central processing unit so if you simulate a system with this causal flow in such a system you'd have to store the values of these variables of external internal and blanket states within the memory unit and in order to compute updates of these variables you have to engage the cpu so the the main causal flow would be between the memory unit and the cpu of course there are also variants of this architecture in which the cpu contains a memory unit but then the main causal flow would be within the cpu and also between the memory unit within the cpu and the rest of the cpu so it's a different kind of causal flow in this physical system although the computations performed by this by these systems can be the same in the sense that um in internal states can be interpreted as conditional probabilities of external states given blanket states okay um so this is we suggest one one of the differences between simulating and instantiating um consciousness that there's a constraint on the causal interaction or causal flow between the system or the system's internal states and its environment and of course this may be different in a computer with a neuromorphic architecture okay then in the final section we relate this these ideas to our previous work on minimal unifying models of consciousness and so just explain this very briefly on in general what can computational models bring to bring to or how can they facilitate an understanding of consciousness so computational models may serve as a bridge between phenomenal properties of experience and properties of neural activity they may operationalize theories of consciousness they may explain theological functions that is cognitive capacities associated with consciousness and um so these are things that for instance active influence models could provide and the free energy principle together with such models can then serve as a unifying framework and how can it be unifying so here's just one possibility um i'll start with what jonathan burge calls the facilitation hypothesis in a recent paper so they're here right phenomenally conscious perception of the stimulus facilitates relative to unconscious perception a cluster of cognitive abilities in relation to that stimulus so this is just a hypothesis which can be empirically investigated you can for instance look at different non-human animals and see whether non-human animals that have one of these cognitive abilities in that cluster also have the other cognitive abilities and whether animals that do not have one of these abilities also lack the other cognitive abilities and so this would then corroborate the hypothesis that conscious that is that there's some mechanism that facilitates or enables all of these or an entire cluster of cognitive abilities and this could be what consciousness does um in addition to that one could also find theoretic theoretical supportive evidence if one builds minimal models of these cognitive abilities and one could show that a minimal model of one cognitive ability in that cluster is also a minimum model of other abilities or whether or if one could show that there's a common computational mechanism that accounts for all of these cognitive abilities and a hypothetical example just to illustrate would be active inference models um if active inference models of such a cluster of abilities or required deep temporal processing or maybe deep counterfactual processing or something like that so if one would have to posit a certain generative model with a certain architecture in order to account for these different abilities and that that could be something like a common computational mechanism which then could further corroborate the hypothesis that there's something that that facilitates these that jointly facilitates these cognitive abilities together um okay and this this could then lead to something like a minimal unifying model i introduced this term in a paper and neuroscience of consciousness a minimal unifying model specifies only necessary conditions for consciousness it involves determinable descriptions that can be made more specific and unify existing accounts of consciousness and i would now add that it can also unify cognitive abilities associated with consciousness not just accounts of consciousness not just theories of consciousness okay but we can talk about this in the discussion and the a general picture would be that there are these different aspects of the signs of consciousness there are theories of consciousness potential teleological functions of consciousness that is cognitive capacities associated with consciousness and there are mathematical functions that are computed by conscious systems there's the notion of complexity here and then neural corners of consciousness and perhaps minimal unifying models could um be useful here as i've suggested and the free energy principle can provide a way of relating these different notions and different branches of research together for instance active inference models can be can be used to develop operationalized theories of consciousness or i mean there's been this recent active inference model of the of global of the global neuronal workspace theory of consciousness and um active inference can be used to model cognitive capacities and saron and [Music] relating these different notions and these different kinds of work together would then could then also lead an insight into why different neuronal structures are associated with consciousness and what play compact what role complexity actually plays in consciousness and many other questions and problems okay so this um is the end of my short presentation uh thank you for listening and i'm very much looking forward to the to the discussion thank you thank you for that that's really awesome so you could great well what a awesome way to begin this conversation with a presentation and um also really appreciated that you're revising the paper and kind of giving us this glimpse of not just a research field that's in evolution but the paper itself is um undergoing changes so for the rest of this um active stream 16 we'll kind of play it by the usual rules if anybody wants to just raise their hand we'll get to everybody who does um also we do have the slides and blue who's um here and i had done 16.0 so we had thought through a couple of topics so um at this point i guess we'll open it up to people who want to raise their hand and can also just say hello on their first time speaking and then um in the absence of hand raising we'll just continue through these slides so thanks again wancha and let's go to stephen first and then anyone else who raises their hand yeah just like to say thanks very interesting to hear the developments on the paper um i was just going to ask about the um well i suppose there's a lot of things that come up but i'm stephen by the way just say i'm based in toronto but i'm working on a practice-based phd through the uk and one thing that came up on your slides there was interesting when you were looking at the potential for the there to be some sort of memory that relates to those states um and i was curious whether that memory itself might have to be connected to sensory states or whether that might be something that which is standalone and if so whereabouts you might think that might be in the brain like is it a hippocampus type question grid cell type question or something like that so thank you thank you and that's a very interesting question and i think it could be useful to show my slides again let's do that okay so you're referring to this picture right um and um yeah so here the idea would be that um what in a simulating in a system that simulates a system that is in a action perception loop um there would be some some internal states that encode the internal states of of this system that is being simulated so this digital computer would have some physical states that encode the internal states of the simulated system and um so coming to the first part of your question that there would have to be some interaction with sensory input but what a sensory input so this in principle one could say well any of the other physical states of this digital computer could serve as a sensory input to these internal states right and the question is what the causal relations between these physical states that encode internal and sensory states of the simulated systems what these causal relations be similar to the causal relations we see here and i would suggest no um and then there's another aspect you mentioned in your question which is whether there could be some memory unit in the brain and um so perhaps you can elaborate a little on this you were you're thinking about simulations that are going on in the brain or yeah i was kind of thinking um or curious if you could have maybe two parallel kind of you've got the real time real sensory input being like experienced and then there's like an update to this kind of memory but that memory doesn't you know that memory rather than it being a representation that memory it could be different and with that memory i'm curious where that memory itself then connects back to the actual sensory organs you know so it's not like it's in some computed form um but somehow the the sense of smell the sense of the multi-sensory integration is still happening at some level you know but maybe in a different way which then helps in dreaming and stuff like that yeah so this is really interesting um i mean you i think it could be useful to distinguish different types of simulation that could occur in the brain one would maybe be involved in um imagining certain kind of factual situations and another would be involved in dreaming and you're suggesting that there must be some memories somewhere and which maybe how which would yeah there have to be some states that that are um that give rise to these conscious experiences but they're not directly connected to sensory input is that what you're suggesting yeah they're not maybe not directly connect they're connected to the sensory organs and so so it's not that it's like a separate it's like you in a way i'm saying it it needs to keep some connection to the sensory what we call inputs i suppose but in a different way so it's not like it's it's completely offline if that makes sense but it might be online in a different way which would then still allow you to have a dynamical system in both cases and applause will root for how they came about um so yeah so i'm curious about that so basically um you you know whatever these priors are this they're still somehow um engaged in the sensory organs in in being alive and conscious thanks steven um wanja go for a response there and then we'll continue with adam and marco and then anyone else so the one thing that um comes to mind here is that it may be useful to talk about nested markov blankets um so you'd have the the brain with uh sensory states and active states and of the brain from in states of the brain would can be regarded as internal states but within the brain there would then be another markov blanket and internal states within these internal states which um would then be directly associated with simulations or with dreaming or other kinds of experience and so the that there would be an indirect connection to sensory organs but the as it were the sensory states that immediately affect these internal states that are associated with from say dreaming would um would be within the brain are not directly connected to sensory organs cool thanks a lot um adam at denmark uh can you hear me yep uh hi everyone uh hi wanja thanks for that that was this is really fascinating work and it's uh disturbing me and that i've recently attempted to present a mineral unify model uh using the free energy principle as a overarching framework and i'm wondering whether i need to backpedal on some of the visit the claims i made so uh i tried to bring together um integrated information theory and global neural workspace theory and but in doing this one of the things i pushed back against an iit is uh sorry i'm not tried my little breathless um one claim i was trying to push back on is that uh you you needed a neuromorphic architecture um to have consciousness and that you couldn't do it um unavoidable i think this issue is probably largely moot um moot do the just what probably would take to get enough compute we probably need neuromorphics just for energy efficiency but um i'm wondering um with respect to that that slide and in terms of causation whether like by the time we commit to a core screening that involves those entities that actually um we might need to coarse grain over the workings of the vinom architecture and actually go into the virtual machine and that the the causation is in terms of like a like in perlian causation we have to like commit like commit to an ontology and then once we have our directed graphical model then we're doing the operations and that's like our causation and so to i'm wondering if there might be like a mixing in that figure and that if we're like i don't know if that makes sense um but and i guess with relation to um nested markov blankets i i and and steven's question um also i'm wondering if this might end up taking um biological consciousness and potentially putting it in the same kind of epiphenomenal boat in terms of degrees of how much directness allows us to um actually treat uh sensory motor coupling as actually being uh real and causal so i'm wondering if we do that we end up like being in the same boat of uh not ascribing consciousness to um biological systems either even though they're using massively parallel operations and they are in their very dynamics having this dual aspect character i do not know cool launcher go for it and then uh marca thank you so much um by the way um adam thanks for attending this session so um and you had some immediately i had some comments on twitter when i posted a link to the preprint and i didn't have time to respond to these comments so i'm really grateful to you for attending this so we can um actually just discuss the points that you raise and yeah so how real our virtual machines and i think there's two issues that have to be separated and so this is one thing i'm sure about and then there's one thing that i'm less confident about so the thing i'm sure about is if you have a course grain description of a system and abstracting away from certain details that doesn't mean that you end up describing things that are not as real as the things that are happening at a more detailed level um so if you're describing a brain in terms of the what's going on in individual neurons and describe that in a lot of detail that's more specific description you have than if you're just um considering the average activity of neuronal populations but that doesn't mean that what's going on at a population level is less real or that these average this is this average activity cannot um have causal powers or that relations that you would describe that way that you could describe as causal relations at this more coarse-grained level that these relations are not really causal relations or something like that so i think we agree on this part and then the question is well if at such a course grade level if i understand you correctly at this at such a more coarse grained level if this instantiates a virtual machine doesn't this mean that the virtual machine is um as real as the physical machine that instantiates the machine and i actually i don't know but there are some things that follow if you accept that virtual machines can be conscious that are really really counterintuitive and disturbing so um i mean if you if you if you what if you've watched a series black mirror there are these episodes in which a um person is uploaded to a computer and then there's a virtual version of that person in a virtual environment and that virtual person is being tortured for i don't know for weeks or even months and the assumption is that this virtual entity is conscious and this happens in over a course of i don't know minutes in the external real world so um if if a virtual machine can be conscious then conscious experience is that we have hearing during over the course of several weeks or months could take place within minutes or maybe even within seconds and that's really strange but you could also run these a computational processes backwards and then you'd have backwards conscious experiences or you could stop them at any moment and then continue them and or you could have physically highly distributed systems that um are connected by some some device which then enable this enables this collection of physically scattered systems to compute some to perform some computations and if this can then give rise to consciousness you'd have a very strange conscious entity and other things like that and then there are these thought experiments such as uh net blocks blockhead experiment in which the chinese nation instantiates consciousness um or at least the functional states that um may be associated with consciousness and yeah so there are these really counter-intuitive consequences and this is the main motivation for me for trying to make a distinction between simulating consciousness and instantiating consciousness and maybe there are some virtual machines that are conscious but i my intuition is that it would have to fulfill very specific conditions and one of these conditions may be that the interaction it has with its physical environment must be the same type of interaction that the simulated system has with the simulated environment and that's what i try to illustrate here but of course you could you could argue well this is just motivated reasoning because you don't want to accept these counterintuitive consequences and why what would you make make this accept why should one accept this requirement thanks it's an interesting style the argument from consequences with consciousness well that can't be correct because then this would be conscious and we know that's not true it's like wait do we know that so marco and then blue oh yeah another question another comment about a nested mark of blankets and um so you suggested that maybe this requirement is too strong if we maybe it's fine if we apply that to um to digital computers but um if we apply that to actual to biological organisms that we would regard as conscious and the result would be that they are not really conscious at all so it's too strong i guess that's what you're suggesting or what you are um the question that you're raising so um yeah i guess one would have to flash this out in a bit more detail but one way would be to say look the free energy principle makes some idealizing assumptions and the very notion of a markov blanket as used in free energy principle is as um i think many know by now uh slightly different from the notion used by or proposed by judea pearl um because pearl's notion of a mark of blanket it's just a feature a pulse markov blanket is just a feature of a model of certain causal graphs and the markov blanket used in the free energy principle is an ontological notion or metaphysical notion because it's meant to be actually part of a physical system and um one good question that it really exists and um yeah so or that optional biological organisms that are conscious that they act actually have these or that they fulfill these idealizing assumptions made by the finishing principle and then for this reason if we derive a constraint on what it means to be a conscious system as opposed to simulating a conscious system from the free energy principle actually requiring that a system has these or fulfills these idealizing assumptions which may be features of a model of a system but are not actually satisfied are not actually instantiated by the system itself um yeah i think there's an ongoing discussion or there will be an ongoing discussion about the notion of a markup blanket and whether it's justified to treating it as a ontological notion as a feature of an actual physical system um and we are committed to this strong reading and in the paper so we're using the notion of a markov blanket in the ontological sense not just in the epistemic sense so this may be a problem cool thank you for that edition so marco then blue thanks um thanks for the uh presentation that rates that paper i've been significantly excited by reading it especially a great uh use of fep uh and buyers because it particularly it's a lot of intuitions i had um and it's also very nice in the spirit of pluralism right this approach of really embracing the unifying quality or power of the fdp and focusing on unification rather than another universal or overly saturated um claim proposition um and it's going to be brought up some scattered thoughts but i'm trying to also touch on what stephen said i'm not sure if i understood what it meant um but i'm reminded of the amortization so so sam gershman has some nice papers on this uh he has a nice paper called uh remembrances of influences past so the parameters or the the components the factors of the strategy used in influence could also be remembered and so maybe that's what i thought you meant when you said something related to or associated with sensory states but not actually synthesis itself maybe in the physical substrate that enacts these sensory inferences and there might be partial many parts or parameters or factors um that can be remembered and that these are the ones can be these are the ones that also are reactivated during dreaming despite the absence of sensory inputs um but but anyways if it's not what you meant then let's talk about that later um uh about about your conversation with adam i um i have a bit of a i'm not sure if you meant virtual machines as popular as uh in actual computers and software for me it's very important to to distinguish a mere embeddedness uh such as the normal virtual machines and an embeddedness where the where the relation between the embedded subsystem and the larger system is one that's related by active systematic mutualistic influence right that these are mutually contextualizing and there is a kind of targeted relation to it which is not the case with normal uh virtual machines especially in the norman architectures because these are more sterile channels as i would like to say um so as for my own points i've been reading it so so um i very much uh appreciate that you elaborating emphasizing the notion of dynamical complexity as opposed to complexity and the and very elegantly avoiding the many conceptual traps that tends to be lurking in these woods um but but but i have a suggestion or question so so you suggest that the notion of accuracy kind of falls away in the determination of free energy uh variation free energy um for honest awareness but i actually would argue that that in fact it accuracy is still relevant but it's a reflexive and of accuracy so because of the embedding this or the mark of like it's immortal magnets or the embeddedness of these ecosystems um you could say that that to some subsystems their concerns of accuracy are relatively the other sources which are eternal system and so you see this in dreaming right so so the so the different subsystems of the brain need to update themselves and each in relation to each other and so for example um i think tononi has some work i'm not sure if it's published yet but that the activation of uh during dreaming or the epicenters of the activity radiating outwards seem to correlate with the sites of activation during the waking experience where something new is learned and so this and and under the sap it becomes extremely clear well it suggests an intuitive nature for why because if there's a local kind of improvement or local updating occurred in that model but all the substances in the ecosystem are mutually contingent or contextualized by each other then it would be useful to kind of extend the fruits outward or share the fruits of that experience with other systems and so you get you also then lose the the necessity of a global coherence of the ecosystem because there is no uh unified external world right all the contexts become local it's simply a constellation of local continents which in my opinion is an elegant way to look at it because it seems parallel to the phenomenology of dreaming which um you know argues for your paper's motivation because this tentative computational explanation um has in uh immediately a nice pathway to the phenomenological uh aspects to be explained um so yeah so i would just really like to uh hear your thoughts about but whether out should really be dropped or whether we should see of still being a part of the variation energy in the lower levels so more specifically it's as if the complexity at one level is partially constituted by the accuracy improvements of the lower levels right and so especially because you you you mentioned embedded this but i don't see it as well integrated in the paper but i think it would much more bolster your claims especially because of its relations with other material and consciousness research especially before ecamp and their whole emphasis on embeddedness um and and this also relates a bit well i would say that that in another way you can see this as a contextual reduction can be seen as uh configurational accuracy so if we assume that that's the act of choosing the configuration is itself a question of policy selection or the action of configuring yourself in relation to the context then complexity can be seen as a configurative accuracy right um and so now you get these weird things so decomposition of uh complexity reduction in terms of intrinsic context reduction so kind of the bayesian model reduction you see scene the active pure active influence of curiosity paper and an extrinsic model reduction a modern reduction in relation to the um contextual systems in which it stands relationship anyways i'll stop here i think i'm rambling on too much so um yeah we'd love to hear your thoughts so um thank you so much for your comments and and questions on yeah so this point about embedded systems and nested markov blankets that's definitely something that i have to think about a bit more which will be relevant relevant so when you talked about complexity and how within a an embedded system within the brain it could still be um accuracy maximization despite a lack of sensory input via other sensory organs i actually wasn't reminded of a discussion we had with a reviewer but not a reviewer of this paper but of the markovian monism paper we published last year so one of the reviewers wrought brought this actually this this question up what happens in a an isolated system and they mentioned that according to iit it i i think it could still be conscious something like that it really reminded me of what you said or the other way around what you said reminded me of this discussion we had and the my intuition i had or the the initial reaction i had was that okay well in such systems we have maybe no sensory input from the from the external environment but there's some environment within the system within the organism that can provide some simulated sensory signals and i think this is what you were referring to and i don't really remember what happened i think carl had a slightly different opinion regarding this and suggested that the most the more elegant way of treating this case would be to regard um the system as a system that just minimizes complexity and it ignores accuracy but now that it comes up again i don't think that's the end of the story and it may be relevant and interesting to make this a bit more complicated by actually considering nested systems and simulated sensory signals and so on and i don't think that this is something we can build into the revisions because the paper is already getting quite long but it's something that we should focus on in in future work and um i would be grateful if you could um point me to some of the literature that you mentioned um send and then there's another thing you mentioned a distinction between two types of virtual machines or two sensors in which a virtual machine can be under things that can be associated with that term and um i i don't think i am um aware of this distinction and i'm not sure i already understood it so perhaps you can repeat what you said um so that one is nearly embedded and the other has some some additional features i think that could be really uh interesting and and relevant should do an hour later yeah go go for it the keep in thread and then we'll go to blue and then to dave and adam and another question from chat i try to keep it brief so so so actually one of the reasons why i'm so motivated about this is actually because when i did my master thesis with carl i had the same argument complexity so he's also like no no it's just complexity reduction but but the thing is as you know it's about redundant parameters but i don't think that's going to be the issue in the story that i told you it's not about redundant parameters it's parameters that are mr tuned because of their contextual contingencies having been updated but they themselves not because in the real life awakening experience it wasn't they weren't interacting but in a more extended context it's kind of a weird kind of counterfactual marginalization but but it's like expanding the scope in which the newly accrued experience might be rather easy that's how i see it and so it doesn't seem to me that it's valid to say it's merely about removing redundant parameters um it is improving the configurat configurative preparation for anticipation in the future given the limited scope of the experience that was newly acquired as for the virtual machines one important uh difference is for example the the the mutualistic relation between for example merely the transistors and the virtual machine they're in um it it's it's um kind of flat or low dimension there's no multi-skill multi-level thing there's no adaptivity all the way as in the driving uh the driving force behind the generation of the virtual machine virtual machine itself is is kind of uh is is is simply sent out it is not in the path itself it's not uh contingent upon the path integration so so there is this non is it's not an active integration of their relation okay never mind i'll try to think about how to phrase it but my intuition says there's a very important difference between virtual machines classical form and the kind of virtuality that you that is relevant for minds but especially consciousness thanks marco for these awesome points and that's the fun of the synchronous moments that we share is to surface all these new connections and ideas and then we return to the work and the sharing on you know platforms with each other so thanks again for that ronda if you wanted to add anything otherwise we can go to blue and date okay awesome so we'll go blue dave and then we'll have heard from everybody once then the chat and then return to adam and everybody else okay so i want to go back to um your discussion with adam also uh and it incorporates um virtual machines really so you were discussing simulating consciousness versus instantiating consciousness and the necessary conditions for both and in describing like the black mirror episode which i've seen and is awesome um you know you really implied that there's this necessary like arrow of time going forward for maybe instantiating consciousness that's not present in simulating consciousness and so is that like one of the required conditions like does the arrow of time play a role like this bi-directional like time is like not that that can't be conscious and maybe it would also help um and i'm sorry if i missed this in your presentation because i was a few minutes late but um i i don't know how exactly you define consciousness and i mean i did the dot zero with daniel and it's really kind of difficult to um you know measure or ascribe scientific properties to something that's like not clearly defined and i know that there's many definitions but i just was wondering wanja what you personally take consciousness to be or how you understand it okay thank you very much for these questions um so i start with the second question because that's um in a way it's easier to answer that question not because i can define what consciousness is but because i can just tell you that i don't know what consciousness is um so i mean there are various ways of operationalizing consciousness and ways that are used in research on the neural habits of consciousness in particular and many of these measures of consciousness involve some kind of report rubber or behavioral report and uh for instance the question is than just does the can can the subject report reliably report the presence of a certain visual stimulus or not and that would just be an example and so i don't have a strong opinion about ways of what's the best best way to measure consciousness um but so what we are kind of uh building on here is just the existing work on new colors of consciousness which use various ways of operationalizing consciousness various ways of measuring consciousness whatever it is um but of course this is a bit unsatisfying and we would like to know what is consciousness we perhaps you can tell us how to measure it in different ways but what is it that is being measured and here i actually think that a computational explanation of consciousness could provide an insight into the very concept of the notion of consciousness in that it if if there's a common computational mechanism associated with different cognitive capacities that are facilitated by what we call consciousness and this could provide part of the puzzle that gives us a definition of consciousness so just as an example um think of the global workspace theory according to which consciousness is basically global availability well this you could use that as a definition of consciousness consciousness is of global availability to different cognitive subsystems so information is process consciously if it's not just available to um some individual subsystems but to all or most of the cognitive subsystems within the system and um yeah this is of course a bit unsatisfying because it seems that it doesn't capture everything that we associate with consciousness so consciousness has many interesting features um so if we try to describe the phenomenology of consciousness then we can say that it's typically structured we experience spatial relations between things so you're not just experiencing say a desk and a computer screen and whatever else you're you're consciously perceiving right now you're not just hearing something but you're experiencing these things in relation to each other and there are spatial relations that you're experiencing they're pothole relations and there are also temporal relations of course you're experiencing not just isolated events but successions of events and it seems that what is happening right now is not just a an instant but it's somehow extended or a um yeah a specious present as william james called it and and there are further general structural features of consciousness and what a definition of consciousness should provide is a means of making sense of these different properties so for instance if consciousness is global availability of information in a cognitive system why does this mean that why does consciousness have these phenomenological features that characterized characterize most or maybe all types of conscious experience so that's something that a definition of consciousness should provide in addition to just saying what's happening in the brain and computational models i think could provide a means of breaching levels of descriptions let's say descriptions of neural activity and neural mechanisms and then phenomenological descriptions that um describe features of consciousness as they appear from the first person perspective and integrated information theory is of course an example of a theory that tries to achieve just that starting with phenomenological axioms that characterize consciousness or typical forms of conscious experience and then trying to operationalize these axioms in a formal way such structure they can be applied to your neural structures on your activity or other kind of system and in principle i i think this this would be how one should try to define consciousness um finding computational mechanisms underpinning cognitive capacities that are associated with consciousness and trying to relate models of neural activity or maybe computational models of cognitive capacities two characteristic phenomenological features of conscious experience and then yeah so that this is what what i think how one should approach the question what is consciousness can i ask a follow-up really quick yes uh so i just wanted to um so you're just you're describing really this global availability and like able it's it sounds a lot like um david krakauer described individuality like in 2020 with with like this bi-directional information flow in in an individual right but there was no like conscious um so you have to have that downward causation which is which sounds a lot like the global availability that you're describing and so i just was wondering i don't know if you've read the paper or not but um it also like i got that i got that out of what you were responding but i also tended to think that um you're maybe only talking about human consciousness but then when you went into iit maybe not so so do you actually draw a line like what what kind of the system can be conscious does it have to be biological does it have to be like higher order vertebrates or um can do you think that this is especially in terms of computationally like defining consciousness which i also think is a great place to start um but does does this mean that maybe all systems up and down the um you know evolutionary ladder can maybe have some degree of consciousness or what do you think what are your thoughts on that thanks for the question and so where to start maybe first um you mentioned top-down causation i wouldn't assume that consciousness involves top-down causation so i i don't see a reason for assuming that and that's not what i meant by global availability um so um i i'm not familiar with this work by david krakow that we mentioned but um it seems to me that it may not be um may not be directly connected what i was trying to um and then um the question where where to draw the line between conscious and unconscious questions uh systems that's i think a really interesting question and a further i mean i'm in the beginning of the talk in the beginning of the presentation i mentioned one motivation for um going into research on computational correlates of consciousness and that was trying to have some um correlates of consciousness or criteria for consciousness maybe necessary conditions for consciousness that can be applied not just to human beings or to human beings in ordinary states on or human beings with a normal brain as it were but also to unusual cases and i in the presentation i only mentioned the possibility of islands of awareness isolated systems but of course unusual cases also include non-human animals that are physiologically different from human beings invertebrates artificial systems as well and this is largely i would say an empirical question where where to draw this line and there's recent work for instance by um eva blanca and simona ginsberg who um have a an evolutionary hypothesis about consciousness or hypothesis about the evolution of consciousness and they suggest that a particular type of learning that is a learning capacity can maybe the evolutionary transition marker of consciousness or creatures that during a species that um developed this capacity were were then also conscious and the capacity learning capacity they call unlimited associative learning which involves particular types of learning such as cross model learning and other types of learning and so their suggestion would be to look if there are what systems are capable of this or have this learning capacity and what what other um what other capacities cognitive capacities they have and um so this is basically an empirical question trying to determine which creatures are conscious and which are not and we of course need to have some criteria that can be applied to determine whether the creature is conscious or not and if we don't have a definition of consciousness or a theory of consciousness that can be applied to non-human animals and it's really difficult to tackle this question but focusing on technological functions of consciousness and cognitive capacities of consciousness is one i think particularly promising strategy and i was also mentioned jonathan birch who um has proposed this that there may be clusters of cognitive capacities associated with consciousness such as trace conditioning or cross-model learning other capacities um yeah so i i don't know where where to draw the line i think it is an empirical question and um these more empirical projects trying to investigate cognitive capacities of certain types of non-human animals can i think be complemented by theoretical research using computational models because that's one of the advantages of computational models that it can be applied to different types of systems not just these systems that have a central um nervous system such as ourselves but um also systems that have a different physiology or in principle also artificial systems i wouldn't assume i would make the assumption that artificial systems um cannot be conscious but i think it will be really relevant to determine what constraints what conditions have to be fulfilled by a art by an artificial system in order to be conscious and which systems which types of system will never be conscious and if for instance this black mirror scenario is a a physical possibility if we could actually build such devices that simulate conscious beings and thereby create real suffering for instance that would be horrible and it would also mean that we could just make hundreds of copies of these virtual systems and instantiate the same type of um maybe horrible conscious experience in multiple times that would be a really um a disgusting scenario and so i think it would be it's extremely relevant to investigate the difference between simulating a system and instantiating consciousness to avoid creating unnecessary suffering or disturbing conscious experiences and um you had a very interesting question about the arrow of time going forward and yeah this is something that i guess i have to think about in a bit more detail so what what happens to the causal flow or the action perception loop if you reverse the temporal direction of the simulated system and um so intuitively i would say that it's important whether the arrow of time goes forwards or backwards that you wouldn't have conscious experience if you rewind the experience of a simulated system but i'm not sure about that it's a really interesting question thank you thanks for the response so we're going to do dave then a question from the chat then adam and marca it sounds like getting a handle on consciousness maybe suffering because it's been deflated too much if you assume that something you're examining is a predicate it gets really easy to turn it into not just a predicate but a thing and things are pretty hard to work with mostly they seem too easy um the first first over deflation is assuming that conscious acts or conscious activities are persistent long-term things a predicate of an organism the organism is present now if you if you subscript that and say is present of certain things now and then under some conditions well then you can ask what conditions enable consciousness what's achieved what's the evolutionary advantage of being able to be conscious when you need to and people as disparate as gerald edelman talking about conscious flashes in the brain the the dynamic core crystallizing now and then and then there's conscious activity and then maybe it's not needed because the the problem has been resolved earlier alfred north whitehead talking about threads of higher experience within a brain for instance something that happens now and then uh and then the transformational psychologist gf actually gives an exercise to force people to be conscious and he said he his background is most people are non-conscious the great majority of the time however if i ask you are you conscious the answer is always yes because i've made you conscious by asking you that specific leading question now the notion that consciousness is something that comes into you from the outside that's been around for a good while that's what the physiologists in these 1870s and 80s assumed so freud since he was busy thinking about so many things didn't want to tackle that and he just assumed oh yeah consciousness comes in something's going on the outside of my brain my neocortex responds to that and that's where the consciousness comes from and if i'm not being stimulated well i enter into nirvana i don't dislike and i don't like i'm just in a great blah state and the engine is just idling and waiting for something else to make it do something so oh and the other way conscious activity is getting deflated too much is as i said being treated as a predicate well it seems that if you expand it to a relation it might be a lot easier to start dissecting i'd say off the top of my head a relation among 10 terms self a is conscious with self b of object x in that conscious relation between a and b of x given the presuppositions of a b and the emergent relation and the three sets of dissatisfactions or goals uh whether it's uh an unpleasant stimulus wanting to get away from something or getting towards something of a and of b and of that emergent relation between the two so that's ten a ten place predicate right there and when i'm thinking hard i don't really see any of those ten dropping out because when i'm in dial out dialogue with myself i'm at least that complex and is so so is my dialogue with myself how about you thanks dave for the question uh laundry and then i'll ask something from chat thank you um yeah so you made some really interesting remarks and in fact i i think it's important to see consciousness not just as as a property that an organism has um static property but as something dynamic that can be transiently lost and so on and um so your suggestion to suggestion to treat consciousness as a phase relation um is pretty interesting and i think this speaks to the the idea of investigating nested markov blankets and interactions within between subsystems within the brain or within conscious organisms and i think you're completely right that a lot of the relations that can hold between um a conscious being and other conscious beings or conscious beings and parts of the environment must in some way be mirrored in mirrored by relations between parts of the system of subsystems of the system and yeah these are really complex issues that we try to um try to try to ignore in the paper but which i think we should get into at some point especially the idea that they're on nested systems nested mark of blankets within the brain so i think this is really important thanks for the response so i'm gonna ask a question from cambridge breaths in the chat and um just for this last you know 25 or a little bit longer i have the slide up for 16.2 so we don't need to worry about answering consciousness in the next 20 minutes let's actually build the energy for continuing our discussion as we do in between the weeks so cambridge breath's question was and this is related to this discussion does every memory or thought episode um have a markov blanket of its own that can be mathematically characterized and measured in other words can we consider every thought episode every minimal phenomenal experience as an independent island of awareness on its own so wanja i'd love for you to give a thought on that how do we operationalize the markov blankets are we a singular blanket or is it just this constellation of little transient bubbles and how does it relate to this islands of awareness idea that you mentioned in the paper thank you um yeah so i'm not sure how to um how to formalize a memory or thought episode but i'm sure it would involve some mark of blanket but that doesn't mean that there's an isolated system or island of awareness within a conscious system because there would be some interaction with the internal environment as it were um another question that comes to my mind now is that if if you um imagine something i have a thought episode there will be some part of and if we assume that some part of your brain realizes this broad episode what would there be a conscious system within a conscious system because of it if it has the right engages in the right cause of interactions with its environment and i think this should be added as a or could be built into a further constraint of what what it means to simulate as opposed to instantiate consciousness um i'm not sure how exactly but so no i wouldn't say there's an island of awareness when you have a thought episode or remember some events um but there are some really i think relevant points associated with this yeah thanks and it's just funny how we talk about instantiation and for instance and we talk about instances of virtual machines in the linux sense so it's actually a nice computational um language so i have adam and then marco than anyone else who wants to um raise their hand so adam or yep go ahead uh hi can you hear me yep as usual stop asking then so um i you mentioned like with the reviewers um uh so like you have these like lists of like places where like your intuition is balking and i i share a lot of those like um could a nation uh be conscious could like um so a bunch of tin cans and ropes strung together and there's a sense in which like just like church tearing thesis i guess like there should be a certain universality of computational realizability and maybe we can do like um in in principle tin cans and string but it just might not be practically realizable because there aren't enough you know there's enough aluminum in the multiverse or sorry in the universe but i actually liked your proposal um that like you kind of dismissed it as like maybe being like a little bit special pleading but i didn't find that to be the case at all so like if you actually created a rich embodiment in embedding within a virtual world and then if there's some sort of like homomorphism between this and i guess i actually i don't even know if it needs to actually couple with the physical world like the importance would be that you you're having this sort of um functional closure with respect to active and perceptual states in this sort of self-evidencing and self-making that you're having within this within the system you're doing inference on this interaction and within the context of which um within the context of the virtual machine virtual world which you're running this um it seems like we could say that consciousness is arising but i'm not sure um in terms of functions of it so i've been wondering whether we can think of consciousness as a kind of like world model and we're doing a divergence minimization with respect to and it's having some causal significance of this kind and where it needs to have certain um additional necessary uh properties for it to be a world model capable of generating experience of having um basically coherence with respect to like spaceships oral and causation and causal kinds of coherence and um i'm wondering if that could potentially speak to the issue like to bring up functions of which systems um are more or less likely to possess consciousnesses like we just add in a additional stipulations as additional necessary conditions and i'm i'm basically wondering if like quasi kantian like a priori categories or some of the things people talk about in terms of core knowledge these might just be necessary for any kind of world to appear to any being whatsoever for any kind of coherent sense making and with the function being you have you have um some sort of a data structure corresponding to the agent and the situation to the world and this is acting as a kind of top-down controller and in ways maybe is a virtual machine of sorts if you want to if you have any thoughts on that otherwise um yeah thank you adam um and does the system need a world model in order to be conscious or not and um so i think if if it needs a world model it doesn't have to be a very compact model um and [Music] i mean it would have it must have a world model in the sense that its internal states must encode probability distributions over external states given blanket states so but but this is not the world model you were thinking of right um well you were i'm wondering if we need a little more to actually have like composition and i guess like the kind of accents like to have like things related to things with particular like distinct properties um where those properties are situated for the various things that are in relationships and whether this is basically serving the function of uh allowing you to surf uncertainty basically like allowing you to both uh be informed by and reform these action perception cycles and maybe you need additional like structure built in for this to actually be like a world represented by the internal states not sure though i i think perhaps um you don't have to have a model that contains objects like um a system that is conscious you can have a conscious system that doesn't know what a tree is or what an apple is but i think you i mean you hinted at um spatial temporal and causal relations i think it must have a sense of spatial and temporal relations and causation that would be candidate i guess so that would be a world model which um could be fairly abstract in a way about gives the system a notion of time and space and causation and this is what i guess um i mean one thing that i um completely sidestepped here is the issue of benefit has caught minimal phenomenal experiences that it's conscious experiences that are characterized by the absence of spatial relations and or even temporal relations in which there's no subject object structure structure so um these are also sometimes called non-egoic states or states of pure awareness or consciousness as such and are this described in especially in literature from eastern traditions because these are states that can be experienced during meditation but also um in other um during other states and perhaps you don't need a world model and these types of experience and minimal phenomenal experiences thanks oh yeah adam go for it do you think it's possible and those sorts of non-dual states that there could be like a very minimal coherence along those lines like sometimes like um who's that gun toku like sometimes it's like described as like just spaciousness it's like just the space of awareness as these stripped down non-dual states like could there still be sort of it's not enough that it's necessarily um a movie it's not clearly like identifiable it's like here's a movie from your point of view and you're separate from the world and you're an agent represented but still there's just enough of a point of view just enough um like they're they're in there but they're not like reflexively modelable i don't know um yeah that's a possibility and it's actually um a really interesting option so with respect to a so-called um non-egoic states of consciousness or states which the ego dissolves there's a possibility that actually the subject object distinction disappears so those are non-dual states of awareness but not because there's no subject but because the subject is expanded and contains everything there is so i mean in such states or in the transition to such states subjects often describe a dissolution of the boundary between themselves and the environment and one possibility is that what happens is not that the the word or that the boundary becomes blurry and then the self disappears but actually that i become the world and so i cannot distinguish myself from the world anymore and that's why people say there was no i so that's at least one possibility when um i think sasha i think has argued for this point and one could apply the same idea to space i would say and that's what would be similar to what you suggest that if there's there are no points in space anymore no place where i am but if i'm the world then i'm everywhere so there are no points in space that i could distinguish from the point at which i am i myself am located because i am everywhere and have become one with the world and so there's only space as such perhaps only the world but no places within the world no points that could be differentiated thanks for the response makes me think of king of infinite space and a circle with circumference of everything and inside of nothing or however you want to phrase it so um we got 10 minutes left marco and then anyone for closing thoughts and again i have this slide up where we're writing things down for next week so marco and then anyone else who wants to give a closing thought and there's one last little question to chat to so go ahead marco i'll do my best to keep it short um regarding the last few comments uh safely say that which is called space is called by buddha as no space therefore it's called space and so your conversation was very much about the notions of emptiness or the rate of luminosity which is a fascinating body of literature in buddhism but but i would argue against the notion that we should take as valid context the idea that oh now we contain the whole world because it's exactly in that phenomenological stage where language breaks down and so any reports of that feminology must be taken with a heavy great results right so one alternative is so so the way i would see it in in semi fep terms is that it's a state of maximum potential reach right so it's it's not that everything is contained within that state it's that everything is reachable from that state it's like uh it's it's a kind of optimal readiness but contrasting that maximum potential reach is a minimal determinant which relates to the buddhist ignotion of no grasping or no clutching right you're not grasping to any duality any determination you're just letting go of any determination whilst being ready to determine whenever it arises or is necessary which is related to the co-dependent arising which again is beautifully uh congruent with active inferential tracks um so uh i've tried to refrain too much from trying to solve conscience but but since since something's already been raised so i think i've uh pinned down why i talked about the virtual model in that sense the virtual machine and so just like um adam as always i i also agree very much with with emphasizing the cybernetics compared which i found surprising for you not to have included from what i saw so far in the paper so so so um i i think i think one problem in the kind of the folks ecology slash language we're used to when talking about these things so we say that the brain is conscious but we only say that because we don't have the ability to specify in what way the brain generates or relates to consciousness and so following david chalmers i would argue that that it's better to use something neutral right instead of saying consciousness uh we could take seriously the phonological invariants across people's reports and say that this must be some integrated a reality let's call that a local space or a locatology i prefer to call the c space um and so you could say that that the c space embedded in the mind's brain which nicely has an acne within the marketing and is that the c space effectively has primarily a cybernetic imperative but in the pursuit of that cybernetic comparative it actually finds itself doing diff influence or inferential initiatives simply by the virtue of the fact that it needs to bundle its environment which for the c-space for that's a sensible conscious space um would not be the world as such but but the world modeling mind's body uh system and so you get this weird a tripod type distinction which actually joshua bach's first one for actually pointing to this to this idea so that you have embedded in the universe in the physical world a particular system called the body with a brain uh with the systematicity that we call the mind and embedded in turn we have something called the c space i importantly but that itself uh due to its imperative cybernetics or good regulation uh learns about that my body and it's the mutual contextualization they're in so upon instantiation or realization of this kind of disease space they become mutually dependent on each other the mind brains activity or let's say fitness would translate transitively to the students of the seizures which in turn transitively translates back to the fitness of the entire mind body and so this is very important to note the the the requisites of uh of the problem that the initial challenge of creating enough challenge enough free energy gradients to necessitate instantiation such as space and so i don't agree at all with these unnecessary debates about whether organelles are goddamn conscious like what the hell what's going to come in for them they have no adaptive imperative there's no incentive to instantiate some kind of regulatory space regulatory subsystem right there is no coherence instantiated to even lead to that there's no initial ignition to scaffold that complexity and so i i i yeah anyways that's my personal annoyance another notion i would like to share is the island is the awareness the fact that the island is embedded in this huge amorphous sea is what we should relate to that seemingly secluded experience um and and crucially and this is very intensive but i want to share it because of your paper so so i i would argue or propose that the c space is there where the intrinsic manifolds of the extrinsic manifold become equivalent or at least approximately or or asymptotically why because uh it's another way of phrasing that it's pure reflexivity those causal flows or spatial temporal causal activity that is purely reflexive therefore would not uh pertain to directly that which is outside there in other words that c space is by definition the self referring island and now comes one more crazy thing because trauma said we need crazy ideas so here's a crazy idea um so the point is that it's possible perhaps that this uh tentative islands um should be embedded in something and therefore we should intentionally say there's a blanket or a encapsulation which can be seen as like a channel like a two-dimensional surface which acts as a channel but each channel mathematically has a bottom a limit so what if what happens when that island that c space um generates more entropy than can actually be expressed through the interface with wichita's relation to the greater world right um and so i'm basically trying to indicate um in part adam's suggestions and intro and elaborate what i meant with the difference between virtual machines because normal virtual machines aren't in this mutually contextualized relation not at the level of the physics as such um and because it's purely in the virtual whereas here the physics themselves are co-opted in that mutually contextualized relation okay sir that's enough for now yeah all good thank you marco so everyone thanks so much for participating if anyone wants to raise their hand and give a final just little tweet length recap but what i'll say here is organizer is our next conversation live is going to be on february 23rd and we wrote down a bunch of questions for next time and there was also things that were asked in the chat like about the difference between quantum and classical computers and simulation so i know that we're going to have so many fun directions to go to in our next synchronous and asynchronous conversations and communications through the interface so if anyone wants to give a final thought um wanja thanks so much for coming on it really it was um it's a it's a special paper it was a lot of fun to work through enlivened our lab and our collective thought processes and um yep so if anyone else wants to give any final thoughts um steven and then anyone else raises their hand just for the last short thoughts steven unmute just one just wanted to say thank you very much and uh i know there's a lot of stuff has come at you so you you've managed to field a huge amount of questions but i uh yeah i'd be really curious to maybe see your thoughts on how this this work can pan out in the world in terms of people's practices and ways of thinking about other research and how this kind of way of understanding consciousness could you know play out possibly in because a lot of work in psychology is using brain neural correlates to justify what they think people are thinking you know so it'd be interesting maybe next week to think about how how does some of these ideas have implications and application which uh i think they do so thank you thanks steven adam and then anyone else with a raised hand um yeah this i mean fantastic as usual and this is uh my it's the most fascinating thing there is and i also think though it's extremely important we get this right so like marco is just discussing i'm organizing we're talking about islands of awareness like how quickly do we move forward with this research program which could potentially you know help countless people but we don't want to inadvertently create like black nourish scenarios so if we can place a very low prior unconsciousness for organoids due to not having the quite the proper cybernetic grounding contextualization and betting that would be great like similarly like with artificial intelligence we might eventually uh create systems where it is in question and so then we're dealing with um you know are these ethical beings on our own are we like doing mind crime against these beings or like both avoiding negative outcomes and then maybe wanting there to be positive outcomes in terms of creating beings who are conscious and are of value in their own right as subjects who experience and there's something that it's like to do them so they should be part of our moral communities all right these are maybe you know it's it's not we're not there yet but we might get there eventually and these might be the questions that are the key questions for what really matters for those events here yep and what's on the table adam as you often highlight is everything from who we include in our moral community false positives and false negatives are quite vitally severe in this issue um so looks like that's all the raised hands 16.1 thanks everyone for coming this is really a great discussion and look forward to continuing this discussion in 16.2 and beyond thanks