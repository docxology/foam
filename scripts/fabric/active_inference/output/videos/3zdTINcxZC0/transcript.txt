[Music] yeah okay it's recording um so for for Joseph for Daniel for the rest of the Institute who's uh following along remotely uh or via YouTube um welcome to the active inference Institute uh we do a uh weekly textbook group readings uh twice on Thursdays once in the morning once in the evening uh if you're if you're roughly in the Western Hemisphere and um yeah so so today we're going over chapter 7 in cohort 7 um and we'll be talking about essentially the Exemplar discret State space models and active inference that would be hidden Markov models and uh partially observable decision- making processes we have a nice Taz example which is pretty classic in in the field as well as in kind of Behavioral and and and and Neuroscience and neur neuroimaging studies in general and uh again we're in the second half or part two of the book which is highly dedicated to to constructing uh essentially active inference models as opposed to part one which is much more focused on Theory and and definitions um and so it's for anyone who's interested in computational modeling whether it be doing it themselves or getting a clearer grasp of kind of what are these computational models or cognitive models that we're putting together in order to say collaborate with a uh Team of of people who do do comp cognitive modeling then it it's very worthwhile to spend time with the second half of the book and then furthermore there are just there are many discreet State spaces in in in a very general sense in a in a physical sense and a kind of decision making and and and and human belief sense it's referring to scenarios where essentially if we think about it in data it's sort of like U categories it's thinking about binaries uh it's not so much thinking about uh fully continuous number series such as a range over one through a million where we could have so many different uh different values including with the decimal points um potentially an infinite number of values instead we're looking at say RGB color scales on old televisions so red blue green um we could have a distribution between the three of them in the sense of how much red how much blue and how much green uh but nonetheless it's it's a kind of um discreet uh categorization process so um yeah I normally I give a rather long uh kind of chapter summary but I'm happy to just kind of open up the floor if uh Joseph you have any particular questions or things you'd like to discuss here um I I would like to hear your summary um I also do have some questions um I haven't fully been able to understand um for example figure 72 so it if we could like I I I guess I just don't understand what plotted and and where the lines are coming from but um yeah I I'm I'm very interested in and trying to build these models and understand how to build them and um and so this seems like a really critical chapter for for me yeah yeah no that's great um yeah figure 72 which I I personally have not spent a lot of time with recently so coming back to it is rather interesting yeah so the the this is where the chapter it essentially opens with hidden marov models uh before we jump up to uh partially observable marov decision-making processes so it's like we have um I believe my is my screen still being shared appropriately or can you not see anything uh no I don't see your screen okay got it thanks for that just realized I doing a a little bit of uh display gymnastics um that should have done it so this is and a quick reference this is the Koda for the textbook group and so we have this available to all members and we have um not just a overview of the textbook group and you know dedicated uh meetings and and time availability and and all those sorts of things for the separate cohorts we also have a pretty good breakdown of the active inference textbook especially since it became open source so um so yeah figure 71 I know it's not the one you referenced but just to make sure we're like familiar with it enough this is essentially hidden Markov model and so understanding kind of the core Dynamics here uh we ult ultimately only have three different components in this model so we have D our prior over initial States it's kind of like what are our beliefs about States whenever we come into a situation as it were um and then from there uh that gets linked to uh observations that we receive right and the relationship between what we believe and sort of what we see is linked through this this likelihood Matrix a so it's represented as is kind of this broad categorical probability distribution that is conditional uh observations given our beliefs s or in States s and then there's also a temporal Dynamic over time right so we have the B Matrix which is kind of our beliefs about how transitions change from one to another so that's why we have this sort of State at T minus one to State at T to State t+ one so it's this this temporal Dynamic playing out over discrete time steps um and so from there you can move to 7.2 a little bit of context so this is this is an example that relates to the idea I believe it's of of someone who's playing uh an instrument and they're trying to actually play a song in accord with the the script the sheet music that they're reading and so we can quickly imagine scenario where there's someone who is still learning how to play this is practice for them even if it's someone who's very skilled there maybe learning a new piece for like I don't know an orchestra performance they'll be doing soon um and so there's all kinds of room for error right and to to accidentally hit the wrong notes at the wrong time maybe you're still on beat but uh but you know you hit an a rather than an A flat and so that's sort of what this this example is about is being able to to use the Dynamics of a hidden Markov model including What notes play over time and then also what does the musician believe the note should be versus what we we actually hear um right and so so by being able to like hear and it's obviously a very kind of abstracted example there's a lot more to it there we're not saying anything about what the sheet music looks like but here we're just simplifying it to saying the musician has a belief about what the note should be versus what they actually observe that they're they're playing so with this figure 7.2 I think the the quickest way of kind of getting at what the authors of the textbook are trying to demonstrate for us is sort of an illustrative example is this upper right this is like beliefs about hidden States so the agent believes that the first note uh in terms of over time time should in fact be the first note as in it's in row one so that this row is like its own set of notes row two is the uh yet another note so it could be read as like the top row is a the second row is a sharp the next row is B the next row is c not to confuse anyone who's not familiar with scales but we there is no B Sharp essentially unless you're concerned with Naturals and all that so um so each row is its own note and then I here it's kind of like the the musician believes based on the sheet music that they should start with the first note a and then move to a sharp move to B move to C and then go back to a for the fifth note so they're just kind of descending a chromatic scale before they get back to a again that's what they believe should happen meanwhile with our O's or observations what really happens is that we get an A we get an A sharp and then rather than getting the expected B is here we the the actual observation is that they realize oh no they hit the same note twice in a row which is not correct but by the fourth time step they get back to what the note should be and then here uh at time step five they also hit the correct note so kind of what's What's Happening Here is that there is at at S3 we would say that this is prediction error right and so with whenever the agent infers their posteriors over hidden States here they're so conditioned to believe that it should be the right note that that they kind of shaded this in as if it's correct but there's this grayness here it's very it's so light but but the essentially the idea is you know this is the note that should have happened their prior their observation however gives them evidence that it's actually this Grace space this is maybe actually the that they heard um or that they believe really happened so there there are aspects of this that can seem kind of counterintuitive but really just being able to grasp that the upper right are their beliefs about States the lower uh the lower right is the observations they actually receive and then a good reminder is you know in a discreet State space setting it's like oh we're not looking at you know the interim between a and a sharp you know some kind of microtonal situation we're just looking at uh we're assuming there's a and then there's a sharp and then there's B and there's C and ultimately they're that's the Western scale of 12 notes um in the way that I'm describing the examples so now these these graphs on the left are a little more complex um and it will be good to spend a little bit of time with them I'll first just like read what it says in the textbook so so we'll go with lower left we're looking at negative free energy gradients or prediction errors the rate of change of the beliefs in the upper left plot is determined by the value of these errors at each time step and so what we're seeing these are negative free energy gradients in terms of active inference we're wanting to minimize prediction error we're wanting to minimize variational free energy so if these are actually negative gradients then that means that at each time step we should actually want these to be rather large um if they're negative and so the the smaller they are you know it's kind of like saying that it's sort of the the worser scenario and then that seems to kind of constrain the agent's ability to to be confident or or or kind of understand the the situation so um see and then upper left beliefs posterior probabilities about each note in the sequence of each time step yeah so the these are the posterior probabilities it's kind of like their ability to understand a particular note here of what it should have been has gone down right so there's a kind of constraint there that's been applied and then once they start to hear the correct notes again their confidence kind of kind of increases I will be honest these kinds of these two graphs on the left they're much more rarely used today in active inference like these specific types it might be because of this sort of kind kind of uh counterintuitive nature because more often than not we actually are looking at you know the inverse of the situation so we're looking at positive free energy and so actually this would look quite bad it would look like step three is actually you know a rather good thing in terms of minimizing uncertainty and it's the other way around but I I think that the authors who who compose this are really trying to get at a particular kind of Point by representing it this way I I think I'm still maybe a little lost about so um you said that just focusing on this top left um the the y axis you said is probability so okay so I I get that and what are all the different Minds um are these different trials [Music] or yeah yeah it's quite yeah one of the things I'm really wanting to understand better is um uh how the agent actually is learning and and maybe that comes later on this is after this example but um where do do we adjust the weights of this of the the in these matrices um according to the result we get from our minimization step oh yeah no that that's an excellent question very relevant so so in this example they don't actually apply learning at all so this is essentially like a static model yeah um and I think they're just using it to try and kind of get at the more inference level Dynamics as opposed to learning and so I I know that this is you know this is pretty familiar terminology for folks who do other forms of kind of computational modeling data science deep learning uh I heard you say that you know referring to it is adjusting the weights all of that is very relevant here but so so for this particular model yeah there is no learning what would what could happen is that refer to this again um this what typically what an agent learns are these kind of probability distributions over time and so D you know the your your prior beliefs in the basian sense those could be updated so you could the agent could effectively learn its D matric um a could be learned B could be learned actually any of its parameters including c and e which C is just a prior over observations e is a prior over over policies or actions um so so that way you know that's what's missing from this and it's also missing from this because we have no policy inference right so the hmm is kind of like can be viewed as a truncated version of the pmdp which is exactly equivalent until we get to hear it this connection to to Pi or representing policies or action and so so an hmm is kind of like perception and no action uh partially observable markup decision-making process is perception and here we have the connection to actions that it can take so we get these these kind of relationships we suddenly State transitions the B Matrix recall earlier 7.2 the B is simply what is the next state going to be given the current state whereas here we now have yeah we now have what is the next state going to be given the current state as well as an action I take so that is sort of the kind of the the agency so to speak that this this agent has um and its ability to kind of infer which actions to take and furthermore um e the prior over policies I mean that so that can be learned that gets Incorporated C preferences prior over observations there're those are more often left static so those are more often left static in the sense that they become a kind of attracting point for particular observations uh in that case that kind of supplies the agent with a sort of goal um and kind of like it's seeking out particular observations that it it it prefers or EXP specs um in the sort of theoretical literature the idea is that it relates to the notion of homeostasis so I prefer to observe my blood oxygen level being at a certain point or I prefer to not observe my stomach growling because usually it's a sign that I'm hungry and I need to do something about that which in a that context it would that that c would then inform what I should do via the B Matrix of oh my stomach growling must mean I'm hungry I know that if I eat food I will S I will no longer be hungry at the next time step so to speak um so so again any of these can be learned and I know that there's a nice figure that kind of HS at the point of this but I don't want to get us too Lost In The Weeds um thing Hybrid models the essentially what happens is that we end up with this kind of hyper parameterization of all of those model components okay I'll stop I'll stop scrolling around because it might be that we missed that one still relatively new um so you would have like a prior on D which itself is in the discrete State space setting so it would be it would just be like a kind of so it' be a de lay distribution so so it would be like a categorical distribution but the values can in fact exceed one they don't have to sum to one you could almost think of it as a kind of like dear L count system where it's like oh I I you know I have my D Matrix but then I've also experienced like more of this state in many more time steps so it implies that there that an agent actually has a history of states and a history of observations that it's kind of accumulated and then that ACC is a kind of hyperparameter on these matrices which then proceeds to modulate their values and then learning them itself will involve free energy minimization I it's just applying another free energy minimization rule um and so and so like minimizing free energy is sort of the global uh functional or optimization problem here for just about everything like State like inferring States URS by minimizing variational free energy inferring policies heavily relies on uh minimizing expected free energy which uh earlier on in the textbook I think it's somewhere in the realm of chapters 2 through 4 uh there's kind of a broad breakdown about those terms so feel we should be able to find those we have the equations as well yeah great so something we'll notice very quickly between f and g f is going to be for Again State inference which ends up being treated as a separate process related but if if you were to code this like if you were to write this in code you would actually write like separate Logics and even executions of State inference versus policy inference State inference will involve Min minimizing F which without getting too lost in the various ways we can break this down we could look at it as complexity minus accuracy which is the sort of classic you know machine learning imperative of how do you create a model that can both be generalizable and not excessively complex and overfit um but we need it to be accurate so we don't want it to heavily underfit either so the the complexity term there is just what is the K Divergence between your posteriors what you think now versus is your prior what you thought before and then you want to also keep accuracy high so we want to be able to find the expected value of the log of essentially observations given X so so all this is to say each one of these terms depending on what the sign is in front of them um we'll we'll want to either maximize or minimize so we'll want to maximize accuracy right because it has a negative sign in front of it vice versa for complexity we actually want to minimize that because at the end of the day we come back to F we're just trying to minimize that um similarly for G but the interesting thing with G is that whenever we look at just the Top Line Information Gain and pragmatic value we notice that there are negative signs in front of both of them and we're trying to minimize G so we actually here want to maximize Information Gain meaning the the the expected value of the kale Divergence between sort of what you your posteriors what you what you think now conditioned on what you've seen and what you've done and the the Divergence between that and just what you think now versus uh are conditioned on what you have done so this is kind of like saying oh now that I have actually observed the outcomes of what I have done in what I believe Now versus simply just relying on you know sort of what I've done it it it kind of prioritizes this idea of oh I need to collect more observations and I actually want those observations to be quite different from what I you I'm used to seeing before what I'm so used to that I can practically sort of marginalize them out right so that that's sort of the exploratory Behavior we see in active inference and furthermore just the fact that we have this functional that actually allows for both exploratory behavior and pragmatic value which is simply your observations conditioned on C so like I mentioned earlier C it's often used for defining like goals that the agent has it's kind of like the what you expect is what you want which is what will help you maintain homeostasis or it's what will help you achieve that goal you have in the far future or however you want to look at it um or go in the mediate future and so you want your observations to essentially like you you you want what you see to actually be accurate versus like your priors over observations right so that that's that's kind of I don't know if you're familiar with reinforcement learning but anything like uh Epsilon greedy Behavior or having to like Define manual rules such as like oh you should always exploit with a probability of 0.9 and you know 10% of the time just do something random and see see what happens instead instead this is what it defines exploratory behavior is directly implicated in Computing minimizing expected free energy so the that's the claim in the textbook that these sort of naturally balance out which is not a sentence that really really should be said that way in the sense of you could still have a scenario where a policy is evaluated such that it might not have any information gain at all it's just the fact that both are computed and so there is no manual definition here we're just relying upon minimizing G and these particular equations I might have gone much more deeply into that and I I hope I didn't miss no I I really appreciated that I liked your explanation about um the the the meaning of those different distributions that you're me measuring the kale Divergence um that that was very helpful actually uh especially when you were talking about the uh Information Gain and the fact that you know one is conditioned on why hat or why Tilda and one is not um I I had I just sort of not connected those dots before so yeah yeah no problem at all I me can you say again I think you said it already but can you say again the difference between p and Q is this related to the world model versus the uh agents model like the truth the true World versus the agents what's in the agent's head or is it something else yeah sure no good question so um if we were to kind of use the phrase is it just in the agent's head or the world it's all of this would be in the agent's head okay so yeah yeah so and then uh in the sense that like this is the agent's beliefs about States this is the these are the this is kind of the breakdown of the overall rule of computing free energy Over States so States as and my my beliefs about the world and then G will be used for inferring what policy should I use so that's as opposed to State inference it's policy inference right okay so the and and and all of it is in the in the agent's head as it were and then for Q and P so Q is always used they they always use that to denote posteriors so this is sort of like what what do I think after a given instance vers you know and versus what did I think before right so there's this kind of model inversion process that that that happens where where we compute F by sort of fig sorry about this figuring out our posteriors figuring out our relative to our priors and that's what allows us to kind of score the final like what is our final belief now that that we have all of that and we have now minimized the free energy right so just basically basically employing basian inference but in the context of a fuller functional equation yeah yeah so so you know there are a lot of interesting ways that the these terms get broken down and I know that like for for a lot of folks who get involved with the Institute and their Learners uh especially those who are not terribly uh you know comfortable with math or or probability Theory it's sort of like these things can be very intimidating right to just like quickly see like you know let us tell you all at once all these different ways you can break down G whenever you're still trying to learn what this little C is over here and what how what that means relative to anything else right and what's the Tilda about right so and I'll also I guess I'll mention that the Tilda is U so G what you're doing here with G and the the whole G over pi is you're you're inferring your beliefs about each policy you have available over some inference Horizon which is why we call it expected free energy it's that it it also is variational in the sense of like there're we're relying on different kinds of you know approximate methods variational methods here but the the the expected part is emphasize because if if if I notice I'm hungry and I'm trying to decide what to do it's like well if I need to if I decide one policy I have is that I need food there's going to be a series of steps that I'm going to need to take do I have food in my fridge do is it at the store if it's at you know if I decide going to the store is what I want to do then like what are the intermediary steps of what I need to do from between now and then um surely I don't think about every single thing like the entire process of what it will be like for me to walk down my apartment complex's hallway but more discreetly know that I will need to say stand up and then I'll need to leave my apartment and then I'll need to travel or take transit or otherwise to the store and so on so so it's kind of like the Tilda is just acknowledging that this is a sequence so what what are the kind of expectations I have over what will happen if I follow a particular policy and and how I think it'll occur so um which is why we also don't see the Tilda up here because this is more about momentto moment inference which again relates back to earlier the point about learning right because it that's that's an important distinction between the two and active inference is uh State inference is just about you know in real time you know you think about again the the musician playing notes in real time and and it's very directly related to their ability to hear the note that they're playing versus what they think it is um so that's that's one way of kind of just more conceptually conceiving of uh of these things so okay and and so I guess um especially with G it seems to me that the reason you would choose one breakdown over the other maybe with not just G but all of them but um is because you're you're trying to figure out what's actually computable like based on the Computing resources you have maybe it's a lot easier to compute um that fourth line there with the expected energy and entropy and the other options in terms of uh what's actually solvable um that that's possible I mean it's never made very EXP it why we have all of these different breakdowns the The crucial thing is that like the way the math plays out it's necessarily the case that this first line like one version of G is Information Gain minus pragmatic negative Information Gain minus pragmatic values G it's necessarily going to be equivalent to the value here um given the the equation right and so it's sort of like now another sort of tricky thing is that the this is sort of U they will be less than or equal to these other terms so the this allows for almost I don't want to get too abstract and mix things together but it's almost like this is an additional like this is the the the lower uh excuse me the upper bound of the upper bound on like expected future surprisal right it's because we already know that um f from definitions of surprise like f is just an upper bound ons surprise that that works out mathematically and so it's kind of like saying well these are the upper bounds on surprise expected in the future and these are kind of the upper bound to the upper bound because of this sneaky uh less than or equal to sign here right um so so another thing is that it's sort of like the another aspect of this beyond that is uh the authors try to supply their own definitions of the different ways of breaking these down um and so expected free energy in just about every as broad generalization but I would I genuinely think maybe 80% to 90% of all literature you read in active inference most people are just going to focus on describing it this way information gained pragmatic value because it comes off as the most intuitive and it's also the most interesting in terms of viewing things uh in like explore exploit paradigms it's just this is the clearest way of kind of um kind of expressing that and so it's it's just intuitive like oh yes I want more information that actually increases like what what I thought before versus what I know now it's almost like I'm seeking out I hesitate to use the word learning because of it's It's technical connotation here but sort of like I want to look learn more or I want to find more you know it's a sort of a someone who wants to have fun doing something new for the sake of it you know I sort of think about that as a more colloquial way and then pragmatic Valu is just you know is what I'm seeing or what I expect to see is it going to be aligned with my prior so so is what I is this particular action this particular policy I'm evaluating like going to the store to get food is that going to help me achieve observations that match what I want which is attaining food and kind of watching and myself eat it and then watching my stomach no longer growl is a kind of odd way of saying it but it it's closer to the the the ideal of of how it would work yeah um I I think part of the the break the breakdowns here is just sort of it's taking into account so like with expected ambiguity and these other terms uh some of them additionally get introduced in in chapter 7 as well um posterior predictive entropy those kinds of terms it's kind of like just saying like oh it's it's sort of like this this idea that you know there's a lot of other dynamics of how we could view these playing now so expected ambiguity is like um oh that that for example is sort of what is the entropy of what I expect to see versus what I expect will be going on so um you know if if if I expect that the it will be light outside because I believe it's daytime and I expect to see that it's dark outside as I believe that it is uh nighttime then it's sort of like we have this nice one toone discreet mapping it's light out it must be day it's uh dark out it must be night um however if uh if we if we expect more of a one to mini mapping such as uh oh it's light outside it could be day or there or excuse me it's dark outside it could be night or it could be a solar eclipse right now right and I'm just not sure which I didn't pay attention to the news I don't know like what's going on right so um that's suddenly where you get more of a one to mini mapping between like what could this particular observation mean um and here it's a it's a positive value so we'd actually want to like minimize that kind of that that entropy of that situation thus expected ambiguity um and then risk over outcomes so these risk terms that get related to because there are a lot of people who go into this field not just because they're interested in seeing Neuroscience but they might be interested in other fields I I myself have a a background in um I did my uh Masters At You Chicago and this very broad social sciences program that's part of what part of what brought me here uh including an interest in economics so looking at risk so this is like your posteriors versus your priors like what what do you believe right now about what you would expect to see versus versus Pi versus like what what do you believe after right so what what do I expect to kind of see from what I do versus what I I I'm used to sort of um and finding the K Divergence between those right and you want to minimize that which is which is a little different so it's just there's so many ways to break it down and so many different ways to attempt to put into words why these equations work the way they do that's sort of the the trick isn't it of not just active inference but sort of cognitive and computational modeling in general like well if we have real data and we have real approximate methods of mapping things like beliefs and actions and inference learning processes what how do we put into natural language what any of those things mean so it's a I think it's a fair attempt and I I personally like many other people kind of prefer this top one but it is interesting breaking the rest down and then it's it's kind of like the further you go down the line here we get closer and closer to theoretical physics which is yet another kind of partner field of active inference and a lot of Carl friston the other folks's um research right so just having energy terms that yeah my my background is in physics and I think that's a large part of why this um appealed to me as a I'm I'm working on a robotics company so um that's how I came into this and found it and and why I think attracted me was the analogy to physics and um I'm hoping to figure out some ways to reduce the amount of data that it requires to uh for an agent to learn a task or to decide to do a set a sequence of motions or something yeah no absolutely I mean it's uh I don't know if you skipped ahead to um chapter 10 in this textbook at any point but I think they spent some time on on Robotics and and you know the the the more theoretical aspects of like cybernetic processes and sort of feedback loop operations but then like suddenly with active inference In This Very broad interdisciplinary aspect it's sort of like like because it can go so many ways it because it comes out of Neuroscience I mean one way one kind of funny takeaway and maybe it's not funny but I I I think it's kind of fun is uh like a lot of this relates to the brain and inference processes as well as looking at Behavior like the tze example that we get in his textbook so uh you know like a similarly to actually having a real mouse in a lab but then if computational Psychiatry plays into this a lot and it's actually like it's a growing field and many people are trying to cognitively model and I'm working with a research group here as well uh but we're working on a on creating an active infer inference model of PTSD actually and uh sort of what are the behavioral Dynamics there and and sort of the belief Dynamics and can you find a a generalizable but still a approximate um sort of model of these these Dynamics in relation to neurobiology and then in future like being able to kind of fit that model to real empirical data that could be collected from you know willing participants uh who have PTSD um and and all this is to say the reason why I'm kind of going on about it despite the fact that you we start with robotics is that it kind of in an interesting it's almost like like is a bad machine learning model or reinforcement learning model like if it's bad should we view that as a kind of like like a pathology like can we View suboptimal Behavior as sort of something like pathological Behavior so I I just think that's a that's a fascinating kind of take and i' I've talked to a few people who are more focused on sort of sort of optimizing models rather than you know they're not worried about attending to human beings and modeling that they're wanting to create very efficient you know kind of engineering based you know tools and and and and so on U right and we've always had interesting conversations about that because because an active inference it's like there's a term that's used sometimes um the term is a h hyper precise priors and so that that would kind of be like a scenario where to bring it back a little bit it it could be for example a situation where skip it because it's fig so like imagine an agent you know imagine our musician example again we don't have even have to worry about policies just yet um imagine this agent like had a very precise hyper parameter like on their a like a very strong prior for their a matrix and for some reason they're just learning it at either an incredibly slow rate or not at all um and say the a matrix was really biased so like with the with the this example like say the agent was heavily biased to assuming that like if their like their these beliefs are necessarily like one to one with with these to where even this kind of light gray space didn't appear so to speak it's like they wouldn't track the prediction error because they believe in the basian sense so strongly that this is in fact how things work you know perceptually given that the hmm is just like a perceptual model then it's kind of like saying like oh that's you know it it in in robotics that I mean that would be awful that means it's not able to like read the data streams in a way that allows it to make any kind of like flexible adjustable inferences instead it's it's going to just assume whatever it sees regardless of what it is just reconfirms like its beliefs about you know what's going on and If This Were a PDP then also what it should do in response to the data stream is receiving but uh simultaneously the pathology bit is sort of like oh that what for a musician that would mean that they believe they're really good at music all the time right because they're they're sorely convinced that their beliefs about what should be heard or what should be played or what's going on uh necessarily match the incoming data stream and so the incoming data stream will just kind of like it won't the variations in it won't really matter it's like there will still be it won't even hear that they made a mistake essentially they'll they'll believe that it's right exactly and so not to you know not to sound in you know insensitive or or phrase this wrongly but it's I mean if you start thinking about things like schizophrenia or psychosis where something in reality is not going on at all but the person believes it is it's like in a certain way it's they're they're perceiving reality in a way that's heavily biased by these kind of priors that they're coming to the situation with right so you're sort of stuck in that uh if so long as the your your inference process is kind of you know kind of path pathological in that way so um so yeah it's I mean with robotics I'm not that's not my my key field as it were so I can't say too much other than anytime I've skimmed it there's always something being something interested being said and that you know with a lot of these models in various areas um they're highly performative including in When comparing them let's say to reinforcement learning right because it's another way of looking at active inference especially especially pdps or any any model that is actually augmented with the capacity for Action we're effectively just looking at okay here's an agent who receives observations and infers an action it should take um if we just look at it as a mark of decision making process like remove the partially observable part then it would just kind of be like they are it's as if they already know what the state is they observe a state it's sort of more one to one we don't have this extra observation versus States part right but but but nonetheless I mean this is still the canonical bit because I mean this is as far as kind of the Neuroscience Go I mean we're always in a partially observable environment right so we we don't always know what an observation means and what we haven't even said anything about like you know the possibility that the sensory receptor itself is bad whether it be you know a person who's nearsight and so anything far away is just very hard to see or it be um you know a robot that has a a receptor that is just like not functioning correctly or um an EEG headset that like you know someone left the ground uh you know they didn't set that up right and so it it it sees really high amplitudes not because of the person's neural activity it's because it's getting like supercharged um so in a inaccurate way so that's sort of the part partially observable bit um they do I mean anytime I look at model comparisons it's just you know people do very regularly Benchmark um yeah they they regularly Benchmark these models against reinforcement learning models um usually again and again you know Furnishing an inherent balance of exploration and exploitation like so so a lot of people point that out that they just find a lot more sort of credibility in thetion of like oh these agents because of G uh they're sort of naturally internally you know architecturally incentivized to seek out more information and nonetheless like once they're affirmed that they have enough information they are over time more than likely going to become like oh no I've seen that before so that they kind of know not to go necessarily in that direction however it's over a policy Horizon so maybe that policy Horizon is five steps in the future or in a continuous State space it could be you know a minute into the future uh and what they expect to kind of see over along the way um you know so so even if they think they've explored a space well they might not necessarily know the full sequence of transitions that'll happen over time so it's it's very likely that they'll be able to depending on your setup continuously find ways of like getting Information Gain out of a situation Computing that like incorporating that into their decisionmaking but once they become much more reaffirmed that they've explored enough then they start to you I'm I'm speaking from from experience sorry oh in Montery Bay I'm not sure he's talking to us no uh here you go um so uh yeah I that kind of interrupted my sorry train of thought but it's just to get to the point it's like very often the not I I think that t- example and spending just like an extra not a long time but just like a minute or two um because I know with robotics you're usually looking at continuous streams right I mean maybe there's a I mean go sorry go ahead yeah I was I was just going to say yes um what I would love to be able to do is take a six degree of Freedom robot arm and output the six joint variables uh that are continuous you know and U between some limits and um that would all be given some sensory input and then beliefs about what that sensory input indicates about current state yeah no it's incredibly relevant to a lot of the work that is done here and it's sort of you know one way that they break down if you if you've looked at the book and and become more familiar with the notion of like an observation modality which is you know a very common sensical thing if you get rid of the jargon it's just sort of like you know I can see things things versus feel things versus taste things like there there are different ways that I can receive information and and so they tend to break those up into three different categories and so there's um exteroceptive which would be like information I receive from the outside which includes all the things I said earlier that it's sort of common five senses of like I can see and touch and feel things interoceptive would be like internal bodily signals um that are coming like sort of from within that I'm not necessarily feeling them in the in the typical sense of like my skin is in you know sort of dermal aspects um and then there's proprioceptive and that that's if you come across that term Sorry by the way I don't mean to sound kind of sending if you already are familiar with it no this is great thank you cool definitely yeah so so proprioceptive I'm just trying to find it quick and we might have to wrap up then is uh oh I know it's in here um somewhere there's there's they have a nice sort of visual description of like the idea of like that ah proprioceptive afer so this is you know this is still related to more like the brain but proprioception you know has much more to do with like movement and kind of reactive systems so so there could be like a reflex arc sort of logic for example um that that might be of interest to you if you want a model that's kind of a little bit more complex or sort of sort of Savvy with with putting together the architecture in regards to active inference there's no necessity of it though and I think you'll you'll you might especially find chapter8 interesting which is going to explicitly go over continuous models it of course is different because of the continuous Spa like time and continuous State space sort of setup um but then in that case we're looking at more like kind of these attracting points or or here this is uh this is rather complex but this is a Lawrence you know chaotic system and sort of like different kinds of Dynamics and viewing them in different ways and viewing how you know like I mentioned with the discrete State space setting there's C which is sort of defining your goals or your expected observations you want um the the sort of equivalent of that in continuous State space models you could view that as uh sort of like an attracting point or or a distribution of attracting points or an attracting distribution uh over a continuous space and just modeling it like that um you'll also it it relies a lot more on sort of tailor series approximations there's this notion he calls refers to it as the uh generalized coordinates of motion so it's sort of like the through these more sort of working more with more directly and explicitly with derivatives um over time so um yeah it's yeah I realized that that actually kind of shot by the time um but uh thanks for this uh opportunity to get into uh to sort of the deeper theoretics and and and really honing in on some of these equations uh you know some yeah it's sometimes folks with the here I'll I'll pause um also thanks for bearing with the this is my first time ever when I'm doing this purely solo so yeah um so I'll go ahead and stop recording and thank you everyone and see you at the next see you at the next one