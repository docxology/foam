hello and welcome everyone it's November 16th 2023 we're in acin live stream 55.2 and it's our second discussion with Magnus here on the message passing papers in this dot two we're going to begin with some code Explorations see where it goes and then in the last section have more open discussion and see where that goes so thank you for joining again Magnus and looking forward to seeing this code yeah thanks uh thanks for having me so yeah one of the things we we talked about last time um was that it would be nice to go through what all this Theory actually looks like in code so I brought along something that I prepared uh earlier which is basically the experiments uh that go with the paper papers and I think the one that makes the most sense to look at is the one where we do direct policy inference because that's sort of one of the really novel things where we don't encapsulate uh PRI algorithms we we actually get something new so that is uh what we hopefully have on screen right now all of this is implemented in Julia and it is all publicly available so like for anyone who's interested you can you can go and mess around with this on your own if you want but basically the model we're going to be looking at is the same discret pum DP that we all know and love but instead of uh Computing expected free energes and comparing them to finding our to find our policy we're just going to infer the policy in one go as part of optimization uh so would it make sense if I just go through it cell by cell or yeah maybe make the font a little bigger and and then go sell by sell yeah perfect thank you all right this better yeah sweet so all of this in Julia um and runs the notebooks there's um yeah instructions for for how to do it and the repo so the first thing we do is we just load a bunch of things that we need uh these here are all message passing rules for the transition mixture model the transition mixture model is what allows us to do policy difference it's basically saying instead of having one transition Matrix one B Matrix we have a number of candidate ones and then we have a variable that just picks one so let's say I have either one transition Matrix or another transition Matrix if my indicator variable is 1 Z I pick this one if it is 01 I pick this one that's sort of the logic here then we have this goal observation node which is a goal constrained observation node and this is where we do uh the GV based message passing so this is where we have the P substitutions going on that we talked about last time and where we get to get all our epistemics and then we have like a bunch of healthful functions to set up PRI for stuff whatever uh so what this code implements is the teamas experiment and again I think we all we all know how the teammates works but just for completeness sake uh actually I think I might be able to pull up the paper and show the graphic for the teamas because then we can see exactly what it is for modeling we can see how that corresponds to the graph and everything just give me two seconds I'll do that uh here we go so this is the teammate environment um and this is the reaper where you can find the code in the teammate environment the agent starts in one of four positions which Rel one and in it's designed to simulate a rat looking for cheese in a maze there's a reward at either positions two or three so either know one of these arms the main point is the agent doesn't know which arm has the um the reward at to learn that it needs to go to position four position four has a que that tells it whether the arm is uh the the r is likely to be to the left or to the right this means we need four transitions want to go to States 1 2 3 and four uh and then we need a um a likelihood Matrix that sort of maps to different observations that we can get I where are we at L location do we get a reward that kind of thing from all these uh four different locations so I think I actually have yeah so I can actually show the the constraint FFG of the model that we're building and it looks like this so what's going on here is that we have a vector D that parameterizes where we start that says this prior then we have this transition mixture node the transition just from a laden State at time T to time t plus one it has four candidate transition matrices again because we need to move to either one two three or four and then we we have this indicator variable uh that we call U and that just picks whether we go to 1 two 3 or four down here is where we have the uh to me the really interesting bit because this is where we perform this P substitution you can see here we have a meanfield factorization and a p substitution and we have uh this composite node this goal observation node with a goal prior so what this says is um do GV based optimization around this note and the goal is is given by this categorical down here so it's going to have some C Vector that parameterizes it this same kind of notation you're going to find in most of the papes and we're going to infer a two-step policy so we need to go from initial Point ZT to T plus1 to T plus2 think I should need to put T plus2 in here but no matter uh this does this model and this graph make sense perfect like the thing that I want to sort of point out is that we can see all these things where we do the E based things uh the kind of messages that we need to pass the factorizations all of this is something that we can see from the graph because this a constrainted FFG uh instead of a normal FFG so let's look at what this little guy looks like in code code uh and it looks like this so we have a categorical uh with a d Vector that starts we have C which is going to be our goal PRI we have our Laten States and we have our indicator variables each indicator variable gets a flat prior that's what this do yeah so for for all time steps I for all future time steps want to plan over we need an indicator variable with a flat prior so we are a priority indifferent as to which state we want to go to we have um a transition mixture that maps from oops Z at the previous time step so Z at the current time step G um and it takes us input the indicator variable or switch and our candidate the transition matrices we have the same thing we the next note which is this gold constraint observation node has a likelihood Matrix i e this oops a here and a goal uh prior which is the C here see see input from the state and the um the likelihood Matrix and this here is some um Plumbing that we need to uh get the messages to work out so don't don't worry too much about this and this is actually the the model specification this is all there is to it um which to me is kind of nice because if we can do inference in the back end then if I want to build mods and with design agents all I really they need should be able to should have to worry about um is to set up my model specification correctly all right uh right this is another a little bit of Plumbing um and these are like optional constraints that we are going to introduce later what this basically says is that we want to Point Mass constrain um the the switch variables the indicators so instead of saying um I'm sort of indifferent between going to two and three and put equal probability on them we're forcing the agent to make a choice we're saying you can only pick one and this is identical to having a point Mass constraint or a Delta constraint as described in part one that's what this thing does uh right so then let's set up everything we need we have a planning Horizon of two we want to run uh 10 INF iterations because this uh optimization procedure is an iterative thing uh that we also need to set some initial marginals like a VMP thing and we need to construct our a b c and d matrices uh which for those of you that had worked with pmtp or SPM or similar these work the exact same way uh and then all we need to do is call the inference function function with you know the model with all our parameters so theage model we just defined with our a b matrices our D vectors to start within our time Horizon C we set our goal prior as data inputs which is just a convenience thing we did to make life a little easier sort of programmatically um and then then We're Off to the Races so let's actually see what happens when we run this so if you just load everything should be up to date in a minute here yeah so let's uh create a model create our constraints configure our experiment and now we are here so we can run inference so do you have any guess as to uh how long long is this going to take 11 miston it's pretty accurate so it actually takes a little bit the first time because the model needs to be compiled now you can see we have new posteriors available for our switch variable our state variables and our initial State variable so let's see what we got so what we're printing here is just the um distribution the posterior Marg over the indicator variable I.E the probability of choosing go to one go to two go to three or go to four and what we get is uh this long number that we around to about 25% chance of going to four about 20% chance of going to either two or three and 35% chance of going to four that means quarter of the time I just as much as we put in our price we're staying put about 20% of the time the agent wants to go and explore either arm because I mean there's still reward there and the most likely option uh with a grand 35 uh probability is that we should go and investigate the que so go to position four if we then compare what that looks like at uh T equals 2 that is what does the agent believe it should do after the first time step so if it goes in this is the queue what should it do next here we see that it is quite unlikely to stay put uh sorry to go to um to the starting position doesn't probability there it's coming from the prior it is indifferent between going to either two or three because even though we know that we're going to get information at Q we still don't know whether it is going to be right or left and that's represented here by these two being you know equally valuable um however combined it means we have about a 60% chance of going to uh to an arm which is probably going to be the right one and then with about again the quarters about what we put into the prior probability we're going to go and look at the queue again and this is the same result yeah yeah so as you pointed out one of the big contributions here is this direct policy inference so let's just say that we have the probabilities of different state occupancies at time one and time two yeah but how do we really retain the fullness of a path for example um time one and time two might not be jointly independent so for example um yeah the probability of being at the Q in the at time one and time two might not be just. 35 time 026 no no so what what this is uh I think we can take a look at this see this actually was here uh like just just to clarify the question while you're typing it um if we're not explicitly tracing the branching Paths of specific sequences of states and rather we're getting over each time step then how do we know that just because something is 10% likely on here and 10% likely there but then that doesn't mean it's one in 100 together that those both happen it might be more or less depending on how the paths actually flow right so the thing is here we're inferring a distribution over possible futures um so we don't have any observations available we don't actually have a full action perception Lo we're just doing planning um and since we don't have any observations available the best we can do is get a distribution over all the different possible features that we have now we could do that but get that by like an exhaustive search say I want this and I say this this probability or we could just infer it so what this tells us uh is not like it's not a sophisticated scheme it doesn't have sort of the counterfactual thing going on what it tells us is that if we want to optimize um this combination of Bey and generalized free energy expected free energy uh before we get any observations what is the optimal distribution over paths that we could take okay so the t2 distribution is basically T2 distribution as viewed from t0 yes and then when we get to T1 we'll update but we're basically taking uh something like a mean field or an aggregate over all possible paths to T2 and those May radically shift like obviously once we go to the queue and find out some information then okay yeah yeah so this is this is at T equals zero this is the agent saying well if I have to make a decision right now for the whole game for all my time steps for my whole plan what is sort of the the most likely thing that I should do and how should that distribute itself once we actually start to get observations in there um then then yeah these are obviously going to shift yeah then it's very notable that the two arms do have this High quantity taken together but absolutely equal it's and that's that kind of like being precise about uncertainty that we would want such a planner to have yeah exactly what what this is saying is the agent is really confident that it needs to go to one of the arms but because it hasn't got an observation because it hasn't actually seen the queue yet it doesn't know which one so it assigns them equal probability but it is pretty I'm sure that it needs to go to one of them which again makes sense if we think of this as doing planning at T equals zero yeah it's it's really cool like especially after so much expected free energy branching time active inference exhausted searches just to see this kind of non path-based direct policy inference that's the contribution of the paper yeah I mean I would say this is closer to a path-based formulation what you don't get is a search tree thank you thank you yes consistent with the path-based formulation of basing mechanics but not necessarily resulting in an exhaustive search tree exactly so what what we can actually do here is we can look at the schedule what's going on here so this this actually shows us the exact uh way that messages are flowing under the hood in the code and the main thing that's different to most other approaches is that we have things flowing backwards we have these red guys here and we have these here flowing in the opposite direction so this information flowing from what we think the future is going to be towards what the present is and what this is telling us um is given that I want to end up here and I reent backwards what should I do and that is why we don't need a search street because instead of having to run forwards we can do the forward sweep here say well this is my distribution over things that I'm going to do if I don't know anything and in the other direction we have well this is distribution over what you should do a why expect to end up and if you have where you expect to be and where you expect to end up figuring out what to do is is like what what what happens here and that's how you do the the policy it's because you have an idea of where you're going to be and you have an idea of where you want to end up if you know those then inferring what you should do is at least in these simple cases quite straightforward and again sort of the one of the contributions here is that because we do this backwards smoothing sweep we don't have to do a a forward search tree because around usually these notes all we have to do is care about we care about a message flowing in Here We Care message flowing in here and then we can do our inference it also means that every time we add a time step if I were to take uh this wait this guy here and extend it once more to the right I wouldn't have to like an exponential explosion I would have to evaluate the messages flowing around inside one of these sort of slices once more so we need another backwards message another backwards message another forwards message and that's sort of where um one of the things I want to do that we didn't get around to doing for these papers is to investigate how this behaves when we scale to longer Horizons and larger models because this should scale uh much nicer than having to do an exhaustive search treat because every time we add um another slice we just add the same number of computations again right so let's go back here right so what we do here in the last experiment uh is that we add these Point Mass constraints to the switch variables so what does this yeah so on a graph it looks like this this the exact same graph but now we add these little constraints here these little delta form constraints in the beads and this means that when we compute our posterior here it has to to be a Delta so we need to pick one of the options we can't say I'm indifferent between going right or left you have to pick one because you have to be a Delta Spike so in practice this ends up being the same as taking the agmax of um the distribution over over actions so you're effectively as part of your inference procedure picking the best policy and doing inference only with the most like thing so let's run that and again this gives us a much sort of clearer picture where we are now 100% certain that we're going to go to the Q at T equals one before we observe anything and then in this case it picks two but if you change the random seat a little bit it picks three because it's it's IND different between these two guys it just has to pick one because we're enforcing that yeah it's it's still at t0 speculating it and so it's kind of like a a bifurcation where depending on its random seed at t0 it might symmetry break to imagine it going left or imagine going right in that implementation but either way after T1 it's going to have the empirical decision exactly I might actually be able to show this as well if we do this so part two actually has an example of the kind of protocol where we have these this is the thing I want to show we have these updates going on this this one reminded me this one reminded me of the fire escape plan of a hotel I didn't think of that but I can actually see it so effec what's different between these experiments and the ones we just looked at is um that these U's here are clamped the little black squares and P that means we are picking a transition Matrix so we are fixing whatever our transition should be and this here T equals 1 is before uh the agent observes anything so this effectively the same situation we just looked that except now we are evaluating a policy we're doing the same search tree kind of thing uh instead of doing direct policy inference so again these are just Transitions and these appear fixed so we're no longer doing direct policy inference we're just uh Computing the uh free energy including GV terms for this model without any observations and the thing I want to sort of point out is these composite noes down here is that the exact same that we just had um but you see there's nothing going on here like these X variables have nothing on them but once we get an observation it's the same as adding this constraint down here so we advance the time step it goes from one to two and then we clamp our observation and then we do the same protocol again because the model hasn't changed what's changed is that we constrain one of the variables to be equal to our observation and then you keep on going T equals 2 and so on and so forth every time you get a new observation you just add these little uh constraints to the model this and then you'll get yeah this reminds me of scientific pre-registration like we have the whole experimental plan laid out we know what kind of information is going to flow in and we have a distrib distribution or prior over the data and then when the data do come in we just clamp the format that we expected and we don't need to do any structural changing to the factor graph and so it's like as we're moving forward and collecting data we're just clamping timelines on the past and then leaving open what hasn't happen slash what we don't know yeah that that's exactly what's happening um like beautifully but because ex that's exactly what's happening as we get data but just saying well now I know that this variable in my graph that corresponds to my observation I didn't know what it was before but now I know that it was this value so I need to put that observation into my my inference problem somehow my model is still the same doesn't change the structure of how I think the world works like parameters might have shifted and stuff but like the main thing that's changed is that I now have a data point that I need to add in and you can do that by adding these little um yeah data constraints we call them so this here actually shows the exact type of thing that you were uh just talking about how does the model change once we get data points so this case here would be pure sort of speculation pure planning ahead of time and this here is well once you start observing stuff how do you actually add the information into your uh your inference problem yeah one other angle on this again because I think this is really important the structure here is kind of like the Timeless action perception Loop and then we can be anywhere from at the top we have purely speculative purely anticipatory on the bottom we have purely retrospective all the data are already in hand and it's kind of like a solved issue like looking at a a something that already played out or you could be kind of the inmedia res in the middle between which is basically just to say that part of it is already clamped in with the past and then part of it is speculative and all of those settings from like the purely anticipatory to the purely retrospective to in the middle of things with a little bit of both all those are structurally described by the same formalism yeah um and again that's because the model doesn't change what changes is just we need to add in the data now what what is kind of nice about this is that it also allow you to talk about doing different things at different times um so one thing that we for instance looked at is once we have all the data should we do our parameter updates then because you can imagine a situation where you run an episode of something that you are trying to do some psychophysics uh experiment and you have your uh participant run you know an entire episode of an experiment and then you expect them to go and update their beliefs about say uh some parameters for what occurs where you can model that by saying one once all my observations are clamed then I send messages toward my parameters my thetas down here so this sort of highlights the the kind of things that you can start to be very very explicit about when do you do certain updates what should the graph the constraints look like um all these things that um kind of often has a tendency to sort of get swept under the rock a little bit that you can now be like very very explicit about yeah that's super important again keeping with this theme of kind of like looking forward Looking Back Being able to plan out the message passing Logistics and scheduling means that you can have a sense of the computational requirements and so on forward-looking and then also looking back have a better sense of the way that variables were generated and Trace back their kind of informational supply chain so that this is that's what makes it fully synthetic if it were fully synthetic chemical we could trace from the precursors how it were generated this is fully synthetic active inference agent and so that means for every variable we should be able to say well here's how we brought it onto the table and then here's exactly how the messages were passed from once it was placed yeah like what what is the model what is the inference problem what is the data and what are the constraints that's that's like we should have everything be completely transparent um because that's that's in my opinion that's good uh the more you know we're allow scrutiny of all the little different bits and pieces and the more we can investigate how everything is going on um like the the better and this is kind of the ground floor like you could keep digging and ask well how is addition carried out how is multiplication carried out in Julia or how however but this is kind of the layer that is the foundation layer for building then more elaborate graphs like we don't need to finer scale or do we that that's actually kind of funny right because each of these messages we're drawing those just as arrows but each of these messages uh is a computation right you mentioned that you need to do a multiplication like one thing we need to do for these models we also do for for this thing here is we need to multiply um a categorical variable by a uh transition Matrix that is an INF that is an operation that is a message so there's a whole computation and a whole derivation that you can go and and find uh for how to do that operation so everything is uh should be completely transparent here yeah all right I'll ask a question from the live chat Andrew writes okay if the model is CL stop the screen share oh no it's fine it's fine I'll just ask it here yeah if the model is clamped in this manner how does the generative model evolve is there a way that new variables can be introduced during the inference generative Loop right so that's a thing that's actually quite uh interesting and the short answer is not with this uh formalism there is some like nice work on structure learning and how to like do Bas model comparison Bas model reduction and expansion that you could use in in this context but as is what we've done is we've found a way to write down a model and an inference problem and the the corresponding messages that allow you to do um GF based optimization it doesn't tell you how to expand your model now again uh if you want to combine this with something like Bas and model reduction or expansion uh that would be cool I think someone should should really go and do that because that would be an interesting sort of Avenue to go down when we can start to combine these things and one of the sort of things that I wanted out of these uh these two papers is that I wanted to be able to get this kind of thing explicit right I wanted to to have a language that um is not too difficult to pick up that allows people to ask these kinds of questions because there's a very different um at least in in my mind I don't know whether you agree Andrew but to me there's sort of a different vibe to philosophically thinking about what does the model evolve like and saying well how do I add a node one is a very concrete problem that you can start to talk about while the other can be very sort of can end up being very very abstract if you if you let it I like things that are concrete and and transparent so I'm really glad that this kind of question pops up because that's one of the things that I wanted to to do with these papers is to get people thinking along these lines awesome I I'll kind of give another angle on that question so there's a implicit or unprincipled or ad hoc strategy here which would be we're going to be operating our model and then we're going to have some extracurricular diagnostic like if it's taking too long we're going to um then we're going to do something else outside of the generative model and change a generative model like oh maybe we should add another variable so that's kind of like the nonpr principled approach of which there's an infinite amount of these types and they may or may not be documented in the paper whereas the principled approach would be to bring that structural learning that learning structure explicitly in through basian structuring so then you could say okay well there's either one two or three factors in this regression and then we're going to have a distribution prior over whether there's one two or three factors and then we're going to claim lamp eventually and find the best fitting model whether it is 1 2 or three with the appropriate um penalization and then someone says well what if there's four it's like great now we're going to explicitly bring that possibility into the model so that we have a unified approach to talking about the kind of sensory motor lower level parameters and arbitrary nestings of structure learning those shouldn't be outside the perimeter of the fully synthetic agent otherwise to the extent that it's outside the perimeter the engineering perimeter it's out of control yeah I think we we at least um I think we should be a little bit wary about what we uh attribute to the agent and what we attribute to the engineer here because when we set this this approach up an agent is just you know a graph where that that you receives some observations and allowed to emit some actions um so there is still an engineering aspect here about how do you construct the graph what should the graph like what should the model look like um which is not addressed in these papes but you absolutely right there should be a a principled way of doing that which something like Bas model comparison um is is a way to do um but the agent I the graph that does inference just does inference on the graph at least with the tools that are in here you would need to add some something more to do like organically growing graph or something does the graph ever undergo structural Evolution or does the structure of the graph change but then through clamping and parameter updating the graph can even with a fixed topology represent structural differences or do are there ever actual structural changes in the factor graph so I think that's one of the nice um things about the the CFG approach to writing down models is that you can write like you need two things to do to minimize free energy function you need a p and a q that's what you need to write at um and writing p is done with standard Factor graph notation so you write out your boxes and your edges and sort of you construct your graph your model and the constraints and the done in Q that is um you write them separately so you can talk about what happens in terms of constraints and what happens in terms of the structure of the graph sort of separately and explicitly um so the graph uh at least what we've done here doesn't uh sort of intrinsically undergo any um like topological iCal structural changes it's just the model you write down that you as as an engineer as a scientist is interested in uh what you can change is you can change the constraints that allows you to to do different things with the same model for instance incorporate data points as your agent sort of progresses through time um but what you can do then with this type of notation is you can be explicit you can be explicit about when you're doing something with the constraints of the model on the model sorry constraints on the inference problem I should say where you're defining your q and when you're doing something structurally with the generator model when you're defining your P because one is done in terms of this like bead overlay notation and the other one is done with standard sort of factor graph methodologies um so what what in that context what's the the main advantage of these two papers is is that you can be explicit about what you're doing yes very interesting yeah I hope so continue with the code here yeah uh yeah I mean we've actually reached the end of The Notebook uh this is all that is to the policy inference experiments but one of the things that you can do with this type of thing if we go back to the generative model specification um is that this here uh is just defined in terms of noes and edges I I have my variables and I have my notes that Define them and I have my uh goal observation where do the P substitutions and everything um but these units like they're sort of atomic in nature all a node cares about is Zone Mark of blanket so these things are composable meaning that if you want to build them with say hierarchies or heterarchies you want to add multiple modalities all these kinds of things you could might want to do um that still works s of with a caveat that you know there might be some coding bugs and stuff you run into because it's still just still software after all but theoretically once you have these different things as little building blocks you can sort of Lego your way through to any model something that we hint at in the papers as well that we um they're not really sort of explored in the in the experiments which is that this type of uh active inference works for free form generative models that means because it's local all the little changes we do are local on the graph everything else can just be whatever it wants to be so so you're no longer limited to using a pomdp model which is an awesome model like there's loads of things you can do with it it's a great model um but there are things that might be easier to do like operate with continuous variables for instance uh that might be easier to do with a different type of model and if you can sort of Lego block your way to a generative model that uh represents how you think uh your agent should think that the world operates and that is that is something you can now do because model model sort of space has become much much larger right there with the how you think an agent should think is the articulation between the engineer and the agent that's the second level inference problem the first level inference problem is what the synthetic artifact will be doing in its runtime and then the engineering question is how to create that and so it's almost like the engineer is only interfacing with the inference challenge or environment through the blanket of the fully synthetic agent yeah and that's in my opinion that's how it it should be I guess if I put on my my engineer hat for a um I'm a lace engineer like I I can go to if I have a problem to solve I could go to the literature and I could find you know 57 Solutions and I could pick one and I can implement it three a bit and now I have 58 Solutions that's one way I can do it I have several solutions for every problem that becomes unwieldy real quick because every problem you have like bat load of solutions um what I like to do is say I want only one tool I can only do inference I believe that everything can be solved by inference if you set up the problem right and the trick then is how do you set up the problem right so as an engineer what I would sort of take from this um way of of approaching approaching problems is that my task is to build a generative model that does the thing that I wanted to do and there's going to be some iterations and trial and error and some tweaking of things and all these nice sort of practical challenges that that we all uh run into when we try to actually build things um but the tool should always be the same if I want to control a robot or um track an object uh or localization or preference learning or filter an audio signal or whatever you might want to do the tool should always be the same all I should should need to do is do inference it's my job as the engineer to figure out what is the model structure um that produces the the results that I want because it's clear if I want to say move a robot arm and I have a you know a generative model that can only move to the right I'm going to run into issues now again you sort of also hint that there's an an extra level that you can go to uh where it becomes about structure learning now you you want to learn the structure of the model you want to learn P you want to sort of build your graph organically um which would be amazing to solve but uh I'm not going to claim that that we're anywhere near there with these papers these papers is just a way to accurately write down what I as the engineer want to happen is there any more code you want to show um we can take a look at the other ones uh but they they basically work the same way so this is for um generalized free energy with an action perception Loop I.E and and a fixed policy this simulates the kind of things you would get in something like pmtp usbm excuse me so again the um setup here is very very similar Define our model which has um a policy that we want to evaluate uh it has two time steps has our goal priors it has our initial State prior um but now we're also interested in learning the parameters of our observation Matrix so we put a prior a durly prior over our a matrix over our likelihood Matrix and otherwise these here move like they they look very much the same it's instead of mixture models over transitions it's just fixed transitions uh and this here is calling the exact same um sort of P substituted mean field constraint all that goodness node that we used for the last uh like the direct policy INF simulations uh and then we do the same thing we load up our PRI we load up our matrices and because we want to repeat this over and over and over and over we're actually going to run an agent for 100 episode so it's going to play the game a 100 times and try to learn the um the likelihood Matrix and does so by inferring an action as doing infin missing in the model taking an action executing it in the environment here and here and then getting back an observation and this getting an observation does this clamping operation that we talked about so this is literally the thing that's shown here T equals z t equals T 1 tals 2 and in this case T equals 3 because that's how many steps is in the T environment sorry and if we do that and we run this uh 100 times oh yeah so every time the agent has completed an episode so it's done it's it's two steps it gets to send messages towards the parameters a it gets to learn about the uh the a matrix the likelihood of the um of the observations given the states and if we do that we can look at the differences from the sort of very flat prior that it gets initially uh versus the end posterior PRI end posteriors to look at what has this agent learned the way to sort of read read this plot um is that o refers to the origin location so location one in the uh like in this thing here oops Yeah location one in this thing here it's the origin we have the left arm and the right arm L and R and we have C the Q location RW means reward and you get a reward NR means no reward and this here says is the reward to the left the reward to the right um Q points left Q points right what we can see here is that nothing really happens in the origin State we always moving so we don't really get much um we can see if we on the left arm and the Q points to the left and the reward is also on the left arm we are pretty likely to observe a reward if the reward is on the left we're likely to observe a reward if go to the left same thing over here the reward is on the right pretty likely to observe a reward on the right the interesting thing up here uh so what happens at the Q location right because this says that if the Q points to the left the reward is also pretty likely to be on the left if the Q points to the right the reward is also pretty lik to be on the right so this shows us that if we do this iteration the agent can actually learn um the like a pretty good we're going to say the correct but like a really good um likelihood Matrix that predicts where things are and how all the relations actually work in the tmas experiment which is nice because that means that we uh we don't have to put all this in we can actually do parameter learning using this GV based message pausing as well which is really neat and here we have some uh free energy curves so this shows the free energy at the initial time step so before any observations have been received all right yeah and when the uh first observation has been received and then after the second observation has been received and we do parameter learning and you'll see everything get like nice and decreasing but there are these spikes here and these spikes indicate when the agent predicts a win but gets a loss so the agent the Q says the uh rewards to the left the agent goes to the right there to the left but it doesn't get a reward there's a bit of stochasticity in the teammates environment um so it's not always 100% sure even if you que points the right way and everything that the agent actually gets a reward observation in the end and this to me is is pretty nice because it shows that if um the agent's predictions are violated we can actually directly see that in the in the free energy of the model we get these sort of prediction error spikes which I think is is really nice um we have a similar setup which completely identical except we don't do the S A P substitution we don't introduce a gve term we just run Bo standard uh rational Bey free energy minimization and the model looks the same we just uh use like some Bey options for uh this composite node what is meta So Meta is a way this all done in RX and fur meta is a way to pass options to a node um and in this case the Bey meta tells um this sort of joint node composed of a likelihood model and a goal prior whether it should uh optimize generalized free energy or battery free energy so you can think of it as if we look at the graph again and if you had this note here to make this a little bigger we have this sort of composite note here inside the dash lines there's a difference between if we draw a circle here or we draw a square square would indicated we do a p substitution which turns this locally into a generalized free energy if we don't do that we draw a circle don't do the P substitution what we're left with is a mean field factorized Bethy free energy and the way we do uh this s switch between a circle and a square is through this meta object you also use it for other things for other nodes and arer like if you have um say a nonlinear function that you want to use somewhere in your graph you want to pass messages through it again need to do some approximations most of the time so you can say pass your approximation method uh through the meta object and if you know run some iterations or you need to do some sampling or something that kind of information is something you can feed in through the meta object so what this thing is really saying is just swap uh like the square for a circle so that we do bir free energy instead of generalized free energy and then everything else here is exactly the same this is literally a copy paste of the the other notebook because again we're not changing the model we're just messing with the constru the model is the same all we're doing is messing with the constraints and the P substitution and if we instead of saving the figures actually create these yeah you'll see this agent only gets uh stuff like around the the outcome because it starts with uninformative uh PRI over the a matrix doesn't really know what the reward is there's no incentive to go and explore it just doesn't really accomplish much you can see the same thing free energy is flatlining nothing is happening and it is consistently not performing well it is uh it just endured a 100 trials of loss and I don't think it is particularly enjoying itself and again so the the thing that we can illustrate with this um is the power of messing with uh constraints and messing with P substitutions and messing with these sort of local adaptations to the free energy because the model in both cases the P the graph is actually the same there's no difference the thing that's different here is we're messing with the constraints and we are messing with uh turning this local Bey free energy into a local generalized free energy and that produces these quite drastic changes in in the behavior of the agent yes the Bey free energy agent does not do well here just changing that one yeah aspect moves the epistemic phenomena into the present that's that kind of like EF like component of the GFE that we discussed last time yeah exactly so what what this um thing does basically if you if you do the B free energy uh you end up with an agent that does kale control this ends up being a completely standard uh control problem that controls inference there's nothing nothing fancy going on here um and crucially that does not include an epistemic component we get no information seeking and we see that here there's no information seeking anywhere here there little guys just flatlining um but once we start to mess with the function that we introduce as epistemic component we get exploration we get good performance we get this active curiosity information seeking all these nice goodies that we know and love about active inference agents and we can show that very very directly by simply saying well I'm going to change this little circle to a square and then we have uh some aggregate experiments that we also ran which is just repeating the same thing over like numbers of Agents because there's some Stu as this to uh sometimes they get reward sometimes they don't if you want to compare um yeah how this does sort of in the aggregate you need to do multiple runs I'm not going to run this because this takes a long time to run you see the I ran out earlier today and it took like an hour and 20 minutes um but what we can do is we can look at the distribution over Wednesday how many times per run does an agent obtain uh a reward and you can see that it is very much sort of skewed to the right which is good that means the agents learn they learn the parameters and after they learn the parameters they consistantly get uh rewarded on their uh on their trials doesn't always happen right there a bit of sort of tail end down here what this tells us is that sometimes these agents um because there STC casity in the rewards they're just going to get stuck in some unfortunate Minima they get some conflicting information and they never quite managed to recover and that that can happen sometimes you learn something that's wrong and you sort of end up sticking to it uh so it's it's not a bulletproof thing uh the same thing here we can show this is sort of the the ideal uh expected reward you can get that as the trials progress an average agent starts learning earning more and more and more and more reward how you get closer and closer to this line there's some wiggling in here that's because we have to use a sampling approximation when we send the message towards the parameters so there's some stochasticity in here which is why we get this bit of wiggling that and and the reward stoas this basically shows that as an aggregate um agents get better they get consistently better but again the model is all the same nothing changes cool good with a code yeah I don't I don't have any more code uh not for the for these paps at least cool yes thank you then so I'll stop the screen [Music] share okay I'll um share screen um I guess just to sort of transition into this second component like when you sit down at the table where do you begin um usually I begin where I left off the day before uh that's worked for uh for quite a while now and so there are other things that I'm working on right now um that I I can't really talk about yet but just as a general a general um building your own generative model like if you had a new fun nonproprietary study where would you begin with Sketch in out what to be on the road to to working with it this way right so if oh now get yeah so the first thing I would do is that I would start drawing graphs I would start drawing my my constrainted fgs and see if I can figure out what is the problem that I want to solve and then get that real accurate and that is not a trivial exercise um because once you have your model specification your inference specification in place then like the hard to me the hard part is is done because again if I put on my engineer hat the main thing that I have to do if I want to build an application I want to solve something is that I have to come up with the graph and I have to uh you know come up with the constraints that Define my inference uh problem once I'm there I need to figure out are there any parts of this problem that I can can solve is there a message that I need to work out is there uh some approximation that I need to figure out to to put in there um and if so it's nice that's sort of a like a research question on its own quite often um but if there isn't then the next thing I would do is I would try and and run inference in my model if she does this model actually behave as I think it does especially if you're in the game of Designing active inference agents uh sometimes these things have a mind of their own uh there several reasons why that could be um it maybe your intuitions about like the thing that you wrote down uh don't actually quite hold up to to how things manifest in the real world or or there might be like bugs in your code all these sort of very practical things but once we're at this place um like to me a lot of the heavy lifting is done because the hard part if you want to build to me if I want to build something is specifying the model specifying the constraints and figuring out to solve the inference problem once I have that then everything else is sort of implementation details um actually funny story we had this during uh the design of the experiments uh that we just looked at so if you look at the transition matrices that we use uh if you have a the original teammates transition matrices they let the agent get stuck in positions two and three that's sort of attracting stat once you go there you can't go anywhere else and the way that is done practically is that the transition Matrix sends you from state two to state two whatever transition Matrix you CH you choose if you do that with our setup that means you're allowed to send backwards messages uh that say well if I pick action one which is no go to state one and I'm coming from state two I would also go to state two does that make sense so then what's the issue or what did that result in um the result is that the agent starts inferring some very very nonsensical policies um because it'll say well I know the goal is at position three but I'm going to confidentally go like pick the Matrix that corresponds to go to one or go to four go to like two and that's because all these things all these matrices allow the reverse mapping from say uh state two you think the reward is to any of the other states why is there no reverse constraint so I can put this because if you're reasoning backwards you have to think about how do your model run in reverse and what does your transition mean mean the other direction and the way we solved this was that we send uh we let every sort of invalid transition send it back to the beginning so every um yeah basically if you end up in one of the the rewarding arms uh every transition that you pick is going to send you back to the STS it's just going to reset the whole episode and if you do that you get rid of this problem of the backward message not corresponding to your intuitions as as an engineer because we're designed this thing to say well one Matrix is go to one one Matrix go to two and so on so forth but the math doesn't really care about our intuitions the math is is like does what the math does just it's up to us to make sure that we VIP things down accurately okay just to kind of confirm this so the initial strategy was you used as kind of classical in pdps you just had the state of reward be absorbing however it was almost like you could get in but then you didn't want it to leave with the absorbing state but you actually you had like a ladder out with the backwards message and that was leading to some weird things so instead you change it so that the reward State brought you back to the beginning essentially tucking in the end of the trial to something like the beginning of the next trial and then that better cohered with intuitions around how policy should be selected so you can think of it also as the backwards message saying um if I go here where should I come from so if you are in say the rewarding arm where should you be coming from and if you allow the rewarding arm to map from itself to itself because you make it absorbing the agent can correctly that's what you put in infer that it should be coming from the the rewarding arm when it is in the rewarding arm yeah how do you how do you get a million dollars step one start with a million dollars exactly and that that's not the kind of inference that we we want our agents to make it's right if you have a million dollars the most likely way you got them is that you had them a second before you had more a second before at at least a million dollars a second before but yeah that that that kind of inflence is something that we want our agents to to not do H and that's an issue that doesn't at least for now sort of research hadn't arised did not arise when we only did the sort of forwards roll out version because if you never Reon backwards this kind of thing just doesn't pop up but if it starts then you need to consider this this kind of sort of circular uh step one start with a million dollars type of reasoning is that an example of something that that we ran into in the course of this experiment where we were pretty sure that you know everything was sort of set up correct and it was but the agent didn't do like did not behave in the way we anticipated and sort of how we wanted it to behave so we had to go and look at what what is actually happening not what do we think is happening what is actually happening when you sort of start to look at the code let me ask a related question from the chat Ander says what you are saying is there's no conditional probability placed on where the agent thinks it is when doing that reverse message uh is when I think is the question is there a conditional probability placed on the agent when it is doing the reverse message yes but it was conditioned on the future you're doing everything backwards so it is condition sort of conditional on where you should be in the future you can think of sort of the the forward roll out approach saying well if I am here and I do this then I go here and so on and so forth then you have this large tree that sort of expands to uh where you could go and then you look at all the places you could go and say well that one that's the one I like you can also think of doing the same I'm sort of um um approximating a little bit because we don't actually do so indries but you also think of doing the same thing in Reverse right you can say well if I am at if I have already obtained everything that I want in my life where is it most likely that I came from and I say all right if I go one step back where is it then most likely that I came from and so on and so forth and that distribution that kind of logic uh is what we're doing with the backwards paths so then when you have the forwards path saying well this is where I'm likely to end up the backwards path saying well this is where I want to go resing that difference is uh is what you do when you infer the action you have two messages colliding and they spit out something towards your um your action variable yeah okay well that's making me think about is we classically think about new data coming in we're conditioning on the the existing conditions and then new observation comes in things update so that's the sort of March forward then there there's this kind of aspirational or fulfillment based conditioning conditioning on not what has been sedimented and brought forward but conditioning on what is anticipated or preferred and then yeah let's think about the the um three time step case where there's a clear last step and there's a clear first step and they just meet in the middle and it just like the sort of um the conditioning from both ends results in just this obvious solution however also it could be more ambiguous at the beginning of the end or it could be more than three time steps and then that's where we get this um message passing and kind of annealing as data are getting clamped through time and that has a relationship with the search tree based methods in that like the territory of what they're explaining is the same sequences of action but again to be really clear this is not doing policy specific evaluations followed by comparison of expected free energies or any kind of enumerative search at all it's solving one direct policy inference problem as this kind of annealing process so I think the beauty of this approach is that you can do both I like to do Direct policy inference because I think it's it's to me it's it feels like the right way to go but one of the things we do in part one and to extent demonstrate the experiments in part two is that you can recover like these forward rollouts uh these classical algorithms are special cases by only passing messages forwards because when you have these P substituted uh goal observation nodes with your mean field constraints and everything the energy terms of those nodes ends up being equal to one that's what we talked about last time and to being equal to one term in uh the EF of a matching policy so you can reproduce like the forward roll out method by saying I'm only going to pass messages in the forward Direction I'm going to fix all my action variables and then I'm going to compare all the energy terms of these little GFE nodes and that'll reproduce you know the exact algorithm that we've all been using for for years you can also do is you can add a backwards pass what you can also do is you can say well now I want to infer my policy and so on and so forth but because this is a toolbox um it's just a thing like it's just a way of thinking that allows you to do different things um so the policy inference is not inherent it's something that you can do uh if if you want because it's just an inference problem if you don't want to and you have good reasons for doing something else um you can go and do something else um but sort of to to row back a little bit to me this idea of reasoning back Wass also works with sort of my intuitions on on at least how I as a person often operate like I'll have some idea in mind I'll have some goal that I want to achieve uh which could be you know um I want to get a PhD or I want to make a taste of dinner or something and then I reason backwards well I need to do this I need to do this I need to go and buy all the things that I need I need to set aside time for cooking and so on so forth there's sort of a backwards reasoning that if I want to go here I need to do this this this this this this and then there's the forward planning all right if I am walking out the door I need to go to there and then there and if I keep walking I'll end up in the right place and so there's both components to it if you want to put sort of a more anthropomorphic uh intuitive spin on it um in practice uh what this ends up being is just inference on the graph and if you're an engineer it's up to you to design uh you know your model and your inference procedure in a way that you end up with the results that that you're looking for yeah this about the policy selection here the forward pass policy is basically habit coming forward because in the absence of any aspiration or preference policy is just well there's various ways you can select policy still you could select maximum Information Gain you could select different things but the prior on policy it the E Vector is happen so it's kind of like that's the starting position for what actions are selected from now and then it's just really interesting to think about this forward and backward solving which is of of course a classic technique but again being done in this way where you could recover research but it isn't Tre search so the the the the thing you would recover is a trajectory so an evaluation given a particular policy the research uh comes about when you want to try and expand this and be smart about uh how you uh compute things for different given different policies because obviously you can do that for every single policy one at a time and evaluate everything but that's even more inefficient um so what you do practically is every time you sort of reach a branching point you need to select a new policy you say well I'm just going to take my prediction here of where I would be and I'm going to Branch off into all the different um know policies that I could pursue and that'll give me sort of a new set of of branches on my tree and then you do the same thing as so that's how the search tree comes about but each sort of path through that search tree would correspond to a forward roll out on one of these models yeah you said inference is something that you can do I want I I want to go a little bit back um because you mentioned this thing about the EV Vector um which I actually think is is really really interesting I don't know since you have the screen share I don't know if you have the the papers available which papers oh of these your two yeah yeah do you have a part one yep all right can you go to page 33 of course thank you here this is the one one so if you look here um we actually have an e Vector sitting right on top uh as a parameter for the um indicator variable and what we're saying here is just we're literally putting a prior on uh which policy like which action to pick at each s stop but that's that just comes about because we're building it out of these little graphical building blocks so if you wanted to you could put a prior on E and then you could learn e that that's absolutely something you could do or you can say have um a shared e with its own Dynamics so now your habits are time dependent and that'll be another sort of thing that moves along at the top with its own stat based langage everything these are the kind of things that you can start to do um we used flat PRI here but you could just as well say I'm gonna sort of intrinsically have my agent be really really biased to go for the goal whatever it checks uh we can even say I it's going to prefer the left mode goal more than anything in the world and that that is something you can do but what you can do here is you can be explicit about it exactly how your habits work how your e works um practice the sort of semantics intuition you can attach to it is the same as they they've always been but you can start to look at well I let look on my graph what is the thing that I'm updating what does this like actually sort of specifically concretely mean and you can draw it and if you want wanted to do something else you can you could for instance imagine that you have hyper PRI for E set by another agent that just mean you extend the graph and maybe that agent has its own observations and so on and so forth so then you need to attach that to the edge that is currently clamed with E and now you're having an agent that sort of influences prior preferences over policies or habits if you want to say well now these need to update at different rates that's you having to mess with the schedule in which you pass pass uh messages sometimes you pass more than like one agent than you do in the other but these kind of things can be explicit now that's to me one of the the big sort of contributions that we wanted to put forward with this paper is you can now put all of this sort of directly on the page unambiguously what exactly is happening you notice for instance here we have different e vectors for e a t and e t plus1 that's also a choice you could for instance have these be linked together and say they have to share the same thing um and that same thing now has a prior that that's something you could do and that would be a completely valid graph and you could figure out how to do inference in that so like the idea of opening up free form models means that um model structure it doesn't have to be like a b c d uh e FG and so on so forth we can do whatever model we want like the space is has opened up uh for uh yeah things we can try out with with active inference all right I got a little bit of a challenging question nice you're talking about active inference and the toolbox and that sometimes is seemingly not what people expect they come come to active inference or the free energy principle looking for an account of a human experience or of a certain cognitive system yeah which of course isn't to be found in the toolbox you know any more than the blueprint or the house itself is to be found in the toolbox so could you from your view going back as deep into history as you like can you give an account of the AC of inference toolbox what tools have been brought in when which tools were versioned how which tools were deprecated how like when would you trace the active inference toolbox coming to existence with what just to get a little bit of a runup to the state of the toolbox today I think pretty much anyone you're going to ask is going to have a different answer to that there are some sort of big fixed entities that we can all look at the know the big one of course being SPM um which has been the go-to tool for many many years and in many ways still is because it is sort of the the battle tested version with all the bills and whistles that Carl himself has in large Parts written like that would be sort of my my my um proposal for a a grandfather grandmother software package for how to do active infuence this should be the gold standard in my opinion um later on we've had uh p and DP which was uh largely implemented by by Connor Hines and Al chance and a couple of other collaborators which is a great project uh I've used it uh it's awesome which is a python implementation of many of the routines that exist in SPM um so this sort of brings many of the tools to python instead of mlab which is a lot more access a lot more popular these days um and that will sort of to my mind be the two big active inference toolboxes in bias lab we have uh RX infer which is a general purpose message passing toolbox so it is not specifically an active inference toolbox you can use it to do inference and you can use it to do a lot of things that we would like to do with for active inference but it is a a message passing sort a Bas inference engine first before it is a of specialized active inference tool box which is great uh because then you can use it to build your active inference uh Parts uh like we did with this uh a constraint goal observation node that we used for the uh the simulations we just looked at that's using mostly stuff that is an RX and fur and then one little node that we uh added on that does this GF based thing I think what what to my mind is um the more interesting question is the types of algorithms that have been employed because again and I'm I'm going to miss some here and for everyone that I don't mentioned uh I'm sorry uh but like from the original sort of inception in 2015 of the first active INF routines with the sort of forward search tree uh through to um something like branching time active inference or sophisticated inference or there been a lot of really really interesting work with monal Tre search uh and the whole sort of deep active inference field that I haven't touched on at all in in any of these discussions because it's it's a again it's a different approach um there sort been an explosion of really really interesting algorithmic work uh on how to to do this how to solve the the active inference problem in a smart and interesting way and the things that we introduce here are sort of more in that tradition where we're saying uh instead of trying to find ways to be clever about the search tree uh or trying to sort of like the sophisticated inference for for one is is a really really interesting idea but it's yeah it's a different thing instead of be clear about the search tree we want to introduce this backwards path so that we can solve everything as part of the the inference problem um like parts of this is also something you can find in is actually sort of an interesting thing as well that's mentioned in in the papers if you go and look at uh par and friston's 2019 paper where they introduced the generalized free energy uh everything is still conditioned on a policy but the the message passing schedule the forwards backwards thing is actually present and the update rules are based on generalize free energy so they end up looking a lot like the update rules that we derive just quite interesting you can sort of see uh the seats of what we based this work on quite clearly like you can you can look at the equation side by side um which to my mind is really really interesting because that was sort of the the inspiration like you have these update rules you have this forwards backwards thing what if we try and do this but we come at it from uh the point of view of doing message passing on graphs so I agree this is a big question and I'm not sure I can do it justice because there's so there's been so much interesting work on um software and algorithms and simulations for active inference that whatever uh Parts I bring up I'm bound to have missed something um but if you if you put me on the spot these are some of the things I would highlight I would highlight SPM I would highlight pmdp and later on uh know like the message passing tool box boxes from uh from bias lab and algorithmically I would uh highlight the original algorithm along with all the things we have here like branching time onalo sophisticated uh deep sampling based all these kinds of things and and then put our um work in that field more as this is a different approach to solve the problem uh that we're interested in when we want to design active INF agents that do uh do planning in interesting ways with all the epidemics and what we like I don't know who asked that question um but you're right it's a tough one it's kind of a perennial question I mean even with some familiarity sometimes I find myself grasping at how simple and clear the problem is and how we have all the tools to work towards it and then there's just this interesting moment like we know what we're interested in and what we want to do we also know how to do it so then like what do I do then when when we know what we care about and how we prefer it to be and seemingly have no limitation on the actual composability and the implementation these developments happen year by year yeah I mean that that's always going to be more I expect that our method is also going to be superseded by something that's even cleverer and even smarter uh and I look forward to to figuring out what it is whether we figure it out or whether someone else figures it out um whatever it's going to be it's going to be awesome but I sort of want to dwell on one of the the points you just mentioned was this idea that if you have what looks like a deceptively simple problem it doesn't mean the solution is is simple because most of what we're doing like 90% of everything we care about here is really just solving Bas rule like base rule is pretty simple to write down um the trick is that it's really really really hard to solve that's why we have whole fields of variational inference trying to come up with approximate solutions that are good enough why we have uh all the like really brilliant interesting work on Monte Carlo samples and all these kinds of things it's all because we know what the problem is the problem is easy to State problem is just solve base solution can be really really difficult and that's sort of where the the the devil in the details and all the work goes in same is saying I just want to just want to minimize free energy it's easy well in paper paper edit this but if you actually have to solve it you need a lot more paper easier said than done that was probably true about the first uh rocks being chipped into blades just make it narrower just make it sharper yeah it's easy it's easy just you know go go hunt the mammoth it's easy just make make it lie down you can take all the food no but but I I think this is sort of um to me one of the really really interesting Parts about working in this field um that oftentimes it's really easy to State what you want at least for me like I have want to you know be really really good at updating beliefs and I want to do that in a way that's consistent with all these things that we like but really when it comes down to it we just want to update beliefs in a good way how to do it and the details and how the belief updating should work and whether we should introduce all these extra little bits and pieces like Information Gain terms all this kind of stuff to me that's where sort of you you get to dig down and immerse yourself and find all the cool things and try stuff out and sort of that's where the work is that that to me is is really the exciting part but I like that I can always understand step back and say well I just care about doing inference man let's look at where you left the papers off as we kind of think about where we go so other than broadly summarizing all of the acronyms and term that's a lot you clamp the paper in a very interesting way at the end yeah so what what what would you say about this final paragraph and like how did the different authors perspectives um arise to result here so this is this is actually an interesting point um because I think I want to come at it in another way um what we're trying to do here is we're trying to provide tools uh we're trying to build really really good tools for doing active inference um but as you mentioned earlier people come to active inference and the free energy principle for all sorts of different reasons and for all sorts of different backgrounds like they maybe not everyone is interested in in building tools and the measure of whether our tools are useful are whether they end up being used by other people if you or others pick up this this notation and pick up these types of message passing pick up this little grunch and active insance formulation then this this paper uh is going to have the impact that we' like it to if nobody picks it up it's just a cool Hammer that that keeps lying on the shelf and sort of paradoxically um I think one of the barriers to uptake could be that um a lot of the people that come to active inference come from say Neuroscience or or more sort of philosophically bent uh which is which is great like we need everybody on on the trrain um but they might be concerned with other things than we are and we didn't make any claims as to whether this is like a biologically plausible uh update algorithm which may be something that uh you care a lot about if you're a neuroscientist don't have any claims about how this tie is to experience or qualia or Consciousness or all the kinds of things that you might be more interested in if you come at it from sort of the more philosophical side um so these tools uh is the best we could build as Engineers trying to make good hammers but if what you're looking for is not here if your um priorities lie somewhere else then these might not be the right ones and like and that that's okay um so yeah I think paradoxically one of the things that uh could hinder uptake and and the impact of these papers is that the people for whom we're building tools might care about different things than uh what provide with with what we built I if you really really really want to use a screwdriver doesn't matter that we have the best hammer in the world you still need a screwdriver so because this is a a again a tool building exercise because this is about doing things from an engineering point of view uh that means they are tools for engineers so we don't sort of make any claims as to biology or Consciousness or all the other things that people are studying under the The Heading of active INF and the free energy principle will let people who are are better qualified uh and in many ways smarter in these things than we are take care of that what we want to do here is just say here's a really really good Hammer uh if that's what you want then then we got you covered these are really important I I I really respect that approach I think in terms of the otive inference ecosystem even if there seem to be like some barriers or hindrances associated with really the articulation of the tool and the and the and what it's used for um this is how we bring more perspectives in and if the hammer is oh yeah but then we also included this and here's this proprietary phone home and it only does this type of nail and it only makes houses all of a sudden it's it's not really a hammer anymore it's become welded to a larger apparatus um and yet if the head of the the hammer and the handle aren't connected it's not a hammer so then it's a fine line between like what algorithmically in implementation does this essential Act of inference tool do what is it and then what can it be done with it and I think actually RX and fur and bias lab other than Burr and you're all's incredible work over the last years by situating within a broader basian message passing framework that has helped first off bring in insights from the broader basian inference space related to literally fory graphs message passing reactive message passing and so on and then also it helps us separate out the active inference models from other models and there's nothing wrong with other models if you are building a model you find that your system of Interest doesn't require like an active epistemic agent for every single component no one in the active ecosystem is going to get mad at you for like just doing what the situation demands and so that's very cool yeah again one of the the things besides uh the new messages and everything that that allows people to do this uh type of message passing based inference one of the things we wanted to stress is that we wanted this uh constraint FFG notation to something that is quite easy to pick up like it's easy to write down your graph and if you want to do just box standard belief propagation you don't really have to do anything like it it'll just it looks like 99% like a normal Factor graph but if you want to say do a factorization here and there you can like you can just draw that in by adding a little thing for each of the uh parts of your graph that factor together so we want like we really stressed this should be something that is intuitive and and usable because we want this to be applicable for people who don't care about working out the intricate s of message passing on Factor graphs this should be a tool for everyone uh and if you come at it from a different background and what you want is a nice and and clean modeling tool it should be that for you what else do you think could come into play even just totally speculatively or arbitrarily like what do you imagine in the preferred active toolbox even if it's some sort of like synthetic intelligence fantasy like what would be the coolest thing that we could bring a new colleague to the table and set them down to to me I would like um to generalize all of these update rules that we have derived like the general form of the GV based messages to a point where you don't have to worry about whether the thing you need is is there and then I would want it to be so that I can give it to you or I can give it to you know a neuroscience colleague and I can set them down and they can draw their graph in their C5 notation and all of that should get translated into executable code because then we end up in a situation where if what you want is a modeling language that solves your problem you can do that you don't have to be a technical expert and for those of us who want to work on the technical side of things that's a code base that we can open up and and dig into but in terms of a toolbox that I would like to have have an existence it should be something that is really really easy to use generic um and fast because fast as cool do you see any difference or Reason to build sketches in the factor graph form or in the base graph form we know there's an analytical relationship that links them but is there any reason to like work on building intuition about Factor graphs specifically or would it suffice to build intuition for Learners about base graphs knowing that they can be rendered to factor graphs later I actually had this we had a long conversation about this with colleague of mine just just this week and I think if you dig deep enough the representation doesn't matter that much the update rules end up becoming the same I would say Factor graphs uh like to me they're preferred I mean obviously I'm biased because that's what I work on but I also think they allow you to write down more if I have a base graph I can see that two variables are related because I have a node here and I have a node here and an edge between them with a factor graph I can also see what the relation is because a little Lally have a little square that with a simple end that tells me the relation and also if you adopt the CFG notation you don't just write down your generative model you also write down uh your constraints on Q so you can write down your um like the exact inference problem that you're trying to solve and that is something that I I'm not aware of a way to do on on basent Networks so to me I think there is more um information contained in a factor graph especially if you uh adopt constrained [Music] a given base graph there's actually like a few to one projection of factor graphs as constrained down to a same unconstrained base graph and so in that sense they can convey more information if you use the cffg and yeah so again they don't call it bias lab for nothing it's definitely your fellows's uh take but this again this is also one of things we introduce with these these papers um is that we identify a need to write down more than just the generative model if you want to be clear about everything else you're doing because if you don't have a graphical Rel you need to put it in equations you need to put in text and you need like half a page of hieroglyphics to convey what you want to do um and again that is a barrier if you want this to be an accessible tool for everyone which is one of the things that I want um so we augment FFG notation specifically in a way that I'm not aware of anyone that has done for for base baset I might be able to um so in that way you're right there's more information definitely in the CFG that in the Bas net but in general that's also more in a factor graph because you have to be explicit about the relation between variables more than you do with with at least the versions of Bas Nets that I'm familiar with uh again I would say as I put on my practical head you should use whatever is best for for your problem whatever you're most most comfortable with because a lot of the math you end up having to do comes out to be the same like in the end it's just it's just a picture yeah yes but there's there's more to my mind there's more information in especially constrained fgs uh which means you can be really really accurate and precise about what you want to to do by just writing down the picture yeah lot of thoughts there one great you left the door open to somebody introducing a graphical or other than graphical notation for Bas graph to create a true onetoone map between Bas graph and the constrained for graph it's not impossible it might even be trivial no um and then it just makes me think of like a learning language that's more broadly used Bas graphs where the nodes and edges have a semantics that are equivalent to what is usually considered as structural modeling yeah and then you enter into the learning language and and play space and toolbox and then there's this strong relationship between the learning language again just kind of hyperbolizing this and the engineering implementation or specification and then from there it's actually anchored in message passing like that's how we connect the Bas graph as sketched to the message passing is like enter the problem into the Bas graph if you want to enter at that point transfer over to the cffg and then run the messages from there however if you wanted to you could just work directly from the fact graph and a Bas graph would have never had to enter into it I I think I think that's doing a little bit of a dis disservice to to Bas graphs I I I have my favorites um but if you go and look at like the original when in Bishop papers for instance where the original dve the VMP algorithm it's all done for base graphs it's all done for basent networks um so like it's not that you can't do message passing again this is just different representations of a generative model um and sometimes one is more useful uh I find that generically uh Factor graphs are more useful uh when I want to work with message passing but some other people might might disagree um what I can say is that uh the versions of Base graphs that I I've seen I contain less information than a constrained FFG because you don't write down uh the exact inference problems I.E the constraints and the data points and everything so I I wouldn't say that uh you know there's no room for base graphs like base graphs are great uh but I say for the kind of work that um I'm interested in Factor graphs feel feel more natural thanks for that that's great let me ask one more question in the um in the graphical brain 2017 yeah paper um there were three graph types mentioned so we've spent a ton of time talking about the the complementarities and the transform abilities of of Bas graphs and forny Factor graphs could you say a little bit about this last form of graph the neural network or neural circuit um I am not intimately familiar with the formalism of neural circuit graphs uh so I don't think I can add anything above and beyond what's here especially if I want to make sure that I don't say anything wrong fair enough fair enough okay just want to check because they're all like different projections so but just wanted to ask I think so if if you wanted to sort of try and and tie a bow on it um I would say the different graphs different graph types and different types of graphical notations are useful for different things um and whatever you need to do you should pick what is best for the kind of problem you're trying to solve like if I have um if I all I'm interested in is dependences between variables I just know I want to know which variables depends on which others based that's might be the best way to represent that and yeah that that might be the case for the problem that that someone is working on that this is sort of the the central information that they care about if I also want the relations between the variables to be explicit and I want to do message passing um maybe an FFG or a cffg is a better representation and if I care about say uh neural implementations maybe like a neural circuit is um the right choice Choice again I don't want to sort of come out and and tell people what they should do because I think everybody knows what they should do better than I do what I want to sort of propose is that there is a a type of notation that has these nice properties you can easily represent the relationships between your variables and you can represent the constraints on Q thereby also the resing message passing algorithm which is something that I find really useful uh and that I hope other people would but if that is not the type of problem that you're interested in and something else works better for you you should use something else yeah again this is all pragmatics this this is just a notation for how we we want to get our ideas across clearly and in the end what all this is really designed to do is to communicate information in a nice way and you know the the the language which we we choose to speak in uh should reflect what we want to say we could try you and I and have a conversation in German but I I don't think it would end well for either of us at least not for me I'd be there to hear but I wouldn't listen or do you speak German no nor do I there you go again we could try and some ideas might be more expressible in German than in English uh but if if none of us speak it uh it's not going to Aid Us in in transmitting information so for this setting English is probably the right choice of language for other settings it might be bassets and for other settings it might be cfgs I have a bias to watch cfgs because I think they are the nicest way to represent everything that we want to do but um for other people it might be different and that's okay yeah we just did a very long unpacking of the hammer and again as discussed if you're not looking for that function it's not the right language or process I mean it's just such an important higher order point that that clarifies the contribution of the paper and the research Direction and off up something to hopefully many people who will use it at least from my side only in spoken English active inference ontology dialect I hope that people do use these tools there's only seemingly a few individuals who are working in RX Ander making active inference models it is very few relatively so having that number number broadly increase yes also training up different kinds of synthetic intelligences to do active inference modeling that enriches and enlivens our ecosystem of shared intelligence and yeah the engineers also need to have hands-on experience and know and work through simpler examples to build intuitions for larger systems less larger systems be so far beyond our understanding that that it's not useful or helpful yeah and so before we end up wrapping up um I want to reiterate the the invitation that I gave last time as well if anyone out there is interested in in working on these things uh like hit me up I'll be happy to help you work work out your graphs because I think it's important than you know measure of a tool whether it's useful is whether it's being used so if I can assist people in picking this up I'll be more than happy to that's the most helpful and pragmatic thing I believe I guess just in closing like what loads up on pragmatic value for you what loads up on epistemic value for you in our field as we head into like 2024 I think I'm in the very privileged position that I can do research for a living uh so my pragmatic value and my epistemic value tend to overlap a lot if I can figure something new out um that's pragmatic because that means I can write a new paper and that's the thing that I'm I'm producing uh so I think that would be my answer I want to try and see if I can make this practical uh as I work it out for more models and and try it for more things um but it is a very privileged position to be able to do research uh where we basically get to spend our days trying to figure things out you know with with all the admin and all the Academia things and all the sort of other the stuff that goes on but at the end of the day uh I do get to spend a large chunk of my time figuring things out and that is really really nice so the epistemic and the pragmatic value tend to overlap which is is nice well thank you it's a beautiful sentiment so it was a great Series this is really a a mega Tech Tree direction for the actm ecosystem and Beyond so good luck with your continued work and play and research Magnus it was awesome thanks I had a a blast coming on here it's been some really really great discussions and for those of you participate in chat um got some hard questions that's good yes and it will continue in The Institute Discord excellent I I'll see you there see you Magnus bye