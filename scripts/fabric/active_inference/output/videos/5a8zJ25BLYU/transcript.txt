all right it's cohort 4 October 31st 23 and we're in our first discussion of chapter nine so today we can totally look at questions you can look over the chapter but is there anything that anybody wants to bring up about chapter nine welcome Andrew any general thoughts on nine modelbased data analysis before we kind of look through some look over the structure of the chapter and look over the questions okay I have to just hop in to the uh the structure a good overview seems nice on this one cool yeah nil had mentioned epistemology so talk about leading with a epistemological assertion the models described in this book are only useful if they can answer scientific questions is that the only way a model can be useful or is this such a broad sentiment that everything can be cast as a scientific question so even something of purely epistemological or epistemic status could could be under the the scope anyways though in this chapter we focus on the ways in which active inference can be applied understanding empirical data so that's kind of the big plot twist in chapter nine is that up till here everything has been very much like proposed as sort of intrinsically coherent like the rat and the te-as or other um calculations they're all kind of proposed as um just thought experiments more than anything else although there's a lot of citations to the empirical work like in chapter 5 here though they're going to go into detail about how you interface the generative model with empirical data and so in that way it kind of complements chapter six because here we're actually GNA take that generative model and connect it with data our general goal is to recover the parameters of the generative model that a subject's brain uses to produce Behavior the subjective model so we'll see this graphically soon but just to be clear to recover the parameters a synonym there would be like to identify plausible parameters so if we knew that the height of children in the classroom was distributed according to a normal distribution then we have the data which is kind of distribution free it's just a list of numbers and then the task of chapter nine is basically juxtapose the structure of that model with the data and recover the parameters okay section 9.2 goes into this metab basian perspective or metab basian uh methodology they bring up two related reasons for fitting a computational model to observe Behavior the first is to recover parameters to give an account of a single individual from a broader population or comparing two groups or two populations or more against each other so that is taking a fixed model structure and then multip full samples of data could be one person on multiple days or multiple people on multiple days or whatever the design of the experiment is and then for a fixed model structure identify something useful about that data like about one group versus another the second reason is to compare alternative hypotheses expressed as models that represent different explanations for Behavioral phenomena so here instead of preconditioning on one model we're actually going to use the data to interrogate a portfolio of models so we have performance of multiple individuals on multiple days so then we might say okay well the simplest model is all individuals are the same drawing from the same distribution all days draw from the same distribution then we have a model where individuals don't differ but days differ then we have individuals differing but not days and then we have there's an individual byday interaction so this is familiar to anyone who's basically done any kind of statistical modeling which is that adding more parameters to a model essentially or almost by definition always increases the accuracy of the model on the training data that doesn't mean it increases the accuracy in an out of sample so that's like the training and test um relationship of overfitting um but adding parameters basically all always gets you to fit more data um or fit the data more closely however the return on investment per parameter can diminish and so that is why in basan statistics people use methods like the II information Criterion AIC or the basian information Criterion Bic to identify well where are we adding parameters where's this kind of like knee or this um special inflection point where yeah of course we can still add more parameters and get more fit but we're like basically at a nice optimal tradeoff between adding parameters and fitting the data which is exactly what variational free energy is so this is two reasons one is to take a model identify something interesting like a pattern in the data the other which is complimentary is to use data to identify or differentiate hypothesis in the world so yeah actually having the day factor and the individual Factor those add value but having an individual bu day we dilute too much just just like raise your hand if you have any thoughts if you want to add anything so this is a pretty standard statistic framing here is where we're going to revisit some of the key Concepts from earlier in the book like the generative model of chapter 4 discrete time model chapter 7 all of this and now we're going to kind of put it in a laboratory context so we have the subject of a behavioral experiment so let's not worry too much about like objectivity of Truth and subjectivity of experience but let's just think about a behavioral study so the subject of the behavioral study is the one whom we are developing the cognitive model of their cognitive model their generative model might be about the laboratory like the maze however the subject model is what we've been discussing in basically all of our kind of agent Centric cognitive modeling discussions and then in chapter nine here we're just going to pop out and add another dashed ring and a solid box around that to represent the fact that it's kind of like now we have the lab and the subject here's the experimental stimuli going into the black box here's the observed Behavior objectively coming out or could be said empirically coming out so this kind of gives a whole inclosed or closed loop representation of like the person in the fmri machine and then here's the experimental stimuli being provided to them could be a fixed sequence it could be drawn from a distribution of stimuli and then there's the behavioral outcome so even the setting of The the behavioral observational moment the ethological setting this is also amenable to basian statistics so we could talk about optimal design of experimental parameters we could talk about optimal delivery of different stimuli we could talk about the information content for different sequences of behavior we could do a statistical power analysis or simulation should we observe a few individuals for a long time lot of individuals for a short time these are like very empirical laboratory statistical considerations that will be familiar to anyone basically doing a behavioral study or really just any other study even if it's not like squirrel behavior but if you have a limited amount of resources or time or cameras or whatever it is in the lab you're going to find yourself basically in a setting like this and so that's why this is a really nice figure and chapter because it steps outside of just the kind of abstracted generative model and now it's like okay now now we're going to build the setting around that and so it's kind here's outside the building but now it's like the laboratory is like the marov blanket around the rat in the mace which is to say if you control the laboratory if it's a controlled experiment then what's happening outside of the laboratory shouldn't matter should be conditionally independent with respect to the subjective model so this is kind of compatible with a lot of um basian epistemology in empirical Sciences okay any thoughts or questions on that or we'll continue with with n end of 92 then 93 it's called metab basian or I don't know maybe you could say Epi basian or something double basian because of this wrapping with a basian statistician around the basian generative model and then they even say we're using basian procedures Twice first to evaluate the subject's posterior beliefs about action so posterior means posterior to the observation so we're going to flash the red light then we observe an action and then we're going to do parameter recovery about the subject's posterior beliefs on action posterior to the observation of the red light that's the subjective model and secondly to evaluate our posterior beliefs about the unknown priors that we have so it's just a purely basian approach to studying the total setting of the behavioral observation otherwise you'd have this kind of free floating basian model of the subject but it would be like a view from nowhere because you wouldn't have completed the metab basian setup okay that's 9.2 9.3 is going to connect us with a statistical method that's used in a lot of statistical approximation algorithms and it's discussed in SPM a lot as well so variational appuse may be used for more generic likelihood functions than incountered earlier which were defined as gaussian so variational Lao we won't go through all the details here but um usually what the llas approximation is going to do is it's going to find the single highest point on the distribution the single maximum likelihood solution which is why the um why the likelihood calculation from earlier matters and then going to fit an upside down Parabola so if the distribution has a central tendency the Vari variational loss can very well if the distribution has fat tails or has a multimodal tendency variational Appo will not do well if it's gausssum kind of a technical subsection but it's possible to tractably optimize the variation Lelo so it's going to be misleading in some settings but it's going to be super useful in some settings but either way it's very calculable so it's used for a lot of statistical optimizations okay so a common critique and and valid point that people often bring up about basy and statistics is well don't you just get out what you put it like if your prior is such and such then don't you just recover your prior in the posterior and there's multiple ways to address that one of them is if you start with multiple different priors so from the same family parameterized differently or from different different families and you recover your prior then you actually can empirically say that there isn't information in your data but to the extent that those diverse priors Converge on a similar posterior you can empirically say that there is signal in the data it's not binary but those are different heris that are used so one approach to kind of get around or to address This reviewer typee comment that like oh well basian statistics just returns your priors is to try different priors and there's also another approach which is called parametric empirical base so it's called empirical because we're taking empirical data and using that to initiate the cycle of optimization so for example if we were going to be doing a basian model of the height of the children in the classroom one approach would be like we um know that people range from one feet to 10 feet tall so we're going to do six feet plus or minus five as a gaan another approach would be wow we really don't know it could be between zero and a th000 feet or something like that like just try to do this sort of like purposeful open-endedness but that might wash out true signal in the data like even if a bunch of people were the same exact height you've only shrunk your distribution now to between zero and 500 feet tall so what's the way to get around that well one is trying multiple prior sets but another one is to take the first batch of empirical data parameterize a gaussian based upon that and then use that gaussian or an overd dispersed form of it as your prior for subsequent analysis this discussion on design Matrix so representing the experimental design as a matrix like if we had five people on um you know multiple different days you have like data point1 and then in person a they'll have a a one and then all zeros for the other columns so representing the design Matrix of the experiment as a matrix and then putting that within a generalized regression framework is discussed more in the SPM textbook than in this textbook so this is kind of another technical subsection talking about how you can use empirical guidance on initi IAL prior sets in a generalized linear modeling setting with experiments summarized by their design Matrix to do a regression on model coefficients so you can do statistics on those coefficients like testing whether the slope is different than zero so to go back to the earlier example where we had like multiple individuals on multiple days you could you have the simplest model no effective individual no effective day and then you have model two effective individual model three effective day model four effect of both model three effect of both and then you can test the relative likelihoods of these different models and then for whichever one is the best selected model then you can say okay the coefficient was 0.1 plus or minus .5 05 so the slope was two standard deviations away from flat so at this Alpha level it was a statistically significant slope for the best selected model it's kind of speeding rapidly through a few different parts of Statistics but that's basically what is happening when you just smash together the design Matrix and the experimental Co efficients you're basically doing a regression and then you're calculating essentially A P value based upon the slope of the regressions conditioned on the design Matrix okay so just Al just raise your hand or unmute if you want to add anything but just kind of going over this chapter 9.5 is a lot like chapter six chapter 6 was the recipe for making a generative model 9.5 is the instructions for modelbased analysis so 9.5 is almost like a Koda to six so the steps are summarized in figure 92 first collect data or you might collect data later and you might do all these other steps earlier like if you were going to pre-register the study or do a simulation or statistical power calculation so you don't have to collect the data first again just like chapter six this is kind of like more like stuff that has to be done not really a list that must be done in order however at the very least you'd want to know like what sensory input is available to the subject and what data you're getting out sensory data for the subject what data are coming out as kind of side note many Studies have been way layed because there was sensory data that wasn't intended that was getting to the subject so I've heard examples ranging from it was a study of circadian behavior and there was Construction in the building so at certain times a day or during certain cohorts or replicates there is sound or just in a Animal Facility different animal handlers or different light schedules or different temperature different air conditioning forest fires that we had in California like the sensory inputs available to the subject doesn't mean just the ones you want and so that can disrupt a lot of the statistics but this is basically like saying what data are are you going to get from what kind of subject and then what type of tests are you interested in doing on that data there's always exploratory data analysis Eda um also criticized as being kind of like a fishing Expedition so sometimes people go into a study with really specific hypotheses other times more open-ended two formulate a PDP model if you're using a discrete time setting which is kind of how how they're going to do it they're not going to um template this with A continuous time model so we get a fully specified but not solved PDP so structurally described but not parameterized PDP specify likelihood function we could look a little bit more at SPM mdp vbx to try to see exactly uh what degrees of freedom are here but I I I my understanding is this does not this is not something that needs to be designed on like a per experiment level specif prior beliefs these are the priors that are being proposed for that metab basian setting so if it's kind of a if it's there's some priors that are centered on zero other times you have a prior distribution over a distribution so it's like it's bounded by zero and one at which point you could use uniform or or some other um proper distribution solve for posterior probability standord standard inference scheme again we could look at this routine but this just takes in the above information and makes a calculation so the output of steps one through five is going to be these estimates of different parameters of the model then you can do the group level analysis [Music] um we can look at SPM DCM this is the dynamic causal modeling um so that's one approach that's discussed more in SPM or you can B basically use any statistical method canonical variance uh analysis CVA similar to The Independent contrasts I or principal components analysis PCA or just Anova any number of statistical analyses so here's the summary we have the data we have the PDP and the way that it's going to get calculated all of this gets brought together so U Tilda Tilda always means through time and here's U Tilda the action as selected as observed Behavior so that's what that's what we're getting from the data is the behavior survey data movement data i gaze data and then here is going to be a we're going to get a a likelihood function for saying what's the likelihood of given Behavior conditioned upon the parameters big Theta all the parameters the sequence of observations received and the generative model M so that is kind of what the subject is doing is it's doing this likelihood calculation what's the likeliest behavioral sequence given basically all the parameters accessible to me the cognitive model as well as the observations and then we invert that and that's now happening on the scientist's side so we're going to invert so that we can recover the parameters big Theta conditioned upon on the model the observations of the subject and the action of the subject that is proportional to so it it might be off by a multiplier but it's it's monotonically proportional to a factorized disassembly which is the parameters given the model which is something that that um can be taken off multiplied by here exactly what we have here P of utila conditioned upon Theta observations in M now we can specify our prior beliefs on this left side because this is basically our our beliefs about Theta given the model and again we would test a range of these Andor use parametric empirical base to generate empirically grounded beliefs but with this inverted model we can then do statistics on calculation of theta we have X the design Matrix beta which is a variable with the regression coefficients and a noise so it's kind of like y equal MX plus b plus noise term except it's like a mega regression on a much more sophisticated kind of data but in the end here's those five groups and then here's this one's parameter range was you know 0 2 plus or minus one this one was 5 plus or minus one this was four plus or minus one and this is like the kind of figure that you'd get in the paper where there'd be like a a bar a saying oh these are not significantly different Bar B these two are not significantly different and then an asterisk saying these two you know individual one two and three were significantly different than four and five at this P value so chapter six builds us up to constructing the generative model if you have empirical data already at hand or if you intend to collect empirical data and you already know what structure it's going to be you kind of design this complimentary relationship between the observations that the generative model is going to get and then the actions that the generative model takes which are going to be observations for the behavioral scientist and then if the behavioral scientist can specify this likelihood model and a prior belief distribution we can invert what it is that the subjective cognitive model is doing to recover differences or patterns amongst parameters for different individuals or times or contexts so this takes us from a chapter six which is just how to build the generative model and lays out the big steps for like how we would then get to the figure for the paper or to test okay here's five different doses of a drug and then on the y- axis is the attention parameter no variance on this which is we're going to interpret that as attention and here's how it differs across doses of a drug so it's a lot of Statistics here being very very quickly recited in a basian cognitive behavioral modeling setting could you repeat the example with the drug dosage again yeah so let's just say we had five different um doses of drug or control and you know low medium high and very high and then um each of those we're just going to do you know of course you could have n individuals on N days with n doses so that' be like a fully replicated experiment but but you know you have some some's upset so we five groups and then we have 30 individuals per group um rats so that we don't need to talk to the IRB or we probably still do um so then you have the rats do these different behaviors now if you just wanted to do statistics on their behavior you wouldn't even need a cognitive model so if all you were going to do was going to do like I'm going to do the average um percentage of time moving between the five groups well then you could just read that off from the data and just say okay it was like you know 10% 15% this and that so you could have a little scatter plot and then you could say do these groups statistically different in the percentage of time moving so that would just be like literally from the data to to step six however you might also have a cognitive model and so there you would take that data and then see the actions of that which the data which are the actions of the subject and then utilize this constructed cognitive model to do inference on some cognitive parameter of these five groups so not simply descriptive statistics on the action output and then and then these these scores are referring to some proposed cognitive parameter it's like an example of a cognitive parameter I'm very interested in like I like uh yeah I like that I just want like an example of yeah that's a good question um let's look to some of um the work of Ryan Smith okay let's see if this will have a nice generative model some of these well perfect so here we see our classic downstairs bit figure 4.3 down downstairs um the empirical data are going to be the uh heart beating empirically um and then the hidden state is going to be I believe in this situation the interception of the heartbeat and then a which of course Maps between observations and hidden States if a were the identity Matrix this would be a fully observable setting but of course a isn't exactly the identity Matrix and in fact they're interesting in the Precision so then they can parameterize this model and you could say um and then you can um test whether the Precision the interceptive awareness differs amongst individuals or between individuals who have this or that diagnosis let's see if there are other models here's another one this is this one has to do with um gastrointestinal perception again we see the downstairs so then what would be the next step what would be the way to integrate action well we know action intervenes in the B Matrix so now if you had a descending action condition and then you could say okay now do or don't try to pay attention to your um you know hard you're a gut and then you could do statistics on the efficacy of the B Matrix but now these are making larger and larger experiments and that's where you get back to like the resource limitations and laboratory and stuff like that but yeah then you could say well I'm now I'm interested in this summary statistic describing the difference between the average Precision when we do and don't tell them to do this and there's some individuals where there's no difference like telling them to pay attention or not didn't differ their precision and there was some individuals where telling them did alter their Precision but of course those Precision estimates are features of a statistical model which is why we had to construct this those Precision estimates and the statistics about how those Precision estimates differ those are not features of Simply the heart rate but this is a big this is a big apparatus to bring out so if you just are actually interested in the descriptive statistics if your question can be addressed with description description of behavior you literally don't need a cognitive model this is being brought out when you are trying to make inference about something that's not directly objective yeah then they reference these two papers that do have slightly different ways of looking at this IE Cade situation um again to this kind of basian epistemology and also like sociology of neurodiversity really we hear so much rational irrational irrational nonrational all this um and act active inference or just basing epistemology has kind of like a simple and a deflationary but also kind of a maddening answer to that which is well all the generative models are on their path of least action so those paths May differ but the paths are all alike in that they're all Paths of least action so it's all based optimal one person might be engaging in a repetitive Behavior another person might be ruminating another might be having anxiety could be human could be nonh human but those are all Paths of least action um so obviously there's a lot to say about this but um that gives a little bit of a space between like disorder implies that there's like an ordered and a disordered but here it's saying inference is working fine but on a flaw generative model but again you could even go further than that and say well there's nothing flawed with it like so but that's the kind of way that it takes that that conversation table 91 reviews a bunch of settings where these kinds of parameters have been done addiction impulsivity compulsivity delusion hallucination interpersonal personality disorder oculomotor syndromes pharmacotherapy prefrontal syndromes that's seems pretty general not sure that one is visual neglect disorders of interceptive inference and that's where the Ryan Smith work especially is relevant and it's the sumary so it's kind of an interesting chapter with a few of these Mosaic sections beginning with a large scoping frame on cognitive behavioral modeling then having these two kind of technical subsections then going into a chapter six like interlude about how this work is empirically deployed providing two examples of generative models in the ey tracking setting going into this kind of like social definitions of pathology and empirical work on pathology space summary people who are interested in this like design Matrix or just like experimental statistics SPM textbook is really the place to look another kind of interesting note is um the methods for active inference are very welldeveloped in the SPM toolkit in Matlab and so for neuroimaging in those kinds of studies there's a ton of empirical work however most people who are in data science today are not using Matlab using Python and pmdp leading python active inference package is only now beginning to include these empirical going from data to the model type model inversion tools those do exist in the Julia RX and fur World um which is one reason I believe why such a common kind of paper today is like we built a generative model we synthesized some data we simulated data because going from the data back to the model is actually not so easy today for a full active inference model but as that becomes more plausible there's going to be a whole new Forest of lwh hanging fruit for those who choose to forage because this data could be any data so this could be click data it could be Ant foraging data I mean these are just regular data sets that people have already analyzed and then it will be a purely empirical question what cognitive modeling can add in comparison with purely descriptive statistical methods so as that capacity becomes like unlocked in the active inference ecosystem It won't always be like situation generative model simulation we'll be able to like start with empirical data about real systems economic data psychological data and so on and then already have that while we're having the generative model discussion that's going to reduce a lot of degrees of freedom on the generative model because we're knowing that it's going to interface with the data a certain way so honestly it's like a whole mode of discourse and work in the oim space that we just don't see but this chapter lays it out because where we do see that mode of work is literally fris in at all's work in mat lab with the SPN package is the SPM like textbook is that like on the website for The Institute or is that a something I should just be find online I'll I'll um I'll put it in the chat I believe that this is the most recent yeah spm2 manual is probably the most recent [Music] um this is probably one to look at the last print version is this one from 2007 so it has some typos it um is less about like action in the loop and the control theory however the relationship between Sensor Fusion and hidden State inference neuronal Dynamics inference this is absolutely a slam dunk so it's very very and even just like [Music] um e even just seeing the kinds of statistical methods that are that are in play between neuroimaging observations and then just the cognitive modeling and then having action kind of come out of the neuronal Dynamics one I think that that is not the only learning path but I think that that can give a more solid basis for the kind of downstairs part of the model um and then also when we're talking about like attention as mental action or covert action common theme that's basically like saying well we have this descending um internal action that's intervening in Hidden State dynamics that um so then it's kind of like but but SPM is like the train and then now it's like okay AC action intervening in attention is like you know I don't know throwing like little throwing something in between the spokes of the train wheel or having a hand on the train wheel but this train downstairs is basically St statistically summarized in SPM so it doesn't go as heavy into the action embodiment all of these topics that we like know and love as part of the discourse today but I think it's a really useful prerequisite or background work so that's different from the manual that's um yeah I I'm not exactly sure what the overlap is between the manual and the 2007 book um we should definitely use a language model to find out I may or may not have a PDF of this book um yeah and it just helps giv us a sense of what friston at all were working on only 15 years ago primarily and that helps contextualize like oh this whole like physics of cognitive systems versus just the statistics of neuroimaging but how how nicely they come together and as someone who's never done neuroimaging it was just kind of cool to like see the kinds of challenges that they have and there's a lot of transferable uh information like the fmri Bold signal has like a latency of several seconds so you know it takes two seconds to come on and it takes 10 seconds to fade off or something so then if you're timing experimental stimuli the stimuli can't come in every half a second but also if you wait five minutes you're going to reduce your statistical power because you only have a fixed amount of time in a scanner so like these are like very empirical very pragmatic aspects so it just makes me think about how in kind of future textbook groups or future internships or whatever it is like there's the Journey of chapter six there's you know Journey on the low road and high road then there's the journey on chapter six to build a generative model for a situation of interest and then there's the the Journey of doing an experiment and of how you reckon with these resource constraints and with um data that don't uniquely identify models all these different things so there's a lot of a lot of cool things that chapter nine helps open up any just thoughts or or ideas that people are having at this point in the group um yeah I I find this chapter to be just incredibly important as far as application of active inference goes and uh I I think it was great that you kind of directed us to Ryan Smith's work and others I think that um there are enough general principles here for this chapter to be used useful and that said uh actually being able to see like direct examples of the application in already published Works um seems very you know useful and that we we actually have something kind of more concrete to to get our hands into yeah so here's like interceptive Precision so this is a this is a they did parameter recovery on the a matrix then you can say well what's the correlation coefficient between this coefficient we identified IP and these empirical phenomena Reaction Time Reaction Time variability these are just descriptive statistics but on the x axis the rows are precisions a difference in the Precision prior learning rate and learning rate so the rows are actually cognitive parameters and then the columns are empirical descriptive statistics blue is a positive correlation and red is a negative correlation so this is like immensely transferable if you think about well the observations and the summary statistics are on the blanket so if you're just trying to draw conclusions about just the blanket um you know there's a factory it's making blankets if you just wanted the average color or the average size of the blanket you could just measure but then if you wanted to do some sort of inference about what was happening in the factory and then you could do this kind of correlation between here's the outcomes and here's the cognitive parameters that might lead to those outcomes this starts to really find some [Music] interesting possible because you and you could also look at the correlations of different cognitive parameters and then here it is with the statistical statistical differences say well you know we we don't find a statistically significant relationship between interceptive precision and BMI negative slope but it's just insignificantly different from zero but we do find this statistical relationship here so yeah and that's what differentiates cognitive modeling from purely behavioral work okay well let's see next week we return second discussion on nine there are some questions already on nine and there's probably also um a bunch more we can add but we'll come back to nine next week any last thoughts on this yeah um you know maybe uh as an extra thought maybe next week uh uh unless there are any new incomers uh who do want a refresher on as a summary or something maybe going over another example or two uh that you that you could think of I mean even this just at the very end was super helpful like the the correlation graph between the cognitive variables and um you know collected data so if you had any thoughts on maybe something like a simple walkthrough or two or maybe opening that up um could be really great um yeah just the direct application in the concrete example super super helpful cool sounds good yeah maybe if you find a paper that you like then send to me and we can see if it's um relevant or maybe we can look at one of the papers in t 91 or one of one or two of these iade papers okay or we can look at another one but yeah cool all right thank you everybody farewell thank you so much bye thank you