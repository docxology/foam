all right welcome back cohort 5 we're in our first discussion for chapter 7 so Andrew thank you for facilitating feel free to take it from here um so yeah uh first meeting on chapter seven so I'll do what I did with chapter six and maybe just give a I'll attempt to give a brief rundown of the chapter outline sort of layout and what's going on um that said there's a lot going on in this chapter and I would say aside from chapter six those who are interested in getting started on building actor inference models are going to want to kind of give a a kind of special amount of weight to this particular chapter um we do we are introduced to Hidden Markov models and partially observable Markov decision-making processes which PP is a shorthand for that and it's it's just as of right now that's kind of the not completely ubiquitous but most widely used models for a variety of Behavioral experiments um that said U couple weeks from now we'll get into chapter eight where we talk more about continuous models and um that'll be for looking at instances where we're looking at very you know uh fast time scales and great for things like motor movements um and just with things that are much quicker but usually we're dealing with discret variables so um chapter 7 quick intro 7.1 uh we focus on models of categorical variables in discrete time with examples increasing in complexity so we're introduced to models of perceptual processing decision making info seeking learning and finally hierarchical inference um and the authors try to um kind of get at different sorts of emergent properties including measurable physiology and behavior from these models 7.2 were introduced to the hidden Markov model on a perceptual processing experiment um this is where where the the model is of our beliefs about how a musician's audible notes which are the observations or outcomes in the model are generated from a written musical score which are the hidden States the actually written notes um they're uh they simulate the Dynamics of basian belief updating induced by a sequence of her notes again the outcomes um and then from there it gets into before I move on to the next section it's just worth noting all of these experiments in this chapter more or less to note very specifically what are the hidden States uh what are the outcomes what are our matrices that we're looking at um the a matrix or likelihood Matrix probability of observation given State um our B matrices which is our beliefs about how one state moves to the next uh from time step to time step we have our D Vector which are our prior beliefs uh about States just starting at time step one that's it once you move on from there uh D is no longer it's in the equation but it's not repeatedly used at every time step um although it can be updated depending on how you're running an experiment such as from trial to trial um so we get to see how it evolves over time time there are five total time steps it's another thing important to note a lot of experiments you'll typically decide how many time steps occur or will occur over the course of your experiment so here there are five a total of five notes that are played thanks Daniel for going to figure 7.2 um I guess to get a little bit more in depth here uh upper left of that figure shows the the models kind of um posterior beliefs about States and it's confidence and so like these lines start to stretch outward at first it's getting the the right notes for time step one and time step two Suddenly at time step three we get an incorrect note or at least an unexpected note uh there's a difference between the observations which are in the lower right corner um a certain note is played twice in a row incorrectly versus um what should have happened or what we expect expected which is the top right graph unless you can see that lightly gray square that is and the model correctly inferred the right note one that's in black but it was thrown off by that incor incorrect note occurring that's the gray there so the these are all basically outputs of the the model as the experiment is run um at the time of this experiment I believe they were using mat lab and SPM is their um kind of you know packages and software used for for deriving this experiment so there's a lot of um um nice functions for producing graphs and outputs of the experiments so you can actually see what's going on when we run run them um moving on to uh section 7.3 now we have what is arguably a really key sort of model to dig into this is where decision making and planning are introduced into um an experiment and so now our hmm becomes a PDP Um this can be seen let's see figure 7.3 um is a great one to it's kind of a um Exemplar uh image of a of a PDP where we have many elements as we had before we have our uh D Vector the initial States starting all the way to the left from there we move on we have our states we have our outcomes s and O we have our a likelihood matrices which map the two we have our B matrices showing going from state to state um but now what we've done is we've added policies which are sequences of actions um that the the agent holds like it could do this series of actions it could do this series of actions and so on um those are all kind of scored so to speak by what's another quantity that's been introduced here expected free energy um which differs from variational free energy that we've seen before F uh in that g now invol it takes into account preferences which are also c not imaged here but you can see on the the right um they are denoted in that like list of equations um and those impact how G is calculated that is you kind of are trying to score should I do this to get what I want should I do this to get what I want should I do this to realize my expected or preferred outcomes or should I do this instead that is impacting how the agent from there computes its transitions so what we've done is introduced action into the model where Action Now impacts the environment and it's expected to impact the the hidden states of the world that are agent is inferring so it's it's just kind of like the the prior example but now we've just added this whole new dimension of action where there's an actual bidirectional um relationship between the agent and environment um we are also introduced to a couple of Concepts whenever um the authors break down expected free energy so we have like negative epistemic value and we also have pragmatic value um this relates to the famous exploration exploitation uh debate or dilemma um there's a broad statement in this textbook that uh active inference naturally balances out exploration and exploitation that's its own kind of conversation to have and some people debate that depending on how you want to Define what balance means the key Point here is that exploration and exploitation for uh EX epistemic value and pragmatic value changing your mind or changing the world um are included within the same cost function if you want to look at this from a machine learning um perspective for example or more statistical perspective or calculus what we're doing is that we're minimizing this quantity that is expected free energy and you're minimize you're minimizing it by engaging in both kinds of behavior both of them contribute to minimizing the same quantity and I believe that is what is meant whenever they state that um active inference naturally balances those two uh kinds of actions that an agent can take we're also introduced to other things that you know for the sake of time I won't go too deeply into but we are introduced to factorization here so there are different kinds of States um that that the agent can try to infer um they're also so different kinds of outcomes or sensory modalities that the agent can try to infer for example um the yes that figure 7.4 uh the A1 Matrix is just mapping uh exteroceptive the exteroceptive modality or set of outcomes which is basically where the rat is in the Maze but then the second Matrix uh a a superscript 2 is denoting uh the rat's beliefs about uh the stimulus if there's no stimulus an aversive stimulus or uh an attractive stimulus and so the word tensor is used in this chapter and in previous meetings there's been some debate over if this is actually a tensor I believe the idea is that we're ultimately looking at outcomes locations and contexts right so it's this kind of outcomes by locations by context so it does kind of in a way if you attempt to to put it together would be more of a tensor object as opposed to a matrix it's it's a collection of matrices here um but hopefully that helps for anyone who's been wondering uh what that term how how that term's being used um similarly we have multiple B matrices because we also have um multiple contexts uh related to location skipping ahead a little bit we're are at least skimming the rest of this section we have our C vectors which are composed of um preferen preferences about location outcomes um which is C1 uh the rat dis prefers staying in the same spot whenever it starts out uh so it's set to a negative one right um You can you might expect that if you set that to zero and kept them all zero there's a chance the rat might just stay there for the entirety of the experiment probabilistically um negative one kind of gets it moving um C2 meanwhile has to do with the stimulus zero uh it's neutral towards not getting a uh not achieving any kind of stimulus six because it really wants an attractive stimulus neg -6 it doesn't want the aversive stimulus right it wants the cheese it doesn't want you know a tiny shock or something along those lines and finally Ma just as far as the mathematics go those are all passed through a sof Max function um that allows you to both include positive and negative values whenever you're initially defining the vectors um and then also it it normalizes everything such that we're we're looking at more like probabilistic um probability distributions so actually after the math there um for example the attractive stimulus is actually 400 times more probable or we might say more desired than the neutral stimulus um and then again we have our D Vector beliefs about um and and again multip it's factorized there multiple modalities so uh the the rat believes that it's starting at the starting location with full probability that's how we said it meanwhile for D2 uh that has to do with where is this where is the stimulus that it wants uh it doesn't know so we have a purely uniform D Vector there uh half and half so um that experiment also includes things like a big part of it is there's a q um in the lower position of the Maze and so this is a great experiment to look at as well because now we get to see both epistemic and pragmatic value epistemic it doesn't know where the reward is uh the Q gives it epistemic value it gives it information about where the reward is such as in the image there it's it the time step one rat starts at beginning location time step two rat moves to the Q at the bottom of the maze it uh the Q tells it that it's at the left position that's the location of the reward finally time step three rat smart rat follows that Q moves to the attractive stimulus at the left position um which is pragmatic value realizing its preference that it does want aches so so we we we end up with kind of this you know two-step process where it derives epistemic value from the que pragmatic value from actually achieving the the attractive stimulus um so those are some key ideas in active inference in general and yeah again kind of an Exemplar model to wrap your head around um especially for those who are still kind of trying to understand the material here because there's a a significant amount but this is a great starting point point for really trying to get into to modeling um section 7.4 have a model of information seeking this is a kind of an a cade experiment um where an agent looks at four different squares um this does start to break down things like uh it breaks down epistemic value further um rather lengthy explanations of these terms but but they can be very important and they're uh very helpful for understanding the behavior so um those uh squares there uh all the way to the left this is where all of the squares are given uh equal epistemic value right they're um the agent will look at any of them with roughly the same frequency uh in the middle one however um in that situation the top left square is uh by the person who set up this experiment and its distribution um they gave the top left Square more expected ambiguity which is one of the the terms that's derived from epistemic value um beliefs about that square are less precise they're more uniform uh in the a matrix for this Square the likelihoods uh you can kind of equate this with uh the street light effect the idea that if someone is out at night they drop their keys um they don't know where they are they will probably look where there's more light first so they the idea is oh it's expected that you will actually be able to minimize your uncertainty or reduce your uncertainty by looking at the other three squares here uh whereas the top left one just imagine it's as if that square is is is blacked out or or not well lit or dim or something right there's it's going to be hard to derive any kind of epistemic value from it because it's not things are not very visible um so that's an attempt at getting at how to understand expected ambiguity and finally top right uh or excuse me the rightmost um image uh here all of the squares are still visited um but the B transitions are made to be less predictable um and so it's uh the lower left yeah the lower left square is given a higher post posterior predictive entropy so the agent is less sure about what what will be seen if the cic is chosen for this square if they keep looking there it's as if it's as if they uh are unsure if there will be new information to get out of that square so they keep returning to it again and again um whereas for the other squares those are more equivalent in the idea that oh you look at them they don't really change why look back again right so it visits all of them but it keeps checking out the lower left one um we do have box 7.1 which introduces us to it's been mentioned before in the textbook but this is just a quick rundown of trying to describe uncertainty and precision or um priors over Precision so that's a term that basically it it start you it's like game control uh from the neural process theory that we've looked at uh previously especially in chapter 5 um this is kind of like at amplifying not adding to but rather amplifying certain neural signals um Precision is inverse variance so something has very high variance it means it has low Precision um kind of hard to trust something that has very high variance or very low Precision meanwhile um High Precision means oh that's seems more trustworthy whenever carrying out your computation so it's in a in a sense it's like waiting uh differently all of these different objects the different matrices or particular beliefs or otherwise uh within the model um section 7.5 learning and Novelty um this is where we're introduced to beliefs about parameters um so this shows us we're showing this on page 141 figure 7.10 or 7.10 um it's basically the same model as before except now we can see all these additional nodes uh where all of our different objects are parameterized so our likelihood matrices the a matrices those are all parameterized by tiny a or small a um same thing for the B matrices parameterized by small B um C Small C and so on um those mesh really well with looking at things like Precision because we're looking at things like waiting um so these are all parameters that can be learned as well over the course of a series of experiments um and typically in active inference models this uh learning occurs at a slower time scale um which is also kind of seen in the final example of the tech of of this particular chapter um that has to do with hierarchical inference but basically these parameters can be learned as well like how you can learn over time how much to trust uh certain likelihood mappings versus others or trusting uh your beliefs about transitions versus others or which um policies to take versus others things like that um brief mentions about other things um I'll leave them off here but it has to do with choosing conjugate priors whenever uh developing um variables that are linked together concentration parameters uh the importance of dislay distributions for categorical variables where observations and states are kind of counted over the course of an experiment um the more you see this the more you expect it to happen it also helps us to remember that like a model like how quickly it will update so you know you flip a coin five times you see it's always heads uh you might quickly come to a conclusion that uh it's an unfair coin meanwhile if you flipped it a hundred times with a 5050 of heads or tails and then you just happen on the next five tosses to get um all heads you're much less likely to think oh well this is an unfair coin because clearly it was pretty fair the first hundred times around right so by accumulating more counts um that informs us about how much the the model or the agent might update its beliefs after the next you know new observation that comes in we have a mention of structure learning which is an entire topic on its own and there's a lot of work still being done uh on structure learning but it's rather interesting uh I'll just briefly mention this is seen as something like offline learning um it there's a metaphor used for it like that occurs during sleep um that is the model or agent is not taking in any new data and yet it's actually updating its beliefs um but more specifically what it's doing is that it's scoring the model it currently has of the world uh let's say and uh during this this sleep or resting offline state it can actually uh compare that model that it has now with let's say a simpler model that maybe has removed some of the certain variables or or or readjusted and minimize some of its matrices or otherwise to see um if it can produce uh an equivalently if not better performing model um that nonetheless has some of its complexity reduced and suddenly it's computationally more efficient right so so basically in a sleeper resting state we have basian model reduction um is is one way that can occur where the model is kind of pruned and and improved um even though there's no uh additional data coming coming in at the moment and then finally um SEC section 7.6 hierarchical or deep inference um this again is very similar to um the PDP that we we saw previously except now we have an entire additional level added above um this is figure 7.12 on page 148 and so um I would say briefly that um the observation nodes uh in the middle between level one and level two that's a bit of a misnomer um there what's going on is that the state inference occurring at the lower level is being fed up into level two uh as if they were observations at level two um there I I note this because in a lot of um other papers that I've personally read uh usually they would not include those extra observation nodes in the middle they would just go straight level one up to level two um but this is a nice way of illustrating like it's as if those are observations for level two so what's going on what we have at level one multiple time steps like three time steps uh vers for every single time Step at level two so this demonstrates that there are multiple time scales going on here level one is happening at a faster time SC scale there three time steps for every individual time Step at level two um the state predictions occurring at level two are being fed into uh the uh computations at level one and so it's like we have these descending um predictions coming from this higher level and then meanwhile level one's State inferences are acting as ascending um observations uh if that's useful and um so this is um I would maybe I'll drop um an article uh link into the chat but I think that there's a rather nice um paper that that I've spent time reading and I I find personal interest in it um but it's it's a paper called deeply felt affect uh which came out a couple of years ago and it relates to I would say Fields like computational psychology or psychiat um but why I think it's useful for anyone who wants some further reading here is that um it one is a recreation of the tze experiment uh that we saw earlier uh and on the other hand it adds a hierarchical layer um that is theorized in the paper to relate to emotion and so we have a rat that is um given this kind of emotional or affective layer to its model and it actually dramatically changes its um Its Behavior whenever navigating the maze uh it becomes much more confident uh depending on its emotion in its own model of how to achieve the reward um sometimes to the degree that it will skip the epistemic value from the que it'll go straight to the reward why because now it gets an extra time step that it gets to spend with the reward um so it's it's again it's just we have the teamas we have a hierarchical model and then finally we get to see how like Behavior changes whenever we add this hierarchical layer and whenever it comes to cognitive modeling that's a big aspect right it'll be to kind of fiddle around with what variables do you need uh in your model what are the layers to be included um and and how do you kind of tune or prune things from there um I feel like I've spoken more than enough so I will now uh open the floor if anyone has any thoughts or questions or just general things you're considering um sorry is pach uh actually I have a very basic question uh since we have been talking on matrices is the D Vector just like a onedimensional vector that's it yes nothing else oh wow so I will say in the so we saw the te uh experiment for example and in that case we did have two vectors uh one for each uh modality so the first one was yeah the the rat's belief about where it starts so the location uh factor or modality and then D2 its beliefs about where the stimulus is is it to the left is it to the right here it's uh you know hardcoded as uniform uh as a starting point but yes uh they are each uh 1D vectors um and that's all you need to kind of kick off or start the the simulation whenever you're running it yes so that is mapped to the number of factors of the B Matrix right yes to the edge The Edge length of B okay yeah because B maps from hidden state to Hidden state so whatever the number of hidden States is that b is just that per slice and then the number of slices of B is equivalent to how many affordances there are and then D is just the edge length that's carried through with s and then a is O * s because it Maps o and s pi is also a list with the probabilities of the actions in the policy space summing to one thank you yeah I have one question sorry uh in like actually like when you normally try to initialize these vectors say uh say d can you just also hardcode it like just like one and then just pass it right yeah yeah that's right you would you would hardcode it um that's one of the kind of theoretical discussions around active inference right is it's it's more about the what and the the why and the how but not so much about how it starts um that said what you can also do as far as learning and inference goes is you could have um you could set up an experiment that runs over multiple trials uh in that case you would still hardcode D but going from trial to trial you could have it such that the agent actually updates uh deep over time so that trial one it begins with those hardcoded values trial two it might update um you know now that it's the start of trial two what is what are its prior or initial beliefs about the state yeah yeah so like in this case let's just say it was GNA do three rounds of the teamas and then initially it has a a flat uh you know it has a certain it believes it starts in a certain setting that that could be that that would be the D1 location or maybe that one stays the same the one z00 Z and then the one it's like it learns by the end that it it it always starts on the right or updates it towards the fact that it's that the reward of the other the stimul food is on the right side it could learn that by just counting up okay it was on the right it was on the right it was on the right and than that kind of like putting Pebbles of different color into the N that's the learning by counting which is reflected here with a little D over the learning by counting on this one so this one starts out Z 0 it starts out one one and then you add one every time you observed it and then that ends up that's a hyperparameter on the D distribution so hence the kind of strategy of the book of showing like the minimal figure 4.3 type model and then here's like one Motif which is hyper priers for learning or attention like Andrew mentioned here's another Motif nested models but the hyper priers are not shown you can start to combine these but then you can also imagine how the models complexity grows a lot sorry what do you mean by motive like here The Motif that it's showing is just nesting it's not including Little D so it's not like it this is not a generalization of the previous one this is just like a generalization of the of the figure 4.3 one as this one is but this one's not nested so like here but the difference that it's trying to show is just about the idea of putting this the metallic lowercase above each of letters a b c d e and then these motifs get kind of combined okay um question yeah slightly slightly tangential so Andrew in the react FX um project that you're doing are you building these types of models I know it's very simple question um well we've just kicked off the the project a couple weeks ago so we're still kind of trying to decide what kinds of models to build but anyone who's interested can can certainly you know hop on and and see what's going on we do have a a distinct Coda for that and so right now people are running project ideas and so they're like in a table that you can just put in like oh I would like to try doing something like this and then maybe put a couple notes on on what direction you think it might take and um yeah as Daniel's pulled up here uh for me I wanted to see if we could do kind of reproducibility project where we take some of this you know few years old code in mat lab or SPM can we run it through uh Julia and RX and fur instead um just basically to see if we can to help with reprodu reproducibility and from there we could even potentially start moving into like modifying those models or maybe improving them yeah generally I would it seems like pdps might be one of the primary ways that we go so hopefully that answers your question and is that all I I see Daniel posted the links in the chat and I I'll take a look um but is that all happening at this point asynchronously through Koda or are there um Thursday meetings Okay this what you got up here on the and we're developing out the GitHub repo and so on so there'll be but there's there's more than ample sync and synchronous opportunities if people want okay and and it's I mean and there's no chapter six for RX and fur but these are some of the um it um I mean we can go more into this another time but we've like talked about how we can best contribute and right documentation so for people right so it's more I mean it's still sorry sorry but it's it's still the underlying strategy yes we're learning this package for generative model that's also over the course of the Year going to increase a lot in its capacity and usability okay cool all right I'll pull up that Cota thanks a lot and just to note it'll also be incorporating um you know some research that's been done since the publication of the textbook we're looking at so um you know there'll be a little bit more uh up to dat so to speak the the textbook is still a Cornerstone of of active inference but that said there than some you know certain kinds of updates that have been made since then yeah I would just I would just love to see this um those modeling applied with you know actual values if you will it will help me wrap my mind around it cool yeah absolutely um I did notice uh Susan posted a question in the chat how would reflection versus indecision be represented um I find that to be a rather interesting question myself and uh while that's not directly answered by the textbook uh surely there are papers that have been published on the these kinds of topics um but in any case intuitively I would say just to kind of stay with the chapter itself uh chapter excuse me uh page 145 146 um this is uh an agent navigating a maze and in this case um uh the agent is does not have any specified like particular preferences or rather that it's like purely uniform preferences and so the a it's as if the agent doesn't prefer anything in particular uh whenever the model is set up in that way then the agent uh engages in what could be viewed as an emerging Behavior called novelty seeking where it just keeps going to places that it hasn't been to yet uh you know there there's no preference for a certain reward or stimulus to where it knows that it wants to go to a space that has a reward right so it doesn't go there it just goes everywhere um that said it's still doing it's still following the FP it's still minimizing expected free energy or variational free energy and so uh it will avoid spaces it's already been to because it's already been there um nothing new to get no new epistemic value to be derived from a space you've already been to and so it will just kind of like run the full gamut of all the spaces it's never been to uh this is a phenomena that's also been referred to by pner in 1985 is the inhibition of return so that's that's a pretty interesting thing to see and it makes a lot of sense because when we look at our equations C uh C vector or preferences are uh heavily informing G and so there's nothing informative about SE because it's purely uniform you know uh nothing nothing's particularly preferred then it doesn't really help with you know figuring out what to do other than just keep keep running around getting getting all that novelty you can U maybe it's thrill seeking um it's a joke but reflection meanwhile not so sure intuitively might have a relationship with structure learning um you know an agent doesn't necessarily have to be sleeping in order to uh kind of tune or prune its model um but the idea is that yeah there's no incoming sensory data so reflection might relate to that process you know someone who's sitting there reflecting uh kind of thinking through things through even though they've not pres been presented with anything new um might be a for struct learning there might be a paper out there um or or a dozen uh on these kinds of things worth worth uh searching for that um but yeah very interesting question um yes uh Anna I see put you're muted MD can you hear me okay now yes okay okay sorry about that uh yeah I was wondering if you could elaborate on a little bit on um the offline updating of of beliefs um and I'm curious about the scientific studies that have been done demonstrating that this in fact happens that you can you're you know during sleep or maybe meditation or daydreaming right can you elaborate a little bit on how active inference scientists have looked at this aspect about active inference so just be direct in saying that yeah I'm not personally too sure of the research that's been done very specifically on structure learning in related in relation to actual sleep um or meditation I mean I'm seeing just from a a quick all show one this is not exactly AC but it's it's B it's right there I'll put it in the notes this is kind of a g there's many similar architectures done in machine learning where there's this phasic um approach W with extending and updating during the day with a fast inference and then um using different techniques for sleep and I mean it it probably gets into the specifics of the machine learning architecture and then also there's the whole other angle you said like empirically um what what's happening at the synoptic level and at like the cognitive level for humans and skill consolidation and sleeping learning consolidation naps there's there's probably a lot of different phenomena like that thank you just drop this here but this looks to be a manuscript by a couple of researchers as well who uh make a mention of particular meditative Traditions um but they claim there that as opposed to it being um structure learning per se they they claim that meditation is more of a form of like it's it's a particular It's A peculiar policy itself um that is you're choosing to do that um that actually looks to be a really interesting paper and I appreciate that there's a more U kind of culturally accurate as opposed to highly abstract um kind of theme to it so um yeah Susan so yeah this is fascinating um thank you Andrew so what uh what keeps coming up is is is the reference to you know learning is takes time or um or you there's no fast learning and so I'm I'm wondering if maybe the concept of learning itself is you know is is um more specific than I guess what we um what we generally think about learning so I mean how would you categorize learning or or well maybe why that why it keeps referencing that learning is slow slower than what yes this is a great question I'll give a first thought um especially as as more complex cognitive models are built there's several ways that different time scales of adaptation happen so the fastest layer is called inference and that's kind of like the rubber hits the road like each um new observation coming in but also the whole process is called inference um but also the fastest layer is called inference then um the next slowest layer is attention so like attention modulates um on over multiple sensory Cycles like just thinking about the visual case or the the dinner party like overhearing multiple conversations like attention module faster than um slower than the words coming in but um faster than updating like deeper factual beliefs and then then learning um as distinguished from those other kinds of parameter changings is like here being modeled explicitly with these Hyo priers and updating the distribution that the kind of rubber hit the road parameters are being drawn from that's not the only way to model learning so hence chapter six and really digging in with like what time scale analysis is being studied because maybe whether it's learning over seconds like uh repeating back a phone number or whether it's like a um semester level maybe that's the time scale of your model so then that learning is happening on the kind of primary time scale of the model it's just way too General to say without specific system of interest but that being said there are multiple ways that like to kind of and the hierarchical models like looked at so multiple non-exhaustive or exclusive ways to model the multiscale temporal dynamics of change yeah I I would definitely second uh Daniel's point about yeah it's it there is no single answer to the to the question again we have this final example in this particular chapter referring to yeah words and sentences right so those time scales don't seem dramatically different actually right like we're looking at a matter of let's say milliseconds versus seconds um like predicting what the next few words are going to be an entire sentence to try and capture something like what the sentence is about versus the the faster time scale of word word word word like word to word um in the in the sentence but I I did want to mention a couple other uh as far as like um trying to map this to uh Concepts or behaviors in Neuroscience um another way of thinking not necessarily thinking about it this way but one potential analogy could be something like working memory versus versus long-term memory uh right like like here's your quick access like working memory memory of the Moment by moment long-term memory occurs over time you know it gets Consolidated um slowly over time and it it can lead to long-term changes in things like uh your daily habits um what comes more naturally or quicker to you so it just describing it a bit more intuitively that way yeah I like to sometimes think of it as like yeah shortterm versus long-term memory then in addition to that as we saw in chapter five uh in terms of the neurobiology um it's commonly been found or or theorized from from various studies when working with neuroimaging data that uh whenever a participant in these studies is learning something uh what happens is that that you tend to find when looking at brain rhythms from let's say EEG um that uh more Theta and gamma waves are present simultaneously and so they view it as what would be called a Theta gamma coupling we do get a reference to that in chapter 5 in this textbook um Theta rhythms are slower uh in a in a particular kind of Rhythm band or frequency band uh gamma are faster and so what happens is these rhythms kind of um you know it's kind of fun you can think of it as like music you know you have some fast drums and a slow piano or you know some such thing but there's a kind of sinking between these two uh differing rhythms and it it is seen as refle um being a kind of neural correlate to someone learning something um so multiple time scales in a PDP multiple time scales in how our memory Works multiple time scales in Neuro Imaging data just a couple more uh kind of metaphors is Food For Thought So one one more question that um it almost I think I'm wrong but it almost makes me want to wonder um if um if the learning in this sense isn't um is it an effect of um [Music] um actually acting on a policy but that may or may not be correct but the other thing that comes to mind um would be would would surprise occur at either level in other words if they if they don't learn or if their policy doesn't um get the expected payoff then you know there would be a surprise but also learning would could actually Encompass a surprise as well so makes me want to say that there's a almost a surprise tolerance yeah like thinking back to the chapter 2 the basian surprise is the Delta of learning and so if if the hyper priors are fixed the basing surprise is going to be zero no matter how surprising in the first sense the observation is like we think that the room is this temperature and we're not open to learning we' have fixed hyper prior and then even shocking observations just continue to shock US versus if you had um like a kind of adaptive learning rate then there would be some like a surprising observation would come in that would catch your prior wherever it initially was and then that would also move your prior some amount over to the new observation so if that kept happening eventually like you would converge so that that was no longer surprising and then how fast you converge is attention slash the learning capacity over different time scales so learning capacity would would come into the level of Tolerance I guess would coincide thank you and uh forgive me um to clarify so learning capacity is represented through those hyper priors yeah um indirectly yes like like when Andrew mentioned the coin flip it's like when um and this is just um you could see this in any kind of uh basian statistics overview like if you only had flipped the coin twice and you got one in one then you'd have a wide prior centered around 50% because your maximum likelihood estimate for the fairness of the coin is 50/50 but it's very dispersed it can't be zero can't be 100 because you know that you've seen at least the other things but but it's broadly like it's it's a very over dispersed and then if it was 1 million and 1 million then the the the the two millionth and one flip would be very tight would only move the distribution a tiny amount so that's like that learning by counting and then as you count up more and then so then then you have this whole question of okay well then but then that just slows down our learning rate but maybe it's a cell differentiating and it does slow down its learning rate to a halt or maybe there's another process that takes a look at that um learning by counting variable says when this is 100 we cut it down to 20 and and that's this whole challenge with the iterated modeling is figuring out all these like second order rules and diagnostic to be like okay should we cut out 20 or like 30 and then those are all the like simulations and sweeps that you do and then investigate how the agent is behaving in different regularities got you thank that's a good explanation that's a lot of compute it is so even that's one very interesting thing which is like even when like the model as parameterized might be low compute to run the open question is how much did it take to get there not dissimilar to like a language model where in the forward running of a trained model it's not Mega computationally expensive but then training it can be expensive and similarly while updating the model might not be expensive so for a toy model we're just whatever you make you let it run and it's all good but to find like adaptive and resilient ranges of parameters with many many possibilities that can be running these models many times cool well we return for chapter 7 part two next time or feel free to join chapter two it's kind of funny chapter 2 and chapter 7 they are very similar and in Chapter 2 we kind of reference chapter 7 because that's where the discreet time models are then chapter 7 took us back to chapter 2 with the surprise and everything so oh thank you Cheryl I'm going to copy this in you're welcome thank you okay farewell bye he bye bye every