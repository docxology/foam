hello and welcome to the active inference lab today is april 14th 2021 and we're here in guest stream number 4.1 with elliot murphy so this is going to be a really interesting talk and elliot we appreciate you coming on so i'll just pass it to you to introduce yourself introduce the topic and then share some slides so anyone who's watching live or i guess later in the comments just ask questions and we'll have time during this presentation and at the end to go over some comments but thanks again elliot and looking forward to hearing yeah thank you very much and so i am a postdoc working at ut health uh in houston texas i work on intracranial recordings of epilepsy patients doing a bunch of language tasks trying to explore the neural basis of language and today i will i'm going to present uh some slides kind of reviewing um a recent preprint that is currently in review on active inference and the free energy principle so i'm going to share my screen yep looks good okay so yeah i'm going to give you an introduction to the topic and then talk a bit about linguistics and then try and wrap up um you know we'll make this more of an obvious conversation uh comments questions uh objections obviously uh more than welcome from anybody so yeah like i said this is the preprint that's currently uh in review uh with emma holmes uh carl first and ucl uh the argument is that natural language syntax complies in some degree with the free energy principle so just um outlining some kind of really cool uh you know basic principles i want to make this conversation a little bit more philosophical than than normal and obviously this is this this lab is i have influence lab so i want to kind of introduce some linguistic topics more than the fap topics and because the audience might be less familiar with the kind of synthetic theory so to begin with natural language syntax yields what linguists then call an unbounded array of hierarchically structured expressions unbounded meaning that they can potentially go on forever there's no upper bound on the limit of a sentence length right you can always make any sentence longer by simply adding john said that at the beginning of it and it gets longer the only thing that stops sentences going on forever is the working memory the age of the average human being and indeed the age of the universe you know sentences can't go on forever they have to stop at some point and but the generative component in principle could do that so that's the interesting part about language the fact that you could in principle generate an unbounded array of expressions so i kind of argued in this paper that these are used in the surface of active inference which is in turn in the code with the community principle so the general goal is to kind of align certain concerns of linguistics with those of the normative model for organic system behavior associated with the fpp so i'm going to be relying on theoretical linguistics with special analysis on syntax which is the system that look that determines the kind of order that words and go in the structure and organization relation the organizational relations between words and sentences so a lot of these design principles are kind of um general to the biological component of language not just for specific instantiations of language so they're not specific to english but french or swahili they're kind of general design principles um about how the language system seems to be like organized and so like i said i want to kind of emphasis emphasize more on the linguistics and uh topic more than active inference just reverb to the exposition and although the pre-print itself has all the kind of relevant details if you're interested in reading more but um yeah but kind of the brief historical background here is that since around the 10th century about 20-25 years ago lingus has basically been and developing theories of linguistic computation and which invoke economy principles so um basically the idea is that when you pass an instructor sentence there are certain computations that enter into that process and it's not just a single kind of chunk that you memorize or produce you do it in discrete operations and yeah for some reason there's currently no means of grounding or motivating these ideas through more general non-linguistic domains so recently proposed principles of economy um such as minimal search or least seven criteria which i'll explain and give you definitions up soon and i i argue that they adhere to the ftp and if this can be shown then this permits a greater degree of explanatory power to the fbp with respect to higher language functions and also presents the language of the first principle grounding of notions pertaining to computability and so on so in other words the idea is this um natural language syntax is a system it's an organic system it's a formal system it can be described using the the language of uh theory of recursive functions and computability imported from computer science and all the rest of it but there's a problem and all of these principles of economy and language design in the literature are unsurprisingly kind of language focused right they have a very kind of linguistically encoded background and so there's a bit of paradox because one of the original goals of this program was actually to see okay let's see how much of language is actually uh can arise through the kinds of formal principles that govern the organizing uh shape of you know snowflakes or the mythology of lightning blocks and so on kind of just general um uh domain general uh laws of nature essentially that can give rise in different ways across different domains and yet the linguist the the teacher is still kind of uh encoding these in language specific terms rather than relating them to more general principles of brain organization or mental computation so that's the kind of background and so many historical insights into syntax i argue are kind of consistent with the fbp which provides a novel perspective under which the principles governing syntax are not limited to language but they actually reflect domain general processes that underpin a variety of positive computations this is also consistent with a strain within theoretical linguistics that explores how syntactic computation may adhere to general principles that may well fall within extra biological natural law in particular considerations of minimal computation such that certain linguistic theories might be engaging with general properties of organic systems that impact language design so i think that's kind of a beautiful idea the idea that and the language that we speak english french german not only are they biologically grounded but actually the rules that kind of govern the uh what you can say what you can't say and and indeed how you how your brain computes in parts of sentences part of that process you kind of get for free if you just assume um extra biological natural law one of which you can you can argue is is the french principle so that's the kind of background here so um the structuring influence of the fb can be detected in language and not only has been argued recently at the complex level so the um kokkerson and his colleagues have published a bunch of papers in the last year or two mainly in 2020 arguing that and active inference can relate to narrative comprehension interpersonal dialogue like when two people talk um cooperative intentional communication and speech segmentation and but i'm going to kind of argue that all of these things rely on something much more lower down and that syntax if you can't um synthetically construct the phrase if you can't put two words together and form a phrase you're not going to get very far right you at least need to do that in order to engage in um you know intentional uh cooperation and narrative and storytelling and speech segmentation you at least need to be able to form a phrase right if you can't do that you can't do anything to do language so i'm i'm kind of arguing that and you know all of the all of the ways that the fep can be related to all of these things here is arises from all kind of fundamental lower level consideration to do with the way that phrasal computations are executed um have you changed any slides we can't see any slides changing just okay uh yeah i have changed a few slides just maybe unshare and reshare but it stands alone what you've said as well but it will be also good to have the visual you see okay how about this so we see your mouse and then just advance like on the left bar like click on to your next slide yep perfect okay what about now no okay oh i see my bad my bad okay um well i might have to like window in focus yeah this just as a non-linguist was really interesting that you appealed to like kind of bigger laws than linguistics like complex systems theory or snowflakes and lightnings like you said and then yeah just pointing out that you're kind of bringing linguistic level rules or patterns into a bigger scope okay cool so we see it with the chomsky now perfect perfect okay well um i've only skipped like five slides so everything i've just said is so yeah i show the the paper here's the paper and i simply bring it to this i exercise this all of these things i've already read out so yeah if you heard what i said then that's good great okay so can you see slide number eight yep cool awesome okay uh okay so um if the process of constructing hierarchically organized sets of linguistic features into words and phrases and sensors can be shown to adhere to principles of efficient computation then this process must also operate within saying fundamental constraints on your dynamics such as those implied by the fpp to which the homeostatic brain minimizes the dispersion of intraceptic and heteroceptic states the ffp can also allow us to understand how natural language complies with the constraints imposed on worldly interactions deriving certain features of language from first principle so again at the moment this is all this is a little pretty abstract and i'm going to give you some much more like concrete examples so i apologize if it's a little bit philosophical at the moment and so a number of robust findings from theoretical linguistics can be used to support the image of the brain as a constructive organ assembling and unfairing linguistic representations in the sense of surprise minimization and related goals those syntactic structures are not minor external entities they're not the kind of thing that a physicist could easily examine but they're rather actively inferred by the brain uh seemingly with the help of endogenous slow frequency activity coordinating cross-court organic itemization of linguistic features according to a bunch of recent models of neural linguistics which i'm going to get back to at the end um so in other words we have a general framework for how syntax is implemented at the abstract level and we also maybe have a kind of a decent understanding of how that's implemented in the brain so that's the kind of the long vision of this presentation at the end we're gonna i'm gonna try and wrap it all up by returning to the brain um but for now uh i'll give you a more kind of concrete example so i said that syntactic structures have to be unfair they're a form of inference they're they're inference generation they're not just um passive um perception so um if you have a structure like we watched a movie with jim carrey that can mean two things and it can either mean um the movie stars jim carrey the truman show say we watch the truman show or you actually watch the movie whilst um sat next to jim carrey it depends on how you pass it right it depends on um how which phrases get merged with um in in which order basically and so there's a kind of general roof uh schematic here um that you can kind of find a collaboration of in the pre-print paper that i mentioned but the basic idea is that there's structural ambiguity that arises from syntax so therefore the whole process of syntax has to be an inference process it's not given to us uh on a spoon we have to kind of we have to do some homework in order to construct every possible pass so that poses the problem how does the brain do that and it is an inferential process so what are the operations involved in that so here's a kind of a standard what called free structures and linguistics there's there's two different ways of passing the sentence and you can either shoot an elephant who's wearing your pajamas which is definitely possible or you can shoot an elephant whilst wearing grandpa jumps and it depends how you in which order you merge the phrases right in which which hierarchical relationship is exhibited if that makes sense so uh there's kind of a lot of theoretical background here in linguistics that i won't go into because that's a whole other you know the whole whole different lecture um but the basic idea is that there's a ambiguity in structure generation it's not always a language is not um a system of beads on a string it's a it's a structure so along along that kind of theme i'm also assuming a distinction in linguistics between what's called i language and e language and this is a really crucial extension to get clear so i want to make sure that um this makes sense to everybody so an eye language is um the actual internal knowledge that an individual human being has in their mind brain i stands to individual internal and intentional intentional with an s meaning meaning that which literally means meaning generating meanings on the other hand you have um what i call the e language perspective and that is actually arguably not really a vulnerable or coherent it's the idea that language is a kind of external um a system behind external an extra mental system like the english language is somehow out there in the world and when we learn the english language we kind of approximate some kind of mind external system and we all have something in common i.e we all approximate to the english language in different ways but the island perspective assumes that when we actually communicate with each other and the reason why we can successfully communicate is not because we share an e language in common it's because our languages successfully and sufficiently overlap to the degree that we can actually communicate with so everybody's our language is different it's often said that there's you know seven thousand languages on the planet but that's actually not not true there's actually you know however many people that are on the planet i can't remember it's like seven billion or something less than half and that's how many languages there are and every human being has a different language faculty and it's set that parameters are set differently we all have different um idiotics different understandings and so on and that kind of sounds really obvious right when you think about it it's like obviously that's true but um the implications for the study of language are actually pretty uh kind of natural in other words when we study um linguistic confidence we are studying like a mind internal computational system and we're not studying something outside so the english language is not something that lingua is actually study we don't know that's not a coherent concept uh it's like the concept of culture or um community or whatever it's these are not things that physicists could you know identify and that's not to say that we can't say things about them you know we can't abstract the human mind count stroke theories of e-languages i guess you know you can talk about the english language you're changing over the decades which is you know kind of definitely way to talk about language and you can study in that way but from a naturalistic perspective from a biological perspective that's not use the only way we can study it is based on what an individual mind brain is doing so to give another example here um if you take a recent film that ign released a trailer for yesterday hitman's wife's bodyguard and the defining property of language is said to be this unbounded array of hierarchical organized expressions via location so radication is a defining property of natural language syntax but it turns out to be distributed unevenly across the world's languages so some people have argued that certain languages don't exhibit embrocation at all and but this actually tells us nothing about the actual biological language so in other words speakers of these languages can easily learn portuguese right so the idea is that and language can easily and readily be exhibited by most of the world's languages again when i say words languages i mean a kind of convenient description of every individual's fixed language faculty so most germanic languages only allow a single pronoun and genitive limited to proper nouns but german doesn't allow recursive embedding that possesses though in german you can say john's house beyonce john's sister's friend's house um so the above movie when it's released in germany ashu is going to have to be called something like the bodyguard of hitman's wife which it has a slightly different tone to it so the idea is that um this fundamental capacity of language to execute location and you can find it all over the place you can find it in uh phrase embedding center embedding phrases the way that we actually convert phrases or you can find it in other ways like in this example here so um relating this back to the fbp while the mpp is a variational principle of least action such as those that describe systems we've considered one of these a relatively recent program in linguistics has suggested that natural language syntax adheres to principle principles of least action and minimal search engines so modern theoretical linguistics remains comparatively remote from other fields in cognitive science but certain postulates from this field resonate with the fep and it's the proper so many biological and cognitive principles of efficiency might be special cases of a variational principle of free energy pursuing this assessment should allow researchers from distinct disciplines to re-evaluate their hypotheses and empirical evidence in terms of lower level free energy which is the kind of goal i have in mind here but we should also stress as a recent paper does that the fap can be used as a methodological heuristic for research and but it's not a theory of everything it's just a framework and actually i find this very similar to a framework in linguistics called the minimalist program which is a program it's not a coherent not yet at least it has a couple of theories working theories but it's not a coherent body of doctrine at the moment it's an ongoing research program with a particular ideology and framework and that's exactly what the what the f is right it's a formal principle which yields that i can contribute to a discrete individual and separate and sort of theories depending on what domain you're looking at and so i kind of like the idea they're both programmatic notions and and you can implement them in different ways so uh so we'll be working here with the fab and the paper in question doesn't work on the fpp so much it's not an applied approach to the fpp and so the fab has been argued to be more of a kind of conceptual mathematical model for self-organized systems and as a recent review by andrews makes clear there's a number of ways that the fbp has served as an aid to scientific work without constituting like falsifiable stations about the state of nature it's a it's program it's it's a way of usefully describing uh certain uh joints of nature without necessarily carving them so while we argue that natural language syntax and so when we argue that natural language syntax complies with the free entry principle i'm not necessarily implying that the fap you know necessarily bears specific direct predictions from the behavior it's rather way of motivating the construction of neural novel conceptual arguments for how some property of organic systems might be seen as realizing with those so as i said uh repeating their language language distinction and focusing on knowledge which is internal to the mind of the speaker exploring their apparent competence rather than you know what they happen to produce so one example of what you can think of immediately is the existence of syntactic structures can contribute to a unique form of episode encouraging through maximizing model evidence and minimizing surprise and variation of energy the actual paper in question that i'm i'm citing here kind of goes into more detail but the basic idea is that the highly restricted uh set of syntactic projections while called um the way that you can categorize a phrase and nouns and verbs and complement as a brace and so on that restricted set that finite set of ways that you can define a particular phrase as being a bit of phrasal vampires and achieves that goal so you can also assume that languages and language users during comprehension select the phrase structure that's least surprising from the perspective of the hierarchical gender model again this goes back to the um we watched a movie with jim carrey example it's a prediction and uh uh plays a pretty big role here so our search for a structure building can be cast as an internal action it's an actual internal action that an individual does in the sense of the active inference framework that selects someone competing hypotheses i.e syntactic structures you can pass it this way you can pass it another way and also the same goes for lexical semantics not just syntax and in other words you can you can you know interpret an individual way of one by one and i'm going to give some examples later so pretty much all of language from individual words up to sentences and are inference generated and also a little side point here and relating more directly to existing work on active inference and communication and it's i think it's pretty interesting to know that the recursive combinatorial apparatus of syntax has been argued to facilitate recursive theory of mind right so the ability your ability to know that someone else knows that she knows like you know this and so on and it could be seen there for his deriving or piggybacking in some way and active influence-based properties of higher order linguistic communication and which in turn serves to unveil the latent or hidden states that are other people's mental states which is the kind of standard assumption and theory of mind active inference research but i think it can be related much more directly to language once we understand that the extended kind of uh uh use of our theory of mind in language is it comes directly from this because recursive property right okay so i mean i'm going to move on to kind of the more concrete examples everything i've said so far is pretty philosophical and pretty abstract but i want to kind of give you some actual concrete examples here so it makes a bit more sense so free energy provides additional constraints on what a computational system can be physically realized as which is very useful so take the first three principles in classical recursive function theory which allow functions to compose substitution primitive recursion and minimization these are all designed in a way that you might think of as computationally efficient they reuse the output of earlier computations uh so for instance substitution replaces the argument of function with a function primitive recursion defines a new function on the basis of repairs of call to itself bottoming out and a previously defined function and minimization produces the output of a function with the smallest number of possible steps so the notion of minimizing surprise can be used to ground observations from theoretical linguistics pertaining to the grammar's propensity to reduce the search space during syntactic derivations permit no tampering of objects during spectacular derivations so it restricts the number of resources able to be combined in a given moment so in other words this is very kind of an abstract example but i'm going to give you some more concrete ones soon uh during any particular part of a sense you don't just search the lexicon and extract a new unit just because you can't you only do so if you need to if the current and number of lexical items that you've already said so that's gone for and suffice to generate a given interpretation then you don't need to search for more items that's sufficient right so in other words don't just do don't do more search when less search is possible and also relates to the grammar principle to limit the range of representation of resources able to be called upon during any given stage of comprehension so examining some core principles of occasion natural language clearly exhibits minimization right while binary branching of structures limits redundant computation so binary branching uh calls back to that slide i showed a few minutes ago where you have these uh binary branching pre-structures so i shot an elephant in my pajamas and that all uh relates to that can will be derived through successive implementations of a binary branching operation that simply puts two things together and gives it a second identity so natural language syntax also exhibits discrete units obviously there's individual words and which leads to a discreteness continuity so syntax is driven by closeness of computation so objects x and y form a distinct object x and y its objects are also bounded so there's a fixed list so there's nouns verbs adjectives adverbs prepositions converses uh tense objects uh little nouns and little verbs aspectual elements and so on all these different particular functional and content related notions their hierarchical ordering is based on a specific functional sequence as well and which imposes direct restrictions of combinatorics these objects can be combined into cycles so recursive embedding which can be extended to form non-local dependencies so an example of that is you have like number um plurality marketing so you can say the keys to the old wooden cabinet are on the table um not the keys to the old wooden cabinet is on the table because obviously it is those are keys that they have to mark but they have to agree in the number feature right so that's an example where you have a non-local and dependency between two elements so you have to hold it in memory uh even though they cross phrase boundaries so syntax still keeps on generating new structures but you have to associate one element with a new element further down the road and so these properties are in turn guided by principles of minimal search unleashed effort which i'll show in a minute fulfilling the goal of active imprints to destroy meaningful representations as efficient as possible and i think that's really what it comes down to that's the kind of core crucial message here and if it can be shown that the the language system uh constructs meaningful representations as efficiently as possible then therefore it must be on accord with the fdp and again that contributes to surprise minimization amongst other goals so from the perspective of fep and the range of possible structures available to comprehenders provide alternative hypotheses that generalize as such preclude overfitting sensory data so if the complexities of linguistic stimuli can be efficiently mapped to a small series of regular and regularized in the sense of blend syntactic formats this contributes to the brain's more general goal of restricting itself to a limited number of characteristic states so in other words only change you believe about things if you have to right and by mapping syntactic structures to language external conceptual interfaces now that's a kind of a key thing language external interfaces what that means is you have a narrow component of language the kind of narrow what's called the narrow faculty of language we can call it whatever you like it's just the capacity to construct phrase structures but then you have the language external mental modules and memory systems and what have you and attentional control systems uh in the mind that are located across different political surfaces you know it's it's the kind of standard the standard modular framework i guess and so by mapping these structures that i've been generating language to these language external interfaces in a minor adhere into principles of economy language can be seen as engaging in a series of questions and answers with sensory data specific so all the recent work in theoretical semantics assumes that there are and there are fetable concepts that language can interface with and they're also non-fetchable concepts so in other words that there are systems that there are properties of human thought that the language system seems pretty keen on um and it likes to use and exploit but there are other uh modules of cognition that for some reason um can't be linguistically encoded as easily that's kind of a weird property right like why should we why should we be able to linguistically encode and communicate about certain thoughts from others and so one example is many of the world's languages again when i say words languages i mean that in quotation marks and what i really mean is many of the individual human beings they make use of quantification or numerosity often you know typically seen as via this frontal parietal uh quantification network which is interestingly closely linked uh to major language sites so that might be one of the reasons why it makes use of it and but they make little use of color despite color featuring just as prominently an ordinary experience right our sensorian is filled with colours um and this might be due to the remoteness of occipital visual regions in the brain maybe maybe not uh that's one explanation maybe it's wrong but it's one possible reason so for instance one might imagine some functional morphine um encoding brightness or shade of coloration in language you can imagine it like if you were to invent a new language you could you could encode color uh features somehow or some kind of inflectional but that doesn't seem to be the case even though like i said that color is a pretty important part of your life at the same time language seems to make considerable use of certain contentful concepts like eventuality but not others like worry if you're like you know no things mark how concerned somebody is about something i'm very concerned i'm a little bit concerned i'm not very concerned which seems to be making potentially neurally efficient use of specific access easily accessible representational resources rather than less easily accessible cognitive modules right to map complex meaning onto natural language expressions so it's fairly observed that language acts as an artificial context which helps constrain what representations are recruited and what impact they have on reasoning and inference so words themselves are highly flexible and metabolically cheap sources of price throughout the neural hierarchy this is a really cool idea that i'm going to expand on in a few moments of time so to give you some more examples of again writing this back to active imprints and take the second ball second blue ball to the left of the large box that's a pretty simple spatial direction but it can only be encoded via natural language syntax right you need that kind of hierarchically organized phrase structure to generate that particular thought it's a very simple thought very simple direction but you can only generate inferences about it through using the recursive um combinatorial apparatus of language similarly you have structures like the young happy eager either going to oxford cambridge pleasant man this involves unbounded unstructured coordination involving disjunction too so the disjunction is you know x or y uh which is a highly complex structure to compute it's a highly complex conceptual structure and your language users can easily do it and readily infer it but it also um opens the the door for a whole new species of inferences to be generated new thoughts about the world new possible hidden states new things that you can think about and one of the examples of jerry photo or the philosophies together and uh he said you know if you take a a a pen and a piece of paper you can easily draw a man you can easily draw a zebra but it's kind of difficult to draw the thought that there is not a zebra next to you right so if you try and conceptualize the thought there is not a zebra next to me how can you draw that how can you depict the fact that there is no zebra next to me that's kind of weird it's kind of difficult to do i suppose you could draw like a man and then a zebra and a line through the zebra well that's kind of weird that still presupposes that there's a zebra next to him that you reject um so language uh generates a whole new species of a new format for thought basically it generates a new format for thought and which seems to be unique in other words not readily and translatable into other domains like visual representations or so these rapid inferences are our properties and states hidden states can be generated relatively effortlessly by language and like i said no other computational system and human cognition can achieve with this well that's the idea so a number of economy principles have been proposed in theoretical linguistics these are all very you know kind of technical um syntax related notions so i'm not going to explain too many of them but i will give some examples and these have all been framed like i said at the beginning they've all been framed exclusively within a linguistic context invoking highly domain specific notions despite a core part of the intended project of modern theoretical linguistics being to embed linguistic theory within principles general to cognition right so for example the inclusiveness condition maintains that no new elements can be introduced in the course of a particular syntactic derivation so once you integrate once you're passing a particular sentence and you're deriving a particular representation of that you're not just going to randomly introduce a new lexical item or a new um hidden element just just for the sake of it you're only going to do it if it contributes to immediate interpretation and so only existing elements can be rearranged restricting available resources but it's also unclear to what extent this computational principle finds analogous examples in non-linguistic domains right so one way of motivating these language-specific and generalizations by making direct reference to the fpp will not only foster i think uh more crucial relationships between their theories of hypocrision and neurobiology but will also broaden the explanatory scope for the existence and prevalence of particular syntactic phenomena and but what's interesting to know is that linguists readily admit to lacking a specific theory of computational appreciative language like i said at the beginning it's kind of a program right it's a programmatic notion so in a recent paper galeo and chomsky point out to be sure we do not have a general theory of computational efficiency but we do have some observations that are pretty obvious and should be part of that period right that's basically the state of the field uh linguists have very um uh well uh theories of uh language specific uh efficient uh efficiency uh criteria but they're translating that into kind of more domain general uh gold areas has not been so successful but we can at least suppose that whatever definition will be forthcoming will be related to more generic notions of economy like hamiltonian notions or minimizing energy expenditure during language processing uh shortening the description of your minimal description uh reducing pomegranate complexity and the degree of necessitated belief updating again like i said you know requires one's belief if needed and more happy so one of these uh so-called minimal computational procedures is uh what's called relative rise minimality so this is the principle that states that given a particular configuration x z and y a local relation cannot connect x and y if z intervenes and z fully matches the specification of x and y in terms of the regular features i'll give an example of that so in other words if x and y attempt to establish a syntactic relation if you go if you call back to the long distance dependency thing i mentioned and put some element into beans and provide and can also provide a super set of extra particular features ie x's features plus some other features this box of relation that sounds extremely abstract so i do apologize for that but this is a more concrete example so in the sentence you haven't won and which game provides a subset of the features um hosted by howe which results in unacceptability but the equivalent does not obtain in two and so a relationship between both copies of which game can obtain licensing interpretation uh and the strikethrough denotes the originally made position after uh after movement has taken place so it's originally merged down here and then you move the structure to the beginning of the sentence in order to form a question so question formation often uh involves just moving all in uh in the middle of the end of the sentence the beginning to form a question and so if you say how do you wonder which game to play that sounds pretty ungrammatical but if you say which game do you wonder how to play which has the same approximate interpretation that's okay and the reason why is because uh uh howe hosts only a queue feature a question feature and as it searches down the structure um it encounters which game now which game bears a cue feature because of which but it also has a now feature because of game and so therefore it can't reach its final destination the final destination being it's originally made position after play which only has a queue feature on the other hand if you move uh which game to work do you wonder and you leave howe in situ then which game carries these two features here and it skips over how because how does does not satisfy the full feature specifications of which game right so it needs to search further and then only when it searches back and reaches to its originally merged position does it interpret it so this mean what this means is when you say which game do you wonder how to play you're interpreting which game at the position of play because you're not into you're not definitely that edition of wonder you're not asking which game do you wonder how you wonder you're asking which game do you wonder how you play it's all about the question is about the plays on it and so this has been argued to emerge directly from minimal search uh allowing this higher level representational principle to make directly from properties of official computation this is one example where you have a kind of a rhetorically kind of gross higher order uh principle in linguistics being reduced to a lower level and more simple kind of element so translating that into minimal search you cannot even consider as i just said when you search the structure for matching your features in two the minimal stage procedure would simply skip out but it will find the original copy of which game because it's searching in across the full structure so another example of economy can be found in the principle of full interpretation which simply states and that there are no superior symbols allowed at the two linguistic interfaces so these interfaces are assumed to be the conceptual and sensory window systems so in other words the two things you can do with a linguistic structure is you can interpret it or you can externalize it you can produce it you can you can say it you can sign it uh you can translate it into braille and so on and or you can simply think it so this ensures that the system need not compute symbols that are ultimately subverted to the goals of either interpretation or externalization so for instance in three this has an argument that does not have a semantic role so therefore it's unacceptable so you can imagine um the sentence walt gave jesse a gun that's fine all the semantic roles are filled there's an agent there's a patient and there's an instrument being involved the particular tool but then if you add to saw there's an additional kind of location preposition and being marked there but it doesn't have a semantic role so it can't be interpreted and that sounds kind of trivial right like most people when you when you give them these examples they say well yeah obviously that's kind of not a very deep thing but actually it's it's a pretty puzzling like phenomenon it demands actually it has to be explained somehow and so one of the operations that's been involved in this uh minimal computational procedure is called mage so the operation image simply takes x and y and forms the set x y it just puts two things together and this constructs the binary branching structures that i mentioned but merge itself can also derive some core uh separated properties of linguistic relations such as membership uh dominate team off and so on these different hierarchical relations between nodes and in a tree uh you know one one uh branch of a tree being higher or more deeply embedded than another one and as well as other relations see what's called c command we can put that aside and so in brief much of the complexities of syntactic relations and can be derived from successive instances of this simple marriage computation and reducing complex visibles to simple invisibles so for instance if you the example i gave here with which game you can imagine uh the set a b being constructed and then if you take the same element b and you merge it with the set again you get b a b and then you can delete the first appearance of b and then you simply get the linear order would be b a right rather than the actual set being interpreted as b a b and this seems to be what's happening here where you have a b over here which game and then an a the whole structure and then b again being originally measured so all of these kind of con superficially complex linguistic phenomena which on the surface seem very complicated and very elaborate they can all be boiled down to a very simple operation which is just take two things and put them together uh and what's interesting about this is that related when i'm not like a junction uh do not involve modification of the semantic content of the structure the adjunct is a candidate room so an adjunct is something like uh like a prepositional object like in the path or to the beach and so for instance if you think a structure like john and mary talked in the bar the fact that they talked in the past doesn't change the fact that they talk right so the actual original interpretation the syntactic structure of the integrity and the meaning of john and mary talked is not changed by adding an adjunct so you can add an adjunct in the park but that doesn't change the fact that john and mary still talk right again that sounds like a very trivial property it's obvious right but again it demands explanation it has to be a reason for that and the reason why is it turns out that and when you add an adjunct you simply um you simply linearly add it to the end you don't change the actual identity of the phrase itself uh there's no what's called labeling there's no labeling involved in prepositional major merging so you don't change the identity of the pocket rates so the ftp has been equated with the principle of least effort and its process theory is active inference so strictly speaking the fpp basically is a computational principle right uh the probabilistic beliefs it's concerned with are directed absolutely namely external states of a self-organizing system and in in a similar way that the entity is a researcher is that so too is much of recent theoretical linguistics guided by programmatic concerns right like i said it's a program it's an ongoing kind of ideology on the other hand linguists have developed theories of syntactic least effort like i said um but the process theory is a little bit less clear right how it's actually implemented is slightly less clear and what i would argue may become more clear if it can be accommodated within existing frameworks and knowledge that the active infrastructure can bring with it to help solve the puzzle of how a language is uh implemented in the brain so here's another example for you and routinely poems that rhyme evaporate so in this instance routinely exclusively modifiers evaporate so the wave routinely goes with evaporate that's how they're injected so it can't modify rhyme even though this word is closer in terms of linear distance to routinely right so ryan comes one two three words after but evaporate comes one two three four in terms of linear distance so they're more they're more linearly remote and yet in terms of interpretation they go together and the reason is and the matrix predicate evaporate is closer in terms of structural distance to routinely and the reason why is um that rhyme is embedded within the phrase headed by poems so it has it's on it exists on a different hierarchical plane if you like it's kind of lower down in the hierarchy and evaporate and so language computes over structural distance and not linear distance so sensors are not simply beads on a string they're not linear objects and they have to be linear objects in terms of centromere externalization because we live in the universe we live in we can't we can't speak in parallel we can only speak in linear linearization although there is some evidence that uh sign languages uh can communicate somewhat to some degree in parallel like you sign one uh element with one hand and then another word so there's some evidence that sign language might be able to defy the laws of physics but it turns out it's that's probably exaggerated to an extent it is still a form of linearization but just kind of a co-linearization development so language prioritizes the demands of the syntax semantics interface over other subsystems like morphophonology so while two structures might exhibit different linear orders they may exhibit the same underlying hierarchy so here's a really good example in english and fast the verb direct object dependencies are the opposite but the interpretation is concerned so john has read the book you have john then auxiliary then the then the object and in basque you have a different order you have john then the book then the verb and then the auxiliary and yet they mean the same thing they have the same interpretation right so this suggests that um a kind of more fundamental uh operations going on here namely syntax encodes the verb and direct object as an abstract phrase which omits the subject so in other words in the syntax in english and basque you have the same underlying syntax which is subject and then verb and direct object being merged so you merge the b and do first and then you accommodate the subject and then when you linearize that when you you know communicate it in the externally and you do it in different ways in english you do it one way in past you do it another way and but the basic idea is the same you have the same underlying interpretation that also accounts for something pretty obvious mainly the fact that you can translate you know one sentence into another language right that's that's a fairly obvious thing that you can do with language and so therefore there has to be some kind of commonality somehow but the commonalities might be much more deeper down than most people kind of appreciate so through the various stages of language development as well innocent children don't typically produce expressions that deviate from general grammatical principles pertaining to the structure dependence of rules even when they produce so-called mistakes so there's been a lot of research on a child language development so in other words when children do make mistakes they seem to make mistakes which accord with the grammatical rules of their language which suggests that and sensitivity to um structure dependence uh forms a core part of language design so corpus studies of infinite language exposure reveal that there are actually very few diagrams let alone trigrams so statistical procedures can help but there seems to be some more kind of innate sensitivity to structure dependence which seems necessary and as a recent paper also reviews human learners prepare to induce hypotheses that have a shorter description length and logic with simplicity preferences possibly being a governing principle uh of uh cognitive systems all in the code of what the fpp would predict so simplicity-based preferences i've got a range of formal language models too relating to the notion of minimal description and you know you might also invoke principles of minimum redundancy so this is kind of a really important idea uh minimal computational efficiency seems to be a really general cognitive goal of the brain and there's a couple of recent papers one of them in text i think that came out maybe last year or maybe this year i can't remember um i think was called memory as a computational resource which showed that um across a bunch of domains human memory in its various guises also exhibits uh an adherence to principles of efficiency so i think it's not too surprising to when linguists come along and say you know language also adheres to principles or efficient computation so all of these ideas seem to be you know out there at the moment and everyone's kind of coming to more or less the same conclusions but just using different language just using different background assumptions but the general idea i think is is kind of they all mesh well together so as i said linguistic computation seems to be optimized for the generation of interpretable structures rather than for the generation of maximally communicative messages on specifics so in other words and whenever there's a conflict between principles of computational efficiency on the one hand and principles of communicative clarity on the other performer typically wins now this is not to say that when we do communicate with each other as we're by first and then when we do communicate we do do it in an efficient way but that's a separate question from wherever the language system is designed in a way as to maximize that communication so the normal functioning syntax seems to lead the instances which reduce communicative efficiency and prioritize inference generation so that so the goal of the language system is to generate particular inferences and representations about the environment in an efficient way so here's a pretty clear example of this right if you take the sentence you persuaded saul to sell his car the individual and the object can both be questioned but questioning the more deeply embedded object in terms of the hierarchical structure forces the speaker to produce a more complex safe locution right so you can say uh who did you persuade to sell what but you can't say what did you persuade who to sell even though they mean the same thing right same word same interpretation all it means is you know what is the individual and what's the object that's it just just tell me who the individual is and what your objectives but you can only say it if you construct it in the most computationally efficient way i search for the first possible element to question right if you search for the more deeply embedded object can't do it and so the structures in 11 involve the same words same interpretations yeah the more computationally costly process can't be licensed so this is a pretty good example and there's plenty of examples like this by the way i've written a paper about it in gloucester plenty of examples in which there is a clear conflict between um syntactic uh priorities of just generating a meaningful structure and generating possible structures that would actually aid communicative efficiency and communicative flexibility that's not a priority of language so other examples show that the acceptability of sentences can be impacted based on the extent to which the construction makes and it contributes to a novel non-redundant contribution to one's mental models beliefs again this is really directly in the core of what active entrance would predict uh rather than those of course specifics again reinforcing the role of syntactic processing and inference generation rather than communication so the greater acceptability in 12b that the real by the way the reason why these are number 12 13 is that's that's that's the numeration in the in the paper so the reason why 12b uh seems degraded relative to 12a seems to then from the fact that the speakers are unlikely to be ignorant of the relevant content right it's kind of a pragmatic reason so kim knows where the soul's in bed that sounds okay but kim knows whether i'm in bed sounds kind of weird even though it's a technically grammatical sentence uh it sounds weird because you would never say it it doesn't contribute meaningfully to revising or contributing to once mental models or beliefs about the world so therefore the language system doesn't like it which again brings in closer contact uh language design with the fdp and also cases such as 13 real how even processes such as contraction are sensitive to hierarchical structure and can't be executed over any other and any random word boundary so you can say source taller than chemist but you can't say source taller than kins and the reason is because there's a invisible phrase boundary between those two elements other examples that related to it are in 14 and 15 you can say what do you want to do and you can contract and say what do you want to do but if you say who do you want to read the book and you can't contract that to generate dude you want to read the book that sounds a bit weird i mean you technically can't say it right and if you say that to me i would know what you mean i know you mean straight away there's no problem but it sounds a bit more awkward again the idea is that there's an invisible phrase behind you there but commits that stops contraction uh efficient computation uh or at least structured dependence actually i should say is also exhibited in more classical examples in the literature so if you say the man is happy you can uh question that that structure by moving the auxiliary to the front and saying is the man happy and so you might be forced to conclude well maybe to form a question you simply search the structure and take the first possible element but that turns out not to be sufficient so you can say the man who is tall is happy but you can't say is the man who tall is happy uh because who is tall is again similar to poems like ryan routinely is embedded more deeply in the the man-headed phrase is that the the question uh is happy which is higher of the hierarchy and easier to search for so therefore you have to say is the man who is tall happy because it is is actually closer to the element you're questioning in terms of structural distance than the ears in who is tall so again syntax cares about structural proximity and i'm not linear proximity and there are also constraints on this as well and so you can say john a chicken and bread for lunch um and you can question the whole uh phrasal conjunct chicken and bread you can say what did johnny punch but if you want to efficiently question one element that's how you already knew that giant chicken but you're not sure what else he ate you can't say what did johnny chicken and lunch which there's no reason why you can't do that right like why can't you say that it's a terribly fine thought and you already know that i ate chicken you know someone just told you ate chicken and something else um but you can't say what the giant chicken had for lunch because the whole the syntax respects the integrity of the phrase chicken and bread it's it's it has a phrasal identity that syntax respects and you can't violate it you can't just violate the phrase boundary and only interrogate while other examples relate across phrase boundaries and confidence boundaries not just within them so you can say sam gave a guitar to me and loaned a trumpet to you and you can question both elements you can say what did sam give to me and loan to you but you can't say what did sound give to me and loan the trumpet to you even though again in terms of communicative efficiency that's a pretty simple structure to generate you already know that he loaned the trumpet to you you want to figure out what sam gave to me right and again these uh relations of hierarchy you can find them all over the place uh so pronoun uh reference is a pretty good example you can say mary said that he has a lot of talent and that peter should go far in which case the pronoun he is being connected with peter uh in which case you have the pronoun here coming before the actual element peter but then when you simply take that phrase and question it and unstate it it's no longer acceptable you can't say he has a lot of talent and peter should go far that sounds a bit strange he should refer to someone like john and and the reason why is because when you embed that structure in one a large structure it changes the actual identity of the conjunct so the conjunct that headed by that is a compromised phrase whereas the conjunct headed by he and peter are simply tense phrases tv and other puzzles exist here as well so you can say john you can say john said he is proud of his house in which case he goes with john but it sounds weird to say in john's house he organized a meeting and when he refers to john again you can kind of pass it you can force it but it sounds a bit more awkward it's more natural for he to refer to peter if you say in john's house you know someone else that organized anything and the reason why is because code reference via this phenomenal phrase blunting is bad since syntax preserves interpretation across movement so the original structure that's generated is uh john said is he's in john's house he organized a meeting right um that's that's generated from a more original structure uh he organized the meeting of john's house in which case you have uh he uh coming before john in the same kind of tense-free structure that i mentioned earlier so in other words syntax seems to win over linear precedence although kind of quick examples like this too so you can say i gave her the book that sarah always wanted but if you say i gave her the book that sarah wanted but again that sounds slightly strange changing the syntax by adding the adverbial element changes the actual uh content of the phrase itself which allows more easier co-reference so um stepping back a little bit this this whole framework of like merging and generating hierarchical structures has been argued in the literature to kind of uh boil down from a more domain general lower level uh computational procedure so some people have called it the universal generative faculty which is just the ability to construct hierarchical structures and map onto different interfaces so the idea is that and when we have our system of moral judgment formation which involves agents patients events and so on that still requires some kind of combinatorial apparatus to generate those judgments same with music it's been known since the 70s that um musical structures have a kind of higher up correlation to them and then also with a numerosity with numbers you can imagine that it's been hypothesized by chomsky that if you restrict this operational mage to a single element and simply reapply it you can kind of generate the natural numbers right so you can kind of form the empty set and then merge it with itself and then merge it with that object and so on and that kind of gives it uh you can call it zero and then call the next one one and call the one after that too that gives you the natural numbers but the general idea is that you have an underlying generative and faculty that can interface with different subsystems so in interfaces with the sound system you get music when it interfaces with whatever morality is theory of minds structures or judgment formation or whatever you get moral judgments uh when it interfaces with the system of quantification and numerosity you get the natural numbers and then when it interfaces with the lexicon whatever that is even more mysteriously you get language which is exactly what the album i'm saying right here so huge f major sound music and so on and but interestingly only language seems to attribute to these most elements an independent identity so with quantification music and morality you simply involve a generation of a chunk some kind of chunking that's happening but with a natural language you seem to get an additional operation you don't just chunk things you chunk them and then you give it an additional identity right you give it a kind of the the um the sum is greater than the parts i guess you can call it that that way so and what's interesting is that you can use that merge structure to then call it again so kind of you can kind of treat it as an independent object independent of its constituent parts whereas you can't you don't really seem to be able to do that in music or language uh sorry uh non-music domains so there's also um a recent paper uh by stan duhain who argues that this capacity to generate nested tree structures um is a human specific kind of species property right and it gives a bunch of different um neurobiological instantiations with this and but the basic idea is that you know only humans have language uh along alongside that you get an interest in a unique level of tool complexity possibly due to this linguistic capacity so for instance um spheres are constructed from a rock and a shaft but it's not just a sphere it's not just a rock plus a shaft right a sphere is a rock muscle shaft with an additional abstraction the functional use of it the utility i'm going to come back to that in a couple of minutes but uh language seems to be uniquely concerned with functional abstractions like use not just form and then on top of that it seems that only humans have what chomsky has loosely called the science bombing faculty so only humans are scientists not surprisingly um we can do things like peace and abduction or non-deductive complex infant generation where you get you know some weird event ear cares but then we posit that if a is true and then yeah e would simply follow naturally right so we assume a and then of course on top of that we have the other example i mentioned theory of mind boxes by a hierarchical language you know i know that you know that tune so there's all these kinds of weird human specific cognitive traits all of which can potentially maybe be boiled down to or connected to some linguistic capacity so here's a good example that the hane gives and these are five different types of sequence sequences that you can generate only human beings seen capable of generating nested tree structures as i've argued here and in the paper and we generate tree structures not just randomly we don't just do it because we feel like it we do it efficiently and in the for the explicit purpose of active input generation so transition and timing chunking ordinal knowledge algebraic patterns these are all non-human uh capacities too bonobos macaques fade song they all exhibit these forms of things it's only humans that can do uh option number five the nested true structure business so again this is this is the idea that when you merge car factory you don't just manage two nouns right car and factory you actually create a noun phrase a structure that's bigger than the two parts so in other words the factory is not in our phrase and car is not a nanophrase you need both of them to create a noun phrase and then you can use that noun phrase as an independent unit um with a certain kind of computational identity so this leads to a kind of a more um important question i think which is what is language all human beings have language we all have very strong opinions about what languages and but consider the fact that geometry was originally the study of land measurement right back in the back in the day but developed a sufficiently rich body of knowledge to abstract away from original its original object of inquiry and departed also from common sense intuition so our common sense intuitions about what languages actually have no place in science uh data for common sense notions of you know massive energy of physics so mit and professor ed federenko recently conducted a mechanical tech survey a study asking ordinary people what they thought language's primary function was uh now most of them said communication uh in line with common sense and she used this um this data to criticize the uh idea from a saying part of linguistics that language isn't basically an instrument before its primary purpose is to contribute to conceptualization and but a physicist obviously wouldn't conduct a mechanical survey randomly asking people what they thought the nature of light is and a biologist wouldn't concern themselves with people's intuitions about how hearts and livers were um and so natural language syntax i think should be investigated using the same standards of scientific inquiry as any other object in the organic world right there's no reason why people's intuitions about language um should we need it right in fact if that were the case would just there's no need for linguistic departments let's just ask a bunch of random people on the street uh that's that's all we need to do so on top of syntactic phrase generation and we can also frame this as contributing to policies uh used to perform particular free and free energy minimizing actions and not just generating you know uh linguistic objects so the rapid and reflexive identification of objects states and the reds in the in the external world through simple linguistic means uh can yield complex flexible interpretations for some of the most common nominal nominals is just a fancy word for nouns aiding in the successful generation of internal models of the environment using a limited number of resources and so objects in the world have to be identified and they have to be identified now immediately right in order to be successful in in navigating the world you have to understand things straight away and rapidly identify things that's kind of obvious and but at least the certain puzzles are so for instance complex forms of what's called palisami personally just means a weird having multiple senses and and politically also turns out to be much more widespread than most people think it turns out i am almost i remember i think half the way is in the oadlessness and complex forms of i think was 46 percent uh complex forms of policily generated via multi-wave constructions allowed for a more precise and exact localization in conceptual space than discrete symbols science and gestures right with natural language syntax allowing the generation of a more accurate unveiling of hidden states in the world so natural language syntax allows us to more accurately position ourselves in conceptual estate space right again i gave the example of the second blue box to the left well here's another example for you you can say the poorly written newspaper that i held this morning has been sued by the government that's a perfectly fine sense but it's referring to three different senses of a simple word like newspaper so a newspaper can simultaneously be a piece of information it can be a physical object it can be an abstract organization and we can also call upon all of these senses at once and yet notice that this sentence cannot possibly refer to anything in the world right there's nothing this is not a kind of thing that a physicist could explore and something that's poorly written something that you hold something that can be sued it's not a coherent entity and yet language allows this um simple policymaker's word one single lexical item to generate a very rich range of perspectives to interpret experience which is exactly what you would expect from the active infrastructure and so since there can't possibly be any object in the external world that complex business word like newspaper can index a one-to-one mapping with another framework we are developing here lexical items could partially be seen as hypotheses about the structure of likely co-occurring sensory input right or hypotheses about ontological and meteorological relations between objects and states in the world so in other words a word is not simply something that has a conceptual meaning a word does not simply fetch concept and a word is basically an upper eye hypothesis about what the world is what we can interpret experience to be in any given moment and we basically test the hypothesis so we use the word newspaper as a hypothesis about what's going on outside and maybe it fails maybe it succeeds it depends on the context it depends on our state of mind and also depends on our interests and our concerns i'll give some more examples uh in a second just to kind of illustrate that so a recent paper by paul christian's lab headed by the amica setter 2020 in the paper in frontiers they note that from the perspective of active inference things only exist as a label or hypothesis or inference about hidden states so the contention that i'm kind of presenting here is that forms of complex meaning derive from natural language semantics form a core component of this labeling mechanism in active interest so linguists like to talk about you know lexical items both book table walk and so on these are basically just hypotheses composed from distinct core knowledge systems in the mind so our sense of geometry our sense of place our sense of social relations which can elucidate environmental regularities essential to active inference so here's some more examples and again a nice little quote from that they make us the labeling quote and let's take the notion of vagueness so i was once an infant um but i'm no longer an infant but i'm still me right and the boundary between infancy and childhood and childhood and adults there are legal terms for that but that's kind of a just arbitrary choice the actual concept of infancy is an arbitrary boundary uh um some philosophers do actually think that there is a specific nanosecond which transitions you from infancy to childhood but you know i i think that's unlikely i think applying fixed quantified uh uh notions like that to uh intentionally an inherently vague notion like infancy is kind of a paradox it's meaningless you know infancy is just infancy it's not meant to be a precise boundary uh unless you're a legal scholar in which case that's fine you can define legal legal boundaries between things but that's kind of irrelevant for cognitive science so consider something like infancy you also have things like pile so we say there's a pile of sand and you keep taking uh bits of sand off at what point um how many grains of sand are sufficient for to to make a pile right that's called the serotonin paradox and the notion of the vague notion of pile is great for active inference it's great for generating rapid inferences and assessments well you know as soon as you're actually interrogated sufficiently the system becomes exposed the systems flows become exposed once you're actually subjected to all that much scrutiny and then same for things like book imagine you're going to a library and there's let's say a thousand physical books but there's only um 800 kind of actual abstract books in the sense that you know every library has multiple copies of different books so the library will have you know 10 copies of the bible uh ten copies of the quran and so on and let's say john goes into the library and he reads every book in the library and he leaves the library and because he's fed up and all the books are rubbish so he just assisted decides to ban it down in that case uh you can say john banned every book in the library or john read and burnt every book in the library in which case he banned more books than he read right he actually bans a thousand books but he only reads 800 books so the phrase every book does not pick out a fixed quantity there's nothing in the external world actually and exhibits a one-to-one relation in terms of quantification it could be 800 it could be a thousand it depends on our perspective and that's the crucial thing about even simple words like book they generate these very rich pelicans perspectives that you can use to integrate experience but have an unnecessary component to them another example is something like a city so you can say london burned down and was rebuilt 50 miles up the thames and london can still be london even though it's physically completely changed it's in a different location all the londoners are dead and so on uh london has a very complex place in a sense that you can decompose into organization sense uh local location sense uh population centers and so on government institution and but the single word london does not refer to anything in the world so in other words there's no such thing as london that's just a kind of convenience it's a convenient abstraction that we use to interpret experience and but there's nothing that there's nothing in the world that the word london refers to right coherently uh you can use the word london to refer but that's an action it's an act of human will to actively do that and choose to do that so it's a choice and it's an action again while in the code of active entrance you can choose to voluntarily and willfully refer to one specific component of your representation of a city to refer to something in the external world well that's a context by context case the idea that london invariantly refers to a particular structure is just not not it's not true so by committing a more refined accurate positioning in conceptual space natural language syntax aids agents in the formation of novel policies uh to navigate and make inferences about the environment so cognition is an ongoing process of dynamic interaction between an organism and its environmental niche uh yet notions like event are also not predefined external entities but are actively generated and partially divided language system and so again events are not things in the world and events are things of the mind their construction so consider also that simple electrical items like city have properties that go way beyond the somatic complexity of other atomic representations so you can say the large school with large windows next to the river starts at 9am and has a strict headmaster on unruly students so there's nothing in the experimental world that could possibly be a location an artifact an event a social group right surely not absolutely that that's not an ontology and also using these kinds of sentences surely doesn't commit us to the belief that there are such things in the world it's a convenient abstraction it's a fiction right it's basically a picture a useful fiction that is used in the service of active inference and it does a very good job it's very it's very successful and the fact that it's so successful is evidenced by the fact that philosophers have only just begun to really investigate this phenomenon this this phenomenon is called complex gallism and it's taken centuries of inquiry to actually realize that some of our most basic common nominals uh do not have reference to things in the world they're just convenient fictions so take another sentence the average man is concerned about wage cuts because he needs to afford insurance does language commit to the belief that the world is made of things like average men and wage goods and relations of concern right that's not surely not something that we're committed to london can be as i said lonely can be fun and polluted and burned down and rebuilt we will 10 miles up the river and silvico london uh so these nested tree structures i mentioned earlier are widely considered to be abstract or even simple words exhibit considerable abstraction so in other words the the the the paper i've written with fristen and holmes focuses on nested truth structures and but it's also worth pointing out that even simple ways themselves uh exhibit a considerable degree of abstraction and perhaps just irrelevant here is between richard's invitation to consider a blind physicist who knows all the physics right in some kind of hypothetical physics complete scenario so what is it that a sighted person knows that the blind physicist doesn't know if this physicist knows everything certain experience of content surely right so what it's like to see the color red that's not part of the blind physicist knowledge so therefore physics can only capture the causal skeleton of the word we can at least conclude from this that my experience of seeing the color red simply is a property of the world but one that we can't provide any naturalistic account for and the reason why i mentioned that is because i think that may also be the case for words like london and city and book these abstractions are considerably uh much more detailed much more intricate and much more complex than we usually give them credit for uh our minds have managed to achieve an analysis of the concept of number right number theory is very rich a very serious uh field in mathematics about about number theory um but there isn't really much of an analog in linguistics so lexical semantics is nowhere near us nowhere near as detailed and nowhere near as sophisticated as number theory okay less cosmetics is pretty much the meaning of water is you know the set of all things that are water that's basically if you pick up a semantics textbook semantics textbooks are pretty much just that like the meaning of the meaning of it is raining just means uh you know it's raining right that's it just like a redescription and so in other words linguists are very far from actually having a serious naturalistic account for even some of the most simplest ways um which again might be just might just simply be because of our uh cognitive limitations right we can't construct theories for these objects um so one of the other kind of ways to exhibit this rich politically is by looking into the philosophy literature so in the philosophy literature um there's something called externalism which is the position i've just been critiquing which is the idea that words can have a kind of one-to-one reference with uh things in the external world and there was a survey convicted not too long ago which showed that the majority of philosophers are externalists they they do believe in fact that you know the way the water refers to h2o or whatever so consider this famous um thought experiment which i think contributes to our understanding of active inference as i'll show in a few slides in some parallel universe it said that water is not made of h2o but rather some other substance right xyz so in the parallel universe planet f2 the exact same as planet f1 except water is not made of h2o it's xyz so the question is can the inhabitants of this twinner use water to refer to the substance so externally saying no extenders say the meaning of water can't be applied to the substance and on the other hand in contrary in contrast to the externalists there's what's called the internalist position which says that the meaning of words is simply a conceptual structure and that's it there's no there's nothing in the external world that these things refer to so the internals obviously say yes of course they can't use it it's just a concept and so the term water seems to be polissimous between some more kind of common function based sense and a more concrete like technical sense so you can imagine that um let's let's say one of the examples that numerous given is imagine that there's a tea factory that kind of explodes and some of the tea leaves in the factory get into the local water system and so what comes out someone's tap is chemically identical to the cup of tea that they're making in the kitchen and yet one of them one of the substances is water and one of the substances is tea even though they're chemically identical right chemically the same thing and you know one of them's water and one of them is t and the reason why is because it doesn't set one of them satisfies the functional based criteria and the other one violates it indeed you can imagine another parallel universe so then paul petrovsky offers what he calls paternal earth where doppelgangers of our scientists discover that when um what they've all been loosely calling mud in fact has a deep uniform structure so obviously on planet f r planet f and there's no uniform structure to mode right mud can be anything returns all that in this parallel universe uh all of their uh examples of substances of mud actually exhibit a uniform structure xyz and so the argument is that they can use the concept mode to refer successfully to all physical structures of mud okay that's good for them right they could successfully use the way the mode to just refer to xyz what does it follow from this that the inhabitants of fraternal earth could not then travel to our universe and use the word mud to refer to our chemically diverse samples right if they if they came to a black hole and i think the answer is no the externalists would say yes right the externs would say well their meaning simply refers to xyz and since we don't have xyz in our universe when these people jump in a black hole and come to us and when they talk about they have to be speaking of something else but that surely violates the actual meaning of the term word it's conceptual representation it's not a physical structure so the idea that their natural kind of forming use of mud could not readily be extended to a policymaker sense doesn't really seem to be well supported and it's just not a good description of what language actually is or what it cares about language doesn't really care about the world it's it's it cares about when i say you know it doesn't care about the external world as actually is it's not it's that's what science tries to do science tries to achieve direct reference to things in the world but the language system just cares about active inference just cares about making sense of the work that's it it doesn't actually care if water is made of pastry oil right that's not relevant so we can use simple words like water in wood to access multiple concepts and then use those concepts in the sales of active inference in fact the trustee goes further and he shows that using government statistics u.s government statistics he notes how diet coke has a high percentage of h2o than stuff from my world in fact i'm drinking uh a bottle of dr pepper i'm not sure it actually has that information but dr pepper has almost definitely a greater content or pastoral and stuff from uh the world in your backyard and in fact that it's bright and good so there are even more like h2o and yet they're not deemed water for reasons purely to do with intended purposes right so i think a cup of tea is like 99.7 or 99.5 percent h2o and yeah it's called a cup of tea right it's not water it's tea so moving even further away from this consider that even scattered entities forget about water let's talk about scattered identities scatter identities can be taken to be a single physical object under some conditions so imagine a picket fence with brakes or a colder mobile right the latter is a thing i'm a column mobile it's called a thing whereas a collection of leaves on a tree is not a thing and unless of course these leaves are placed for like the purposes of decoration or art installation and so the reason seems to be that the mobile is created by an act of human will again the functional ocean is important here so here's the question how are these human specific notions of function and intention coded into the lexicon and how are they coded as part of any genetic model under active inference that's a really tricky question right and and indeed going beyond this too between russell and famously claim that objective is based on spatial chemical continuity uh but it also seems to be not sufficient so the four legs of a dog can be seen as a single object under many conceivable contacts such as if they were you know cut off tied together and used as a doorstep but they could still be understood by its user as part of the dog so abstract objects do not bear causal relationships and they're also not spatially temporarily located so an object is usually understood to be a concrete thing hence the confusion you know when someone denied space temporal relations so an object is an object if we deem it so and in addition i think it's important to note that a psycholinguistic lens is needed too so when philosophers talk about externalism on internalism they often just talk about language without actually knowing anything about linguistics which is kind of like a philosopher physicist like a philosopher a philosopher of physics trying to do philosophy or physics without knowing anything about physics that's kind of strange right if you want to do philosophy of linguistics philosophy of language you should really know about linguistics so here's one particular paradox in the philosophy literature the following contrast has been called a paradox like a problem so you can say batman fights more mobsters than bruce wayne and but we also know that bruce wayne just is batman right so therefore we should be able to say batman fights more monsters than batman right but we can't say that because it sounds weird and the reason why is is due to linguistics it's not because of philosophy so that there's a constraint on discourse interpretation in language through which whenever there's two referential expressions in a clause they're default interpreted as non-identical okay and this feeds into non redundant computation which again feeds back to efficient computation and the such reference is objective so we have two instances of batman there's a problem so the sentence in b forces us to search for different reference even though we know is the same reference right so this paradox and philosophy of language is not due to mind world relations it's just due to linguistics it's just a psycholinguistic phenomenon that's all it is it's pretty simple and again one of the problems here is that a lot of these uh quotes of language demand interrogation and they're not immediately obvious they kind of language is very good at constructing an illusion that we kind of become susceptible to we like to think that the things we talk about uh are really really existing in the world but in fact you know language use i like to think it was kind of a fairy tale you know using language is kind of more akin to the constructing a fairy tale than it is science because we're just constructing concepts and using them in a kind of loose freaking sony language game sets so here's another example uh what are called escher sentences so there's the famous staircase painting of essa the endless staircases go round and round and round so the the visual system doesn't care about that the visual system just sees what it sees if the world if that turns out to be a physically impossible construct that's not relevant to the visual system we just see whatever it sees and it's the same with language too so language also has things like analogous things what are called lesser senses so if you say more people have been to russia than i have or in michigan and minnesota more people found mr bush's ads negative than they did mr carries that's actually a meaningful sentence it doesn't mean anything right more people have been to russia than i have is a meaningless sense it kind of sounds like it makes sense but it's completely meaningless right because you're trying to compare a fixed finite quantity like 50 to a simple binary yes or no right you've ever been to russia yes or no and more people means five or six people right so it's it's syntactically legal but it's semantically incoherent and there's plenty of sentences like that where it's kind of the language system is very good at generating the illusion of meaning or generating the illusion of our structure when in fact if you interrogate it there's no there's no meaning and again this feeds into the idea of a efficient computation of influence generation doesn't you know the language system doesn't want to interrogate too deeply it just wants to generate an inference right what is this person trying to say here okay on the other hand it also generate it also appears into the idea of like anti-reference so this sentence sounds uh meaningful but there's nothing in the external world i can refer to so there's no real comparisons being made here between more people and me being uh beating through yes or no so in conclusion it seems that any object is much more than its material constitution or its function and we can also use his origin so thomas hobbs talked about rivers his famous example was a river can be maybe defined by its origin and they can kind of scare and diverge and and go into different paths and maybe converge again and but also a sense of continuity so john locks the personhood was that you know a person is defined by a sense of continuous identity and not physical constitution and so when a child watches a cartoon of a you know the handsome prince getting kissed he turns into a frog and then he turns into uh uh you know a human again once he's kissed again or whatever there's a chaos and then and something happens the child knows that's the same person right the child watching the tv knows that it's the same entity it's the same person uh that the prince being turned from a human to a frog and yet again that's got enough to do with reference there's nothing that could be coherently uh existed in that sense so all these uh representations seem to inspire and in addition we also have a kind of a fifth element so we can kind of call this extra linguistic biases the shaping objective so that pertains to one example there are lots of examples but one example is default marking of object services so if you say john painted the house brown this implies that he painted the external surface brown not the internal surface because we seem to have a sense of objects as being concave objects and think of scenes at all like scenes events are kind of you know we're inside scenes or outside scenes so we seem to have a kind of visually imposed sense of what are houses so if john and mary are both stood five meters from the surface of the house but mary's inside the house and john's outside it uh john is near the house but mary's not near the house mary's inside the house even though they're both equidistant the actual physical structure of the house so the house is again a functional notion it's not just a physical object it's also a kind of a functional based interpretation um so in other words at least these five components they all contribute to active inference they all contribute to generating structures but at least these five components are somehow encoded in language and i consider that to be uh one of the biggest mysteries how are these things encoded in the lexicon and how are these how are these networks across the brain interpreted and activated during language comprehension that's an extremely problematic issue because like i said when we talk about schools being having strict headmasters and being large and near the river and uh etc and we're using pretty much all of these concepts at once and we're doing it effortlessly and yet somehow how they're actually implemented is kind of a mystery but there is interestingly precedence in recent active imprints literature for these suggestions so in a recent paper perhaps that they'll argue that the fap is most compatible with an instrumentalist theory of mental representations in which representations are useful fictions for explanatory goals right which is exactly what i've just been saying about linguistics and this is also compatible with certain models in philosophy of language the endless perspective which assume that lester items have no one-to-one direct reference in the external world but are basically useful fictions they're composites of distinct representational domains that are used for successful efficient interpretation and ultimately agent survival right and it's also compatible with continuous models of markov blankets uh which have been argued to be performed kind of neil canton and help help launching the cancer cognition whereby the boundaries of cognition are diluted by the school emphasizing the interactive constructive nature of higher cognition and generating intellectual actionable concept right again the crucial concept of actionable concept it's a useful concept it's something that you can you know use in some meaningful way so the long-term storage of frequently generated lexical characters and the combinatorial rules underlying their creative deployment in language production and comprehension all of this seems to allow speakers to categorize novel sensory data into a discrete set of object hood and event good representations so there are a few events that cannot actually be passed through the simple and unique schemes provided by which increases the likelihood of speakers avoiding surprising states but more efficiently and readily you can pass a particular situation as an event then that leads to the surprise minimization right so another recent paper notes that the active inference model of the brain assumes an imperative to find the most accurate explanation for sensory observations that is minimally complex which has been recruited in barlow's exploration of minimum redundancy and which seems to accord with how the language system provides the most computationally efficient format for solving the problem of mapping linear sensory input right so linear sentences to hierarchical interpretations so from the perspective of inference individuals need to minimize the effort involved in meaning making so we propose that there is increasing evidence from theoretical linguistics and natural language syntax and that this structure exhibits design principles in keeping with this criteria so another recent paper by christian christian's group proposes that the goals of speech does documentation and all sampling data in a way that requires the most faster memories degree of belief updating in a prologue with principle so we've basically extended these claims to the domain of natural languages and indeed active imprints have only one underlying imperative to minimize generalized energy or uncertainty i'm much work in psycholinguistics so things like eye tracking you know tracking people's eye movements during their reading senses uh during fully gap dependencies uh you know long distance species and sentences where kind of elements have to relate or agree within features um it shows that phrase structures are generated predictably in anticipation of optimistic guide in fact it turns out that even something as simple as adjective noun phrases are constructed predicted so things like red boat symbol two-word phrases they involve rich prediction so maybe i should just stop there for a second to evaluate and this has been a uh awesome learning experience as a non-linguist there's a few questions from the chat and there's also just a few other things i wrote down but a lot of the questions have to do with how things happen in the brain so maybe it's worthwhile for you to just um share however much more you'd like to share and then hang out as long as you'd like to answer some questions and you're always welcome back so you know no worries just whatever's comfortable today let's talk through and then we'll continue the discussion cool all right well yeah this is the final section just to kind of wrap it all up together so um only a few a few a few slides more but yeah the question of how all this relates to the brain is absolutely essential and so i'll try and explain that so just to begin with that framework uh yeah so so far i've kind of just outlined the kind of basic philosophy of how language is implemented in in in a kind of computational theory and how it seems to be involved in efficient computation and so on how it might on a cognitive level contribute to inference generation uh but what about actual neural implementation so that's that's that's the kind of the next frontier so there's the recent paper by embargo and arguing that what makes a good theory is not just generating testable predictions it's invoking plausible possible mechanisms mechanisms that are plausibly realized in nature either in your biology or genetics or physics right it's kind of a nice framework the idea is that uh theory is not just there just because they can generate um you know true testosterone predictions it's it's to generate things that are plausible and i think that's a very important point so what's your current um uh the neurobiology language involves quite reasonably testing some hypotheses um and then generating poorly available explanations for results so you'll read the results section uh which shows like let's say for example theta power increases automatically coherent sentences maybe just choose a random example and so the explanation is just kind of a redescription of the results in the discussion section so it'll be like you know we've found a hippocampal theta increases in xyz um so therefore semantically coherent sentences are indexed by a hypothetical okay it's kind of a redescription but then what we need is really a pre-existing mechanistic understanding of the possible computational properties of hippocampal theta so what is civil capital theta in the first place right otherwise what's the point i'm looking at there's no point looking at some neural response if you don't understand the computational capacities of that neural response what is that property what is that lower level mechanism and indeed there might be there usually are multiple mechanisms multiple possible mechanisms for realizing that particular signature that you get through brain data and what if your if your simple output is to simply do the experiment and then redescribe the results using different rhetoric then that's not contributing to um you know conceptual uh progress so a lot of these ideas are uh outlined in the recent book of mine the isolatory nature of language and so i'm kind of just gonna tease them apart but if you're interested in more of the details you know free to contact me and i'll send you a copy so one can derive some elementary properties of linguistic computation through a direct line of communication from the fpp through to endogenous oscillatory synchronicity and linguistic behavior which kind of comes out of the output so under the fpp endogenous oscillations are the type of dynamics brain dynamics that neurons would expect to encounter since they have genetically encoded beliefs that the cause of excitatory postsynaptic potentials follows the same pattern so active imprints can synthesize various and silicone neurophysical responses via a gradient descent on free energy such as the mismatched negativity phase procession theta sequences place cell activity uh feed the camera uh phase amplitude and so on um and the reason why that's important with all these mechanisms have been involved and implicated in language so moving forward with these concerns neuronal dynamics and plasticity appear to minimize variational free energy under a simple genetic model which entails prior beliefs that pre-synaptic inputs are generated by an external state with a quasi-periodic orbit recent paper showed this so the implication is that ensembles of neurons make inferences about each other while individual neurons minimize their own free energy so generalized synchrony kind of comes for free it's an amazing property of free energy minimization desynchronization is induced by exogenous input explaining event related synchronization and structure learning images in response to causal structure in exogenous explaining the functional segregation of neuronal clusters so an external like neuron external state with a quasi-periodic orbit is assumed to generate the presynaptic inputs of a given neuron and what's interesting for me is that low-frequency phase synchronization emerges directly from this assumption and also the coupled assumption that neural dynamics maximize variation free energy so the reason why that's important for language is that one particular implication is that models of syntax synthetic computation grounded in these dynamics so yeah the the model that i have in my book but also some other recent models from guru grafeld and uh savannah can be said to comply with foundational principles of the fat so for instance endogenous low frequency delta delta phase trapping of syntactic nodes could be seen as emerging as a direct function of generative belief updating in accordance with vacuum imprints supplementing the association of delta oscillations with the cortical computations responsible for creating hierarchical linguistic structures so what that means is whenever you get a particular phrasal node and you have phrase boundary you seem to get some kind of unique low frequency response so the details can be found in the actual papers but there's basically some kind of unique low frequency signature which seems to code okay this is a phrase here's another phrase here's a little phrase and that kind of goes beyond the uh level of abstraction uh contributed by syllables and words so it was kind of a phrase-specific signature and so the active inference brainwave provides clear predictions about the neural dynamics of language and can help bring together research programs that are presently pursued independently so exploring the possible neurobiological basis of a core feature of language a recent paper argues that feta gamma phase after coupling in language which codes syllable recognition and also predicted coding and physical covenants been associated with predictive coding for a little while now can be brought together so this theta gamma coupling has been applied to silver passing modeling theta chemical dialogue as well which appears to form part of belief updating fact of imprints whereby beliefs are simultaneously updated at high fast levels but also low slower levels um also their feeds uncovering has been applied to a constraint well it's been assumed to be a constraint in working memory so for instance the idea is that the number of items you can hold in working memory is based on the number of gamma cycles you're going to bed in a given theta phase and there's a kind of trade-off between fidelity if you want to have uh hold more items in memory then you can indeed increase the number of gamut items to you know seven or eight gamma cycles within the theta circuit about seven or eight but then that lowers the actual resolution and in fact uh it's been shown that there's a relationship between a number of items hell and uh uh physical causal manipulation of plato chemically in people's skulls using some kind of extra you know attacks or tms and their actual performance in working memory desk so here the gamma doesn't practice because implicated in just constructing sets in working memory um which i think is kind of a cool idea so in this particular model phrase level inferences generate words contained within the phrase before lower levels reset for the next phrase right manifested as repeater through the phase alignment so each transition at the higher level is accompanied by a resetting at the lower level status so this is in line with the suggestion that low frequency phase can coordinate the bundling of lexical features indexed by class gamma cycles within the given description via forms of crosstreatments equal length namely base amplitude and also phase phase coupler ensuring serial readout are features alongside the transfer of syntactic identities to language external systems so there's the kind of top-down information between uh different types of based on food coupling this is kind of elaborated in my book but the basic idea is that low frequency phases coordinate and set the identity of whatever discrete representations are being fetched and linearized and combined and chunked via these uh faster gamma cycles cross-courtedly uh prescottically just means whatever part of the brain happens to be responsible for storing the representations that you care about again language is very good at talking about extracting fetching concepts from different domains so that will yield predictions for which clausal services are being extracted and kind of spoken to by this uh low frequency phase coordination operation so theta chemical going has also formed part of recent models of scheduling and updating the list of syntactic semantic features being associated with a given chunk of linguistic stimuli with ghana cycles indexing distinct data structures being coordinated by database data structures just means linguistic features so semantic or syntactic features right like first person number gender features what have you so these proposals are potentially analogous from an euro computational perspective so you have the same lower level generic algorithm newer computational algorithm that's simply fetching discrete domain specific representations which also kind of feeds back into the hanes idea i mentioned earlier where you have these different systems morality music mathematics language where the computation seems to be analogous but the representations are different so the same computation operating over different representational domains so through specifying a process theory that explains neuronal responses during perception and action neuronal dynamics have previously encountered the gradient flow on free energy so that's to say any neural process can be formulated as a minimization of the same quantity used in approximate phase imprints so the brain seeks to minimize free energy which is mathematically equivalent to maximizing but evidence and so this view of neural responses can be conceived uh with respect to hamilton's principle of least action right so all these ideas kind of weave together in fact recently a deep temper model for communication was developed based on a simulated conversation between two synthetic subjects uh showing that certain behavioral and neurotheological correlates of communication arise under variational messenger passing in particular theta chemical right so feta gamma coupling arose from this particular synthetic dialogue so the model of syntax i've assumed in the paper assumes that syntax sorry in this particular paper assumes that syntaxes are sequences of states or words with a terminal node at the end of every sentence so a kind of phrase boundary like a wrap-up effect right like a consolidation period and with each form of syntactic structure being limited to questions and answers in a game of 20 questions in this particular paper and but the conclusion this paper are pretty much in keeping with core assumptions in linguistics concerning the inherently constructive nature of language so elementary syntactic units which are highly robust and considered across speakers of the same language provide specific and belief structures that are used to reduce uncertainty about the world through rapid and reflective categorization of events states and objects and their relations again in compliance with the fpp uh sentential representations can be thought of as structures designed partially to consolidate and appropriately frame experiences and to contextualize and anticipate future experiences so the range of possible syntactic structures available to comprehend us provides alternative alternate hypotheses that afford past harmonious explanations for sensory data and as such they preclude overfitting so if the convexity of the linguistic stimuli can be efficiently mapped to a small series of regular syntactic formats this contributes to the brain's goal of restricting yourself to a number of states as i mentioned earlier and again by mapping syntactic structures to conceptual systems in a manner adhering to principles of economy language can be seen as engaging in a series of questions and answers with sensory data itself right but also non-linguistic mental states and only through natural language can we generate the full complexity of wh questions you know the questions i mentioned before cross serial dependencies long distance dependencies where different elements across different structures depend on each other uh fully gap dependencies and so on which permit an expansion of what kinds of querying the brain connects to you over sample data so in a word only with natural language syntax can the brain execute these particular type of queries and generate influences so all the ways that language centered aspects again if you recall what i meant at the beginning or other ways that language-centered aspects of human cognition can be motivated through conformity to the fbp and act of imprints so things like communication and narratives all of that can actually be further be derived from a more elementary focus on synthetic computational complexity and in fact there was a paper published a couple of days ago and by again by kristen's lab which showed that neural dynamics under active imprints are metabolically efficient and suggest that neural representations in biological agents may evolve by approximating steepest descent in the information space towards the point of optimal inference and again there's a pretty no that's not a bad idea to pursue in connection to neuro-linguistics in terms of the optimal inferences afforded by not just synthetic structures but again also lexical semantics so individual words that i mentioned before so just moving to a couple of conclusions from the slides and i've tried to show that natural language syntax renders meaning making and pioneer order imprints a computationally efficient process and it seems we makes it work right for what the cost etel call as a key question for future research for active imprints which is how biological organisms effectively search large policy spaces when filing into the future so regardless of whether you know you're actually issuing particular economy conditions uh x y or z my motivation's been kind of more general it's just meant to consider how the fbp can in principle provide a novel explanation for the prevalence of efficiency encoded linguistic rules and indeed other linguists might disagree with me and disagree with the actual framing and maybe the other linguistic frameworks like maybe ray jack and up spring back with parallel architecture that might be more appropriate too depends on your background assumptions so it's specifically natural language syntax and its capacity for sentence complexity that allows language users to expand the scope of their predictions about their future positions in state space they can think of more possible future scenarios right on platform recording so we've arrived at a number of suggested explanations for the way language implements the construction of hierarchical syntactic objects namely to minimize insanity about the causes of sensory data to unveil a species unique format of external hidden states to adhere to at least epic principle um and in fact in some cases this involves externalization but not always so exploring our and disposal empirically made demand a more mature development of the science of computational capacity in the brain so i basically argue that all the ways that language sent backwards and permission can be motivated for the ffb can simply be grounded through syntax so in other words narratives are strong candidates for constructs adhering to active immigrants and but the generation of synthetic praise structures is a necessary feature of any narrative right you need to at least construct a phrase to generate a narrative so i've reviewed how the fep that underwrites active imprints is an expression of the principle of induction which is additionally a principle implemented in models of syntax so ultimately both the fpp and synthetic theory are empirically and conceptually well supported constructs and as i've argued they share a number of intriguing commonalities so while the ftp has produced formal simulation supported models of many complex cognitive mechanisms like action perception learning and attention and also communication on the other hand models of syntax have explained grammaticality tuitions certain poverty of stimulus issues are how kids acquire language and the pervasive organizational role the hierarchy has in language so hierarchies just seems to pretty much organize and determine almost all linguistic scriptures whether language is not a is not the only domain which exhibits economy right and suggesting a deeper grounding of this bias in natural law other domains include concept learning cause of reasoning centrifugal learning and also as i said before memory so importantly at the conference has been used to account for also creative functions that have to do with exploration and novelty and the reason why that's important is because linguists have also long argued that the hallmark of natural language is actually its creative aspect the ability to freely construct an unbounded array of hierarchy organized expressions with novel interpretations so you can say sentences that have never been said uh linguistic creativity can be framed in terms of adherence to physical thermodynamic conservativity if it does so to minimize uncertainty undeveloped in states within an individual models of external states so in other words the more efficiently a language user can internally construct meaningful hierarchically organized structures and more readily they can use these structures to contribute to the planning and organization of action consolidation microsoftness and endogenous monitoring and adaptive environment i think it's worth recalling gregor mendel's application of complex algebra at the barney at the time this was deemed by many people to be kind of weird and more surreal and but in fact the same may be true double conceptual directions um for natural language syntax and semantics right unconventional approaches that are often soon antenna into the new normal uh only time can tell how far these directions can actually be pursued though all right thank you for listening awesome you can unshare and then i hope we can ask a few of the questions yeah do you want to unshare your screen oh great okay awesome all right i will um just jump right in with the first question i again as a non-linguist was like looking up a bunch of words learning a lot so really fascinating the first question was um and i think it was related to when you said uh that language is not just about communication despite that being a common conception so potentially it's about the structure of thought or the structure of thinking so the question was it would be great if elliot could define what is his definition of thought and what is potentially the contribution of the intracranial language research towards answering the hard problem wow so you started with the easy question okay and yeah that's a really good question so um i i like to think of thoughts as um i do any other language it's kind of like a metaphor so and you know in in some languages and when a a human being does a long jump they only jump so in english they we just say they jump in japanese they actually allow flying so in in if a long jumper in the japanese linguist is at the olympics and they see someone do high jump or long jump they could technically say they're flying um but it's just a metaphor it's a i think it's the same with thor as well so thought is just a metaphor i don't think we have any uh it's not a well-defined scientific natural kind it's kind of a useful convenience so but you know that said i think the way i see it is natural language syntax allows us to fetch particular domain-specific representations from all sorts of different document domains and then construct them into novel interpretations the important thing is that all of this is outside language so the interpretation process is an extra linguistic process the actual only the only linguistic specific process is just combining a phrase they're just combining items putting them together and then shipping them off to an external interface for interpretation and so that's the thought process and i don't have any deep particular reflections on it i think the best the the best book about this is paul batroski's uh conjoining meanings that came out in 2018 um which is the idea that what language contributes to thought is kind of what i said here today like i kind of uniquely encodes a kind of functional abstraction and whereas different cognitive subdomains like the visual system or the olfactory system i guess you know the sense of and nonsense or geometric reasoning these all contribute distinct sub representations of a given uh sentence but language seems to uniquely care about function so abstract function so i guess i would say that if linguistic thought can be defined at all it's almost definitely going to closely approximate human specific interests and concerns and needs and things like that which is kind of surprising because a lot of people just think of language as kind of um you know in an unbiased way communicating with um the conceptual structures like thought like language is the kind of thought system but i don't really see it that way i kind of see as language does seem to bring with it some unique conceptual contributions uh namely it seems to encode these human specific functions um which is why i said you know if you look at water water cannot simply be the finest hazel it's way too simplistic it's the the actual meaning of water is way beyond that and and then with respect to intercranial stuff i'm not too sure like so uh entrepreneurial research is fantastic actually uh examining in real time actual neural responses right so again really down to integrity it depends on the type of electrode that you have it's resolution it's a listening radius how much of cortex you can actually sample and there's also something called the sparse sampling problem in in independent research where you have very great coverage of specific cortical loci but then of course each patient will have different electrodes in different parts of the brain there's no patient that has electrodes everywhere they only have electrodes where their epilepsy is supposed to be targeted so for research purposes obviously it brings limitations so i don't think we'll ever be able to have a coherent global whole grain in you know intra patient understanding what we do is we submit we combine across all these different patients and generate a more kind of average brain response if that makes sense and so in that sense um it can contribute just the same way any other um energy or extra extra cranial ehg can do it depends on the paradigm it depends on the analysis i think the real crucial point here is conceptual innovation so what we really need is we have tons of data and from natural language experiments we have loads of data but we don't have all that much uh conceptual uh novelty so linguists are very good at coming up with very smart paradigms for you know very well carefully controlled experiments but then when it comes to actual novel conceptual frameworks for how these things actually map onto brain processes i think we need much more of that so i think actually um intracranial research will not be as uh useful as uh conceptual theoretical um you know that changes i think that's much more important actually yeah well one area of utility that you didn't even go into at all would be machine learning and the long time challenge of natural language parsing and generation and the recent approach has been throw the big neural network at it with the gpt and just large-scale text modeling and then that reminded me of what you said about multi-scale models how we don't over fit the semantics even when we hear a ton of syllables i really really really really really am hungry and then um you know a computer might just spin out a p value it's kind of like over sampling but we don't want to over sample semantically so there's gonna be really interesting space for active inference models okay here's the awesome absolutely okay i'll go to the next question um can free energy principle and this syntactic theory framework help explain how and why the brain computes inner speech the way that it does and provide the possibility to predict what's about to be computed in the future so how do we think about externalizing speech like vocalization versus our inner experience of speech are those structured are those in our voice like what is happening i think that's a really cool question and well i think that i think there's also a lot of like misconceptions about this so i actually don't see so so most people think that and you know external speech is kind of crude and you know it has nothing to do with thoughts but internal speech is like some kind of angelic platonic space and like a plate like plato's cave but actually i think that's the opposite i think they're both the same so an internalized speech is basically internalized externalized speech so when you speak to when you listen to yourself speaking in your in in a monologue you're doing it in the form of external speech you're not doing it in a different format it's not as if it's not as if in a speech has a different structure and a different encoding and then external speech is something different when i think to myself when i wake up in the morning i still think to myself in english i think to myself in linear sentences right that's that's how the thought is is externalized so it's this is why i said about the interface between language and conceptual systems what's happening in inner speech is that you are externalizing in your own head so that so in a switch is basically a form of externalization okay not all forms of externalization involve me literally saying things you're still externalizing it you just externalize it to yourself so in other words actual linguistic thought is is pre-conscious it's subconscious and that's why we need linguistics departments kind of figure out you know what are these subconscious operations that are happening right so we have a bunch of subconscious marriage operations that are happening but we don't have direct conscious access to those and why should we there's no reason why we should do right but we do have direct access to the externalized output in our um kind of self-generated auditory encoding of that structure so um i've got i can't remember what the question was sorry but uh yeah that reminds me of thinking through other minds um a recent paper in our you know active inference community and then also um yeah there's just so many interesting aspects about how you really pointed to these domain general attributes of language and yes i'm rethinking language and some of the ways that we even communicate on the stream because you'll see people say i'm just not sure how to say it it's like but are you sure how to how to think it oh no yeah because actually if we think through it and then we have feedback writing and we can it's like stigma g we're making these meaning marks and then we're reinterpreting that and so it's going to be quite interesting there here's a third question thanks great talk given that the sequences are generated in nested hierarchical structures where would linear externalization fit in here can we say that they're bound by linear externalizations and then maybe if that's a linguistics term what does that mean yeah it is yeah yeah okay great where do linear externalizations fit into here um uh that's a really good question um i'm not quite sure what the question is is trying to ask i kind of understand what they're saying um so i guess okay okay so it's typically assumed that um linear externalization is like the output of this of this detected system and that it turns out upon better analysis that most of the world's languages uh differences the way that the world languages differ is based on more phonological differences so in other words the way that morphemes and sounds are produced but the actual sciences and syntax is kind of fairly linear so in other words all the universal things about language i kind of care early on and all the differences in the world's languages occur very late on so late in the interpretation process so when you generate a structure that you want to say you at least need to have the syntax first because that's the most uniform structure now once you've got the syntax in place you then go about doing you know filling in all the sound details but the sound details are kind of irrelevant for language that's just an annoyance basically right so i gave you the example of english and basque so english and basque have the same underlying phrase structure but they linearize it in different ways so um they just happen to linearize it based on you know whatever arbitrary conventions haven't do arisen in those different cultures but the actual interpretation is exactly the same so i would say whatever whatever linearization uh whenever it happens happens late stage and it happens as an inconvenience to the language system interesting and sometimes people will point to different languages or a word that appears unique to even a cultural experience but it's sort of the other side of that coin is the truly 99.9 that's structured functionally like that can be translated and so it is the exception that proves the rule just like when we we violate syntax sometimes to make a point like repeating a word and then in our internal monologue maybe even like singing or alternate characters it's sort of like it's a dramatic externalization but it's the exception that proves the rule or it's kind of like the grand master chess player who who violates a principle but that is is mastery over the syntax and we can't let those exceptions that prove the rule lead us to throw out the baby with the bathwater or however else they say it but it it by highlighting language as functional and the structure of thinking rather than like um rhetorical only in its deployment was just connecting two nodes and you're really opening it up to think about what's inside of the cranium as well and taking measurements from there but it's what's happening with our thought that's being revealed in a linear string so so so one of the um one of my favorite and examples of modern literature is magical realism and magical realism is basically uh put getting a blender putting lots of different random concepts in there i'm blending them all together and seeing what comes out so a lot of literary like novelty literary conventions uh novels poetry is novel precisely because it just it's kind of experimental it's saying that's what happens when you put these two random different concepts together let's see if they can generate a meaningful interpretation that is either emotionally resonant or conceptually intriguing and in fact some of the most important poetry that's been written in the english language has violated rules of english grammar precisely to generate these novel interpretations it kind of it's an it's an intentional violation of some kind of linguistic rule which isn't it's like a signal to the reader that there's a reason for this it sends a message it reflects something or whatever right but yeah absolutely like this these these exceptions to the rule are extremely important to like to think about my final question is how might digital discourse and multimedia be influencing structure of language structure of thought like are we synchronizing on the functional aspects with memes or are we diverging in our narratives so how is that playing out when it's more multimedia visual video than ever less of it is spoken in red which is the linearized sort of classical language and now there's unconventional languages so where does that how's that going to work yeah so so i mean i know that the capacity and the prevalence of like uh long-term digesting of like you know long sunday times articles is kind of declining and the propensity for bite-sized chunking information is like easily digestible it involves less critical thinking judgments ideological otherwise can be made rapidly inferences can be made and so on and that is definitely on the rise however there was a linguistics book that came out i believe around 2010 and i can't remember who it was by um it was basically just analyzing the idea that so around the turn of the century obviously mobile phones came about right and everyone started texting and using all these different so instead of saying please you would say you type plz uh instead of saying mates you would say m8 or whatever all these different abbreviations um and at the time and a lot of people in england were very concerned about this because they said well all our kids are going to be you know they're going to grow up stupid because they use all these slang deviations of english uh it turns out that there's almost no evidence that this affects intelligence like people's ability to you know be smart and use language and identify language grammatically because in fact when you think about it if you use text language and if you open a whatsapp chat and you start and you text with somebody and you use all these emojis and use all these abbreviations for words it presupposes that you actually know the correct version because in order to violate the rules it presupposes that you understand them so fighting the rules of grammar and fighting the rules of spelling actually exhibits comprehension of genuine uh you know rules of english whatever when i call them right the in this book on texting again i can't remember it but uh so i think that's a pretty good example of when this panic this this moral panic in in society over like you know text you know it's i i the answer is i don't know you know i have no idea but on that sort of moral panic i guess i thought it's like um then at some point maybe even they forget how to spell it p-l-e-a-s-e you know how to spell please and then it's kind of like our word roots you know oh i can't believe you don't speak greek enough to know that the word roots of this it's like it's become modularized so units that were novelties at first and exceptions become reified within a cultural context a shared niche shared narrative so that like the crying emoji does mean this functionally and then someone could say i can't believe that you don't know that it used to mean this in a different language it's sort of like it speaks to so many of these excellent themes so yeah elliott awesome guest stream thanks for your first appearance and we're always looking forward to what you might want to share with us in the future so thanks again totally well yeah this is an ongoing project so i'm sure i'll have more to share and thank you very much for having me thank you for the conversation i really enjoyed it uh thank you for the questions too so yeah totally thank you elliot and to all audience so see you later