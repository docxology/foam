all right welcome back um 6 we're in the second discussion on chapter 3 so does anyone just have any quote or question or anything to kind of begin with any thoughts or we can just start to look at the the written questions from before or some page or figure from three a a basic sort of disorienting thing uh I guess in the uh diagram at the top of page 44 um where it shows external States internal States action States and sensory States I'm confused uh about what are realistic examples of like influence on that direct Arrow between active States and sensory States like again I understand active State or the point of active State generally is to impact upon the world and like eventually we see the effects of that in sensory States but I guess the direct Double Arrow between the active States and sensory States seems confusing to me yeah it doesn't have to have any influence in the real system but it's it it it could but I think in a if we if the sensors and actuators of the system are separate then that edge may have no effect but in principle it exists because the the the states are not like pre-tagged it's always just based upon the choice the internal state so a causal edge here in sketching out the specific system You' quickly determine like whether it's something causal in the system or not but we'll see if it comes up in any of the examples Susan uh yes um so can you I'm really unclear in what the what are the boundaries between um the um inference the the external and internal States in other words specifically what is action in this context and I I'm wrestling with speech acts um you know not doing something is still an action on your environment if you think about it so yes well one aspect of action that is um not really looked at in the textbook is deciding when to act like in the kind of simplest format of the action perception Loop there's just a perception and an action every single time step so the like the choice to like wait versus to do something at all um sometimes that's brought in as like a null action okay yeah just like an action that kind of like keeps you where you are okay there could be a nested model where the top is a binary decision you know make a move or not within make a move there's just the four moves um okay is is that have anything to do with the um transition function the um the the transition function is describing how the state the hidden States change through time so it's the B Matrix we're getting an observations okay and then the a kind of maps the observations to yeah the hidden States so from the thermometer readings to the temperature in the room and then the transition function is the temperature of the room changing per delta T just per time interval the temperature in the room underlying changing and the formulas on the right there you know that's actually for the transition functions yeah so then what okay three here yep which is B it's saying it's the distribution of the state at the next time point okay conditioned upon the state at the current time point and the policy so say okay the temperature of the room at the next time point is conditioned upon the current temperature in the room and whether I'm turning on the air conditioner or not okay okay and then which policy is like selecting which slice of b gets applied as the transition function and one last question um so is this is this model just saying that there was a move made and there was a and that's a SE is that is that you know as I'm seeing two different is that perception action or is that just a different action repeating in the picture you know just looking at the yeah like the fact that there's two threes here no actually because you got the upper and the lower so is that the higher high road low road no um so 4.3 the top graph is the discrete time model and the lower graph is the continuous time model okay and then in um here's just the discrete time model okay just the top half of figure 4.3 okay so here again we see the B and it's hidden State at the next time point to plus one y conditioned upon the current state and the policy okay thank you and and I think as we kind of may have alluded to earlier there's like this representation of the generative model which is the partially observable marov decision process and then there's this particular partition representation so here we have the kind of hinting at a circularity and the flows because the dot on top is the derivative so the flows across these four states which we can associate with the um from the perspective of these internal States as the sensory action and external and then there's this form which is more like a branching tree a little bit because here like the observations are a dead end okay I don't exactly remember what we got to last week but I think we looked at figure 3.2 with the Scatter Plots um does anyone want to ask a question or should we just try to go through these Thomas oh I'm sorry that was a hiccup by accident I apologize okay all right let's look through some of these um what's the difference between the terms basium brain hypothesis predictive processing predictive coding active inference so here's it's um discussed in several places here's one reference um these terms can be just kind of broadly used to refer to very similar uh theories and kinds of models or if you really get down to Last Mile they could differ um basian brain hypothesis hypothesis suggesting it's something that could be tested and falsified for a given system like you could hypothesize that the brain is doing basian computation or is doing something that can be modeled with basian computation you could hypothesize that and have criteria for why you would accept that hypothesis or not and then you might as well um do that and then find out or not so this is testable hypothesis describing whether brains which are just some systems in the world but a tiny subset of systems do basian computation however they do it or whatever their distributions are um active inference probably a lot of ways to Define it in a broader sense it could just refer to models that have action included within the scope of inference like models that do sense making and action selection or to be more specific we might be talking about active inference models like perception action models under the free energy principle which is kind of chapter three high road and then predictive processing and predictive coding again these are sometimes used like interchangeably but um to get technical about it predictive processing is the kinds of architectures that'll get visited in this textbook where there's a multiscale model and um higher levels are making predictions about lower levels and um the entire processing architecture has to do with predictions hence predictive processing predictive coding kind of nuances that a little bit and highlights that the messages that are being passed incode predictions and prediction errors so you have prediction process predictive processing systems that that do predictive coding you could also design one that had like one or the other so they're all really related but they're not exactly exactly the same CH if one defines preferred States as expected States then one can say that living organisms must minimize the surprise of their sensory observation I'm not clear on the ontology here is it that preference is the same as expectation or that formally we can use them in the same equation these are lot of partial related questions here [Music] um I'm not I don't remember if we looked at this one last week when we're talking about the variable called preference C it's a distribution over observations I think last week we talked about how expectation can be used in kind of two senses one is the expectation is like about an event that hasn't happened yet and that's a conversational usage of what we expect to happen in the future also a given distribution can have the expectation calculated of it that's the fancy e and that's the same thing is saying the the central tendency or the mean of that distribution um in the case of pragmatic value in expected free energy equation 2.6 yes when we talk about we're talking about an expectation about observations but just saying expectation alone is almost not a complete sentence because you need to so it's like saying well I'm talking about the mean but the mean of what variable so there's settings where um it's clear from Context what it means and that we're talking about the same variable but you could also Imagine a situation where you're talking about the central Tendencies or the future possibilities of other variables so it's not the terms are simply identical by definition it's that in certain kinds of generative models they're described the same way to highlight that there isn't the proposal of like a reward function but rather there's a direct surprise bounding on expectation um Aaron yeah well okay this isn't exactly a refinement of this question but it's related I I guess like I I have been struggling with how to think about uh I I guess the distinction between priors and preferences in contexts where you in advance of receiving some observation have reason to expect uh and I think the narrow sense um that conditions will be other than you would prefer right so if the weather report this morning says is going to be 115 in the afternoon and I you know my car rigs down and I got before I I open the car door to get out on the side of the road I expect it to be uncomfortable and challenging to my body's need for homostasis I guess like in maybe it's not an ontological distinction but I am struggling with like the uh uh under what conditions is reasonable to sub uh to substitute um a distribution that describes preferred States into to a context where where you would be talking about a distribution that describes your belief about States prior to getting more information about them yeah great question so let's just say we're talking about temperature so our um C preference distribution is going to be like homeostasis just a a a gaussian distribution around 37c so we're going to talk about that as our preferences now it's also a prior that's why these things are like not complete sentences it's not that there's one variable just only and always called prior because any distribution that could be updated you might be talking about the prior distribution of that distribution so hence it's it's like when talking about like beliefs it's like about which one of these priors prior distribution about which one now when people don't further Define which prior they're talking about usually they're talking about D the prior on hidden States so let's just say that um we're we're in the desert and we we have an accurate thermometer so our a matrix is is just the identity Matrix just the ones on the diagonal so we're just getting accurate thermometer readings um we prefer 37 in terms of our homeostasis but our prior belief is that it's going to be 50 and then we get a thermometer reading of 50 and then that's consistent with the hidden State estimate of the temperature being 50 so there's totally no issue with preferences over observations interfering with the sense making process because C doesn't come into play with a or D however the the C does shape action through a pragmatic value but C doesn't play a role in sense making or in um estimating future time steps at all is there a minimal example of a thing is it correct to say a thing is the temporary Persistence of a marov blanket perhaps that it's the Persistence of something that can be modeled as or with a Maron blanket are all things then just not an equilibrium steady state processes well considered one way they're at an equilibria point to allow them to be measured but by virtue of our scenario overall being dissipative the fact that they're existing is non-e equilibrium do we need to be more strict in saying these things are are only self- evidencing things rather than things in the colloquial sense totally depends on what what's being discussed if it's an inner particle then it may not be providing any extra information to say that it's self- evidencing like to say that rock is fulfilling predictions that it expects to be there um first issue you may not be adding any richness to the account and second off you're proposing a map that's that's indicating some causal um capacities for the rock that are unrealistic whereas when you start getting into that squirrel is acting like XYZ then you are creating parsimonious accounts of behavior that can't be accounted with simpler and you are making maps that have features that that system might actually have so the squirrel self- evidencing um it's probably more of a value ad in terms of the account and it's probably more plausible mechanistically what is the counterpart in an active agent to conserve versus disputed physical systems what is the analogy to friction in a cognitive system how strong is this analogy that's an interesting question to explore one possibility is like what is heat and what is work in basian mechanics like if if all of the information of the observation were used for belief updating that would be like extracting 100% of the efficiency of the fuel if you if you perfectly burnt the fuel the maximum extractable heat as work would be like from the hydrocarbon down to whatever the um terminal product of burning was like you could burn the candle and extract all the heat from the candle um or like how they test how much calor are in food like you just burn it and there's a Max am of energy to extract so then by analogy there's like a maximum amount of information to extract from a given signal like described by its entropy so there might be something like how much of the entropy of the signal is going into belief updating versus how much is being um partitioned to like dis itive but I'm not sure about friction specifically because friction isn't just the same as heat okay chapter 3 High Road starts from premise that to survive any living organism has to maintain itself in a suitable set of preferred States these preferred states are first and foremost defined by Niche specific evolutionary adaptations however as we will see later in advanced organisms these can also extend to learned cognitive goals I'm curious about the applicability of active inference to social structures such as economic systems firms self-organizing groups and government systems amongst others one can argue that social systems have preferred States created from both evolutionary Niche and learn cognitive mechanisms has active inference yet been applied to these sorts of systems yeah cool that some people have added some links yes applied to a limited extent Al explains in the intro video Markov blanket is a boundary in the state space not necessarily a spatial temporal boundary if we were talking about a spatial temporal State space like GPS coordinates comma time then a boundary in the GPS comma time space would be a state space boundary corresponding to a spatial temporal boundary but if our state space were talking about something that wasn't spatial temporal then the mark called blanket is not necessarily spatial temporal even in the map even before get getting into the map territory stuff what allows the agent to be statistically separated from its boundary as observed there's blanket so I take it the blanket is a lens through which the agent observes their environment yes yes would this lens be defined by their affordances yes just like on the inbound it's kind of like glasses like they're a real like a lens and then like light comes in and gets focused in but then also if it were a light source it would defract out the other way it's not exactly the same because inbound perception isn't exactly the same as outbound action but it is like a two-way lens that has communication across interface Susan wait unmute but then yes Susan oh sorry um so I was listening to one of the uh videos and I've heard um priston say that Markoff blankets were you were really just about computational models what do you have to say about that like he meant that they were being introduced as a computational tool rather than like an ontological Claim about reality or what did you okay okay that makes sense thank you like let's just say we had um 10 variables we could test a model that every variable influenced each other so um and and and every three-way combination matters uniquely and every five way and seven and nine way it's like we could test kind of the the fullest possible model but that's more parameters to estimate so it reduces the St statistical power per parameter so this is a fundamental trade-off in statistical modeling which is like um the fewer variables you include for a given sample size the more statistical power you have however you run the risk of partitioning variants inappropriately whereas the more variables you include that may come closer into alignment with what is like believed to be kind of like a a very multicausal Nexus so it can kind of feel nice and appropriate to enable the possibility of of intersections mattering for causality however that can make the causal State spaces very large diluting the statistical power to detect anything and so models are chosen to be at a trade-off point between strongly explaining two few factors and weakly explaining too many factors and that's why how many parameters are included in that kind of best performing trade-off model that kind of parameter estimation is about the map not about the territory does the distinction between active learning and active inference have to do these Imports I don't remember exactly which paper the active learning was but it kind I remembered that like again like sometimes the term is used in a narrow or a broad sense like in a broad sense Active Learning and active inference are the same thing they could both be used to describe a system that's active and learning but then if you really get into it a paper might say well we're using this to to mean this specifically so without the link I I I don't remember exactly what what was meant by Active Learning are internal State parameters referring to the edge of the marov state space no they're referring to the inside the marov blanket is The Edge like the crust around the internal State parameters is there any identification for the edge of a Markoff blanket like whichever whatever variable you're going to point to and say that's the one that I'm going to call internal so that's just that's a choice by the model which one is going to be called internal then all of the crust around it all of the envelope that encloses it is the blanket of that state but if you had chosen a different state to choose internal you would have ended up getting a different set of blanketing noes it's kind of like let's just say um you know we're we're doing Google Maps and so we we're going to say um for this trip from A to B this is the first straight this is the last smile and then this is like the space between but if you had chosen a different Journey maybe the street that was in the in between for the first journey is going to be the first mile for this other journey in advanced organisms preferred states can also extend to learn cognitive goals and in advanced organisms like human they can achieve preferred states by increasing the abstract social cultural strategies my question is if they are cast in terms of active inference must all of these preferred States and strategies be ultimately linked to survival in some way or in the case of advanced organisms can free energy be related to something other than survival if acting that way was inom compatible with existence you wouldn't see it if acting that way was deleterious to Fitness you'd see it less so that's the kind of um Evolution like component which is like it doesn't mean that every single phenomena was necessary and sufficient for existential success but certainly measuring something means that it wasn't incompatible with existence so then by the time that we're talking about organisms that might be um with all kinds of different cognitive phenomena especially if they're going to be like in like a um a new or changing Niche you could have um maladaptive you have a ball rolling downhill and you could just be observing the last five minutes of a maladaptive trajectory of the ball rolling downhill so like past returns do not guarantee future returns or whatever they say can marov blankets be considered as decoupling an agent in an environment yes you could think of it as how they are coupled which is to say how they are decoupled like how they are articulated the joint is both how things are connected and how they're separated can surprise only be diminished through action so here's a quote from chapter 2 um pointing to the two ways that discrepancy is addressed in active inference change the mind or change the world so if we're in the temperature situation we prefer it to be homeostatic and the temperature is higher than we expect then we can either take affordances that we believe are going to bring the temperature back into alignment with our observation preferences that's pragmatic value change world or we could learn and come to um dissipate the discrepancy just by updating what we expect things to be however for like a physiological prior those may not be learnable huh um I guess uh what happens if observation uh were surprising and then they fall back into um non not surprising um values like the temperatures spiked up unexpectedly but then it just came back down to normal yes that's a good question so like um a related question is like what updating strategies are adaptive under what patterns of environmental regularity so if the environment was like very slowly changing then a very slow changing um belief updating might be appropriate if the environment were very rapidly changing then you might want to track it more rapidly if the environment had like one in 10 times let's just say there was like just a erroneous measure so if it was like a noisy environment where sometimes you got like wild measurements but then they were just like part of a a noise then you um would have something like a CET filter which is basically What's Happening Here in the temperature example this kind of looks like CO2 measurements in the in the environment too but it's kind of like you have these noisy measurements coming in and then the colan filter is basically saying how can we smooth this um in the extreme case it Smooths just taking the moving average in the extreme slow case in the extreme fast case the smoothing is just tracing exactly what you're getting but then there's somewhere in between that like Smooths out some of the noise guess could you put this under the umbrella of um the action of updating your own beliefs yeah like moving a belief distribu ution could be understood as a cognitive action like if you had the choice to do it or not or if you had the mental action to attend if you had the um you have mental action on the dial of how much to pay attention to it zero attention to it means it's going to have no impact on your beliefs because if you don't pay attention to something it's as if it didn't happen 100% attention to it means you're going to update your beliefs 100% to exactly whatever the the last thing that was said was so then that would open up an internal policy possibility of how much should I attend to this environment so then in a noisier environment you would want to reduce your attention to make your smoothing slower to average out more whereas if you in a more precise environment with like more um relevant or Salient information you'd want to increase your attention to to maximize information gained but then when we're measuring supplies and you know the you know the temperature spikes up and then it spikes back down I guess you could say that Spike itself is is a surprise but then now your your prize for the temperature is kind of normalized again so yeah I guess you know like it could be a little bit confusing on how you think about um you know supplies is only minimized through action but in this case it kind of feels like it was minimized just from the environment itself yeah interesting Susan so is there a a specific specific parameter or computation uh to account for the capacity constraints and like cognitive capacity or cognitive control you know when people slip into a state of overwhelm they're going to change States so yeah there's that would be a back to the transition but I didn't know if it was that that's really interesting I mean let's sketch it out and explore it because I think that's we're not going to find that in the minimal PDP maybe just be a state change I guess yeah but if we if we um let's just say that we uh we imagine two internal states that the agent can be in one or leg what about [Music] um so how about instead of um too cold just right and just hot yeah there there's an internal model that basically it's the temp you know is this epistemic is this too boring just right or is this too much that may not be um like that doesn't need mean that that's the ultimate truth like you could have a temperature estimating agent that actually thinks that there's a rooms that are too hot but actually it could do just fine to them just because it's the agent's estimate doesn't mean that it is that but so now there's there's um the agent is doing um estimate it's getting Challenge on the y- axis and then it's estimating how difficult is this content and then it's doing some estimate and then like the the accurate estimator would have um basically would would have would know when it was in boring just right or too hard whereas you could have an optimistic estimator that you know skews to one way or the other way and then each of those three states might be associated with um like maybe in the in the just right phase maybe that's when there's a feeling like there's the highest amount of intentional control but then if something's too easy or too hard somebody might feel like they like there's no point in exerting cognitive control because it won't matter well it'll matter if you have like ADHD and so yeah managing the that uh I mean it's the energy it's the um entropy comes back to the entropy yeah to to ordered or too complex yep is does not have yeah that would be good to add to that graph the flood graph to redundant or to novel yeah how's markco blanket a restatement of the classical action perception cycle there probably a lot of ways to um it's a bit of a restatement perhaps because [Music] um that's so funny like cybernetic agent is like I was expecting like this but hey I mean oh so for for many decades I mean this is more similar than not to what we've just been discussing with predictive processing and active inference incoming sensors with a reference prediction and then the differential goes on to influence the actuator that acts upon the external system which then responds and then passes another sensor observation um so conceptually like no one really disagrees that observations are coming in actions are going out and there's like causation on both sides or you know so the reason why the marov blanket is a bit of a restatement is because this cybernetic Loop was phrased before the basian causal graph literature of the 1990s like with Judea Pearl so octave inference restates the classic action perception cycle in terms of the causal graphs that have existed since the 1990s I will say I want to say that uh that the cybernetic um viable systems models come a long way too so don't throw the baby out with the bass water no definitely not um Dave Douglas is uh working a lot with American cybernetic society and um and I think that combining our Journal work with the uh International Society for system science and cybernetics like it will reveal many many yeah parallels like this these are these are like it just here it's like well the circle's kind of conceptual but now we have a technical definition for that boundary in terms of causal graphs mark blanket or in terms of category Theory but [Music] um like Robert rosen's ecology I don't know how useful the um Wikipedia is but he developed a lot of category Theory ecology models again ones that that pre-s a lot of what were like here but it just it just that it's it's a schematic so it's very evocative what's really different now is there are open source software packages and first principles [Music] formalism that's like this is literally me bar more similar than not even this looks like those marov blanket uh Scatter Plots in the chapter it just so so there's just there's more similarities than not hence trying to get them all together living system um resolve fundamental biological Problems by exerting active control over their states is the way to understand why automatic mechanisms are active in thinking that these are still involved in calculating EF but the time Horizon is small is there another concept of passive control or is all control active take the example of touching the hot kettle probably a lot of ways to think about this one um but the automatic mechanisms being active like I I don't think that they always need to have planning like for example secreting insulin and glucagon and hormone regulation of blood sugar that could just be reflexive actions like as something gets to here then more of this hormone is secreted and then I it goes down more of that hormone is secreted so that's still active control but it's just reflexive active control is there a concept of passive control that's almost like action without action or like the ability to like control control something without acting which you could probably design a situation where that does happen like where um by by non-action the agent has the other system um aligned with its preferences but suffice to say it wouldn't be doing so by virtue of actions that it took um Erin yeah I guess riing on this and um in the chapter and a few other places in discussion um the the concept like temporal depth of a system and like its ability to plan has come up and I guess like this is this is partly in this question we're talking about like some actions that a system undertakes are informed by that temporal depth but some actions that the same system undertakes are really shallow like removing your hand from the hot Kettle you don't need to see very many steps in the future to understand that's like the highest value thing you can do um but so I guess that does that that also implies something about they being like like a A heterogeneity in the the thickness of consideration of actions that the agent is is driving yeah definitely like the organism is a multiscale phenomena so even in the same moment like in might the best map of what I was doing 1 hour ago might have been um regulating uh oxygen homeostasis over seconds time scale but then you know food over a dayong time scale but then like Financial you know well-being over a year so you could make multiple time scales of action even a slow action is happening in a moment and then and then the kind of no planning model first off it's the computationally simplest so it's a great starting point and then you can look for deviations um and then ask where do we need additional phenomena to account for what we're seeing like like birds flocking like the famous models don't involve any planning it just involves the visual field and basically it's like if they're too close like you pull back and then if they're like too far you move towards them or something like that so then if you do a model you find that you can replicate the statistics of this this empirical flock there's no need to invoke planning but then maybe there's a residual with some real system it's like oh actually you know let's try let's see if planning could explain that variance Gap Susan wait unmute then yeah um oh gosh was looking at one of your slides and and uh so I I I I totally lost in my train of thought but I will say that I saw a video back on the the Swarm and the studies show that a large percentage of the of the of the Swarm is just following you know whoever is closest to them so you can't overthink this thing yeah that's that's this is a very funny in the ass like I I don't I think it's a little bit sensationalist to call them lazy but it was it's like most nestmates at most moments are just standing there that's excess capacity or could it you know when if nestmates die or some if there's like an alarm and so it's like it if flocking is the goal then yeah well I think it I think there's a link back to the cognitive capacity um and level of abstraction in other words if I'm if I don't understand more than two Dimension two dimensions then you I can only reconfigure two Dimensions maybe ever thought but anyway just a thought H yeah Hector um he he's been sharing some resources about the development that happens in a child between like the ages of four and seven as they're separating out like beliefs from knowledge where they'll do the experiment like a little character sees somebody put a ball into a box then the character leaves the room and then the the ball is moved and the character comes back and then you ask like where does the character think the ball is and then it it's kind of like theory of mind like you have to understand that well the ball is really in a different place but they think it's here so then that um that a l it's the holding up of multiple possible beliefs about where that ball is but if you didn't have the capacity to entertain and like to juxtapose the memory of the ball moving with when they left the room and all that then you would collapse down to either the ball being you know to to to incorrect answer to that question but then what's interesting about it is like one day the child will not be able to do it and then like two weeks later the child will be able to do that and then that capacity will never leave them for their life just like on a random day when they're five years old yeah okay cool well fun chapter 3 interesting High Road um hopefully we got to some good topics uh chapter 4 is is kind of like where the high and low road are G to they're going to merge back together and chapter 4 is going to be the um the formal treatment as they say of the generative models and that's going to include some kind of like background material on the causal graphs and then come to like figure 4.3 which is like the Rosetta Stone image with a discrete time and the continuous time generative models so these are two possible architectures that treat time differently that have the kind of top down characteristics that were discussed on the high road and have that low road mod construction so thank you all see you next time thanks Danel bye byebye