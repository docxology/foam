hello and welcome everyone it is September 1st 2023 we're here in active inference math stream number 6.1 here with Sean Tull we'll be hearing a presentation active inference in string diagrams followed by a discussion this is super exciting so if you're watching live please feel free to write your questions in the live chat really looking forward to this so thank you Sean again for joining and to you for the presentation all right thanks very much thanks everyone for watching and thanks to the organizers for this chance to speak to you and for the Daniel for getting in touch and inviting me to speak so um yeah I'm really excited to share this about this community basically and to hear from those people who work with active inference and do any formal work what they what they think of what I'll present today um so I'm going to be presenting a formal approach to how you can describe active inference in terms of a entirely graphical language called the language of string diagrams and it's based on this mathematics called category Theory and I won't assume that you're too familiar with this already and try and introduce it to you in the talk and ultimately I'd like to sort of convince you that this diagrammatic language will be really useful for those of you who work formally with active infants and encourage you to pick it up in your own work I'll just introduce myself about I'm Sean tile I'm a researcher at Continuum um formerly a postdoc and computer science in Oxford and a Continuum in this Oxford team where I'm based where we study what we call compositional intelligence which includes cat applying category Theory uh to topics in AI um and as well as this the project was supported by a grant from fqxi it's like this at the bottom and posted it at topos Institute which is the Center for Applied category Theory um so let me get started I think so yeah here we go so um for active inference I'll won't spend too much time introducing it I'll assume most people here are familiar with it and many of you probably know more about it than I do intact um so I just mentioned the parts of it that I'll be addressing within in the talk so thinking of it as a model of cognition that hopefully we can think of as applying at many levels say from a whole organism or just to a single neuron um and the key idea is that in the in this approach if you think of an agent is coming with this generative model that it uses to explain the observations it receives from the world in terms of some some hidden states which you might call perception and in terms of its own actions and um and active infants achieves both of these things for this formals um Basin inference or an approximate form of Asian inference by minimizing this quantity called free energy and these are the ingredients we'll be we'll be looking at in the talk and the thing that's really exciting about I think um for those of a formal background as well is that it you know aims to offer like a very principled approach to help Mission um that you can hopefully apply at all these many levels but I think at the moment it could also benefit from more uh formal work and that's what this talks about it's about formal approaches to theory in particular I think a nice clear organizations or active instances would help to to clarify sort of what the core or the key ideas of the theory are so we'd like to be this very succinct um principle that ideally we just apply to a generative model and everything else Scholars from um and once we've got to this we can hopefully generalize it understand it better and also make it just more accessible to those who come from formal backgrounds like mathematics and so on and get them working on the topic very quickly and and connect it with approaches in artificial intelligence as well um but the most important thing about a good formalization I think should just be to make learning or active inference easier make the framework simpler to understand so that's what we're aiming for in this work and I'd say in them in other places already there's been some calls that got some suggestions that a nice organization of active infants should be a diagrammatic one so when you look at the generated models that come up in experiments a lot they're very compositional in their nature and it's very natural to draw them in diagrams um so in the United States like our whole approach to describing it could be graphical in this way so just for example this is paper called The graphical brain by by Kristen power and abuse and in general you know we've probably seen loads of these diagrams describing gender models where you draw many um compositional features of different uh spaces of hidden States observations interacting and so on so these diagrams are used but they just used to represent the model you still have to then go to doing um sort of traditional probability Theory calculations when you reason about them normally and in fact there is a whole graphical formalism and mathematical language for describing these kind of interacting processes um just entirely with the diagrams so the area of an access called category Theory and the language are these string diagrams I'm going to talk today and in particular there's a lot of work going on in applied category now in how you can describe aspects of probability Theory and causality and causal models in terms of string diagrams and these called the models are basically based on data networks so the same formal structure as the generative models in active inference in particular what we'll talk about today kind of draws on this paper Cooperative with Robin Lorenz talking about causal models and a sense of pearl in terms of these string diagrams so it's basically cause of Asian networks um which has the same formal structure as we'll be talking about today so in this talk I'm talking about this paper which is joint work with Johannes Kleiner and Toby saying personality just called active infants and string diagrams a categorical account effective processing and free energy and basically what we do is try to give a formalization of active inference that's nice and clear conceptually just entirely in terms of string diagrams so we're basically taking the kind of formal components in something like the active infants book and turning it into these diagrams and as I mentioned it was done as part of this fqxi project that's actually on a project about Consciousness it was about ways that category Theory can be applied to theories of Consciousness and we've done some previous work looking at the integrated information Theory Of Consciousness and of course there's all sorts of ways that active infants has been proposed to connect to consciousness um but for the purpose of this talk we won't go into any of that it's just a period of cognition I take it to be here and I'll just mention at the end it'd be nice to connect it back up to Consciousness in future and there's also lots of other related work going on in kogi theory that's very close to this that often goes by the name of categorical cybernetics and this might include some of the things Toby has talked about on the on the stream in the past so what I do now is introduce categories and string diagrams and then later we'll apply them to all these basic ingredients of active infants that I've alluded to so far so that would be generated models updating them free energy and active inference itself let's start with with these categories and string diagrams so you can think of a category in general as a sort of world of interacting processes and the categories we'll talk about here are always going to be the symmetric monoidal categories but don't worry so much about the formal language because the way we talk about them is just going to come down to the diagrams here today so a category amounts to a collection of these objects or sometimes called systems like this Capital ABC here and what are called morphisms or you might want to typical processes between them so when you're writing normally you can just write a more person from A to B it's like this at colon A to B in string diagrams so you draw it like this where we're reading all the diagrams from bottom to top in in this talk so you have a wire for the a input at the bottom and a YF heavy B output at the top and the morphism is just drawn as a box F here and you just read the diagram map thinking of okay it takes in this input coming in on this y a process applies and then you have your output type B at the top foreign you can just compose them in sequence and this just means plugging the boxes together in your diagrams and because we're in this monoidal category you can also compose in parallel so you have this operation called the tensor which means given to objects you can put them together to build this composite object so a b goes to a tents of B you'd say and you can also do this to morphisms so you can build this F10 surgery um but in the pictures it just means drawing them side by side and you just think of this as meaning we have F from a to c and G from B to D and they're just running in parallel and they're not interacting essentially um so most of the time you just draw a picture like this and you need to write the tensor symbols so if these two basic modes of composition and from these you can build much more uh elaborate string diagrams in your category so if you're writing things very mathematically if the relative equations that these that are category or minodial category needs to satisfy but when you work in the diagrams they basically do some of the work for you because these things just come out for free so for example you have equations like this that you'd have to think about when you're working uh with them in the conventional mathematical way but in the diagrams it just means if you have two boxes you can kind of slide them along the wires it doesn't really matter where they are on the wires they tends to just be the connectivity that matters similarly we can crosswise over each other because we're in symmetric setting and there's a few like um just useful features that you always have in a category so every every object comes with this identity morphism which you just think of as meaning nothing is nothing happening basically so it's just drawn as a blank wire there's also a kind of identity object as it were called a unit object which is just empty space so you don't even draw the wire it's just this Dash box just means nothing it's going to be an empty picture and another thing is just useful because it now means we can talk about morphism which doesn't even have an input or an output more formally it has this object I as it's input might output and we give these things uh special names so the most important one probably is that of a state um which is amorphism with no input as it were already with input I so in the pictures it just looks like this so there's no y going in and you call this a state of a you can also have a process that takes in a if it has no output that you'd call an effect and if you have naive you just call this a scalar so this is this is just going to be like a number basically floating around next to your diagram so there's there are many categories out there the point of category is extremely General so this could be talking about computational processes or physical processes or Quantum processes in particular and all sorts of things but this talk will only actually need it as well one category we'll just keep it simple with this one that's the category uh call it matar plus so it's positive real matrices so you can just take objects so the wires to be finite sets and they're more prisms to be positive matrices indexed by these as I'll explain so if we draw a box like this m going from X to Y this is like a matrix index by X and Y and for each input in this little set X so in the set X and each output in y you get a positive real number and you and we'll write it like M of Y given X so this box would mean a function like this now when we plug them together they let us turn some things that you normally have to do with equations it's a kind of simpler pictures I mean mainly this middle one so if we have two in sequence we just compose them by matrix multiplication so instead of having to write this formula where we sum over y we can just draw this picture above it where we just float them on top and if we run them in parallel here we take the Cartesian product of the sets and the tensor product of the matrices as it were but it's just the obvious thing where you have two um things running independently so in particular a state here ends up the the minority unit is singlet in the set and you can basically just ignore it so a state here amounts to just a function sending each X to a positive real and effect the same thing and a scalar would just be a positive real the intuition though is that we're going to restrict to the particular more prisoners in here which are probabilistic in the nature so they need to send each X to actual distribution over y so I want to talk about how you actually pick those out next so to do that you use some extra structure that this category has it forms what's called a copy discard category so this is one more bit of mathematical sort of gadgets we have around so that is that each object comes with these distinguished processes there's one that we call copy that takes in a say and we've got two copies of a at the top and one called discard where you just throw a away so you have no output at the top um these satisfy some equations um that are quite intuitive you can like copying and then throwing away one of the outputs is the same as doing mapping so let's explain why I'm copying a symmetric and associative is the Earth One so in this category model plus discard would be just a function sending each element to one and the copy would be like a Delta so a comes in and two copies of eight come out at the top is the intuition the reason we introduce this stuff is because it's been shown recently you can do a lot of probability Theory uh just in terms of these CD categories and particular ones called Markov category so there's a lot of what's going on in applied to category Theory at the moment it's using this language of CD categories let me think go there let me pick out some things to do with probability Theory I'll just talk about a couple of them here the most important one is the notion of a channel so this is what lets us pick out the actual normalized uh matrices as it were from earlier so in general you call a more personal Channel when it preserves it's discarding and a special case is a state in which when it's a channel you call it normalized so for a state this means it would actually be a probability distribution so it's actually normalized if you sum over the values with this Omega you'll get one and for for amorphism being a channel it means it sends each input to a distribution so it actually is a probability channel in the usual sense uh equivalently The Matrix for f would be stochastic so these are the ones we'll use in generative models for example and this as I said Associated probability for you you can describe this with these diagrams so it's very two simple examples you can describe marginalization and probability Theory with this discarding thing so if you have box Omega like this it would be a joint distribution over X and Y if you just describe why you'll get the marginal on X and if you plug a Omega a distribution on X into an effect that would just be any function on X this would be giving you a scalar now and that would be the expectation value of the in this distribution so I'll meet lots more of this as we go but let's actually start doing some stuff uh rated active inference in particular now so I want to talk about generative models and how you view these in the diagrams so as we've said uh we're going to be talking about agents having generate models that relate things like actions observations and World States and these are normally quite compositional and active infants right and it might involve many different spaces of states and observations and these processes relating them and you'd usually treat these with something like a Bayesian Network um claiming you can view it really as a causal version Network because it's sort of describing how states are causing these observations um performing it's the same thing as a spatial Network which you could normally say is described as something like a dag the director is like a graph which describe the different variables that are being related and then sets of values for each and probability channels described in each one in terms of its parents in the dag and then you often look at this whole distribution over over all the variables but already I'd say the way that these uh data networks are drawn in active infants text and stuff is kind of converging on something a bit closer to string diagrams because you don't actually um just draw the Diagon the variables it's very useful to actually give names to the mechanisms themselves as it were like you have in this picture the A and the B's um so yeah my claim is sort of converging on the way string diagrams will look as we'll see on the next slide which is where you really do label everything um not just the the variables so to describe those sort of Bayesian networks with string diagrams the key observation is really that dags correspond to a certain class string diagrams that we'll call Network diagrams um there's a definition here but which is better just to see an example in a second but the diagrams built from copying and sometimes discarding and the key thing is just that they only have processes with maybe many inputs but only one output so this is going to be like a mechanism that produces each variable so the result is that if you have a dag G and you choose some of the the vertices to the outputs so there's a lot of The observed variables you can draw a network diagram which expresses the same equivalent structure with those things as the outputs so here's an example we have a dag with these four variables X1 to X4 and what you do is you have a wire for each variable in your diagram on the right and you draw a box that produces it doesn't really matter what you label the Box um so with this box C for example which uses X2 and you'll produce it in terms of its parents in the dag so X1 and X4 in this case and if it doesn't have any parents it would just be a state as it work box or no input and then what you do is you take each variable you copy it and you pass it toward its children in the dag and also out of the diagram if it's an output so this now means you've sort of expressed the whole structure of the of the day and also which variables are sort of leaving the system the output ones so this allows us to turn dag into a string diagram and then if we want to make a generative model like that expresses the you know like a major Network that's structurally go into this dag you just have to now interpret this diagram in a certain sense so in general working in any one of these CD categories there's copy discard categories we can say that a generative model in there is given by one of these Network diagrams without any inputs and an interpretation of the diagram meaning you actually say what the objects are but each of the wires in the diagram and what the actual channels are so they need to be channels not just more prisms um in your category are for each of the boxes so for a gender model like this you would say you pick objects X1 x64 and pick channels for the ABCD and we'll think of the outputs of the diagrams like The observed variables and the rest of the Hidden ones so for example if you're working in this category matter Plus that is the only category I've actually introduced here so this is going to be a running example this is the same thing as one of these core salvation networks so it just means picking sets of values for variables and picking probability channels for the boxes um so you might ask why would you use this representation rather than the usual watch I think it's a good question so it's equivalent to the the DAC um and probability Channel description um but the thing that's nice is that in the conventional approach these networks you sort of have to switch between the dags which just fit the variables and then doing calculations with probabilities whereas and the categorical approach you can use just one formalism because you can do probability Theory with the string diagrams as well so it's quite natural in that essence you have this one language that both just intuitively drawing what's going on in a model and then reasoning about it it also lets you start to generalize things in a useful way I think so we I kept having to say that you have no inputs to your diagram but there's nothing really fundamental about that one it's not really clear why we need that so what we what you can do instead is start to allow inputs to your model as well so we'll um call this an open generated model so an open generative model is the same thing but now just drop this requirement the diagram doesn't have any inputs so here's an example of a general Network diagram now with these inputs x2x3 so there's no mechanism specified for these new variables X2 X3 they're just input variables to the system and they can be outputs as well for example X3 is broken input and an output here and again an interpretation of this General Network diagram just means picking the objects and the channels and this is the same definition we use to define what we call an open cause and model in in the paper with Robin lorens that I mentioned earlier so an Android generative model is formally just the same thing but we're just thinking of it as a generative model possessed by a cognitive agent if you run the definition in metal plus then this is just like it calls on laser Network and now you have some of your variables just have no mechanisms specified so they're just inputs to the whole thing um a nice thing about these open generative models is that because they can have these inputs you can plug them together and compose them and these things in fact form their own category but I won't go into that today um so that was the general theory release generated models that just actually describe some examples that you'll you'll see in active inference coming up all the time so it's a simple example let's just imagine we have one space of hidden States and one space of observations then it would be a generative model of that form which is like this network diagram where it's just two wires there's just Sno um s doesn't have any parents so it just has this prior distribution Sigma over it and there's just this one channel often called the likelihood from s to O if you just draw that Network diagram and say that that model in in C that or in Mata plus it would be the same as it's one of these simple generative models when you're looking at these you're often introduced interested in this distribution over both variables together this joint distribution that you might write as you know P of s times P of O given s normally or a bit more specifically here introducing their names for the two distribution in the channel here and string diagrams is just the same as this so you just take the prior and you make it an output now and then you compose those channels together and this would give you a distribution so a normalized state m um over Sno at this so this is just the resulting joint distribution you get from the generator model and we'll come back to that later um so a more elaborate example of generative model that you'll see for example in the active infants textbook are these discrete time models that are used a lot so I'll walk through this diagram now so this is an example of a more complex Network diagram and a generative model it describes so here we've got these n time steps going uh remember we read from bottom to top but I'll just talk about the I'll describe things from the top down here so the top we have the observations o102 up to om the observations at each time being caused by these hidden States S1 S2 and SN at each time by these channels a um and the hidden state is evolving over time by this transition Channel B where it takes the previous state as an input and then choose the next one it also takes us one extra wires on the bottom and that's the space p of policies which is how the agent sort of actions enter the pictures these policies describe its behaviors its behavioral policies it can carry out so based on the previous interstate and the way it's acting this channel B would determine the probabilities for the next States and then there was a prior over um the policies that you can think of as the habits or the typical behaviors of the system so you can just draw this diagram and say like interpret this in Mata plus and that would for any given interpretation meaning any choice of what the values for these variables can possibly take are and any choice of what each channels are but then give you this this type of primitive model and um often you will see models sort of at this form or similar forms being plugged together to form these hierarchical models and I think the compositional language is very nice for these because you really want to talk about open models to Define this so hierarchical model you can view as really as just being given by taking off to these open genitive models I mentioned and plugging them together in a certain sense so here you can see a picture of a hierarchical model where we just have these layers where different copies of the same model each layer and the inputs from one layer match the output to the layer below so we can compose them together first generative models um so so far we've just been using the diagrams to represent them we'd like to actually do a bit more their own reason about these models in particularly talk about how you update a model or update the beliefs within a model um which is very important in active inference and how this looks in the in the string diagrams now so let's say we've got an agent M we have a model m and it's just the simple one where we say there's just one space of hidden States s and one space observations o so they have this joint distribution that I mentioned earlier and they had these prior beliefs about s which you can get back from the joint distribution by just taking the marginal if you like so that's a signal so we've got their model they've got their beliefs about what states are likely and then they receive a new observation and here the kind of observations we'll think about in general can be soft meaning they're described by a distribution over not necessarily just one element so that would be one of these normalized um States but we'll try not to use the word State here because we're confusing with this s around so this distribution over o is the new observation this bold font o another one update the margin there I want to update m in some ways so that the marginal basically is different and describes the updated posterior beliefs and this comes up of course in perception where you're updating your sort of state of the world given to Observation you receive but you can also be used to model like planning Behavior where updating your plan of action your policy uh given something like which outcomes you'd like to see in the future and we'll come back to that later in the talk so we want to talk about how you can do this updating so you might think there's just a standard answer or at least in the ideal case which is this Bayesian updating and that's true when your observations are sharp and we'll start by talking about in that case so I'll see what sharp means in a second but first we'll just talk about how you treat Bayesian conditioning in these string diagrams so on the right here we have uh if you view this process from o to S this describes the sort of Bayesian conditional a Channel or in general the partial Channel impact um that the agent would have from the injuries by their model so you can describe this in string diagrams with a couple of extra gadgets I hadn't mentioned yet so you have your distribution M over s and O you can have this a mapper plus there's this effect that we call the cat which is just takes two inputs and Compares if they're equal and as allows for you to turn an output into an input so that's what this part here is and then you can introduce this extra thing of normalization so what you'd like to do is take a general morphism and for each possible input normalized that so that it's a distribution if you can or set it to zero it's just zero and there's nothing you can do that's what this blue dash box is um and in the paper and in the in the related course and models paper we talk about the the axioms normalization feature satisfies and the point is if you compute this thing in Mata plus it will give you kind of what you'd expect so it'll give you the usual notion for each point out of the space o you plug it into this you'll get the kind of conditional Mo of s given o that you'd expect whenever that's defined there's a string diagram way to describe this kind of Bayesian conditional Channel or partial Channel and I said this is what you would use when your observation is sharp and I I also drew it definitely with this triangle to sort of distinguish that case so what does that mean so in general you say that a a state in one of these CD categories um so a distribution basically would be sharp when it's copied by copying that which isn't true for General distributions and if you run this definition in method plus this really means that this thing already is just point distribution at some specific element of O so you know it really is sharp in that sense it's just a point there's no real probabilities yeah probabilistic aspects to it there in any CV category you can just talk about the sharp States like this they're often also called deterministic so for these sharp ones you know you ideally think you'd like to do this patient updating in fact when you've got soft ones so it doesn't have this property uh there's actually two at least two good ways to do this kind of updating I don't know if this is as well known so I'll just mention it um now anyway and they've been studied in some detail by about Jacobs this paper at the bottom so let's say you don't get one of these sharp observations you have just a distribution over o there's always two reasonable ways to generalize sort of the picture from the last slide to give a notion of updating and Jacob's course on Jeffries and Pearls update rules so in Jeffrey's update you basically do it like we did before you you have this station and a conditional kind of Channel or partial channel the normalized boxer and you just plug the distribution into it both in Powers update you turn a distribution into an effect so you just compose it with this cap to bend it around in the picture that's what it means so you plug that into M and then normalize everything so the difference is where the normalization and yes it's basically just interesting that both of these are reasonable Notions generalization they have different properties it's not obvious that one of them is sort of more rational or something than the other they just behave a bit differently um in the formula you can see that the normalization is being applied definitely so if you turn this picture into the usual notation that will look like this um so in the top case you're normalizing for each possible sharp o and then taking an expectation over this distribution here as in the whole case you plug the whole thing together and then just normalize either way you do it though the point is that these things are actually hard to compute so don't expect the cognitive agent to be doing either of these exactly even in the sharp case and so as we know we want to instead approximate these kind of things using free energy so I'll talk about next um so we are able to try and accommodate free energy somehow in a diagrammatic approach and the free energy sort of formerly that come up are often given in terms of what your father's surprise these negative logarithm quantities so we'll start by introducing just a new graphical component for treating those that we call log boxes if you have any function e on that on a set x a positive real remember that made in a that would look like an effect in your category X um then what we want to do is talk about this function X goes to minus log of e of x let's record a surprise so just introduce this graphical feature where we draw a green box around it and say that denotes this function now and using rules the nice property is a logarithm you can turn these into nice graphical rules this this lockbox feature would satisfy um for example this is sort of the way that logarithms have the multiplication into addition on the left here um if you have this around then you can start talking about surprise so if you have two distributions Sigma and Omega you can do the surprise that's the one distribution relative to the other is defined by this expectation value so these just the expectation of a surprise of Sigma according to Amazon which if you remember I said expectation values are given by sort of plugging a state for the distribution into the thing you're looking at the expectation value of so that would be the log box here so we can just Define surprise uh and we're going to in this way if we're not in in the pictures and important special cases for this come up are when you're calculating entropy which is the self-surprise and the chaoted merchants can be calculated from a surprise in the entry so it means that whenever we have formula given in terms of these if we like we can instead denote them with this this graphical symbol at least with the log box so now let's talk about how we use this to describe free energy so what we want to do in the paper is sort of help clarify the kind of different Notions of free energy that we found in inactive inference in particular the variational and the expected free energy so we want one more General quantity that we can understand both of those in terms of I'm just calling out the free energy here I'm interested to know what other people think of this sort of naming for what we're doing the situation is that we've got some generated model that's fixed over these two variables Sno like before so remember we have this distribution this box m uh over Sno it's distribution and let's just say we have another distribution Q now and we'll see examples of this in a second what sort of queue they would be then we just Define this quantity you've got the free energy of the one relative to the other in this way so it's a surprise uh minus the entropy of Q in the string diagrams then it's just this feature we plug so it's like the expected uh surprise of M for Q minus the entropy of Q's marginal on S or this is the formula if you want to use the the conventional notation which is useful for relating it to existing approaches so we can Define this very general free energy quantity and then we'll meet two special cases of it that we're interested in which is the variational and expected free energy it all comes down basically to having a definition of surprise you know you just need this notion of surprise to Define everything else so the variation of free energy so we have this fixed model M and we have the soft observation o like we did before so this box here and then what we're doing is we're considering different possible distributions over s that we think of as different updates we can consider for our beliefs and we Define the variation of free energy of any of those States those distributions Q has this special case of the definition for the previous slide so it's like the general free energy where that capital Q just takes this form so it just consists of our new beliefs lowercase q and our observation O So in Formula you can also destroy like this so you take the surprise from your model M and you just see how it its expected value for those beliefs and that observation subtracted the the entropy of Q um and what you're going to show is that this BFE value satisfies this band with the KL in relation to the kind of Jeffrey update of immortal with respect to this observation in particular when um it's a sharp observation then the minimal of this if you will be given by the Bayesian updating [Music] in general though we might think about what happens um so in general we can think of minimizing this the SP quantity it's doing this as finding this queue that approximates this kind of updates we're looking at earlier and yeah in the sharp case it will coincide as all of the Notions of updating do um so the minimal VIP will be given by the Bayesian FB but for these soft observations um it's something else that's not exactly the the either of the two Nations updating we met earlier so this is actually a third notion of updating for soft observations um which I think is an interesting way to think about the minimization is doing so we just call this the vfv update so you've got many different queue you can calculate the sphere of the quantity for each and you've got a soft observation out here so it's some distribution and if you find the one with the minimal value obviously if you call that the vfp update and this wouldn't be equal to either those Pearl or Jeffree style updates that we met earlier so that's the vfv which will which we'll come back to the other notion of uh free energy one foot that's the expected free energy so that's where we still have our model M where rather than an observation we think of ourselves as having some preferences over observations we'd like to see they're again encoded in a distribution though over over o so that's this C and that and so and just with that fixed we can Define this one quantity called the expected free energy so that's given by the free energy of M compared with this other generative model where you have use the same in so this m here would really be the inverse channels from o to S of them so like the Bayesian inverse here um but where you just assert that the preferences actually is the Pariah and the observation so you're comparing these two um in terms of this generic free energy quantity we defined earlier so again you can turn this into formulas and there's loads of stuff for instance about the different rewritings of the efe and the ways to interpret them in terms of uncertainty and risk and so on and it has this property that you can show it'll be a bounded by um the surprise of those preferences for your model so and it kind of gives you a way to approximate them as we'll see so I went and talked I didn't have time to get too much more into efe but the point really in terms of what this work's done is just to try and um have this one generic free energy quantity we met earlier where we can see the vfp and the efe book coming out the special cases depending on what we plug in here for the the two distributions so what I'd like to do now is to sort of put some of these pieces together to show active inference itself will kind of look like in terms of string diagrams and in particular what we'll do is derive this formula that you'll find in actually inference textbooks um in a graphical in a graphical way and I think uh quite a transparent way but that's the claim so and to do so we basically need to give a nice high level conceptual viewable active inference is so this is the way that we do it in the paper so when inactive inference team the key thing for stating the definitions is that our model takes the following the form at a higher level so there's some notion of so it's like our discrete time model we had earlier but just with two time steps if you like each of those time steps could break down in terms of further sub time sets but that won't matter we've just obstructed here at this higher level so at the higher level we just have a notion of the current time or maybe like all the time steps up to the current time and that's this s and O here so there's current states and current observations then it's the notion of future times so these future States and future observations so that could be all the time steps up to some big number something like that all grouped together in one of those discrete time models and again we have the policies and we have the same sort of shape of model where there's some channels here which have involved with giving letters to but you know showing the way that the policy um influences the transition from the state to the Future State and observations from each so we just have a generative model where we have policies that we have States and observations and we have future States and future observations in active inference what we're doing is we're receiving two things we're receiving an observation in the current time and we have some preferences about what we'd like to see in the future so these are each given by these two distributions however Voltron and the preferences C over the future observations and then we're doing updating with those so I think that dating is just our um our habits the the prior over the policies to give our new distribution over policies which we can think of as the agent's plan about wants to act so we're going to try and do this updating like before to obtain a new distribution over p and that is now telling us how we want to behave in the future in a way that will basically you can think of it as saying we want to explain why we're seeing what we're currently seeing and how we're going to obtain what we'd like in the future so in the in the book scientific inference and various places you can find a formula like this that will be justified as coming from the free energy principle in some way so basically saying you can do this approximately by making a plan distribution take the following the form so there's a soft Max there's a apartment relating to the habits of your model so that's your prior over policies these Pi are the individual policies in p and then there's two there's parts of formulation to the vfv and the efe um and what we wanted to do is see where this formula comes from in a sort of nice uh I love away from the structure of the diagram so the usual there are explanations for this formula there but I found them quite hard to follow to be honest because they were talking about the um efp as being a prior that you then do um DFE columnization on top of but you kind of need to do the the forward the part about the present time first before you can do the efe um and so what we wanted this is a really clear way to see how this just drops out from the structure of the model so that's what I'll try and show now so what we'd like to do then is to do this approximate updating we're going to do the Pearl style updating which look like this in the pictures so we want to get our new plan so our distribution over policies by updating by plugging in our observation and our preferences and then normalizing everything so the thing on the right is what we'd like to have ideally but we're just going to have to approximate it in some way so let's just take the distribution that's inside the dash normalization box now this is the thing you know we'd like to basically approximate this in the structure of our model we can write it like this and I'll just show then some graphical steps for how we can apply approximations to obtain the formula that we saw and obviously we weren't able to go through every detail of the proof but it should give hopefully some intuition for what it's like to actually work with a string diagram so that's really why I'm showing it so when you're a model take roughly this form there's some part relating to current states and current observations and also future observations I just called them both for M here um but you know we just noticed that part of the model relating to the present time and future time and so what we're going to do is first focus on this part of the model relating to the current state and the current observation and we want to approximate what's in that blue dash box and what you can show is that if you do this vfb updating that will be approximately equal to this part of the diagram so this Q is given by for each policy doing this um BSE updating so minimizing your variation of free energy so you do that for each policy then you can view the collection of all of those up belief updates as just one channel from P to s so if you think about it right here basically for each policy you could plug in you would obtain just a distribution now over Sno and you could do updating with respect to that that's what Q of that particular policy Pi would be and you put them all together into this one channel queue and you can show them for each one if you do this overall process where you multiply by this e to the minus V of the quantity here it will be approximately uh equal to this this part of the diagram okay so that's our first step and that's how the vfb entered the picture then we've got this top part of the diagram we'll collapse it together and just view this as one process going into future observations and our preferences and we'd like to approximate what's in this box now and this is where the efe comes in so you can basically show because you have this um yeah that the expected free energy will give you an approximation to this here where this is basically like an expectation value for your preferences um for each policy so this would be like the density C of those preferences being plugged into your model for each policy um so I haven't had time to go into the full details of the approximation steps but they're essentially the same approximations you'll find in you know active inference text and so on just turned into the string diagrammatic setting and we talk about how you figure out how how they come about from Jensen's inequality and things like this so this step where you think about the future times sometimes it's called the prediction step and the previous one was the was the perception step so now we've Rewritten that diagram in terms of some e to the minus of the V of Phoenix the minus of the efe as well as our habits um I remember what we wanted to do was approximate the normalization of this whole thing so that's when you apply this blue dash box around the whole thing and now if we do that we this is exactly the same as the formula we're after so we've obtained the formula now um and that's because you know you're normalizing something but it's got these e to the minuses in it so you can also rewrite that in terms of the softmax where now that you just replace the E with this log and the other ones you lose the exponentials so this formula if you wanted to yeah if you're on a Roadhouse point of what this was for each policy it would be equal to this down here so uh the claims that this is a nice way to derive this formula and it's a bit more transparent than the the ones that exist so the idea was really just to see we draw what's going on okay we're doing updating with the model of this form and we're trying to do this approximate form of updating and just see where we're applying the approximations and from the structure of the model itself to see how this formula comes about um okay so that so far basically just talked about things that are already there in active infants such as new derivation but it's existing and stuff before wrapping up I just like to also talk about something a bit more new that we do with the string diagrammatic approach so that's the talk about the way in which free energy itself uh is compositional so the motivation of this is that you know the idea is that we want to think of this one free energy principle applying at all levels of a system um so to do that you'd want to know that an agent can say if you've got one of these big composite generative models that can do its free energy minimization on the whole thing by doing it on the parts because we want to ultimately think it just comes down to each part doing its own bit of free energy minimization so that's what we want to make precise um and particularly we're going to be talking about the the vfe here really all the time and if you recall in the diagrams it looked like this so we use this log boxes and it just had took this particular shape here so what we do in the paper in order to address this compositionality problem is introduce a notion of this bfp that we can apply not just to generative models but once which actually have these inputs as well so these are what I call open generated models earlier because we need to really talk about pieces of General models plugging together and give them a notion of free energy to even make sense of this notion of free energy being compositional so we propose this definition of what we call the open vfe so now instead of just a distribution M over s and O we have a channel from some inputs to Sno given by one of these open models and our Q the thing we're doing the Via humanization um with respect to so the thing we're calculating it would now have in an input as well so it's a joint distribution over the states and inputs an observation takes the same shape as before um so you get this other formula that's basically just a natural way to generalize the previous vme formula to accommodate this extra input wire I now um and what we show is that this thing uh is compositional in a sensor that I alluded to so I'll walk through that and the way you do it is just using these graphical properties that these log boxes have that I mentioned earlier so you could turn all of that into a proof in standard probability notation if you like but it's quite uh instructive to to voice just be able to work in a diagrams to keep track of the compositional structure of the models and so on so the result says that this openvfe quantity is compositional in two ways the first one here is this quite trivial way so if we have two models running in parallel so like taking a tensor of them and they're just both doing their own uh we're kind of taking the baby for each of them sorry for the whole thing but that's just given by two running in parallel then it's just the same as calculating the bfv of each individually and adding them together so that should that is certainly what we'd like to happen and it just follows from the properties of these log boxes more interestingly there's the second way in which is compositional which is the sequential mode of plugging models together so if we have an open model M1 and some inputs into some outputs o1 but those are now actually the inputs for the second model I have these running for the first generated models so passing stuff up to the second one and now we want to calculate that results um BFB in terms of an observation we can again write it as a sum of two of them but in a slightly different way so observation is just existing on the top wire right because it's just the output of the whole thing against this observations it's just on O2 so first we calculate the vfp for this model at the top M2 in the usual way and then we add on a vfv calculator for the first model but it doesn't really have an observation o1 right but instead the observation it uses is one that's being passed down from empty so that's the um the queue that M2 is using is passed down now as if it's an observation down so M1 so it's kind of like O2 receives this observation does its uh updating about Q or whatever and passes that down to M1 so in this way we can say that the vfp composers in that okay both of these are minimizing vfp locally where for the M1 model we mean it's minimizing it with respect to these cues that are coming for this oh ones that are coming down from above then the whole system is also minimizing its fear because it's just given by summing those two together so I talked about a lot of stuff now I'm just going to wrap up now and then we can get to a discussion I hope um so the main takeaway was just meant to be to try and show that these string diagrams provide some natural language for talking about active inference um and I would encourage you to try if anyone working on the Enterprise for me to take a look and see if they would be useful to you in some way and in particular I focused on some other what I was calling the main ingredients of active inference so that were generated models the way you update them and free energy and we saw sort of ways you can describe all of those Notions in the string diagrams and the thing that I think is useful about them is that they give you a nice representational language of just drawing pictures of your generative models and composing them like hierarchical models and so on but they also let you do the reasoning because you can do the probability Theory with them so you can actually reason about what's going on in active events just with the diagrams themselves um there's loads of directions in the future obviously we can keep absorbing more of the of the works out and active inference into the diagrams um a bit more interestingly um I introduced this new notion at the end of how to make free energy compositional in particular we gave this definition of vfe for an open system now so it has a generative model which can have input so we call this the open vfb and it's been very interesting what people think of this definition we we introduced whenever it seems meaningful secondly um well throughout the top I get talking about just minimizing free energy and that's all I said already the vfe I didn't say how you do it so um in fact this is going to be done with these various algorithms of message passing algorithms so they're an important part of accurate for instance active inference as well and I think it would be great to include these in the in the setup by having some diagrammatic story of them um there's also lots of other questions around so one of them is that I talked about these two Notions updating with respect to soft observations and I think normally people tend to focus on Sharp observations so they perhaps haven't not everyone has heard of these before um but it's very natural to treat the soft ones when you're working in this compositional setup and so there you start to wonder about which of these the Pearl style updating or the Jeffree style updating is more natural to think about in the context of cognition and maybe we'll say okay one really BFE updating is the one you should be thinking about that's probably the claim active infants would make but it'd still be nice to think about how this relates to the other two is it very supportive as approximating the former or the latter style of sort of precise updating and then finally we could try and connect this up to as much lots of further topics I mentioned um a Continuum I'm interested in this notion of compositional intelligence so it would be nice to to connect this now to topics in Ai and so on um and think about how it relates to other basically applications of category theory in AI in particular this is also this whole world of categorical cybernetics I mentioned at the beginning um and I'd like to connect this a bit more precisely with what people are doing there with their stories in terms of lenses and so on and something else we were also interested in that I mentioned is that we got into the topic by thinking about Consciousness and there's lots of ways as a major theory of cognition there's been just loads of reversals for how active infants is related to Consciousness and it'd be nice to see how those can be described formally uh and in this setup and whether the streaming diagrammatic approach helps you make any more sense of those so that's something we'd love to do in future um and I just like to say thanks again to all of you for listening and yeah I'd love to go to a discussion thanks thank you Sean for the wonderful presentation thank you I've often I will first pass to Ali for opening remark please uh thank you Daniel um thanks so much Sean for your really fascinating presentation and I truly enjoyed it uh so I have a number of questions so let me Begin by asking um uh the first one uh so you know uh when Bob kirkey and others uh took hamiltonian formulation of quantum mechanics and uh kind of turned it into uh the string diagram formulation of it um namely ZX calculus uh the claim was that I mean regardless of its possible Verity but the claim was that one of the advantages of looking at quantum mechanics in terms of string diagrams is uh is more than uh it's more than just a convenient way of looking at a Quantum formulation and it actually unveils some properties of quantum mechanics that would be uh extremely difficult to see with hamiltonian formulation and uh even in some of their papers they claim one one of the reasons for uh somehow the um stagnant development in Quantum Technologies and quantum theory is uh is exactly related to uh the difficulty of working uh with hamiltonian formulation so uh would you say a string diagram formulation of active inference kind of uh takes a similar approach to uh somehow providing more than just handy tool for representing active inference modeling and actually it kind of uh opens up new possibilities for further developments of active inference Theory possibilities which would somehow uh I don't know impossible or at least extremely difficult for the current traditional formulation of active inference to see in the current formulation of active inference that's amazing question yeah um yeah I would agree I think the the you know my the idea for work also is actually in this categorical quantum mechanics area talked about so it's string diagrams for quantum theory and everything and I agree that um that language helps you talk about other things that you would maybe never get around to so much in in other mathematical conversations of quantum theory these are these things that make use of the the tensor that the the composition a lot so if this led to stagnation in quantum theories probably because people weren't focusing as much on the tensor and entanglement and stuff which um became very Central obviously like in the end that that's what people needed to to do Quantum Computing and stuff so now you what people are doing with quantum theory is includes going for computing where they're drawing these circuits and so you're drawing they're basically you know like string diagrams describing sometimes these string diagrams sometimes they just use the slightly different conventions for Quantum circuits but it's similarly a compositional language where you have got tensor products certainly as in things running in parallel states of these products so that you can talk about entanglement and so on um as is the language that makes it very immediate to to represent that because you just draw a box of two eyes you don't encodes entangled State and it makes you want to plug these things together and compose them which is what you want to do in Quantum Computing so I think similarly um if you never use that kind of language you might think of a system often as a fixed thing and not about the way it interacts with other ones so much and that could lead to overlooking all sorts of things so let's turn it in any area and I think in active inference um is certainly very true I think it is natural to think compositionally in this way because you're you're wanting to talk about generative models being composed from pieces and maybe thinking about how the whole brain works in relation to interactions between parts of it and so on so if you never use this kind of compositional view there is stuff you would miss I think I think in some sense people weren't as behind already because they already were working kind of compositionally right because they're using these Bayesian networks diagrams like the dags and the way they're normally drawn a very close they're like they are basically the string diagrams they just don't do the the equations and the rewriting of the of the diagrams so it's not as far back maybe as quantum theory was in the sense that people are thinking compositionally but it's like it feels like just you want to go one step further to having a fully competitional language or working in where you know you have the advantage now that you can just talk about taking a whole model and plugging it into another one and it has a completely clear formal meaning and so on which I think is what you want to do in areas like active infants so going from the diagrams as they're currently used to string diagrams is like The Logical next step and in terms of new stuff it lets you do I think um an example with something like this open BFE thing I guess so it's um if you're just always thinking about just a generative model meaning on without inputs you might not think of a notion I'm not saying this is necessarily the right notion but you might not think about this problem of how you want to give a definition for something that is allowed to have inputs as well and once you have that you know have definition you can sort of apply to parts of a composite system more easily so it becomes um more natural to to use compositionally so that's the kind of thing where without something like string diagrams it people can end up overlooking it it wouldn't be impossible to do without them right you need to talk about this notion of a kind of open generative model which just means throwing away so mechanisms to make things be inputs um but you could miss it you know and you but you really won't once you start thinking categorically thank you ollie please continue if you would like uh thanks so much so uh yeah my uh other um perhaps a related question is um I mean comparing uh this kind of formulation to uh this recent uh formulation of Constructor theory in terms of string diagrams or categorical formulation of Constructor Theory uh before going into this question uh you see uh you mentioned that uh this project is a part of a larger Pro a larger project for developing collective intelligence right so uh the similar similar kind of situation happens for Constructor theory in which it is a kind of meta theory that tries to somehow um uh discriminate between uh the uh possibilities of physical laws as opposed to counterfactual laws and how physical laws how there can be a theory against for the emergence of possible physical laws so uh in this sense can you we just say this kind of formulation categorical category theoretical formulation or um possibly this specific string diagram formulation of active inference uh or maybe other theories of Consciousness it can be seen as a kind of providing a path toward developing a kind of meta Theory Of Consciousness and possibly unifying many different strands of uh theories of Consciousness into to I don't know a holistic picture that can somehow be compared and positively reconciled with uh with one another and ultimately reaching uh the ultimate theory of uh Consciousness or uh I don't know do you see this line of work uh I mean providing enough evidence for this line of development research or I don't know somehow maybe even not specifically Consciousness but uh unifying the different aspects of um cognition intelligence and Consciousness all together so uh yeah you're giving me like ideal um selling points so yeah there's also something I would I would like to say you know I mean that that um yeah I I tend to think of it that way in that you know my background is in applying categories very to just lots of topics and I so naturally do think of it as quite a unifying language and you know the the ground and Consciousness that I mentioned was building on earlier what we did on looking at integrated information Theory Of Consciousness which in the end basically was done in terms of categorical probability so it's like the same setup of the diagrams and so we kind of wanted to do the same thing for directive inference so um there it's like we've taken about these things and put them in this common language you could have put them in a Common Language a probability Theory before but I think you know I do have an intuition that um there is something more clear about it does make it easier to get a conceptual graph but both theories I think once you've done it this way and somehow also yeah the diagrammatic view does make it much easier to compare them so the Hope was basically to you know and still is to keep going and to keep understanding various Notions in that language so there's things I've looked at cognitive science I've also done this way since the three of them conceptual spaces um I've worked on creating that in terms of diagrams and so on so I would love to see basically many theories put into this language to make it easier to compare them yeah you could try and compelling directly already but I think you want one clear formalization to put them all in and I would say that the categories and the diagrams is the right one to pick because um it tends to just give a very clear conceptual view of things the question is whether you have some Theory that's very important where the things categories are good at just doesn't quite capture the essence of what you want to talk about there but for things like active inference and and IAT so far it's it's very natural Because by the case of IIT it's about talking about how integrated something is so you basically want to talk about the opposite of that which is something being decomposed and the diagrams basically that you talk about parts and how they're related which is what you need to make sense of that nation of integration so it's very natural there but yeah I would love to basically to see um various aspects of cognitive science um understood categorically that's something I'd love to do myself as well um and as a you know the hope would be then to try and gain insights from all of them you know and build a theory it's not the category Theory itself is a theory of cognition or Consciousness it's just a very useful language for relating them and then it would be very exciting to see something you know natively defined in terms of Academy Theory as well at the end and there's a feeling that some of what's going on in applied categories Theory I think like in categorical cybernetics and so on is kind of taking that that approach for perhaps some of the first time previously I've always thought category theory is basically take existing things and you get a really nice abstract view of them but now I think people are comfortable enough with it that they're like sort of defining things categorically from from the outset um in areas like that yeah awesome well yeah I I have many thank you that's a great question there's like ideal questions in you you've pointed in and we've explored a little bit of the utility and the Simplicity and how that could help with accessibility and rigor and applicability all these awesome things leading to reaccounting and reframing consolidating as well as discovering some new trails between for example expected free energy and variational free energy looking at the equations you might be able to say that they rhyme but you would be many many lines deep into understanding what if any generalizations could Encompass the both of them so that that was just a very Salient example a few different kinds of questions so how is time treated in category Theory or how does active inference treat time today and how do you see the way that time is treated we talk about discrete time and continuous time generative models then there's the past present future multi-agent systems Federated or asynchronous communication so how is time treated and how does that give us a different grasp on dynamical modeling um right yeah I think that's already I'd love to have a better answer for that basically I think it's a tough one you know so at the moment yeah in the talk says I just talked about discrete time and that's sort of very easy to create the division network setup and with the these kind of thing diagrams because you can just lay out the discrete time steps as processes in your picture like we see here where we have the end end time steps here but um I don't have anything satisfying worked out yet to say about how you would treat a continuous time case search I think is important in active inference you'd like to basically um take I guess basically what you want to do is take the way that you describe this thing with the end time steps and kind of have a form of folding it together and just think okay but you're unpacking this thing end times and then you can take that thing and imagine you know this abstract view of unpacking it just not discreetly anymore in this continuous way so you can capture something like a differential equation kind of definition of continuous time thing and active inference um I um yeah so you can certainly work with continuous time things in the sense of you know the stopping on in categorical cybernetics or sort of categorical systems theory I guess it would be called an act world is kind of you know it has continuous time dynamical systems and talks about plugging them together but that that diagram is sort of just relating their variables though is my understanding it's not like a diagram isn't exactly showing the time and in some sense they kind of have to synchronize I think you know it's not an area I'm totally familiar with so it would be yeah it would have been really really cool basically have this work and then have another part of it talking about you know like we've done for discreet time here having a nice description of of a continuous time case I think we'll end up being some some work to take that into account um there would be really nice to see so it just needs the right abstraction I think or yeah taking a picture like this not drawing the time steps as like bits in your diagram but just saying you know that it's like this B thing with like a feedback loop basically is what this is describing and then giving a semantics to that in terms of time evolution um to give a continuous version of this for example with like State unfurling continuously in observations for each time step um in general I wouldn't say there's like an answer to the question of how is time treated in category Theory there wouldn't really be one answer because category is going to be so generally um they tend to be very effective for the just for discrete things in general like like algebra and so on because they kind of are it's great in some sense that the composition is discrete so continuous aspects and things like continuous time tend to be more difficult in a sense or they're just sort of inside the morphisms as it were they're not in the composition so it doesn't end up looking like this when you're composing continuously um but yeah I think there will be people in act who've sort of would come come at you with this particular answer so they they've got a way they like to treat continuous time um no I'm I'm just not familiar with yeah yeah cool a little bit of a more educational or applied question so how do we go about drawing and learning to draw is there a software package is there a way that we can get a step-by-step process to building that familiarity with like when I see this shape then here's what I know and then how do we know what we can and can't do and does that drawing software flag us or do we need to send it to a friend so how do we look at something and then part one build up the motifs in our own aesthetic understanding so that we can understand the compositionality of this as you do today and as we all do today for example for language English and then part two how do we go from having built that Motif based compositional understanding to like now what can we do and then when are we just totally freewheeling and off the rails or the free energy principle or like Does anything go if the motifs allow it um yeah I wish I I should I should have the standard answer the best way to learn string diagrams I think if I'm going to talk about it like this that you've prompted me to to come up with that I don't have something on top of my head that's the best way but there's so much stuff out there you know if you're I think it tends to be because yeah if you want to get really comfortable with the diagrams he was learning category theory in some sense but it's not like you need to learn all of category Theory it's kind of a relatively modern offshoot in this applied category world that's very diagrammatically focused and there will be various nice introductions out there to using them um another way is to so I'm trying to think I'm pretty sure recently yeah there was a nice paper that came out there was an introduction to string diagrams for computer scientists for example so there tends to be different introductions kind of for different audiences because they just want to pick categories um that um that those people are familiar with right so they can actually have some examples you could just learn the diagram Studios strangely but it helps to have some examples um and you know the old category three textbooks are all things mathematicians have looked at and a couple people haven't heard of so they're not particularly helpful so there's like you know Bob has paper categories for the practicing physicist it's aims of businesses that would basically introduce string diagrams to them there's this recent computer science one I know there's some work going on in producing one for cognitive science which I think would be really good um having an introduction yeah to the string diagrams for those people um so you basically look for one in an area you're comfortable with and you find it in paper on it but um it would be nice to have a good online resource I guess right that gathers these together so people can can just see a great guide for all the all the introductions if you do something like um you know there's courses you can do in a sense of the um bulbs but but in the case of learning Quantum there's something like Bob's long book about Kissinger picturing contemporary Services that's the kind of thing I learned from like it was in the form of electrical so it's basically the same book so because then there's loads of exercises that will make you have to reason the string diagrams and then you pick the rules up because at first it's yeah you don't you don't have the same intuition obviously but the rules what can I do with these can I slide them around like this or whatever but it doesn't take too long to get quite used to it I think which is the nice thing about them they're kind of natural they're just these elastic strings and boxes you know and you have a sort of geometrical intuition depends on the lack of exercises are the way I'd recommend getting used to using them um I didn't use any like software in a sense of the diagram to draw on this program called ticks it but it doesn't like tell you how string diagrams will work or anything it's just for drawing them um but I know there's more work to develop you know libraries like the algebraic Julia project is sort of like an applied category Theory um language but I wouldn't know if it was recommended as a way to First learn categories and yeah so I recommend finding a nice introductory paper in whatever field you're most used to playing with some exercises to get really used to them um for causal models there's this paper Robin and I put out it's not necessarily the very first place to answering diagrams but the game is to to introduce to people who've heard of calls and models say in a sense of exposure networks basically but maybe the course interpretation of them um to get them used to string diagrams and this this paper hopes to be a little bit introductory yeah as well yeah cool Ollie please uh thank you so uh getting back to the question about the time representation uh in this formulation uh so uh I take it that this kind of the formulation of Bayesian inference I mean category theoretical formulation of Bayesian for instance is largely based on Toby Spritz definition of Markov categories as a CD categories right so as far as I understand it uh Fritz paper uh kind of one of its basic assumptions is this kind of unidirectional inference I mean from earlier times to later times right or in other words the prediction uh but uh um in uh Quantum formulation of active inference or Quantum active inference uh there's this attempt to also develop uh the retroduction aspect of inference as well right uh so would you say uh this recent formulation uh can also be accounted for uh for this kind of retroduction you know the words uh can this formulation be reconciled with a Quantum bayesianism as well to add uh one more context here uh in uh I think it was in Kirk and speckin's paper there was this clear distinction between classical Bayesian uh inference and non-classical Bayesian inference in which the classical one uh does not um I mean allow for uh the retroduction but non-classical Bayesian inference uh can be applied for both prediction and retroduction as well okay yeah I would love to be a bit more familiar with the the quantum active infant stuff basically to compare with it so I'm not I'm not as familiar with the sort of and retro sorry obviously the elevation prediction is retro retroduction yeah prediction and retroduction yes so I think um yeah I would have to compare with this the another paper you mean Bob's type of problem on both forms of Asian inference to see what they say they're about the classical one um can you give some intuition as to why isn't something you can do classically basically the the Retro one because if that's a general probabilities then it will be true in some sense here right so you know here it's just being modeled in this probabilistic category and so at the moment they're separate in that you know you have the model which basically goes forward and then you do your updating to try and approximate something going back but you don't really have like this one um yeah yeah so uh so um I mean the whole idea was that uh a poor predictive uh quantum mechanics uh we only need to account for the inference from earlier times later times but uh if we want to account for retroductive quantum mechanics as well we need to somehow uh account for uh because as we know um not every um I mean Quantum formulation uh follows the bell's principle of local causality uh so uh that's uh I mean in order to account for all the entanglement phenomena and so on and we need to somehow uh put these this bi-directional uh inference into our model uh so uh yeah that was the basic idea behind developing this kind of non-classical Bayesian entrance does it have something to do with like the unitary evolution in content period the way it has this reversible thing or is it exactly yeah yeah okay yeah yeah yeah and so you don't expect to have something like that classically basically where you have this reversible thing built in well yeah I I so I wouldn't expect to see um that exact feature here in a sense of you know if it's treated if it's basically something that you can't uh have in classical probabilities it's not gonna it won't exist in this category meta R plus because that's I think that would be basically the same category they would use in that paper and they were dagger combat categories and they'll work with metal something like this metal plus category for the classical case um if it's just a general like instead of a sort of physical notion but it's just an idea that the the model comes with a forward part and an accurate part then I think that's the kind of here obviously how you go from a forward path to approximate this backward thing but the sort of lens type view of what's going on that's more voted in category cybernetics would be imagining I think the model kind of carrying this backward influence process with it as well so that it's you know for each forward part of the model you would um have this approximate inference sort of Channel stored with it um so I don't know if that would address what you're after but it would have either a backward and forward part together yeah well Ollie do you have any kind of closing slash opening remarks or questions or where where do you see this going from the active inference side what is this bring to us and what is opened through what has happened largely this year in active inference and category Theory well actually I'm really excited to see this line of development and active inference Theory and uh as you know I'm a big big fan of meta theories and uh all kinds of unification theories and so on so uh it's uh I don't know I kind of have this feeling I have this hands that this line of development in active inference uh Theory uh is I mean it looks quite promising uh especially for um kind of tying up all the Loose Ends and transcending and many many other areas and discourses and ultimately reaching a kind of uh coherent picture of quote-unquote reality whatever it means so uh yeah these kinds of development I mean the last year we had a tremendous advances in Bayesian mechanical theories and in recent months we have this fabulous line of research in category theoretical account of active inference My Hope Is that ultimately these different strands can be unified into as I said coherent and overarching framework so exciting times so do you mean that you're thinking of it as um it sounded like you're basically alluding to the thing going on in cognition and work going on into physics going together right like one really really matter really exactly so yeah the I the idea behind Bayesian mechanics uh one of its uh premises or assertions was that uh there isn't any clear uh distinction between uh cognitive and non-cognitive things or agents and uh they rest uh and Continuum and it's uh I mean the same kind of mathematical technology can be applied both for inert and conscious agents or in the absentient agents uh or whatever we choose to call them uh so yeah this overarching uh Theory uh I mean unveiled many interesting uh phenomena regarding uh well self-organizing systems and um it changed the whole perspective uh about how we can uh look at and Define even define consciousness cognition intelligence sentience and all of these related terms uh so my hope is that uh category theoretical account of active and inference can also be uh used for clearly seeing many of these uh emerging elements in Bayesian mechanics and active inference Theory and hopefully well gaining some interesting and potentially groundbreaking insights oh that'd be wonderful yeah I would love to apply for those topics yeah and I'd be very curious to see how categories can come in there sorry Daniel yeah oh yeah I'll just give my closing thoughts then then to you Sean just just a few loose notes that again open probably more than they close Ali was right in suggesting and and expressing that Bayesian mechanics recently has helped us develop a Continuum of active and passive systems so-called living and non-living or inanimate and animate and that brings us to another dialectic to resolve which is life and mind which is where the physical and the cognitive science come together you said they're on a Continuum maybe we could say they're on a quantinium and what language could express such work well right now we're speaking in English with the active inference ontology dialect however the phonemes are not intrinsically meaningful the um in a Markov blanket or category does not mean something it's it's a sound and so the string diagram language and representation I see as a way to fuse and integrate semantics into the syntax of the actual inscription which enables us to generalize in new ways also recognizing string diagrams or not everything or and so on and then with all of these intersecting vectors from cognitive and the physical sciences we are able to take the compositional cartographic approach for cognitive ecosystems and talk about diverse intelligences biological Quantum classical architectures all of these synthetic intelligences and so it's super exciting and I appreciate again your visit and look forward to people's curiosity taking them and also the development of tools and educational materials that that make this easier and then being able to display and use something where the meaning is primal rather than like well this letter represents this it already introduces such a space between the analytical representation and really the string diagram which exists isomorphically with it hmm [Music] yes but it's very exciting way of thinking yeah it sounds like you're you're um advocating a kind of structural ontology kind of thing in some sense right whether you're taking the compositional stretch of what's going on to really be the meaning or really be the the real thing that's there not just like I don't know yeah we could talk about it for a while I'm actually but I would love to yeah see string diagrams and you know other other approaches I'm sure that um take that role and you've got me very excited about this kind of unification that's going on thank you again Sean you're always welcome and we look forward to seeing where this all goes yeah thanks thanks again for having me yeah really great discussion thank you thank you so much I've done exactly all right bye thanks