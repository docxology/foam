[Music] [Music] [Music] hello everyone welcome to actin flab to act in flab live stream number 32.2 it's november 16 2021 welcome to the active lab we are a participatory online lab that is communicating learning and practicing applied active inference you can find us at some of the links here on this slide this is recorded and an archived live stream so please provide us with feedback so we can improve on our work all backgrounds and perspectives are welcome here and we'll be following good and fun video etiquette for live streams at this short link you can see some of the upcoming streams this is our second participatory group discussion on the stochastic chaos and markov blankets paper and then in the second two weeks of november we're going to be discussing thinking like a state by our friend and co-participant aval and we haven't yet set the papers for december so if you know an author who you'd like to invite or if you yourself would like to join for several weeks of discussion of one of your works then just let us know today in active stream number 32.2 we're aiming to learn and discuss and ask what if and what is and hold all these different questions at once we're in the second participatory discussion around the paper stochastic chaos and markov blankets by fristen heinz ultzhoffer decosta and par and last week we had an awesome discussion in the dot one and we uh got right up to the cliffhanger where the the m word actually was deployed and i think today we're gonna um sprint up to that cliff and then jump off so it'll be a fun discussion for those who are watching live totally right questions and comments in the live chat and stephen and dean and i and anyone else who joins will be just more than happy to hear what you have to share and to um [Music] roll with it as you write things so let's begin just by giving a introduction or warm-up so we can say hello and then also just maybe what was exciting about the paper overall or since we all participated in the dot one what are you looking forward to in the dot 2 or maybe how have you changed how you thought about it over the previous week so i'm daniel i'm a researcher in california and i'll save what i'm excited for until a little bit later and i'll pass it to dean hey good morning uh my name is dean i'm in calgary uh in the in the one weeks time since we left off i i think this paper more than many of the other people we've had conversations around breathes life into something that is typically perceived as not very alive it's just it's just a sort of a point to and an explanation of but i think this is one of those papers where when people pick it up and start having conversations with one another about the kind of work that's being done here it makes it really really interesting doesn't give us necessarily the answers that the authors could project would be given and i really like that thank you dean stephen i'll pass it to stephen good morning as well from toronto um i'm really interested in this paper i think because it it does um give a unifying perspective it does help with that unifying perspective even though at first when you see things like stochastic chaos it can be like oh we're going down another rabbit hole etcetera etcetera um but actually rather than always justifying or talking about applied active influence influence through applications and quotient applications there is some benefit to this kind of coming to the foundation and so i'm excited by that and also actually following the conversation we're having earlier in the fact that we have this um complexity weekend is some of the ways that complexity can be talked about outside of a more woo-woo kind of conversational sense at these foundational levels which again seems to get into unnecessary um detail ultimately i think it starts to allow some of these crossovers between applications of complexity and this to happen which for me personally i don't think i was necessarily able to articulate a year ago so i think some of this work does just help fill in some of those gaps thanks i agree like the system that's picked up and turned around and remodeled throughout the course of the paper in a very narrative driven way is the lorenz attractor which is like the classic complex systems model so it just kind of plays into contemporary work on complexity and so many other areas but and then also dean i i really like what you said like the authors i hope will be able to join for a future presentation so we're in contact they probably won't join today but what the authors are putting out there and then how that's received and communicated around and explored that's what's fun and um i know that we'll have many cool things to explore today so let's jump to the big questions then we'll look at the road map and try to remember where we got to yesterday and maybe a different big question is striking either of you or anyone in the live chat today and then we'll try to remember how far on our road trip we got last tuesday and then let's see which sections we're going to get to and we have a bunch of other things kind of written down so the big questions that we had previously asked were what is a good model of thingness in a chaotic dynamic anticipative world how might this model or approach to thingness speak to system sentience and then there in little tiny font it's a sentence not in the phenomenological like experiential but greeting cerval good to see you but sentience in the set in the sense of feeling thinking and feeling systems without uh worrying about the first person experience side at least in this paper and then the key technical question of this paper which broaches onto a million and a half other areas is what is a markov blanket just what are these blankets and how is one modeled identified defined statistically what are the pros and cons of different approaches and so that's some of the technical details that we'll get to so steven thanks for raising your hand and then welcome serval after that feel free to say hello and introduce yourself if you'd like yeah i'm curious and i notice here um that we start with thingness which i do agree is this big question of thingness um is something that karl fuston brings up quite a lot but i think he's often he's the he in that part i feel he's often a bit of a lone voice in terms of really pushing the thing this side of things and i think it's really interesting that second question then of okay how does that speak to system sentience and you see now we move into systems right and okay thing this sort of to me is it gives rise to systems and gives rise to sentience however i think it's thing this is still there right things are still the game in some ways are being maintained so that first question because when we then get into the third question things can get lost and it's all about system sentience and i think there's something really interesting there because um there is a question of i mean it's it's even as we say in those brackets what it really means to have system sentience or even just system is probably rather ill defined and and i suppose to some extent is how much is it even needed how much can thingness be maintained as an equal partner in the game and not sort of passed over once we scale so i think that's a good good piece of provocation thank you and to what extent can things and systems be identified in a bottom-up way from measurements of dynamical systems serval would you like to add anything or say hello or bonjour hello i'm turbo i'm a complexity scientist i founded the chemistry social laboratory which is interested in building uh inactive models of social political changes and i'm here to see the discussion around the formalism of the paper which i did not revise and i therefore cannot make informed comments on thank you sir dean i just want to add one thing to what stephen was talking about there i think what the big questions and what this paper addresses pretty directly is can we can we accept a minimum of two that there's an abstraction and that there's a material aspect to thingness and one and marrying those two things is is kind of what we're hoping to be able to do but but it it comes in two very discreet forms so so what of that we call it thingness because it kind of spans both realms and that's what i think i take away from the idea that it can be chaotic and not absolutely there's the map in the territory and one thing that we're gonna return to is like the first map that we apply to the territory so already we're separating the territory in the map but even the first map can be chaotic and then can we put a second map the laplacian approximation as it turns out in the paper can that second map be one that gives us some properties that make it more tractable to handle like you know potholders taking something hot out of the oven give a better grip on the first map which gives a better grip in terms of actionable insight into the territory can that second level both be more tractable especially using computers but also retain those chaotic properties which we're going to see when they estimate the lyapunov exponent of the laprosian approximation so that's the recursion prior to the rep the repetitiveness that's what i think this really gets into recursion modeling thanksgiving so stephen yeah i'm gonna put this as maybe a little bit of a question as well but it's rhetorical if it doesn't want to be answered but i think that the thingness and the markov blanket because we're getting to that stage here um there is this there's i think this tends to be a sense that okay we have a thing and because it's it's trying to sort of maintain itself in a just dissipative world then that's effectively a system because it's got a markup blanket and we're in that world but with this it's like look you've got lorenz attractor then lorenz attractor goes into thingness right now what and then thing this potentially goes into probably with a few phases system sentient system so the question is which side of the which side of thing this is markov blanket does markov blanket emerge from him from going from lorenz attractor into thingness and then continue or does it is like lorenzo tractor into thinness into markovian blanketed thickness um curious what your thoughts are on that and maybe this paper shed some light on that great question let's look at the road map and just see how that comes into play so we talked in 32.1 about a few different ideas and we have some slides ready with just tabula rasa if we'd like to add notes or we can always modify slides and deal with questions as people are raising them let's go to the roadmap though okay so the the roadmap of the paper is as follows in section one there's an introduction in section two there is from dynamics to densities so this is one of the really fundamental kind of linking events early in the paper where dynamical systems dynamics are linked to flow systems like densities so this equivalence between dynamical systems and density driven systems is going to be finessed extensively in the paper so that's an introductory step that's very important to note the helmholtz decomposition is a tool from vector calculus that applies very well to densities and flows where it's usually applied but because of what links were made in section 2 it becomes possible to take that helmholtz decomposition approach to dynamical systems that is specifically visited and revisited and then moved beyond in the lorenz system that's one of those classical chaotic systems and a favorite toy of complexity analysis the lorenzo system is moved beyond and then approximated with a laplacian and then even moved beyond there so that's pretty much where we got to last week we tried to do a really solid grounding in first the equivalence between dynamical systems and density driven systems or two kind of representations of underlying systems that could be seen as equivalent and then we talked about the lorenzo system and about how it's chaotic and how chaotic means like sensitivity to small changes as quantified by the lore by the lyapuniv exponent and then we talked about how the laplacian approximation which is kind of like fitting a quadratic which is kind of like letting the ball roll to the bottom of the bowl and this idea of like well if we can phrase the problem like a bowl then all we have to do is roll to the bottom there's only one bottom and we know which slope we're on so that's also called convex optimization because no matter where you are if you know that the problem is shaped like a quadratic curve like a bowl you can just instantaneously estimate the slope of the bowl and then just run downhill whereas if you're on a rugged landscape just you know you're lost in a mountain region we'll go downhill you're going to get trapped into a valley potentially and you might not make it out to the ocean but if you could re-imagine that rugged landscape in a way that was more bowl like it would allow for tractable estimation of which way to move in section 4 we get to the markov blanket concept and the free energy principle and i think that's going to be the really fascinating discussion is like okay so let's get what that markov blanket what is that partitioning what are we really talking about and then are we talking about the territory are we talking about the lorenz map or are we talking about the laplacian map where is this blanket partitioning coming into the picture and then how does that help us reduce our uncertainty about systems and what does it say or not about that underlying system and that's where we're going to be talking about particular partitions of course a favorite and recurrent pun which is that particular means specific but also it means like a particle like a autonomous particle drifting around in space and then section five really comes to a culmination with a discussion of the free energy principle alone and so i think it'll be interesting to discuss like what is subsumed into the free energy principle and why do we need one section mark on blankets and free energy principle and then another section on just the free energy principle so that's kind of the road map steven and then d yeah just a quick question see if this clarifies something which i think is what you're saying dynamics is um the dynamics more particular flow like it's and densities while you could have a density of particles it's actually more like a probability density on wave functions would i be right in thinking is if there's a kind of a wave particle or part actually a particle then wave way into this and that's partly how they're playing this out someone with more experience with the formalism would be totally open to their contributions on that question my first take would be you could make a dynamical systems model like equations of motion for a particle floating around and so it's like you're focusing on the first second and third derivatives of movements in x y z space or tetrahedral space for some particle so a dynamical system model could be describing the movement of a particle but the um flow that gives rise to the movement of that particle is it's kind of like figure and ground like the dynamical equation features the movement of that particle like i want to know the pendulum's location and so that's going to be a dynamical model of the pendulum whereas the flow based models like you pointed out are much more about well what is the underlying space of the flow of that pendulum so it's like if it's to the left of the bottom it's like it flows this way and if it's to the right it's like it flows that way and so that is about asking what is the underlying flow dynamics that do give rise to movement of potentially particles and those are why this meme is on the slide and why they are made in equivalence early in the paper because it is like talking about the difference between the movements of a point through a space versus understanding the flow dynamics of that space that yes does give rise to the movement of particles ah and could i just ask one so what you almost it goes from a just purely the um you know the momentum of a particle to this flow and the flow is kind of a bridge then between density dynamics which is maybe more of a probability density of where it could be which can map both to an object where an object might be but also where a wave function might be in in broad sense so then you're sort of going from literal particles in a classical sense to the flow and then the flow then bridges into density dynamics which is can be more probability orientated is that fair to say yes particle is going to take a particular meaning a little bit later when we talk about the partitioning of states but i believe that's correct and also the density like you suggested ties much more closely to statistical density distributions like in statistics we're often interested in density distributions of special type like the area under the distribution summing to one the integral over a distribution being one allows us to treat it like a probability distribution and so that's the whole debate in quantum is it actually a wave or does that distribution of positional uncertainty merely describe our statistical inference on the location of something that's at a specific point so whichever side we want to come at this from section two says it's gonna be okay to use both and we're gonna sometimes lean more on our left leg and sometimes we're gonna lean more on our right leg because there's tools that exist for one but not the other dean yeah i don't know if this begins to answer steven's rhetorical question but i think where most people get introduced to a markov blanket they get introduced to the idea that it's either a way of partitioning and dividing or it's a way of being able to envelop or blanket and i think what the paper points out is that it's both all the time so you can for example find a box of cookies and see all the cookies settling at the bottom whereas or if the person is you know described as being creative uses that box of cookies as a leverage point to get more cookies off the top shelf now it's a platform and now it's a mound and so we can see it both ways at once if we think of it as both ways at once it's not an or a question it's that minimum of two piece again that that is really fascinating about this that really kind of blows people up it's really both at once then you decide which it's going to be and that's what i think is really fascinating for people who are just sort of start getting into this because they they've been so habituated to saying oh it's it's a cookie jar oh it's a platform no it's both and that's that's what we want to do thanks dean and um marco in the chat wrote like a boat in a stormy sea it moves at the mercy of the seas and winds flowing forces with only its rudder sales and other limited mechanisms to steward the world's influence on its journey so bucky meme one unity is plural at minimum two bucky meme two on his gravestone call me trim tab kind of like the rudder and that is exactly getting at the point of this paper which is if we just were talking about a dust molecule floating in the air then we could say well we could use the dynamical systems model of the position of the dust particle or we could do inference on the flow of the room and that gives us a different and complementary totally co-extant picture but that's a dust molecule what about active systems that have affordances that are able to do inference on action selection choose different policies how do those active systems do inference that's the winged snowflake that's the moth to the flame and those are the really exciting systems and that also speaks to the difference between what's called mere active inference like the particle where we can still have this partitioning and still see it as a dynamics or a flow versus the adaptive active inference which is then we're thinking about those systems where the particle the particular states are able to effect different policies and select between them in a way that acts as if it's resisting dissipation and those are the kind of exciting systems that we want to get to by the end of the paper stephen and pulling on that idea of um cookie jar is the there's the the cookie jar is maybe a almost a particle a thing that is like leveraged and there's a thinness which it becomes so it say it has a thing about it but there's a thinness of of being an affordance so this is an interesting just sort of came to me but the thing this in itself is slightly emergent because there's the the only by the process of trying to use it to get the higher cookie does another kind of thing emerge which becomes a step so there's something interesting there about the nests um as as an emergent you know is it ever a step really but it's always it's never not a thing at the same time so there's a i think there's something quite interesting there in that emergent stroke flow stroke you know literal sense and also i don't know to what extent this is um intentional but thinness we talk a lot about ness the non-equilibrium steady state or non-equilibrium stationary states and so it's like the things ness that's the thing the thing we're talking about is the thing's ness so we're not talking and saying that the system is at stationarity we're saying that the things nests by virtue of how we're modeling it has stationary characteristics and that is the thing as we model it which is that always existing difference between the map and the territory we're not saying that the territory is even a thing we're saying the way we're relating and interacting with the map which we totally made no one's going to deny that that maps things-ness is tractable i can hold the map in my hands that gives me better grip on my road trip steven that's a really yeah i think also the the thickness can tie into the idea that something ontologically comes into being through relation through some sort of network dynamic um in which it acts um and that that actually that speaks to this lorenz attractor you know so the lorenz becomes part of a network of an ontology of thing which is really a thing in relation to what some other thing is like you know because obviously the scale of the cookie box is quite a lot different to the scale of the atoms in the cookie box or the whatever scale we want to so there's that and then there's some sort of affordance level and maybe we can get out of the whole sentient system thing and it's like it's just an affordance for the thinness to maintain thingness it doesn't need to know about systems as such unless someone some creature happens to evolve a brain which starts to sit down working out system diagrams two-legged ones potentially um otherwise it's like it's just things all the way up right let's now that we're 30 fun minutes in let's um sprint up to that markov cliff and then see where we go so figure one of the paper is kind of making this first fundamental connection between the lorenz system as defined by dynamical equations so the traditional lorenz is defined more like the blue version within equations of motion um well it has a positive lyopine exponent which is to say that very closely located points diverge rather than stay in like a laminar flow parallel line world or converging so they diverge and that's what makes lorenz system chaotic ergo difficult to predict measurement error and tiny perturbations due to even thermal noise which is like kind of how pseudorandom number generators work and then there's going to be an approximation based upon this stochastic solution and so the trajectory in the right panel here it's the deterministic solution to the laplacian form of the lorenz based upon the helmholtz decomposition and so this is sort of like the version on the right that's usually seen and the state space model is almost like we're sampling stochastically from points along that race track so that was figure one we could you just could you just explain that just go over that one more time just to clarify that bit i think i understand what you're saying but it's kind of an important point you make there when you say deterministic solution that i didn't there is that that went just to clarify what that means so the deterministic solution would have no noise so that the position at the following time is entirely determined by the non-stochastic state that it's in at the preceding moment whereas when we talk about a stochastic form it's like that's like your car's driving and there's thermal vibrations on the road now the leopard if exponent on that car at non-zero temperature is small so that the flow term dominates the vibratory term but there could be some other system where the difference between well if it's in the absolute zero world and just on the road then it's going to go straight but once you introduce any wiggling when the system is chaotic it has that positive leopard exponent then the tiniest wiggle causes the system to diverge really helpful because and so basically normally we just like with the deterministic thing it's just whatever the initial conditions if it varies slightly but then once it runs it will settle into something but if you you've got the initial condition and you've got the perturbations due to noise as it's actually running which adds i suppose you could say a little bit of an extra change in initial conditions to each state that the lorenz attractor is moving through so okay that's that's helpful thanks daniel yes and i'm gonna just show this image this is um i don't know if it's the very very first but this is the image that lorenz used to talk about chaos in 1961. so i mean what's the difference between 76.8 and 76.85 it's imperceptible if your thermometer only had like three significant figures you wouldn't even be able to talk about the difference but i mean how different can it be and it turns out that these those two systems start imperceptibly different but nearby points in chaotic systems do diverge and so it's like okay but they're still tracking each other right it's like well no wrong later on and so of course a more chaotic system with a more positively open of exponent diverges faster but any system with a positive leading leopard of exponent is going to have divergence and so if you have a purely deterministic system or one without this kind of the openiv induced chaos then you can predict far out however prediction matter experts as dean i'm sure will revisit us too meet their match with chaotic systems and that's actually one thing i hope that we get to by the end which is that by putting action in every loop and inference on action we can still whichever of these timelines we end up on we're going to be doing okay because we're always re-adjusting our inference to whichever of these mountains we're on it's not like one of these is better or worse or more or less challenging like they have the exact same upper and lower bound the only mistake would be to make a prediction and be super confident about it and be on the wrong predictive timeline but as long as we're always updating our predictions then either of these days are gonna be okay but if we put on the jacket because we thought that it was gonna be cold here and that's when it was hottest then we're injured but if we just think okay well we know we have the jacket afford in so we're just gonna keep an eye on that as we watch the temperature and continue to update our predictions maybe then it'll be okay so that was the initial lorenz attractor section three goes into the helmholtz decomposition which we've talked about in several other streams and um we've seen it introduced as the breaking down of a vector field the total current in this kind of electromagnetic representation into the irrotational which is kind of like the straight line there's no rotation in those straight lines and the purely rotational solenoidal current flow currency we've talked about solenoidal ingredient partitioning and then what this paper does and i think the technical details are in the appendix b of 16. so that was the bayesian mechanics paper and then also there's this housekeeping big lambda and that's in one of the appendices to this paper so that would be like something for someone else to help us understand a little bit of because it does matter and it's actually a key contribution because if the landscape is changing moment to moment you can't just do the helmholtz decomposition once and sail on that c without updating your estimate of the partitioning here of the decomposition here that decomposition so again dynamical systems that's like the position of the car and then we're going to because of section two be having an equivalence between that dynamics and the flow modeling and one of the terms that shows up on both of these is the fancy j fancy i it's the self information the first part performs a riemannian gradient descent on the negative log of the steady state density which can be interpreted as self-information so how surprised should one be to be at a given location relative to expectation so we talked about how like the potential energy of the ball higher up on the bowl is higher and then the potential function drops and that's the physical analogy to this more informational statistical distribution approach to the self-information and then in 3.1 that self-information is going to be pursued and expanded upon for a a relatively more simple case so we're going to identify due to the helmholtz decomposition that there's this key variable which can be interpreted as like a potential function corresponding to the self-supprisal and that self-supprisal term may have functional forms so we can have a function that describes how surprised we should be stephen yeah if you could go to that last slide the previous one there that um i like this is quite useful so the the a plank if it's an equilibrium which we're not at because we're doing non-equilibrium steady state but if it were then it would be zero basically it's that equilibrium so it's not changing so there's no there's no divergence because um it's zero and then he's taking this helmet composition which in a way is like a vector isn't it with a vector you've got you know you can split something's moving into x and y so the this the gradient is coming out and the solenoid was at right angles to the gradient so it's kind of like that trick that he does with accuracy and complexity he can say that there's a solenoidal and gradient which go together for when it's still in a kind of a particular angular momentum sense and then there's that ability to with the housekeeping to then move it into the flow which i'm not entirely sure i think that's the bit where we might need some quite what because flow could probably be more specifically thought about so now you've got this ability to sort of sort of structure through the kind of more particular context the solenoidal and gradient and then there's this transition to a more kind of general variable which can be just we don't need to get the particulars now because it's a variational metric that just gets used in an approximation sense and then it follows into what you're saying would i be on the right tracks with that um not sure who he is even though i have an expectation because it's a multi-person paper but just looking at these as like circles and triangles and squares we can see that upside down triangle fancy i of x is in both terms so there's like a q multiplied by something and then a gamma multiplied by something and we see that omega is defined as like the q minus the gamma so these two terms which were decomposed out from each other get recombined into the final flow so that's just like if it were like a times x minus b times x and then omega we're defined as a minus b times x so we're recomposing a flow equation that still features this fancy i and yes we're solving that for a specific point that matters we want to solve that for where the ball is at the bottom of the bowl so there's a function that's going to tell us more or less in multiple ways whether we're more or less away from the bottom of the bowl and again it's not about the system being at stationarity it's about this decomposition having a focker plank steady state dean just one quick thing so at the bottom of the bowl if we the gradient can effectively be zero but solenoidal could still have a value because it can still be spinning around the bottom of the bowl but the gradient then because it i noticed the gradient term isn't in the flow but the solenoidal term is the gradient is because q minus gamma is in omega so omega is this omega del fancy i is just these two terms combined okay okay yeah dean yeah i was just going to say what typically when this is brought forward it's easier to understand the decomposition and the recompiling if we view the blanket as a partition where it gets a little bit more interesting is when the blanket is literally an envelopment and there's a there's a physicist by the name of keith humphries he talks about emergence and he looks at that piece of it as well because there's a polar aspect to this we can think of that in terms of positive and negative uh fields magnetization and stuff like that and how that is also a decomposition and a recompiling so i understand how it was presented in this paper that's the partition way of looking at it right it's more or less but there's also the positive and negative polar piece to this when we look at the markov blanket as a blanket as opposed to a separator and again if we're holding up two things at once this this doesn't this doesn't change that it's just making sure that we hold both things up at once the envelopment and the divider i just want to bring that out there because again we can we can see it easier as a partition like the skin as a partition we don't typically see the skin as a platform for sunscreen that's my point thanks dean stephen yeah this this this um this is a good point in terms of you can imagine at first i was thinking about grading you know it's obvious that you've got a gradient that you can pass through maybe where the gradient's zero but also solenoidal flow i suppose if if things are moving around this way and it starts to turn and it starts to go the other way at some point the the solenoidal flow is also passing through um a zero so they can both pass through a zero point it just wasn't as intuitive when i first thought about it yeah and even if you have something that's spinning around like something that's spinning in a circle like a hammer thrower it's actually continual acceleration that doesn't mean it's speeding up it means that the vector is continually changing so now our map is of that acceleration and it is unchanging in how it changes so that is to say that the system can have at a in a different way of thinking about it yes a solenoidal component we're not saying that balls at the bottom of bulls are dead and unmoving we're saying in our map there's only one point because we set it up to be like a bull map where the derivative is zero that's the solution where the rate of change is zero so that's not to say that the system isn't moving that's to say that the flow steady state has been achieved in figure four well in figure three we see um the three states so this is like the cover of the geb book like kind of three different projections from a three-dimensional system um the second versus the third the first versus the third and the first versus the second and we're seeing how there's movements of sampled trajectories and they're kind of coming back to something that's bowl like for any set of two dimensions so it's kind of like a three-dimensional bowl now because any two of these dimensions it's more like a regular bowl but the kind of soup goes in but then like the the soup bowl is like a down projection of a system that has an attractor in a higher dimension potentially a much higher dimension but lorenz they're only modeling three because the particle only moves in three dimensions when we continue to investigate the bowl-like properties of our laplacian approximation to that lorenz system we see that there's a few different shapes of bulls depending on which angle we're going to shine the flashlight from some of them are like spherical so a spherical bowl doesn't have any specific correlation between the two dimensions it's like a cloud of points the regression is just like flat through it however there are some couplings where the bowl is non-random so it's almost like if we were tracking the movement of a ball it's getting jostled around in these bowls well like the one of the first and third if that was the light we were shining it would just look like it was a scatter plot this is totally the regression's flat there's not much information being provided it's it's within the bowl so it already has some nice dynamics but it's kind of within that attractor zone it's a scatter there's no pattern but clearly for the first and the second we see that there is a coupling now imagine if we didn't see the bull because of course we don't it's not there that's part of our map we're just seeing the movement of that ball as we sample it through time and it would be only on this manifold kind of being like spending more time going from the top left to the bottom right in these images so we're going to take that idea and then connect it to the jacobian and the hessian which is again throwback to our matrix math slide and um an awesome opportunity for anyone who wants to bring more of a technical interpretation to the stream the jacobian is like the first the the unnamed matrix is just like the matrix of position so that's like the first moment of the distribution and then higher moments are like the partial derivatives so jacobian is like the first order partial derivative so that's like the slope of the ruler at that point and then the hessian is the second partial derivative that's like the curvature so if it's a bowl there's only one point where it's flat and then it could be flat and the bowl could be like up or the bowl could be upside down and so that's why it's important to also have this second curvature term to understand whether we're at a bowl like this or a bowl like that so that's kind of like calculus and those bowls those bowl attractor shapes which we're inferring using the laplacian quadratic assumption so it's not saying that the lorenz has these possesses these is these it's saying that's the approximation that we're going to make it turns out that although there is somewhat of an unclear correlation pattern in the first derivatives so like the movement to the left it's not like it's correlated with movement up however the second partial derivative the hessian matrix starts to look a lot like the covariance matrix so the actual positional covariance of these states with each other so like the on diagonal is kind of obvious like of course your position on the x-axis is correlated well with your position on the x-axis but there are correlations between state one and two and that is recapitulated in the structure of the second order polynomial approximation to these attractors that are laplacian approximations so again up until that point in 3.5 and in figure 6 markov has not been introduced nothing has been said that word hasn't come into play it didn't come into the system's definition and so that's just really important because it's just showing like there's a lot of groundwork to get to where we make the markovian partitioning and so there's power and challenge in that stephen would i be correct in saying that effectively it's a it's a case of approximating the states of the lorenz attractor and that's what's that's currently there so this is different ways of showing that within some manifold state space and by approaching it in different ways it reveals a different type of approximation the only thing i would ask i'm a little bit confused why it does squish maybe i'm missing something i can see why there'd be kind of a round one i'm just trying to work out how it squishes is it something to do because of the solenoidal piece or the gradient piece or is that something else okay so if we were talking about just a particle undergoing brownian diffusion in three dimensions so it doesn't have any kind of correlations or any sort of structure just it's just a random walk in three dimensions well then it would have a spherical error profile for all of these images but we are capturing structure that exists because lorenz attractor has a specific specification so the states are whichever are being modeled if you cared about pressure and volume that's your state space that two-dimensional state space of the map it's not saying those are the only states of that system it's just those are the ones that you're modeling in the lorenz system we have three states it's positioned in three dimensions but those three dimensions again also apply to a brownian diffusing particle which has a different jacobian covariance and hessian we're talking about those being calculated for the lorenz system with the parameters that they selected and so we are pulling out covariances and connecting it to this second order polynomial approximation in the hessian for a system where the movement is not just a brownian diffusion and so that's why there's structure in the covariance and that's why the structure is interestingly recapitulated in the hessian dean and all i would all i would supplement that with is things things there we go things that are in something that's moving the thing that's moving that's in something that's moving that's on something that's moving has a tendency to have the negative charge things all congregate in one place down in the lower right corner and all the positive things all congregate in another place in the upper left corner that's the nature of movement and so that polar part of it is in play as well if we consider it as part of these statistical transitions and that's again again it's it it's fascinating if you if you look at it a minimum of two things so not just not just as a partition but also as a an an envelope those things suddenly pop out at you and you see the pattern in in a different way so i want to think about a physical system that's not chaotic but will help us understand what this covariance matrix is so let's think about a children's carousel so it moves the horsey is moving around in a circle and so that's like our unit circle like the four quadrants and then also the horsey is going up and down okay so when you're you know so again it's like you know if you're at 12 o'clock on the carousel it's like you have a one for the y and a zero for x okay so i hope that makes sense now as it goes around it's like .5 and 0.5 again just using simple numbers here like the x and the y are correlated and then they're also correlated here so one could say that there's like a correlation between the x and the y like they have a covariance together but now the z-axis is moving up and down independently so a system like that you'd see a covariance between the x and the y positions of the horsey and not that they're a different system but that they are uncorrelated they don't provide mutual information on the z-axis which is oscillating now if this paper had done we model a deterministic carousel and it's kind of like putting the rabbit in the hat because the covariance matrix it looking like this would be no surprise because it would have been defined that way so that's the key insight and one of the key contributions is like they didn't put this in the hat this is the og complex system yet the approximation the laplacian approximation pulls out a covariance structure that's extremely tractable and it turns out that it still has a very strongly positive and very similar leopard if exponent so it's like now it's like a chaotic carousel and yet our approximation our map of that chaotic carousel still holds the chaotic nature of the system while also giving these bowl like attractors for any given two and even all dimensions or states considered simultaneously so um we're not conflating the simplicity of a non-equilibrium steady state with the complexity of the underlying density dynamics so just because the map is simple doesn't mean that the territory is and so that to me is a knockout blow against realism and for instrumentalism because this paper's saying nothing about the underlying system active doesn't say anything about the underlying system we're talking about approximations and our maps of territories in other words the probability density can evolve in a complicated fashion in attractors go ahead stephen and from what you're saying it's like if i was to take that lorenzo tractor which isn't it's like a saddle often people often talk about a saddle and you took the the most um agreeable way of viewing it so you view it in in a box space this way and look for how it's moving that way and then flip the box like this and see how it's moving that way and flip the box like this and see if it's working that way those approximations would be those three different distributions however if you'd moved it by like one degree that that would be like messy right so it's like they if you look it's almost like this is how the lorenz attractor taking the most um optimal view of xyz can be mapped out as it transgresses through the saddle um if that makes sense to bring it into a physical space um but again it's more complex than that because this is taking it where there's the plank is zero to try and get the most optimum sort of that's why those things are follow a nice stable transition whereas if you're taking a sort of an odd angle it might all be completely a mess would that be a fair way of saying it if you were to take a perspective a projection down that was a mixture of other axes then not sure what is gained by that but i i see what you're saying there's some particle moving in three dimensions and you can have a projection into any side of that cube or you could take some sort of off kilter view that would be still some type of linear summation of all three so it hasn't really reduced the challenge of estimating but yes i mean that's one way to see it dean so i'm in complete agreement it has to be instrumental so that you can compare to the real so the minimum of two you have to have multiple maths not one in order to be able to see this and that basically then reinforces the idea that the markov blanket has to both at the same time be a partition and be an envelope it's both at the same time a minimum of two again so again all we keep reinforcing is this is this idea that if we over reduce if we get if we get to a place of under two we may be deceiving ourselves and i think we'll come back to that soon so now one hour in we're able to get to the markov blankets okay so previously we were talking about the case of a single markov blanket and so if we thought about like a bayesian graph so now we're thinking about three nodes each corresponding to the x y and z axis nodes a and b you know the first two axes would have like an edge between them because they have some type of statistical relationship and then it would be disconnected from the third node because it doesn't have a statistical relationship with them okay so now to actually start to get towards more like the communicating systems or system in the niche that we're interested in that's the target that we're trying to aim towards in section four the authors repeat the analysis of the previous section but approximate two lorenz systems that are coupled to each other through their respective first states okay so we're going to look at this matrix okay so notice that there are two this make that transparent so here's one lorenz system here's the other lorenz system the the red one and the blue one and then there in these other cells these are the coupling terms so these are couples this is saying that the first state of each are coupled so it's a bi-directional coupling and that it's saying like state one the fourth state on the matrix like matrix row and column four is the first state of the blue system so the first state of the blue one the top left of the blue and the top left of the red are coupled through the off diagonal green boxed terms if this was all zeros nine zeros here and nine zeros here we would have two independently evolving lorenz systems so we kind of took the lorenz system the three by three matrix that specified the dynamics of that system and then we just made another now a nine a six by six matrix with two lorenz systems that would be evolving statistically independently unless we introduce this targeted coupling and they say this form of coupling was chosen to be as simple and symmetric as possible okay so this is like the next step but it's a difference that makes a difference because this is going to change the total dynamics of it if you had just the red and just the blue you could like split that matrix and you'd be getting the exact same model ability because the systems wouldn't be interacting but now we've introduced these terms even if we only had one where you can no longer divide this matrix into sub matrices that perfectly describe the time evolution of the system so 7 figure 7 is like figure 3 so it's similar we're taking like sort of pinpoint locations on the laplacian approximation of the lorenz system and now seven is doing the same but it's looking at these two synchronized systems or maybe it's better to say that there's two coupled systems entangled do yes and due to the structure of their coupling it results in an entangled movement pattern such that their trajectories have very um [Music] have their trajectories are informative of each other and that's also reflected by this flow diagram where clearly their movements exist on a manifold that's in a restricted space of the possible so two people brownie and diffusing in two separate rooms you'd get that circle scatter plot okay two people who are tied to each other you would see a perfect correlation and so we're seeing something even though each system has complex and even chaotic endogenous dynamics we're seeing that the introduction of this coupling term allows the systems to actually empirically have entangled behavior steven we've now got lorenz systems as opposed to an attractor and i'm curious when does it go from an attractor to a system is it because is that does the coupling effectively start to create more systemic nature or you know i'm just curious about that is it because it's the systems the lorenz system is just these three this is the specification of a lorenz system and it has an it's a chaotic attractor and so now we're talking about two of them so there's two coupled lorenz systems but it doesn't make sense to talk about two coupled attractors because that's not what's coupled the systems are coupled as per the matrix specification and then it doesn't even make sense to couple the attractors the whole point is there is an attractor now by virtue of the coupling of the lorenz systems is it almost like once you start to have something and you're going to relate them as soon as things relate there's a system because there's a relational quality to it whereas previously it could be a mathematical for formalism it was still described as lorenz system so i wouldn't get too hung up on that word but yes systems have multiple interacting parts okay figure eight just like seven revisited the format of three eight is going to revisit four and now we're going to get to where we start to see the partitioning figure eight's gonna revisit four so remember the first partial derivative the second partial derivative in the jacobian and the hessian respectively and then the covariance so we're gonna do that same thing that we did on a three by three but now we're gonna do it on a six by six the log jacobian looks a lot like this the non-zero cells in the specification of the system which is the system itself for systems that we get to specify it looks a lot like this but again for territories we don't get to specify the map looking like this does not say something specific about the territory but it's not super surprising that um we actually see like similar this top right has like no correlation like one and three are not correlated and we see that little chip missing here and here so we see a lot of the same dynamics at play within each of the coupled lorenz systems but also we've introduced a coupling now that coupling induces a new and different covariant structure look at these gray squares in the off diagonals now one and two of system a are coupled to one and two of system b less so a little bit grayer than the one that are within each system but still very marked bi-directional synchrony is being induced the interesting piece comes into play when we look at this second order approximation and that's where we see that several cells can be identified so here we have like the magenta mute please stephen thank you the magenta and the red are those first states of system a and b those are the ones that have the coupling on the fourth okay then you have the other two states of system a in teal and the other two states of system b in dark blue so it's almost like we have four kinds of states four kinds of states out of these six of course you could say well each each state is unique great totally true they're different states that's why we have them as different dimensions in our model but there's going to be some interesting things that fall out as a function of us separating out the two systems and then separating out parts of the system that interface internally and externally and then parts of the system that do not directly by virtue of how we know we specified the system do not couple out again the fascinating thing is that like state two of system a and state two of system b have a covariance that's like maybe a mirror neuron again using a more like realistic mapping it's like there's some internal part that is only interacting with other internal parts by virtue of how the system is defined and yet these two parts in each system which are not directly interacting by virtue of how we set up a system have covariance so that is what leads us to this numerical analysis which kind of provides a little bit stronger and different empirical evidence so it's not just an analytical formulation it's um a numerical simulation where we can see that when we actually let that system play out that even when states start very correlated like we just start the third and the sixth state in a very similar position so they start out with a high partial correlation coefficient when we run out that simulation through time it ends up trending towards zero partial correlation so we're recapitulating the correlation patterns of the system even when it starts in a more correlated place and so that suggests that some very essential aspects of the system are being isolated and tractably handled by this kind of partitioning of coupled systems so it's pretty interesting now we've seen this top oh dean yes and then we'll just grow quickly danny what it also says is that hype hyperbolic is also existing in this pattern although it's not necessarily being materialized in these diagrams it's it's also there i mean what that's what this explains is that there can be continuity there can be alignment and there can be the hyperbolic function within all at the same time again that's i just wanna again i just want to point to the non-obvious but it's still there great so we've seen this top of steven yes yeah it's also going to add that so by showing the nature of the coupling like varying the nature of the coupling that you showed it's sort of reached it can reach a different sort of state non-equilibrium steady state in some ways would that be true there's a different pattern emerges as so it could speak maybe i'm over extending this but changing the nature of how things couple could in a way change the sort of states which things can fall into so use you could almost have multiple states being inferred based on how the coupling is set up oh yes i mean we would throw out any model that treated two people tied to each other as simply moving the same way as two people in different rooms it is the coupling of systems that influence their behavior and again we're not talking about the causal structure of the world territory we're talking about the models that we got to specify and how changing the coupling pattern changes their behavior dean yeah and stephen when you're taking the abstraction here off the page so you could set up your programming for example that says okay i'm willing to sponsor somebody because i want to mentor them in bricklaying you can start with that or you can start with there's two people that are interested in brickling but the first thing we have to address is whether there's a potential for positive entanglement that in real terms that's how you can flip the the abstraction into the the likelihood of those entanglements turning out extending because the people will actually be relatable to one another so that i'm just again i'll bring that up because that's how we turn the abstraction into the so what does that look like okay stephen something quick on this otherwise we'll get to 10. oh yeah just very quickly yeah so that that speaks to because we often think about the the the model on the inside the generative model and the external states and you know the model trying to understand what the external states are doing but this speaks to the ability to to change the way the blanket is coupling so that you know how much work is going on in terms of enabling there to be information variational free energy information being used in the generative models how much is being done by actually changing coupling dynamics in the blanket is that make is even feasible that you know because that in itself will start to yield more and more ways for the generative model to start to extract and make inferences based on what's going on externally so that that also helps i think so as a heuristic fat pump yes great but actually the blanket is downstream of the coupling pattern so it's like changing the coupling of a blanket is a little bit of a cart before the horse because there is the coupling of the system and that is what induces partitionings which we can then map to markovian assumptions so it's like you know when you put on headphones and you're listening to this yes a different blanket is in play because the partitioning is different because the coupling is different between other states in the world dean on this and then let's go to 10. yeah i'll be real quick so like purses at charles sanders versus abduction doesn't guarantee that when you walk in later and see beans in one hand and a bag of beans in the other that you'll get to okay i just lost dean so i'm going to continue so in figure 10 but he was talking about abduction and c.s pierce kind of that flipping that i understand you're right it is the card ahead of the horse and sometimes that helps not saying it's you and i know you're you're agreeing with me but all i'm saying is we shouldn't just assume that it only goes one way horse first card second yes and in fact partitioning differ imagining differences ways it could be different absolutely this is how we are going to apply and ask what is if for active inference like what is is like what is the blanket states given a coupling pattern well there's an answer sensory states are the ones with incoming statistical dependencies and active states are the ones with outgoing statistical dependencies those are the blanket states what if the coupling were different then what would the blanket be so yeah it's not that that first blanket that you identified is being changed what's being changed is the coupling which induces a different kind of covariance amongst the states but it can be almost thought of in a cart before the horse way so just to recap this nomenclature which we've seen before so we have external states sensory states active states and internal states b is the set of blanket states so this is one of the innovations of fristen at all beyond pearl 1988 beyond markov at all earlier earlier earlier is that blanket states which in the non-fep active world are the set of states upon which conditionally internal and external states are independent those blanket states again we're going to partition them into incoming and outgoing statistical dependencies sense and action now we have autonomous states alpha which is the set of active states and internal states autonomous states are the ones that for systems that we design we get to control we get to control our internal states we get to control our actions we don't get to control directly the sensory input you can close your eyes so that it's darker or you can move your head so it looks differently and maybe you'll even be right about what sensory information comes in then but you can't just say i want different sensory information coming in so those are the autonomous states and then there's the particular states the particular states are the whole particle that's the blanket states which are sense in action and internal states so the particular states are like the whole particle floating around in the room or the whole cell or whatever again to give physical interpretations and then the autonomous ones are like the things that the cell can control or the thing that the system designer can control so we have autonomous states which composed of internal and action states blanket states action and sense particular all three on the right side and these are all being defined as a partitioning a cleaving apart from external states as modeled not of the territory of the map only stephen and as we uh mentioned here this this this paper gives a way for this to emerge from the very most basic principles and then we have like you described the the blanket states your autonomous states but of course in um any sort of system organism non-equilibrium steady state they're always a nested so it's not like the the the there will be a whole nested levels of markovian blankets so these when we emerge at higher levels we might be emerging it's not like unless we made our blanket there'd be no blanket there's going to be blankets all the way up and down so as a blanket starts to be formed or brought into play then the the nature of how that um those blankets come into play um starts to draw in the idea of coupling you know so maybe there's there's some of those things that you just mentioned there about um there's not a dependency the fact that we're creating something from first principles doesn't mean in the real world without the first principles we can't make a blanket because in in other cases it's going to be sort of emerging through more coupling between blankets because i noticed we're talking about the blanket but it's also good about the blanket in this scale and then the blankets in plural at the multi-scale okay yeah so within internal states again these are statistical dependencies so this one can have other things happening it's like a black box within each one and so that is um for people who expect to see the mechanics of the system that's a disappointment but for people who want to model real systems including machines it's a huge advantage because it allows us to go in keeping the map territory distinction totally apparent which is the real transparency we need from our machine learning models you know there's not going to be a transparent explanation for how 500 variables interact but we can transparently see potentially how the map and the territory are distinguished now that's where we get to their definition of markov blankets so let others read through and work through the math and critique it and build on it but we're talking about boundaries based upon sub matrices of the total state space of the system so like this is just two coupled systems that we're talking about here where we were studying like system a and system b each is lorenz we we added the coupling to one and four and that's where we saw a new pattern arising why does a particular partition comprise four sets of states is it because tetrahedra are the minimum polyhedra in our world in other words why does a particular partition consider two markov boundaries the reason is that the particular partition again particular states blanket and internal the particular partition is the minimal partition that allows for directed coupling with blanket states sensory states can influence internal states incoming dependencies and active states can influence external states outgoing dependencies without destroying the conditional independencies of the particular partition as shown in the upper figure upper panel of figure 10. so very cool how we went from one coupled system or from one system with chaotic dynamics made an approximation and then showed that it had certain well-behaved aspects and then we took like the next step up which is to couple systems in a way that we designed and that gave a new manifold that the systems were acting entangled as if within and then here's where we get that very fun claim that there's no claim either the original lorenz system or coupled lorenz system possesses a markov blanket we're not describing the territory i don't even think we're even describing the first level of the map the claim here is that there exists a laplacian approximation to these kinds of systems that in virtue of the zero elements of the hessian feature markov blankets so this is one reason why you know while also acknowledging that there's many perspectives and so much to learn that we do want to be precise with our language because it's easy enough to say that the system has a markov blanket or there is a markov blanket in the system or all these other ways which prepositions and nouns might be combined and those might be so misleading it could be truly incredible so it's a word of warning because as we figure out exactly what the technical details are and continue to clarify here it will matter which natural language words we use to describe it and it really does matter dean so let me add to that daniel so on an abstracted level uh we can we can frame the problem as as in or out and then when we translate that into the physical space we know when something is entangled or when something is different but i think what this allows us for on the abstract level is to examine to sample the markov blanket as in and out it gives us the potential for a second way of looking at how those two things work together the markup blanket has both a divider and an enveloper do you agree so so on an abstracted level it's very important to be precise but if we include this we have to accept the fact that on an abstracted level in and out must be part of our our repertoire not just in or out because one is more precise what is active inference while the other allows for what if active inference the and would you agree i think i agree i'm just imagining that it could be possible to design a system where there was one coupling going out but there was no coupling back in so one could devise edge case matrices that have various kinds of properties would those even be good maps of systems that we care about probably not so for some of the basic attributes like of communication of course if one person's not wearing their headphones like yeah the systems are coupled one person's talking and the other person can hear them but not return any information right so it's like that's interesting it's a type of coupling that systems can have and it has a different blanket because of that but especially for systems where we want to think about states that are influencing the environment and vice versa whether it's communication or stigmargy or coupling or collective behavior that whole class of systems it's extremely important to have of course in and out and more and less and different i wouldn't drop i wouldn't drop or sorry steven i just got to get this in i wouldn't drop or i'm i'm actually say suggesting that we we this tells us we must also contemplate and so that's like saying okay so which is more important that the positive ions are the or the negative ions they're going to congregate and we can tell the difference but we shouldn't apply a value all the time there are times when we can just see the difference and accept that for what it for what it is as opposed to always jumping to the idea that something has to be more or less that the positive ions are more important than the negative ones maybe they are but we shouldn't jump to that right away done i'm done okay closing thought on this steven and then we're going to get to free energy principle yeah um so yeah just just mention a little bit of what dean said there like the the yeah if something's a charged particle how much is the system of the molecule showing the distribution of charge and how much is it the quality of the thickness of that thing right so i suppose and and that that becomes i don't think there's a definite transition and and i'm glad you went back to this diagram this is the one i just want to mention there then just to that the arrows there on the right so where century states um it shows that the sensory states are related to the same external states as the action states are affecting so it shows it's almost like there's a there's a regime of attention um so it's not the century states aren't reading something coming in about something which is completely unrelated to you know it's not like i'm looking i'm acting in front of me but actually my eyes in the back of my head right so my sensory there's there's a there's an alignment and and then when i look to the the left this this coupling that's going on then between in in the way that it's set up between this it's saying it's it's it's um it's not a markov blanket when it's saying it's a coupled system so it's sort of saying in a way that this more slightly more nascent version there's this kind of um there's still a spookiness about it i suppose we might say it's is that this sort of a it's not it's not it doesn't drop out as far as a markov blanket as there's that kind of resonation going on and i suppose somewhere between left and right you start to become you you meet the the conditions of a mark of blanket and i suppose you know that's an interesting point i don't want to just make the water murky with too many terminology we haven't brought attention in it's not related to what we are discussing here but i i see what you mean that yes if actions were influencing state a you know your actions were moving something up or down but you were observing the left right yes that's a different kind of system than if you're observing and acting on the up down movement of something that's not a regime of attention but i know what you mean let's get to the fep okay finally remember that edges reflect statistical relationships which is the other side of the coin of saying that the absence of an edge reflects a conditional independence so one can provide a definition of the conditional density over external states so probability density that's why it was so important to go from dynamics to densities in section two and we're going to think about that external density probability density conditional statistical density as being parameterized by the conditional expectations of internal states given external states here's the formalism see how the coupled systems have a flow where like given where you were on one even if it were a little noisier as long as it looked like an oval as long as you were like you'd know that if you were low on the left you were going to be low on the up and down if you were further to the right you'd be higher up on the y-axis okay but imagine if the flow was like in a circle you'd say okay we're at x on the zero well you really don't know because you you could be low you could be high or imagine if it was just like a scatter cloud no matter where you were you would like it was everywhere in the box you wouldn't really know but because there's a manifold then this admits the possibility of a which means like sort of like function-like and stretchable map between the sufficient statistics of the respective densities not the territory not the first map i don't even know if we're at the second map anymore we're talking about sufficient statistics of densities of approximations okay i think it's a death blow for realism personally but i would love to hear somebody who thinks it's not the existence of this mapping rests upon a continuously differentiable and invertible map which is linear under laplace approximation there's a few technical pieces there but it's almost like we set up the system in a way that was defined only by its coupling but then we also got to choose our approximation approach and it turns out that with a system with interesting coupling dynamics enough to befuddle almost every other kind of model out there the approximation has a really nice relationship that recapitulates aspects of the system and lets us map like if we're on low on here we can go to low on here and low and here to low on here like it's a kind of it's a function that can go both ways and so that is going to be the relationship sigma mapping between the internal and the external states which was explored more in the bayesian mechanics paper number 26. this means the autonomous flow can be expressed as a gradient flow on a free energy functional of the variational density here are those four states four kinds of states external sense action internal and those are going to be expressed as a tuple it has to be because it has to be all four of those states co-evolving and those have a gradient flow gamma and then the upside down triangle plus the solenoidal flow so remember way back when when we saw that we had q solenoidal g gamma for gradients and then lambda for housekeeping okay so now that is coming back into play the solenoidal is still a single term that's just the potent the potential function that self-supprisal and one solenoidal term then we have a gradient over these four states and then the lambda housekeeping that can be rewritten and now pi again let let the ontology develop so that we can resolve some of this but notice how particular states pi are the blanket and the internal okay but how have we usually used pi policy action selection so this is a different pie the free energy functional on particular states particular states let's call them equals the self-supprisal on the particular states plus a divergence term that is the form that then lends the ability to interpret that free energy functional on particular states to be rewritten in several ways including energy minus entropy that's a very classical chemical way to talk about free energy to be phrased as the divergence between the q distribution on external states and the p distribution on external states conditioned on particular states plus the self-supprisal of those particular states or the one that's closest to the bayesian information criterion bayesian modeling world but still being shown as equivalent to is the accuracy minus model complexity and the evidence lower bound so that's where we we've seen these formalisms come out many many times but we're approaching it from a very different way and a lot more bottom up and i think but again would be open to people correcting in a way that fully embraces the map territory distinction and makes no bones about it whereas if you just pop in from the top with these formalisms it might seem like they're describing aspects of the system but i hope that the way we've approached this over the last like six hours of live streams literally and the way that the authors wrote it so carefully makes it clear we're talking about these formalisms on probably not even the first map and maybe the second map or summary statistics thereof these are relationships that are intrinsic to our approach of map making because we did the laplacian approximation et cetera et cetera et cetera stephen d yeah this is very helpful um i might just ask we go and walk through the very first sentence again in a second just because i think it'd be helpful just to reiterate but from what you're saying and i i hear this really clearly is the the particular states and and the thinness of maintaining particular states potentially or of whatever those particular states persist effectively then is the same when at a higher order level over time and space as a policy because let's not bring policy on picture yet we just haven't gotten there but particularly okay i won't i won't jump the gun on that but um i i suppose the the the point is just to say that we we can still be in the realm of thingness okay we can still be in the realm of thingness because we're talking about particular states of things so we haven't got and i won't say beyond that and then maybe just just to go through the ontology not just of that first sentence just to reiterate just to get it clearer again of autonomous flow expected internal state could you just recap that just to just unpack that a little bit more just so it's a little bit clearer okay so the autonomous flow so autonomous states are actions which are not we're not even bringing policy selection in so it's the right direction we want to go we want to be talking about internal states that are actually doing planning as inference that affect their action states but for now we're just treating the action states as outgoing statistical dependencies and then we're just we're pausing on how the actions are selected we're just because in the lorenz system there's no policy selection but there are external states that are modified by states those are the active states so it's almost like we can talk about action without policy inference so the autonomous states the autonomous flow at the expected internal state so these are expectations of autonomous states action outgoing and internal states can be expressed as a gradient flow so now it's like we're going to um rather than have a six state system we're going to rewrite as four remember we pulled out four kinds of notes and now we're going to express a flow not in a six-dimensional space where each dimension was a measurement that we were making but express a flow over the four kinds of states and then that means that even if there was a bunch of internal nodes that were communicating to each other we could still use this four-fold breakdown william blake hashtag bucky hashtag fristen no matter how many specifics were inside the internal or how many blanket states there were like how many nodes in the graph or how many external states because now it's like a flow over kinds of nodes and then that because the nodes are defined by their directionality of relationship like how they influence each other right like internal and external we know don't directly influence each other they're conditionally independent based upon blanket states etc that allows us to write formalisms that are like evergreen and can be expressed as flow dean yeah i would just say that when i when i first encountered these kind of formalisms i always ask so and then what happened and that's when i coined the phrase when in doubt zoom in zoom out but i didn't want that zooming in and zooming out to be collapsed just to the optical zoom i assumed that your legs wouldn't become paralyzed when you picked up a telescope or a microscope that you would still zoom in and zoom out by moving right so the inference is in doubt so how do you overcome that you become more active but not just with your eyes you actually pick yourself up and move yourself around a little bit more a la you don't just have to be on on the on the north end of the carousel sometimes you can pick up a ladder and look down on it as well and that will give you a whole new set of data whereas more complementary data a different collection and so that's why i think it's really really good to be able to see that and understand what it implies it basically says if you're if you're if you're not sure act some more how could i disagree with acting first serve let's just look at this slide and then stephen so this functional the flow over the four kinds of states that is using this blanket partitioning paradigm can be expressed in several forms expected energy e big e is expectation expected energy minus the entropy of the variational distribution remember the q variational distributions like the one that we get to control q of mu that's the q of internal states which is equivalent to the self information fancy i plus the kl divergence between the variational and conditional density that's this kl divergence term which is always greater than zero so self-supprisal is always zero or greater you can't have negative surprisal um and then the divergence is always going to be positive as well this can be decomposed into accuracy and complexity so that's this third framing and in that setting negative free energy becomes the evidence lower bound or elbow which will be familiar to many people with machine learning backgrounds okay amazing so those are the three different formalisms and how they fell out this is the basis of the free energy principle wait what's the free energy principle put simply it means that the expected internal states of a particular partition at non-equilibrium steady state can be cast as encoding conditional or bayesian beliefs about external states that's the free energy principle in this paper not the first not the last way it'll be defined and not the only but that's how they discuss it here and basically we moved from early on it's like tying our shoe with a density to dynamics side and now we have this whole universe of formalisms that connect free energy-looking formalisms like chemistry with information theory formalisms so thermochemistry with energy and entropy with thermal information divergence and self-information to information and inference with accuracy minus complexity as a modeling heuristic so that's big free energy is big and then they immediately go to talk about physiological perspectives and how that surprisal function that constitutes the potential is log model evidence so if you're getting good evidence for your physiological model you're spending a lot of times in your preferred and expected physiological states and that can be seen as simply a statement of homeostasis where outgoing statistical dependencies i.e active states maintain anteroceptive signals i.e incoming state sensory states within characteristic physiological ranges you minimize your surprise about that until you don't and you dissipate stephen amazing this this really ties into the active infant's paradigm shift because okay traditionally if we're going to have represented like traditionally representationalism for instance like you were saying it does really really throw that out in a way because if i have 100 cameras normally and i took 100 different cameras all taking a photo it's going to make it 100 times more blurry in in that sense however in this sense a hundred sources in this noisy dynamical gives me a hundred ways to integrate free energy reduction and even actually the noise of me moving it 100 ways by 100 cameras gives me more and more informational thermodynamic ways in so that that just shifts very much the way we might think about how to know about how to act in the world cool um let's continue on and we'll probably do a few minutes after the hour to get to these last few points so we've been talking about different ways that we can talk about the free energy functional of the particular states 5.1 is where this idea of the internal states as a generative model and self-evidencing or like the minimization of self-surprise is really leaned into so the alternative and deflationary perspective rests on noting that the free energy gradients are also the gradients of self-information so it's like the free energy itself is a wind that's difficult to latch on to so the free energy bowl the one where you'd be doing strictly as well as you could if you were on the bottom of it we're not on that bowl but the one that we can be keenly aware of is our self-supprisal so if the self-supprisal gradient is aligned with the free energy gradients then reducing our uncertainty about sensory states incoming helps us on the way towards minimizing free energy gradients reducing that difference the organism wants to be minimally surprised about expected and preferred observations and those are the perceptions or measurements more technically it wants to be minimally surprised about expected states and then preferences are going to play a key role when we talk about active systems that do resist dissipation this is where the moth to the flame comes and i just i love this plot arc from 2006 ice queen fristen with the winged snowflake and then you know things are heating up for act dimp others are being drawn like a chaotic moth to the flame they come close but then it burns and then they fly away so one could imagine these kinds of systems to be like stochastic chaos and that could be now again we're thinking about previously the lorenz system was like maybe a particle in a chaotic weather system so that was like the weather systems that lorenz was studying but now we're going to be thinking about particular states like a particular moth that's able to not just have outgoing statistical dependencies but to do policy selection so this is where we actually get adaptive active inference and again you know if there's two kinds of moths some of them are just particles and then they get burnt when they get burnt and they don't when they don't some will survive and some won't but the ones that do enact effective policy selection are going to be persistent even a day later so then the lap adopter have been around for millions of years so is it really any surprise that the moths have adaptive behavioral systems but that when the niche changes like with electric nighttime lighting that that doesn't entail that it's going to instantly be the adaptive system for that new niche we can think about the motion of a moth attracted towards a flame but constantly thwarted by turbulent or solenoidal air currents because active states influence sensory states and possibly external states this would look as if the particle the moth was trying to attain its most likely state in the face of random fluctuations and solenoidal dynamics so it's like there's those arrows the irrotational component are like converging the moth to the flame but then like overshoot and the windy room are blowing it around and so again there's the moth in the room and we could look at it in the x y and the z and if the candle were in the center of the room it would be like a bowl that the moth were converging towards for any two dimensions that you looked at and indeed all three dimensions when considered together and the movement of that particle the dynamics of that particle would be related to something about the flow of the air in that room with a pullback attractor this perspective emphasizes the active part of self-evidencing sometimes referred to as active inference so we were calculating the potential the self-supprisal the outgoing dependencies for particles before we talked about policy selection and that is showing mere active inference as it's called but then when you think about the real world where only the anti-dissipative systems persist and the successful ones even more so that's where policy selection comes into play and that's actually where this paper ends they basically just bring it to that point and say all systems are going to be able to be framed within this way again not the territories all of the approximations that we make will have these attributes and then we can imagine that there are some systems where the internal states are not just acting as if they're doing conditional expectations on external states but then they take those conditional expectations like about the weather and then there's another node for policy selection like the other pi policy selection in the models we've seen and then that pie can be like put on the jacket or not those are like the two affordances or those are the two states in the affordance variable and then in a dissipative world you're going to see adaptive agents as the ones that are able to recurrently adapt to changing and chaotic circumstance to repeatedly come back to that candle and so just to summarize then we'll just um if there's any implications we want to touch but we'll summarize um they constructed a laplacian system that's quadratic aka bowl like in its potential function and flow operators state dependent flow operators by the way that includes the housekeeping function that evinces so provides evidence for stochastic chaos because it has that leopardive exponent that was very similar to the ones that were calculated from the non-laplace assumptions or approximations the way that the polynomial approximation played out with the first and second order approximations gave a sparsity which can be interpreted as conditional independencies and a partitioning into four kinds of states coupled systems have a conditional synchronization map in this example it had a linear form they were on that manifold that was like y equals x and this is important because in these laplacian coupled systems the conditional synchronization map is necessarily diffiomorphic again would be awesome if somebody wanted to come on and like help us see what the technical implications of the diffiomorphism is but the simplest level is like it doesn't bend back on itself so it's a good function so that means for every sensory state there's a unique expected internal state and conditional density over external states so it'd be like yeah when the thermometer says like 20 it could be 20 or 50. that's not a very helpful thermometer now if you say the thermometer always reads half of the temperature or it's always double or it's a little noisy those are all workable but like the thermometer could just totally be deceiving you and then it just is a it's a wacky world with what the thermometer says not going to be a very adaptive system the resulting formulation can be read as bayesian mechanics in which internal states on average parameterize belief of a bayesian sort about external states just like these equations when applied to particles and statistical distributions were statistical mechanics we're taking the full bayesian insight and reading these equivalencies and formalisms as a bayesian mechanics so that's the summary of 5-2 and then there's a few implications that they get into but we'll first have any questions so dean and then stephen well i just think it's fascinating that it doesn't matter whether we're talking moths or or people we're kind of blissfully unaware of the statistical uh manifold floating around us prior to us being pulled back to that flame and either becoming the hero moth or the fool moth we don't discover that until afterwards but if we could be aware of the kind of the statistical distribution in play prior to us being pulled it might help i don't think it don't i don't think it determines much but it certainly enlightens as opposed to being lit up yeah in the moth example i think well you know but if it really reaches the flame it dies so maybe that's not even the final perfect example for how attractors work that'd be like a a bowl with a hole at the bottom so it again it just it's it's a heuristic it helps us think about the kinds of systems that we're talking about but um realism interpretation asides even that example doesn't capture the whole thing because it's a metaphor because it's like map of map of territory so it's totally chill stephen yeah this uh the evincing or evan says stochastic chaos which wasn't a word i'd heard of before to be honest um is this idea of equality um so that that's really useful that how it gives a sense of how um both there can be chaos in the attractor and the noise maintained at least at some level throughout because the um in most cases the noise is just um averaged away in in other scenarios so this is this is this this piece where we tapping into that um quality is really interesting and then that then tying into the bayesian mechanics all the way down um is uh very very useful especially when i think the bayesian mechanics as i mentioned before is stacked it's like it's not saying this is where everything is different to evidence in it's like like we're going to give better evidence which or better evidence which make us think we're going to show a more accurate lorenz attractor solution no it's evinces and we can stack up and add these fuzzy qualities together and we'll still have a fuzzy quality but maybe we'll have one which is a little bit more practical and that's the game in town here is practicality so i like that who also um it does average out noise the internal states on average the expectation parameterizes beliefs and so that's going to be a major challenge is to go from these expectation and mean field approximations without going into ergodicity that is literally the whole challenge the laplacian is like a quadratic that's fit on another distribution which we're not even caring to learn the form of and so we could say that the expectation the single number expectation the expectation of a child in that class is five feet tall but maybe it's all four and six feet but if we fit a laplacian on that it's gonna be at five looking like a quadratic so the map and the territory are not the same and it is a significant challenge across domains to connect some of the approximations that are double-edged swords like let's approximate over a bunch of possible futures or let's approximate this way like there's no free lunch so the internal states on average parameterized beliefs but does any given internal state parameterize beliefs evolution kind of helps us out by saying like well on average it's going to get better and better that's like the fundamental theorem of natural selection ftns and then the ones that don't do as well they die so evolution in a way helps us get out of this but the formalism doesn't have that as we saw it right now yeah stephen on that that's a really good point and and so it also does tie into the so it gives away to sort of the word average but yeah to average or approximate these faster perturbations yet it still maintains perturbations which could be used at the next nested level or in other ways so it's so there's an interesting question there's an interesting thing about yeah it's reducing and um the the the overwhelming noise yes it's also not doing what traditional statistical averaging does which is basically crash everything down to you know a fixed point um it gives this ability for it still to be alive in a way absolutely like if we just took the expectation the single average point of the lorenz attractor itself it would just be one point in the middle of the saddle okay [Music] but we have something that still has a dynamical nature with a tractable approximation and the dynamics have a positively open of exponent so we didn't squash the system down to a point or a racetrack we kept some of the dynamical and even chaotic aspects of the system while still being able to talk about things on average but we're not talking about the average position of the particle yes even quickly on that yeah i think that's a good point and also just one thing that caught my mind is with that butterfly with the moth going to the flame is it talks about a perspective that's been taken yet and this is probably useful when people are thinking in the applied active inference this is the perspective that we're having and it's possibly more used the word perspective because we see a picture of a moth if there was no picture there we might say a description of the scenario okay so there's this perspective on the moth yet the moth isn't necessarily in this scenario taking a perspective it's doing it's it's doing its mothness yep it's definitely moth's gonna moth although the picture was not in the paper i added that one but yeah the winged snowflake was indeed in the 2006 paper so let's just recap a few of the domains of implication and then we'll close because like what a fun paper and discussion so one implication um but also sort of note of caution is that just because we took the og complex system lorenz and coupled it so two chess moves beyond some sort of simple scenario it doesn't mean that a synchronization map exists let alone a good one for any given system but one of the implications of this paper was that now we know that for a non-zero subset of dynamical systems a non-zero subset of chaotic dynamical systems and a non-zero subset of dynamical chaotic coupled systems we can get this well-behaved approximation map so not to say all but we know that the answer is now not none so that's a key implication another implication was to really start to bring in the variational free energy in terms of generalized synchrony that's what opens up to the de costa discrete state space synthesis active inference bayesian surprise optimal bayesian design intrinsic motivation info masks risk sensitive policies kl control occam's principle decision theory utility theory max ends constructal theory like this is not all in this paper so a lot more has to be done to get to some of these places especially in such a reasoned and bottom-up way another implication of the paper is that took the helmholtz decomposition in a new way and it might provide a genetic generic model for random dynamical systems importantly because we connected it so closely to bayesian generative modeling it could be possible to do less just like let's do descriptive analytics on flow to like let's have a map that we can run both ways where we're updating the parameters of our flow map with what we're observing but then we can also generate data sets we can generate counter factuals and so that is what allows sparse data sets to play into active and fep and ties us more closely to modern machine learning like variational auto encoders another implication um or next step would be like we were looking at a three-dimensional system the x y and z of the lorenz attractor but how could we approach high dimensional dynamical systems so attractors in 50 dimensions that's a little bit different and hard to solve so they suggest that a way to approach this probably hint cough cough wink wink etc is to learn the state dependent flow operator and the nests using deep neural networks so it's kind of like you have a neural network now that recognizes bulls and so we're still kind of keeping some of the same things that we had a intuitive interpretation of in lower dimensions it's just like you go like okay i know about the cube and then someone says well there's like an 11 dimensional cube and you okay i kind of know what i don't really see exactly what that means but i kind of see what is being discussed how are we going to compute on the 11-dimensional cube though well neural networks and that is actually very close to neural stochastic differential equations and i think this is super exciting one could ergo hasn't been done probably hint hint use a separate feed forward neural network to parameterize different components of the flow operator and self-information so in other words we're taking what we talked about here and we can think about combining it with the types of stochastic differential equations neural stochastic differential equations and bayesian neural networks all of these uh advances in machine learning and now we're actually coming back to the table with new architectures for deep learning because this would be a deep learning model it would have a deep neural network you know you would benefit from having a gpu but it's an architecture that was inspired by the discussions that we had here and then just the final implications and then we'll just have a little closing um the flows across the boundary also notice that a little bit of the blanket starting to potentially be inched away from talking more about a markov boundary and connecting the fep to the constructal law and the work of professor adrian bajan who honestly is a lot like fristen in some ways and a really gracious person and communicator as well with a long career of working towards this kind of a unifying flow theory to get towards the evolution of design in nature and as the image shows like kind of like from flood plains to lung alveoli that similar kind of a unifying model and then a final set of implications is that we take covariances all the time from time series that's like how we do crypto trading that's how we do weather prediction that's all kinds of systems take covariance matrices on time series and so potentially because the hessian is recapitulating elements of the covariance matrix it could be possible to have an empirical data set and empirically take the covariance and then learn something about the hessian the curvature of the underlying flow states so in other words if you use the laplacian approximation even if it's stochastic chaotic and dynamic that empirical covariance could identify conditional independencies where there's a zero and so in principle you could furnish a description of expected flow and the leopard of exponents to establish whether the system was chaotic or not because we found that the hessian approximation gave us a leopard if estimate that was like the actual leopardive exponent in the lorenz system so we could go from empirical covariances which people do literally every day cov open parentheses and r empirical covariances two estimated hessians two estimates of things like the lyapuniv exponents and that's why people often study chaotic systems in the super toy models like the double pendulum and lorenz because it turns out it's shockingly hard to find chaotic systems in nature outside of those toy examples because the real noise in the system swamps your signal and so in conclusion having a simple functional form for the flow of random dynamical systems may be useful for modeling and analyzing time series generated by real-world processes that are far from equilibrium and may or may not be chaotic and so that's the closing question human flow physical flow supply chains information flow narrative flow semantic flow ants bringing seeds in and taking the mid and out all of these flows we have data sets for them and now we have a new kind of model that we can use to model those flows so we'll just chill and talk for a few more minutes but what a great discussion so thanks a lot dean and steven and everybody else who participated in 32. so stephen first deep breath yeah i i i there was a couple of things there that seems to have a big impact um because as well as markovian monism and the idea that active inference brings in this um this this this partitioning and the the active inference field um which can then also have relationships to deep learning and how predictive processes thought about what's being said here that there's also evincing or evidence the idea of the evidence and statistic chaos itself also been a paradigm a paradigmatic uh part of evidence in being arrived being being arrived at so which in some ways is not dependent purely on markovian monism being the the the overarching game in town so that is quite a um that's quite an interesting additional way for deep learning to be um to be impacted by is this a field of practice is it a domain i don't know what we call it but the that that does it does it does make everything does it doesn't mean it's all markovian monism in terms of the ability to then infer you've also got this other um ability to get information and from what you're saying it almost as much by inferring if there is chaotic presence or if there's not maybe not having maybe going and seeing if there's not chaos present may give an idea of there's maybe no correlations present and that's that's to jump in the gun but you know maybe there's other ways of being able to interrogate statistical dependencies in time series analysis that can be sitting alongside the markovian monism so how that's connected to mycovian monism is definitely a good question but yes these are general tools for complex systems analysis so i hope people don't read this paper as like fristen apologetics or some sort of fringe development in act imp only it's pretty clear from the nouns that we used that this is actually like a fundamental contribution towards analyzing dynamical stochastic chaotic systems however just because you find for example a lowly operative exponent of your approximation it's not a statement about the system so we will always be able to just halt right there you know it's not that it's not a chaotic system your approximation doesn't have a chaotic leading lyapunov exponent so say what you want to say about your map now if your map isn't chaotic and you're doing a great job predicting explaining controlling designing creating wonderful i hope the business is successful and that you can you know have friendship and health and all those things but that's not a claim about the territory and so there's going to be so many fun ways to talk and explore that dean what are your closing thoughts just real quick i think what the co-variation piece which is basically statistics and an abstraction to to sort of explain things points out is that if we can understand the world as being both within enveloped and between partitioned at the same time that's a good fundamental place to start and i completely agree with your last statement we're not we're not counting fairies on pins and then just leaving it there we're actually hoping at some point to be able to turn this into something useful and functional but if you can't if you can't start from a place of seeing it as two things at once within in between you're going to struggle you're going to tend to collapse prematurely and you're not going to get very creative and active inference will well you'll be able to define it and describe it but i'm not sure how how much new information you'll be able to derive with it unless you can hold up both at once it's within in between great there's so much more that we all have to learn the appendices the formalisms we didn't go into the citations other perspectives you know it's just the beginning and that's what dot two is about like how far we've come you know when we can go back to our road map and just be like wait dynamical systems and densities and then we're you know 300 miles past there but where have we gotten and so it's like just really exciting and i'm glad that we're doing it in a participatory way because there's a lot to it so dean and steven thank you again everybody watching live and in replay hope that you get involved and stay involved with actin flap see you later bye you