all right welcome back cohort 6 we're in our first discussion on chapter 5 so let's jump over there um where does anyone want to begin could be like any page or figure or quote of five that they remembered or that they're curious about so first we'll just see any thoughts or ideas people have about five otherwise we can look to the chapter and the prior questions sure I I thought that um this was a cool chapter because it for the first time I think kind of gets a lot of the message passing mapped on to um specific biological systems that we actually have quite a bit of prior knowledge about um so I thought that was pretty neat like to actually see the sort of detailed example of the Pathways in the basil ganglia and that kind of thing I thought that was great yeah it's go yeah I was taking a look at that myself I don't know if we could go through that a little bit because I was just having some trouble with the path tracing it might have just been the model itself and the way that I was misreading it but uh uh figure 51 and 52 yeah 5 five is like the big overview abstract it's gonna it covers the prefrontal cortex graph the dopaminergic the basil ganglia graph and the spinal reflex arc graph and then earlier in the chapter so now we'll go back to 51 and so on earlier it's going to go through those examples and then those examples have a composition it internally that's the graph and then this is kind of the cool thing in the interoperability is that because of that compositionality internally you can kind of do wiring across graphs so that also speaks to the compositionality um of the models and I think maybe one theme that we'll try to draw out and and explore and see limitations of is like there's a massive underlying hypothesis of computational Neuroscience which is like computational models can map biological territories so there will be like some regions or some actual tissues that either do that's more of the realist angle or can be modeled as doing that's more of the instrumentalist angle they can be modeled as doing certain computations and if that's a viable hypothesis then the maps that we make with the math that we have are going to be very apt if it's an inviable hypothesis it could be extremely misleading because there could be a tissue and potentially the toolkit of mathematical operators just isn't adequate to make a good map of what that tissue does leading to like a false confidence about um the understanding of of of what it's doing in the body or like its role in development Evolution all of that so so it's kind of like it but that's so sublimated into the field decades on that it's like of course we're putting mathematical symbols on top of pictures of tissues but yet that's actually like that's kind of the invisible elephant in the room is like what what has happened here applying these mathematical models the graphical models and like using them as overlays to make sense of the functions of different tissues yeah that's that's really interesting I'd Wonder um so perhaps kind of like in addition to that one of the things that I had heard a few neuroscientists raise is the question of whether the um whether the kind of known connectivity maps in the brain suit themselves to this kind of explanation as well um oh I'm sorry Andrew you have your hand raised would you like to would you like to go oh yeah uh thanks um yeah no it was just a quick comment following from kind of what Daniel is describing this sort of like mapping the physiological to the to the comput or mathematical I suppose um I heard this nice explanation uh it's in a previous uh live stream with the Institute with uh Ryan Smith the computational psychiatrist and um it was just whenever I was watching it it was like my first time hearing like this kind of attempt to directly relate neural activity to to like I guess the mathematical equation so he was like breaking down computation of like variational free energy and expected free energy and like equating like basically attempting to equate like whenever a term is subtracted from another term that could be viewed as like inhibition as opposed to excitation with the like addition and uh whenever something is Multiplied in the equation we could view that is like modulation in synaptic gain so this just sort of I mean it it might be taken a little too literally to just state it as simply as that but it was just yeah super fascinating to me to think like oh like this is like really attempting to map neural computation with like you know a written symbolic mathematical formula like subtraction addition we have inhibition and excitation and then we have you know modulation with the multiplication so anyway that that was it was so it was more just a comment I can share the link to that stream if anybody would like to check it out yeah like whether we're looking at um gene expression levels of two loai we're measuring with RNA seek or something we're looking at the firing rate of two different neural populations or single neurons or like two metabolites in the cell it's like okay we're making measurements of some biological properties and then that's the kind of o to S mapping okay we have the RNA seek data that's the observation and then the hidden state is the true gene expression level so that kind of is the partially observable component so let's just say that we're in a highly um observable setting or whatever the case may be we've gone past our measurement into this underlying space of of how are these two Loi related well one option is like the mutual information between them is zero like their co-variance profile is just a scatter Cloud there's no correlation so that point at least at a first past there's not like an interesting or non noise relationship between the two variables not everything is causally connected that's the sparsity of the biological system so it's like that's okay but then if there is some kind of informational mapping then it's like okay at a first pass does one go up when the other goes up or not you know is it a positive or negative relationship is it a U-shaped relationship there's not an infinite number of possible mappings and then how complex does the math need to be to capture that mapping well that's kind of where we get into this accuracy minus complexity question is it enough to just say yeah linear aggression or is it better to do it as a um piece-wise defined function but like well it's a linear relationship up to this point and then it's flat or would it be simpler to just fit a single parameter linear regression and then still just try to capture as much variance as possible or should we go with this two parameter model with a slightly sharper slope and then a flat Bridge me those are the model comparison questions that make but the space of all the possible relationships does map pretty clearly to these operators that we have mathematically they David yeah I was just wondering if you can classify velosi as uh different nodes in that sense yeah exactly like a gene regulatory Network the the nodes would be the expression values of the Gene and then the edges would be like a causal connection different kinds of edges okay and and this also relates to the the earlier point about the connectivity maps and about different Notions and different Maps that are made also this is kind of like the genealogy of active inference with the SPM package which is exactly about well we don't only just model the anatomical structural connections that's basically known I mean largely we know where different tracts of neurons project to um now there's also like the electrical field propagation so on so for for another time but um okay so we have the structural connectivity that's something you can just slice an image and reconstruct in 3D but then what happens is the dynamic causal modeling approach says what are the causal the statistical edges and then there's a few different kinds of statistical edges you might want to just take a first pass which sections are correlated with each other and then the edges basically reflects a correlation value so it's like these two have a 08 Edge between them because they're 80% of the time correlated um so that's one kind of connectivity map another kind of connectivity map would be looking for the causal consequences of one region on another so let's just say two regions are 80% correlated but it turns out that that's because they're both Downstream of another region well in that case the two Downstream regions the 80% is kind of it's a consequence of some other [Music] causal relationships so these are like different kinds of maps that that um connect nodes in ways that aren't necessarily only the structural and that's like a kind of tension that comes up again and again where we have nodes that are structurally connected to each other like they're actually sending neural signals but for a given um pattern of Interest that may not be causally relevant and then on the other hand you have nodes that are causally relevant to each other so you want to model like a causal Edge on the base graph but they might not be directly touching through a neural contact okay so then we it gets a little um both like the simplest system slor would be like all the anatomical connections have causal efficacy and none and there's nothing else so just knowing the topology of the system as it's actually connected that is the computational architecture however it's it's generally not that way especially when making Ultra reduced simpler models so like here it's kind of a mixture of like on one hand talking about the projections of different anatomical regions but then on the other hand that's not exactly or only the edges that are described here like there's other projections that just aren't being focused on here okay I guess that relates to um there was I'm trying to find the term in the in the paper or in the book actually um with the crossover effect and I forget the name of what it was what I just saw a connection with uh interlan and crossover and there there seems to be some correlation I just don't know enough about the uh the interaction if that might be related to you know different nodes interacting at different different time periods as like things stretch out for example in the frequency parameters this is a great point this is basically as presented here it's just a model of like kind of the realtime fast neural inference but we're not seeing like the effect of glea or interlukin or slower changes let let alone even just synaptic plasticity so that's definitely a big open area is how do you incorporate this kind of runtime inference structure with like development and and and neural and immune development which change the structure um another fun um paper I'll I'll put it here could neuroscientist understand a microprocessor and this just you know shots fired in the first sentence why are people collecting all this data I mean why are there massively funded projects to sample synapses and all this stuff well it's kind of related to this belief that if we had more data we better images better satellite images of the territory then we would have better Maps um so this doesn't conclusively put that idea down but they do something very fascinating and empirical which is we know the topology of a microprocessor and there's simpler microprocessors that can be simulated in silico that's kind of meta but we have the full processor simulation being emulated at like a tremendous overhead right on a more modern processor and then they're able to take neural recordings from the microprocessor and you could look at like the activity of a transistor you could also look at like the local field potential around a region by by averaging out the the electrical activity and then you could do single and double lesion experiments and that kind of like loss of function is classic like in genetics like that's what people do for a long time it's like gain of functions and losses of function and looking at single and pairwise losses of function and then they presented that data set to neuroscientists and used their kind of general information processing algorithms and we see this kind of coming up like in integrated information Theory like they're talking about microprocessors and they're talking about neurons because both of those are being abstracted to the graphical Network information Dynamic setting and um it's not to say someone couldn't analyze this information usefully it just turns out that there are like things that are causally connected because we know the software running not to say that they software running on neurons but again it's an allery there's things where we know what computation is happening and the lesions are uninformative about it and there's it's just there's there's false positives and there's false negatives that's not to say it's uninformative experiment but it it it really calls into [Music] question a simple or kind of uncritical mapping between how things are structurally connected in the real world anatomically and then what's the causal architecture of the system this comes up in the ACT INF side like the whole marov blanket discussion our marov blankets reflecting the spatial and the temporal boundaries of things if so where Andor do marov blankets just more generically reflect boundaries in state space which might coincide with a spatial and temporal boundary but it doesn't have to there'd be boundaries in state space that would be um within what would be called a spatial or temporal boundary and and vice versa it would all depend on your your sensitivity thresholds and whether you were trying to capture 80% of the variant or 99 or 99.999 so that's what's so interesting again in this chapter 5 which is like we had the intro chapter one low road High Road two and three chapter four with all the math and getting to the actual kernal of the generative model and then now that is kind of perfect in and of itself but it hasn't really been applied to any system and then chapter 5 immediately is kind of like here's some of the amazing insights you get by applying active inference to real territories and here's some also of the fundamental challenges and limitations that that come into play when we are talking about maps and territories rather than just kind of the pure science of map making because you could just make these kind of pure um maps of just just well I'm imagining a territory that's you know there's a street map on the head of a pin but then you can't necessarily build that or measure it in the real world but you could talk about it in chapter four um yeah I guess that kind of speaks towards like some of the granularity as well in terms of like in silico versus in situ if you're doing a comparison you know going back to the transistor model you know how fine grain can you get I was just wondering if there's any correlation uh in that sense as well so like you know the smaller the transistors that get you know nanometer wise um is there any correlation there with you know Inu with um with the Dynamics of of neuron interaction yeah I think a great example of that empirically is like it's often this is like early 1900s the debate was like is the brain composed of cells that have gaps between them or the reticular theory that was the neuron Doctrine there are cells that are separated and then there's the reticular Theory which is kind of like maybe it's all like a mycelial connected um W with actual direct connections um and then that's the GGI stain and ramonica Hall and they were like no you can stain a cell and it fills out the cell and ends there so that was that was like a big W for neuron Doctrine then you know there's a little bit of complications there's some big holes and St and there's Parts where they are connected and so on but but largely the cells are separated okay so then the neuron became understood as kind of like the unit of modeling but then there's several issues there um one is that's a huge model even for a simple brain region so often we're modeling populations of neurons like these nodes represent not single neurons but populations of neurons and that's the concept of like the population encoding and those are easier to work with in terms of coar graining because you're course grading to fewer regions and also the statistics of individual neurons are very spiky and noisy whereas when you're looking at the population statistics of a pool then you can model it much more easily with like continuous distributions and like rate encoding it's more like looking at an analog signal that's smoothly turning up and down rather than like spiking but then going down a level people have found just incredible computational Dynamics in dendritic Arbors so even just choosing the neuron as the unit of analysis like you can look at the computations or whatever you want to think about it as at a sub neural level like if a neuron had two branching sets of dendrites you could look at the summation Dynamics and the Decay Dynamics at a really fine scale level and then at that point like the neuron would be kind of your higher level outcome and that's part of the multiscale um processes that are happening again with a map territory none of this is an issue it's like well if we wanted to do that um millimeter scale analysis of the sidewalk we could or we could just have the 1 kilometer map but then that's where um realist approaches to computational Neuroscience are are in a in a difficult position because it's so clear even at the single neuron level that you're corar graining and abstracting over like many levels of function right I mean I I see that and like I mean I haven't done too too much research but just from what I've read um you know in terms of like zor Gates and how an individual neuron interacts once you get a field of neurons as an array you know how can you tell what gates are opening closing and what's interacting or cascading across the entire field yeah like right now the nodes are AR just in this image the nodes are arranged to resemble the histological layers of the cortex the melan preval cortex um but all the causal relationships are topological so like technically you could like kind of rearrange the nodes on the page as long as you kept the connectivity and that wouldn't change the computational function here it would just make it look less like the layers of the cortex would you oh yeah good would you have any gradient like layer shifting so would you have you know any of the modalities shift between the layers because of that interaction or would it be like would it remain in situ and just the connections themselves would be the things that are changing that's a good question I mean if your map were purely topological then geometric changes wouldn't matter however if you did want to include like bulk propagation of electrical flows or spatial effects then that would be a different kind of model so then that which is not even represented here and then there's like different ways you could incorporate that into the model like you could give each each node could have a position in in a coordinate system and then you could have another component of the model that had like a propagation through that coordinate system and then this then this would be have why it's so important to have the portfolio of models you say okay I have the I have the 100 parameter model here that's purely topological and then I have the 109 parameter model that has this and then the 120 that has this other thing and then accuracy minus complexity um Bas information theorem we want to reward explaining the variance in the data we want to penalize having more parameters how much should we penalize having more parameters well that depends on the modeler's sensitivity are they seeking to explain a higher and higher fraction of the variance at the cost of a model increasing parameters or are they looking to actually take just a lwh hanging fruit and have a kind of sparer model accepting that it will explain less variance there's no a prior answer because there's a setting where you want 99% variance explained at all costs and then there's a setting where you want four variables and no more and whatever variance you explain is just what you're going to get with four so those are just the the pragmatic considerations of of making a given model great thanks okay so here we're in the um cortical layers um as always the disclaimer this is just the Maman nervous system plan analogous and other functions are carried out by other like diverse nervous systems so I think it'd be epic to have like a chapter 5 insect supplement especially because it's a fully observable neural system you just lay it flat and just take the picture so I think there could be Mega insect brain modeling but here's the cross-section of the neural tissue and here are those six layers here's the surface of the prefrontal cortex and then here's the columnar architecture which are these kind of hexagonally tiled columns that are the basis of of for example thousand brains by Jeff Hawkins this [Music] um this is an interesting work definitely I wonder about what they know about actim um but it has to do with um the the parallel parallelized decentralized architectures and kind of the um columnar um architecture okay Peter I'm going to look at this [Music] awesome yeah wow I mean you know these are the silent elephant in the room isn't that great yeah it's like can we really just take highlighters and like shade different parts of real tissues well this one's doing prediction okay but can can in a different cognitive setting or a different like attentional regime is it always doing predictions it's like about what so those and uh you know where are the glea there's all those slower questions so that's that's always really fun but this is this is great and it's looking at the [Music] um relationships within and across the columns yeah one thing I really like about this paper um and both these papers actually is that for some of the underlying like architectural stuff like looking at the connections between laminer gradients across the brain and that kind of thing um it depends on the track tracing work of Helen barbas um whose work is like some of the most like detailed and exacting that I've seen in the neural literature um so like if we're going to try to do this kind of mapping onto biological systems uh which seems difficult enough you know with all all the things that you you said Daniel it's at least nice to like have uh have a sort of tracing of the actual circuits that it feels like we can kind of take to the bank you know yeah like we we take the tracing to the bank as kind of a fact and then that is like our that's like a very strong very informative prior for causal relationships like all things considered where there's an actual Edge it's pretty viable to suggest that there's a causal efficacy to that edge somehow and then when there's no causal Edge there could be you know telepathy there could be like long range bulk electrical transport there could be relationships mediated through other local connections but I mean on the first pass nodes that aren't connected it makes sense to exclude those as having causal relevance for each other at of first pass so the structural is a great or if we were talking about a communic a network it's like we know which people talk to whom or which computers talk to whom that's a starting position for the causal architecture of the model so that's like that is I think an important role for the neuro anatomy in this um okay so here it's going to be focusing on one columnar slice and it's going to show it three different ways moving from on the left we have the labeled cell types so this is the most neuroanatomical where the edges are reflecting the actual projections anatomically again not all of them Etc but this is just describing Anatomy it's not even a base graph yet then in the middle we see kind of a classic predictive coding architecture where we have m's for means and then like e for errors and then we see the eyes for a given level and i+ one for a higher level so and and the X and the V subscripts so this notation which of course is resembling but it doesn't pretend to exactly recapitulate this is the predictive coding type computational map on this territory which is is you have top down predictions and then you have incoming bottom up sensory evidence for example uh which could be coming from a thalmus or or lower regions and then you have an error term that's just checking the differencing so this is going to be a lot like what we see in the spinal reflex arc but this is happening in kind of like a more cognitive setting the more general setting in predictive coding and then here on the right we get to the PDP this is the most kind of chapter 7ish generative model and although there are some letters that are a little bit outside of the figure 4.3 broadly we're talking about the hidden State inference so we kind of move from the most anatomical to the very highlevel computational predictive processing Paradigm active inference is basically predictive processing with action and then but this is in a continuous setting and then it's brought into the PDP setting So within a column whether we think about it like kind of purely anatomically whether we think of the column as doing predictive coding predictive processing or whether we think of it as doing a PDP with like it's carrying an S estimate through time and then observations are coming in and here there's kind of anticipation the O Tilda through time and expected free energy and all that PDP if we think about that within a column there's an interesting way to talk about the relationships among columns which is that they're arranged side by side in the cortex and each one of the columns is like one layer of a hierarchical basian model and then you have lateral connections which that was a a great paper um Peter that showed these kinds of lateral connections go between the layers and then also just like mentioned like where we see summation like excitation empirically we can associate that with addition where we see inhibition we could talk about that as subtraction of like a a likelihood or like a rate constant and then when we see multiplication which is kind of the same as division just whether you do like one divided by multiplication division is like a neurom modulation because it doesn't directly summate or subtract it's like a a coefficient that multiplies that so that's neurom modulation and then here is like we have like a three level nested model so the six layers perform the role computationally of one layer of a nested basian model so this is like we're we're predicting a three-digit number and we're breaking down that problem factorizing it into the ones tens and hundreds place and then here's the the top down prediction is you know what it is and then the bottom up is coming up and that's how the these three layers even though each one is only holding one digit that's how the three of them you know just super broadly would would um do a three- layer model here okay so now we're going to trace this path this is the the descending motor prediction coming out of a for example motor qual region it's going to be projecting down to um regions involved in motor control and neurons that have motor right in their name so they have to be doing it right here's the projection down now we're going to go from um Mew till the here basically the bait cells might be interesting to look at them um sending down the paramal tract and it is synops onto this essentially combining and differenc her here we have observations why coming in from proprioceptive tissues in like the elbow so there's like all these different kinds of touch and appropriate receptive receptors like some of them look like onions so when they're depressed they activate other they have different like shapes it's kind of cool that is coming in and then it is being passed along from the dorsal horn through an internal synopse within the spinal cord to the vental horn population and then there's a differ thing and this is just a first pass approximation but where the descending prediction is ex exactly matching the incoming propri reception no action is required and um that that kind of makes sense that's consistent with this differential driven approach to basically making moves um and then if you have I guess it's in a different chapter where it shows the um the set point changing and it shows how the um how the the the motor Dynamics are but this is just this is like a kind of sub motif of predictive processing but this is just a frame differenc sir and and this is also very related to um PID control you set point and then you know again two a first approximation if the temperature of the room is if you set it to 72 and it's at 72 too why would you take an action maybe you have a more sophisticated model you have a super strong belief it's about to get hot and you know it takes time to turn on the air conditioner so even though it looks perfect right now you actually want to turn on the air conditioner so that it counteracts like alost statically a future prediction but just at a first pass if you don't have that kind of allostatic anticipation if something is at the level where it's set to be no further action is taken David yeah I mean you you had mentioned also in terms of like message passing like what would the dopam energic response be would it be an initial kind of like packet you know that would be sent that would be a priority of you know the cognitive response itself before the decision is made and how might that actually you know be false signaling in some some cases if you're unaware of you know in a priority situation for example let's this is one a wonderful title and also very interesting paper and and I think there's some other ones on this um topic but Columbo investigates from like a history and philosophy of science perspective on the on the dopamine discourse the metad dopamine studies about well dopamine I mean if we look it up let's predict is it going to call dopamine a reward molecule pleasure satisfaction motivation what is dopamine does desire so it's framed in this kind of salience motivation reward reinforcement but but there's other functions for it and in the body like we were talking about before um recording like with DOP mean and in the immune system and all this but Columbo kind of looks at and he says look the empirical evidence even for the dopaminergic brain regions like we're about to get to in the basil ganglia like there's populations of neurons that do have a correlative firing pattern with reward there's also patterns that are firing more with a surprise model so there's one other Columbo paper that's like an unrelenting pluralism it's just it's like it's just very interesting like and I see as as a a way that um forward-looking philosophers of biology can actually sty me I don't know effectively or not but um like kind of absolutism or like closing the book on what dopamine does when it's like actually we already have enough evidence to say that there's a plurality of functions for dopamine so then further empirical evidence can can be brought into a plural framework where dopamine plays different computational or cognitive roles rather than the debate needing to be like what computational role does dopamine Play We Found evidence that it plays a role in prediction dopamine plays a role in prediction but if we have already kind of laid the top soil with pluralism then it's kind of like all right great more points for pluralism um more points for this function of dopamine but but it can't be resolved in in principle what dopamine does because we already know that we have some tallies across different functional categories um there's no dope mean in this um circuit yet no no Dove me directly okay I don't think I don't know exactly when neurotransmitters are here but um dopamine plays more of a neuromodulatory role than um especially in the it plays a role in the central nervous system that way but in the in the peripheral um system and then like famously at the um neuromuscular Junction so this synapse here is like where you have acetal choline okay so this is just kind of a simple um differencing algorithm but differencing algorithms are like I mean they're they're they're the heart of many things happening and also of course going to to the history I mean predictive coding predictive processing predictive compression was invented by frame differencing video compression in like the 197s and 80s and then there is the r and Ballard Mega citation 1999 paper that was like oh classical means whatever was done before right so then there was these extra classical effects in the visual cortex and then they were like wait a minute if we think of what the visual cortex of doing as protction then that explains why there are these extra classical phenomen that are not being evoked by the stimuli because they're being invoked by the predictions so then that kind of like broke I'm sure there was like a little bit before too but this big citation kind of broke the wall with just the idea that like hey even primary sensory processing is predictive so it's only been 25 years and that's the interesting thing is moving from the concept of well signal processing and sensory transduction is related to processing and Distilling and extracting the information from the real good sensory data it's so much out there and we're just getting the good stuff from that versus actually the sensory data coming in are like lagged noisy biased partial Etc however they just kind of help tune an ongoing multimodal generative mod model and that's why our visual field has color everywhere and no blind spot attention all this other stuff because we're not just like processing and sharpening the TV signal and then displaying it on the inner screen there's an inner generativity that's multiple hierarchical nested levels away from the primary sensory information but this is a this may have made sense to look at before this one because this is a simpler Motif it's kind of a core Motif is this just concept of like take the difference and then just use the sign of the difference positive or negative like at a first pass that's the direction to go I mean if there's a speed limit and you're over it go down you're under it go up unless you have like some other higher order reason to do other than what seems obvious which is right what which is very like cybernetic right yeah it's it's just like for it's like bread and butter first order cybernetics and then second order and higher order cybernetics would would kind of be like where and when and how does the first order cybernetics lead to like a death spiral um and then also could relate maybe to system one and system two and just on a first order take theistic but then to to override heris which is now this is exactly where it takes us here is a more challenging um path so here it's an interesting like visualization I think of this one is kind of like the scales of Justice in a way because it's not saying that the left and the right basil ganglia have different topology it's actually just like showing to modes of function that are like it's like a RAR Shack blot unfolded and then the two sides are like the two different modes but it's not it's not saying that it has these two different architectures on either side but basically there's two path they could the indirect and the direct pathway are both bilateral as far as I understand but they're just separating them out here so these are different brain regions um with their acronyms it's a simplification okay um in the indirect pathway observations are coming in um and then there's o Tilda o through time of a given policy and here's e The Habit this is the prior on policy and then basically those indirect the the prior on policy habit is just being passed on to what we choose so this is just kind of carrying forward our prior on what we do and then either choosing the best ranked option or sampling from that policy posterior so it's kind of like saying let's not do a policy update let's just make our policy posterior or policy prior so no F no calculation of expected free energy no G coming into play um equivalent to not paying attention to observations to incoming observations or expected future observations so it's Mega simple and there might be an Adaptive habit that in a given environment for which that habit was developed adaptively for that's super simple on the other hand there's a direct pathway mode that they're analogizing as actually doing the expected free energy calculation G and updating the policy prior into the policy posterior according to equation 2.6 the whole part about pragmatic and epistemic value so it brings in a ton more variables and computations but it sharpens the prior into the posterior based upon the incoming observations and expected future observations so that's kind of like the deliberative system to thinking whereas this might be analogized to theistic faster system one thinking David yeah how would that affect like if you're skipping that first part would that direct to the map of like the ontological directly without then prior updating so you're kind of going without a map in that sense on The Logical side of things right that's interesting like habit is kind of like map free it's kind of like no just this is my turning distribution I I I go 50/50 left and right and I'm just sampling from that wherever I am versus like the more deliberative would be like well if I turned left then this is where I'd be if I turned right this is where I'd be so the habitual is a more implicit ontology it inherits from the the ontology that that supports that habit whereas the explicit all basically enumerates the onology and creates like a local projections that could be based could could be also leveraging information like from external Maps or other Maps Andrew oh yeah I was just um wanting to relate it to like the the authors in the textbook relate um like the the comp computation and use of the empirical prior e in policy selection as like being akin to like a model free system versus um the use of G and computation of that to inform like um policy selection and policy comparison is like a modelbased system which kind of made a little bit more like helped me track that a little bit more as someone with more of a data science rather than Neuroscience background but um yeah no it's it's super interesting I'm trying to remember another paper that I had read that you know with with with the ACT in terms of like confidence and so on there there was one paper I'll try to remember it but it Likens like an agent in its environment who has confidence in its model and also receives some kind of confirmation that its model is actually helping it to realize its preferences it's as if it becomes more confident and its um ability to use a model to compute G parentheses in order to realize its preferences right versus if it repeatedly hits like obstacles there might be a sort of regression into like oh my my confidence in my model is is very poor it relates to that um relates back to that gamma term is a kind of like Precision or or or or temperature uh term that gets involved with the G computation in any case um yeah whenever the the agent becomes less um confident in its model and its ability to actually compute uh like like useful policies it starts to kind of regress back into like habitual behaviors that it is previously relied upon and then from there I presume like you know over time you might very well end up modulating your your pro your your um have your habits as well like if we view those as like in values over over policies that are sort of ready to go like those get changed too over time but uh anyway I'm kind of rambling at this point but those are great points this has always been where where I get hung up on with a model fre which is model fre just means simpler model so you know when something's like next Generation this or or this you know none of this but then it's like it just it's hard to keep track because people are using using those as kind of like relative terminology but it's even used in chapter 10 I think it would make sense to to have mentioned model 3 in chapter 5 because it literally is mapping to this arbitration and then just in the last minutes I mean 5.1 describes work in the act in space and by others with linking the computational function oh and dopamine is playing the the um judge jury executioner role with a dopaminergic tone setting the difference between habitual and deliberative behavior but there's a lot of like there's a lot of effects of dopamine so that's why it's not so simple as just taking like you know you know yay dopamine or nay but these kinds of things some some with friston and others others just more generally [Music] um the pupil I think the icade and the pupil modeling is really interesting those are things that most people can't control consciously and they're not aware of consciously but they reveal a ton about the generative model um and then they kind of uh allude to the the the um the uh favorite synthesis topic of this textbook which is continuous and discreet hierarchies coming together like every chapter it it kind of comes up and then here we have it if we use that the rightmost form here the PDP discret time like form so we have a discret neuros symbolic model here and then down at the Frontline workers we have the continuous sensory motor form and then we have the the policy selection module is not directly linking down to the spinal cord so it's not like prefrontal basil ganglia and then sends the message down to the spinal cord it's like the prefrontal and the basil ganglia are in a dialectic and then there's a descending motor prediction that reflects multiple scales of the resolution of that Dynamic by just passing down the set point and then that is enacted autonomously with What's called the reflex arc they bring up a ton of other questions cool topics for future discussion okay thank you everyone see you later or next time have a good one thanks everyone