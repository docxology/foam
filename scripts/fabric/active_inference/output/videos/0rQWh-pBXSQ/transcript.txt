okay it's July 29th 24 and we're in the first of the applying active inference sessions for the third week of the cohort 6 and Andrew is going to give some overviews and share from his recent tutorial so go for it perfect um thanks Daniel and um as I mentioned to the the other session attendees today um I did have a bit of a full work day so not fully prep to to run through all this but um I am happy to present what I have and just kind of bear with me a little bit um I presented this tutorial um about a week and a half ago July 17th at University of Pennsylvania for the 10th International Conference on computational social science um wonderful experience doing that by the way super cool to see some other folks doing active inference research uh at that conference um and uh super great to to meet some folks there in including Brennan kleene who I believe has had some degree of affiliation with the institute in the past um and he is also a pi mdp uh package developer one of many uh but I got into contact with them while I was developing this tutorial so it's just excellent having him in the room um it seems I did their pack package some amount of Justice uh in putting together this tutorial so um the intention of this tutorial was multifold uh one uh I myself is am an internet The Institute and so I'm still learning a lot um that said I've been involved long enough to where I've been itching to uh learn much more about coding uh more directly as opposed to reading the the text book inside and out as I have been for for at least a year now I've been involved with the textbook group in facilitating meetings um so so a learning experience for me um a teaching experience in the sense of presenting it at a conference as well as hopefully developing it as a resource for uh other folks at The Institute who might be coming at active inference from you know maybe an angle that they don't have a lot of Prior experience with say reinforcement learning the logic of which can is is very relatable to to active inference and and building simulations um that way and uh and finally the the theme of the conference was social science and so I uh wanted to develop a multi-agent model and I spent a little bit of time looking at at kind of popular uh agent-based modeling paradigms uh in that that people still kind of use and study in social sciences today uh so agent-based modeling being um again this is one of those moments where I'm GNA be kind of adaptively figuring out how to go about this um but um yeah let's let's jump to the slides maybe so there we are um yeah I want to develop a multi- agent simulation that basically recreates a popular social science Paradigm uh in this case it's creating in number of Agents however you prefer to set it um it it it groups them together it connects them uh via like Network logic um the code I wrote has two different options you can either make say in uh groups of M agents so you could have like four groups of five agents uh or otherwise however you want to set that um as I was discussing briefly uh with with one of the other attendees today um I work in education so I particularly like this Paradigm because uh I think about students in our classrooms at the the schools that I work for and the idea of like okay what's what's a good way of facilitating them learning learning to cooperate with one another uh but at the same time not not get too dependent upon relying upon each other for like you know putting together a group project or something that is it that they should be able to learn in such a way that they develop both a p a sense of like personal efficacy but also being able to work within a group um and so the the Paradigm has to do with something like Collective problem solving where where you you have agents and and networks they're collectively solving a problem uh the problem is uh based in um what's called an ink landscape and uh it's basically just um just basically agents seek to find the best solution to to a an abstract abstract or hypothetical problem you have a whole ranking system where there are a series of potential Solutions uh represented in some number of uh bits or or or or or digits um and um and then there's some degree some kind of complexity parameter that you can use such that you know it's whenever you increase it it's as if you know changing your problem a little bit can dramatically change how good it is uh one quick way of saying it is like well you if you want to design a car if you remove the wheels it's going to be a terrible car right like you just changed one piece of it and yet the entire solution the entire schematic that you've built for for your car is is is kind of junk now right um so so that's the idea there are these these Solutions agents are trying to solve that have a lot of interdependencies between the different parts of the solution um so let's see so the tutorial it related to different themes with the conference agent based modeling which I've kind of already touched on and it highly relates to active inference in the sense that we're building agents agents are in an environment uh agents uh interact with one another and the environment itself um like that's your overall kind of simulation right um and uh the way that this has traditionally been done in uh computational social science is to use software like net logo is a very popular one but the idea is to just write a script for your simulation it has its own kind of syntax um and and variables and so on specifically for net logo so it works really well whenever you you can write the code a certain way and then you can easily create buttons and interface options whenever you go to run your simulation all of it's kind of really integrated um but that said it you know it doesn't allow too much on the back end it doesn't allow for too much complexity and so usually the agents are are not really they're not agents in any way that we've learned about agents in uh active inference they're more like a they're more like a a non-adaptive thermostat right if it's too hot uh they warm things up if it's too you know and vice versa so that that's it in regardless of circumstance they don't learn anything they're just kind of hardcoded and so my whole plan here was to recreate that popular the the ink landscape parallel problem solving paradigm but using active inference agents such that the agents kind of have something like autonomous uh decision making and um let's see if we can find something that represents that I supplied some definitions of how we tend to think about agent-based modeling again I didn't I I wrote this for an audience who might not have any awareness of what active inference is but it's still computational social science audience and so they will be presumably familiar with with agent-based modeling I included some guidelines on just modeling really anything agent based modeling but otherwise um four common principles the first one vers similitude which is basically just to say are we making realistic assumptions do are we whenever we set up a model an agent and we make we we have to make a series of assumptions and so do we does the audience do the readers do they agree with those assumptions are they reasonable to make is there any kind of empirical support for the assumptions you're making um if if not then then suddenly we have some very faulty scaffolding for for the entire simulation that's to come um that said we also need robustness tiny changes to the model presumably should not lead to dramatic changes in the final outcome uh otherwise you would otherwise what you done is that you've you've created a created a simulation or a model that's a little overfit that that really works well for particular circumstance but not so much for just about anything else you'd want to use it for um reproducibility is really important I wanted to be very transparent in making this tutorial I made all the slides available all the code is available um we have added the the GitHub for this tutorial to uh the active inference applications or implementations on the I can share that again with you all uh and then finally number four non-triviality which would just be um you know that ideally we would actually extract insights from doing agent-based modeling or in our case active inference um if we end up just creating a simulation that shows us something that we already know from a ton of other literature doesn't come off as particularly useful it can be in the sense that you know it's reconfirming something from another angle angle um but but but but you know I don't know where we're at as far as the cost versus the benefits um yeah idea of underfitting versus overfitting versus producing balanc models um you know this is just fitting a polinomial function to a to a sine wave and so like the overfit model is great until it isn't right skyrockets uh uh and and predicts that y will reach up to 400 ,000 there um so anyway it's many of you will probably be familiar with this already um the the idea of over versus underfitting um in computational social science there's been a turn that I I I argue exists um in in the literature over the past 10 plus years um people have identified you know there is a a a kind of beautiful Simplicity to um to using that logo to make these kind of hardcoded Agents because the point overall presumably is to try and see okay whenever you take these hard-coded agents and have them interact there emergent behaviors and properties that come come out of running that simulation um but it it's really foregoing a lot of you know we're we're we have such incredibly simple assumptions that we're using whenever we make um simulations like that there's kind of been a call in the literature for making kind of more Rich something like cognitive models where agents have something like uh can make their own autonomous decisions um here we are uh and so there's been a push for that so having like mechanisms for for uh say like memory uh decision making um and uh and being able to to act and react in different ways actually inferring the agents being able to infer kind of for themselves what is going on as opposed to a flat agent who just does X if y else do Z kind of kind of logic um and uh and there's been a lot of references to interdisciplinary work which also I think is very much up the alley of active inference in the sense of you know we're we're looking at Neuroscience we're looking at information Theory and cybernetics uh we're looking at biology we looking at reinforcement learning machine learning we're looking at many different things um and and a lot of kind of Rich conversation that comes out of out of that um so and then in addition the kind of benefit to social scientists is that what happens whenever you're engaging in in more like on the ground work where you actually want to design uh interventions or or or policies um that that impact ideally people's beliefs not in a kind of pure social engineering kind of way but what what happens whenever people are engaging in self-damaging behaviors or or in my case what what happens whenever I would like to just design a new way of doing group projects that that maybe better foster learning for the students where that they can learn to work on their own and with others um you know that the the kind of hard-coded agents in net logo they don't they don't learn anything there's no change in their beliefs uh technically they have zero beliefs they're just you know they're a couple lines of code that that tell them to do X if y otherwise do Z so um you know there General description all of you familiar with active inference will be familiar with a lot of this language uh cognitive modeling uh perception and action uh generative or generative models um I try to describe this as like we're kind of looking at models in a model so it's like um you know we have the typical environment and then the agents are within it but now within the agents you know is an entire generative model itself rather than a couple of of hard-coded roles um and you know there are other aspects here that we've touched on in the textbook group before there's the whole uh explore exploit tradeoff I kind of detail how how that is uh implicated in active m there's been different discussions over what the authors meant in the textbook over what that means whenever they say that that exploitation and exploration is like naturally balanced uh in active inference and I try to provide a little bit of clarity to that in addition to the fact that many computational social scientists who are uh wanting to do cognitive modeling are usually working with reinforcement learning based models and so they are familiar with uh this this explore exploit tradeoff kind of issue um the point is not that active inference magically balances those two things it's that they're both kind of included uh as as terms whenever minimizing particularly expected free energy in in the forms of epistemic and pragmatic value um that said an agent could still only ever encounter pragmatic value or only ever encounter epistemic value the point is that both of those terms are within the same function whereas in reinforcement learning a common way of doing it is to kind of hardcode a rule uh for what defines those two things and it's kind of left up to your own decision making on that like Epsilon greedy behaviors where you just say okay with probability one minus Epsilon you do an exploitative action um and and with probability uh V Epsilon you do an exploratory action so it's the point is that the agent here by minimizing free energy is kind of choosing for itself um which which kind of behavior to go for and and there's a there can be a blend it doesn't have to be one or the other it can choose an action that will have both epistemic and pragmatic value so just I think it's just conceptually uh a new way of maybe even thinking about what defines these kinds of behaviors in the first place um I find it personally very interesting but um I go a bit sorry uh I go a bit into uh reinforcement learning but I've already kind of touched on that uh one of the attendees here uh Zach brought up uh uh pdps and uh you know those are used in reinforcement learning they're what we'll be looking at uh today in the tutorial generally um but the the the main idea here is that we're looking at you know an agent has beliefs um about latent states of the environment that they can only infer um they engage in actions uh in order to kind of attempt to pursue their preferences or goals or realize their priors over preferences and then uh that action is kind of sent into the environment which then itself produces a new observation for the agent based on that action the agent then takes in that observation to update their beliefs about the the hidden state so we also have um Markoff decision-making process which is the same thing but but a little bit more simplified the agent has no latent states to infer um it's just a very direct the act the environment tells them the true state of the world and it's just back and forth so you have those two things um here I um discussed uh Epsilon greedy Behavior transition active inference um you know I kind of describe active inference is like well it's kind of it's kind of filling in some of the gaps that reinforcement learning generally seems to miss in in my view um I I don't mean to pit these fields against each other I think that there's a lot of uh valuable communication that that can come from uh H trying to interface between the two of them um but that said specifically for agent based modeling and something like cognitive agent based modeling active inference uh provides just a lot more for informing like what should those initial assumptions that we're making when we set up a simulation when we build an agent so on like I I think it it provides a lot more material in in support for those kinds of assumptions that we're making I think that it's it's it's pulling from uh decades now of of of uh Research into neuronal Dynamics uh into psychology and other fields such that you know I I don't I don't at all mean to say that it's kind of the end all be all of things I think what it does do is provide a very nice framework that reinforcement learning is missing uh reinforcement learning is usually focused upon task optimization in most research that I see um it's usually it's you don't the point would not to be to emulate uh humans it would be to emulate like super humans like you don't want humans you want some kind of AI tool or assistant who can do the absolute best job possible which makes plenty of sense for uh you know trying to optimize prop uh different kinds of processes and so on but um for someone like me who's interested in social science and computational Psychiatry I'm I'm looking for a little bit more more realism as opposed to to building super robots um even though you can kind of do both uh with these same tools so you know active inference as we know supplies the neural process Theory um it that said it doesn't you know I didn't want to scare Everyone by suddenly talking about neurosciences if you have to be an expert in that because as friston and others say in the textbook the the idea is not to emulate the entire brain it's to find the generative model that describes the problem the brain is trying to solve so that helps us to kind of stay kind of balance the balance things so to speak between the active inference approach and maybe what agent-based modeling is attempting to do which is to make things as simple as possible but no simpler than that um which albums uh algorithms and which architectures for for approaching cognitive agent based modeling so we we have the free energy principle um this supplies explicitly belief-based framework uh for engaging in agent-based modeling which is which is perfect for the social scientists who claim that you know we're we're lacking the ability to make agent-based models that maybe are directed at people's beliefs um that's yet another reason why I think active inference kind of supplies like this is a belief based framework that that is precisely what we're trying to to look at a lot of the time in addition to the the various dynamics that are playing in um again exploration exploitation balance kind of touched on that agents are self- evidencing which is um you know they're trying to to realize what they they already believe so to speak and so they can engage in perception or action they can change their mind or they can change the world uh and then variational Bas and inference as we know from the textbook group um um surprise basing surprise is often intractable whenever it comes to working with um you know continuous cases and in and the integrals or otherwise um just the amount of of of computation invol computation involved or being unable to find the analytical solution in certain cases um oh yeah and then uh as I mentioned with the whole kind of superhumans development approach from reinforcement learning with active inference there's this notion of false inference so it the the whole point of introducing that is simply to say we're not trying to necessarily make superhumans we're trying to to kind of better replicate what a human might do or or some other uh sentin organism or otherwise um including just the idea of optimality is relative to the model someone with say uh post-traumatic stress disorder has their own particular generative model and certain kinds of outcome behaviors that you see from them uh while they might not appear to be uh uh optimal for the current social situation they're in if someone you know suffers from from symptoms um I say all of this as someone who also does research on on PTSD um quick disclosure but uh um you know it might be it might not be optimal for the actual situation that that person is in but it is optimal as far as how their generative model is currently functioning as far as parameters and values go right um so that that's that kind of realism element here um then the last active inference agents are highly competitive if you're trying to develop them for task optimization um so I just cited a little bit of research on that um and then the model architectures that are often used in um reinforcement learning like mdps and pomdps are also heavily used in the discret cases for um active inference agents as well um supplied some further reading you know three hours sounds like a long time but there's a ton to to cover here I appreciated talking to a couple the couple folks in who do active inference at that conference and they said they they did not know how they could have fit everything into three hours and they they said honestly they think I did about the the the best that one can do or something along those lines which was a big um compliment and and I felt really good about that because I was a little stressed on how do you fit everything in any case uh further reading uh for folks who are interested and so there's further reading on active inference in relation to free energy principle reinforcement learning uh llms um which you know there there's been research on combining active inference agents with llms you can have use llms as if it were the agent it can like kind of report uh in in natural language uh what it kind of thinks about the observation it just saw and why it's choosing the action is choosing um that kind of lot so so it's a kind of like kind of having the the code for the agent coales with these you know chat gbt or otherwise prompts that kind of keep that sequence going along the way um just a quick comment on that Andrew yeah absolutely single even single successful examples of the active inference syntax and semantics go in in like this multi-agent setting go an immense distance towards using augmented coding methods and saying okay that was four clusters of five edit it to do six clusters of nine or now add this like that gets things going in a in a huge way so it's like kind of see the point that he's pitching but also think about the bigger distribution you'll see adjacent posses and many of those are like directly prompt right now sure yeah no no very very good point like to have that degree of integration um I think is is awesome and it's really exciting right now too um yeah no thank thanks for that note um I just kind of keep pushing on a little bit uh so again this was for social scientists so here are some you know example kind of theory building examples of how active inference has been applied to the social sciences um examples towards development of treatments or other kinds of interventions you can think of like kind of policy interventions too at a broader scale um uh here just general examples of active inference and uh changing contexts and these are only three examples out of many of them and I would argue it seems like many of them involve changing context just to show how agents adapt to a change in the environment um but I I think it's just worthwhile to to have a look at them and many social scientists are especially interested in things like say multi-arm bandit problems or or cooperative joint actions so I want to include those um some other example work um Daniel fredman of course being one of the major co-authors of the active infrance uh paper and the ongoing research uh for that simulation as well as um another Sim simulation uh a couple of its co-authors were also developed the PDP package it's actually from reading the epistemic communities underactive inference paper that I wanted to uh contact the folks um who have been working on the PBP package because it was uh Curious how they did that but both of these papers were a big inspiration for me and trying to figure out as a new active inference coder how do I make uh you know networks of Agents as opposed to just making single agents which i' done up to the point where I read this paper um so worthwhile to have a look at those for anyone who wants to get into multi-agent modeling more re learning resources um for the social sciences and active inference uh I would recommend these to to Really anyone including folks the uh Institute for sure um there's also the the entire constructing cultural landscapes course that was uh delivered for the most part last year with the The Institute um so that's also a more verbal uh verbally kind of delivered um series of courses that that that that relate this study of say culture and Society to active inference uh but many of these other resources have more to do with coding like looking at sanj's upcoming fundamentals of active inference textbook and the like a lot of good stuff here um you know I did use a series of images from the textbook always citing them of course but it's kind of like trying to move from from this traditional rules-based modeling to inference-based modeling um so I I I recognize that I'm I'm just getting to the point where you know this would have been roughly an hour into the presentation as I gave it at the conference and which would also mean that there would be two more hours uh following that and we have 21 minutes and for our session here and then also I think there were other things that folks wish to discuss so maybe what I'll do now is just kind of shift into what I would say are some of the core components of putting together a mo an agent uh like just a single agent um and then that way anyone who's here today or watching this being streamed um or or the recording of it maybe can refer to the code if they would like to to dig further um but for for building an active inference agent which uh this is one of the figures uh in the textbook again it's PDP um what we'll do this is kind of the idea of what from from my perspective how you might want to design an agent um you want to define the states what are they uncertain about um the what are the hidden states that they're trying to infer uh what are the observations that they come across uh those observations are being generated by the environment uh so think about the environment and what kind of observations that it kind of sends out that the agent can then uh come into contact with uh defining controls which are something like that the actions that are available to the agent um but there's a you know there's different language there's actions there's there's controls and there's policies and so the way I kind of interpreted that as a policy is like a sequence of actions that an agent can take an action is an action is just like a a single unit like action available to the agent and then um a control is like an action that necessarily does something to the environment that actually controls something because if you look at some simulations there will be actions available to the agent that don't actually do anything they just need to kind of be there to make the uh State transitions model that a PDP has uh work so um which really simplifies the computation in that case it's just anyway uh once we Define those three things States observations and controls we would want to define the um these different probability distributions in the generative model of the agent so it has an a matrix b c d and e uh matrices um and and so we'll get into that and then after that we can Define the environment which we've already been thinking about because we're thinking about actions and and and and observations Each of which have have a relation a direct relationship with the environment um then after that you define an active inference Loop like you'll have already defined and built your agent you you then you compose an active inference Loop where there you actually have the loop of an agent committing actions receiving observations so on uh from there and then uh and then ideally when when running some kind of active inference simulation or any kind of agent based modeling simulation you want to have some kind of utilities that you've written out to help you plot and to record and and plot or otherwise the results of your simulation you can run the simulation various times with different parameter settings and then compare the the the results for each time you ran it to see uh you know what makes more sense or what what is more accurate or whatever makes sense to you whenever you're testing your models um but yeah I mean why why do this if you can't see the the results and gain insights right and so that's that's the significance of comparison and rision um so the the the agents that I had everyone build where um this is kind of my attempt to illustrate what that looks like like as far as an active per action perception or active inference Loop goes um so we can start let's say here an agent explores or exploits um those are just kind of uh pseudo clever names for the actions available but uh to explore means to work on uh developing a better solution than what the agent already has available uh and exploit is to steal kind of steal the solution of a neighbor U this is referring back to trying to engage in parallel problem solving to solve a problem uh based on an inced landscape right so so they can either try and improve their own solution kind of like studying or they can steal their neighbors solution uh one that is better than what the agent currently has U but they get to choose right it's not hardcoded and the original simulation it's hardcoded to such that agents will necessarily steal a better solution if it one is available to them and um and and it's also a fully observable they always know that there's another solution that's better um so in this case like we have a partially observable environment I think it's a little less realistic uh or excuse me the one that I built I think it makes certain assumptions that are a little bit more realistic how do know that everyone's Solutions are better or worse than yours um wouldn't you have to kind of study and understand their solution to a complex problem in order to adopt it for yourself and recreate it for yourself just at you know so anyway there are different ways that could be viewed but it so that this is the general gist they they have actions available to them those actions are sent into the environment which processes the action returns in this case two different observations based on two different what are called Observation modalities or you can think of them as kind of observation categories uh so the agents will see based on their action if they were uh kind of attending to themsel or their neighbor uh if you attend to yourself the idea is that you're studying and so so there's kind of a linkage here between exploring and observing yourself or exploiting and OB and and observing your neighbor whose answer you just stole if you exploited um and then they get to see if the new answer whether they explored or exploited if if it was actually better than their previous um uh answer or solution that they had and finally they use that information to both infer the hidden State um if if they were attending to themsel or their neighbor is a little counterintuitive but uh the idea is it's it's always the way that I have the agents programmed it's always going to be one for one they're always going to know for sure that if they explored it will lead to them having observed themsel and they'll believe the hidden state is oh I'm attending to myself and I'm seeing improvements um you know or or not or not seeing improvements whichever observation outcome there is um for improve versus nonimprovement and then uh it incorporates learning which is another concept that we'll have learned from the the textbook group uh especially um chapter I think it first is largely introduced chapter 4 maybe and then there's a lot more detail on it in later chapters especially the second half of the the textbook so um that's I don't want to take up too much more of the time and I think I've I've spoken quite a bit here but just so you know um there are you know there's a lot more in-depth information and I you know used latex and all of that to recreate a lot of the the formulas involved here uh that we'll also see in the textbook um but it's just you define your States Define your observations and Define the the controls for the agent and then they get linked together so to speak through all these different matrices so the a matrix will be conditional probabilities of your observations given all states B Matrix will be uh you know what will the next state be based on a combination of the current state as well as an action that I do so if I'm hungry right now if I believe I'm hungry right now and I go eat food uh the next hidden State at the next time step presumably might be that I am now satiated um or will be satiated you know that's a way of thinking about that c Matrix or your preferences to be realized or your prior over over observations to be realized your D Matrix probably the simplest one is just at the beginning of each step you have a belief of of what's kind of going on they're like your priors over uh hidden States um you know at the the initial point of the the simulation you need a starting point basically and then finally the E Matrix uh which you don't necessarily have to Define in p and BP I can't quite recall what the cas is in RX and fur but the the idea is you know the the agent starts off with some beliefs about um kind of which action they have available is better versus others and oftentimes you can just um oftentimes this is initialized to be uniform uh you know maybe you're having you intend for the agent to learn a task and so you you yourself don't know what the better action to take is and neither does the agent and the idea is to start them with uniform beliefs over actions such that you can you can then apply the learning rules so they will kind of update what they think is better on their own um and then there's direct you know these direct code references like just as you know we see this kind of s uh superscript ATT like I I basically I try to keep it consistent sat at like I I aim to keep these things consistent across the tutorial and that way if if it's unclear what something means you can double back but once you catch on to to the Gomen clature then suddenly becomes easier to kind of follow things along so I I I included again code I included some outputs to help understand um because pmdp uses a kind of intricate way of of using numpy arrays as object arrays and and allows storing kind of these uh you you know irregularly shaped arrays um uh whenever composing these different variables for your model but the point is you know you can construct an a matrix B Matrix c d e introduce learning um just what kind of what that is about usually it occurs uh not at every time step you know it's kind of like oh you have to take in an entire experience before you necessar necessarily learn something that's sort of the idea um or one of the ideas behind actual implementation of learning roles um learning we're about out of time I'm so sorry um yeah and then once you have those things a b c d e and then um there are priors over a which are used for updating a um you can plug them into this agent instructor that the pmdp package uses and there's a little bit of detail on the other arguments that I included um but that's that's about it and then from there it'll go into these kind of core functions of of how the agent in first States based on observations how it learns which is just where it updates its variables a b or or D um I will note that I will be extending this tutorial in future for things related to like model fitting um at the at the time that I was developing this tutorial I was using kind of like this standard uh Pi Pi uh version of PDP I did not at that moment in time realize that there have actually been an incredible amount of updates to that package that are really exciting related to model fitting to other ways of doing inference like sophisticated inference uh just a lot of stuff so um this is as of now this kind of a living docu it's not quite finished um but but it's good enough to kind of get you to a point where you can develop um uh an agent using PDP and then with the later code that we don't have time to go into um putting them into networks and then having them run through the uh active inference Loop so they can attempt to to solve the problem and you know the kind of what that looks like is this so um my kind of personal you know my sort of finding is that you know the the way I have the simulation coded right now um I would I would really want to find different ways of grouping students together um as opposed to say uh a fully connected network of of students right like everyone work with everyone right um that's what I did here uh a random network but it's fully connected so it's not really random at all it's everyone's um everyone can take each other's answers as they want uh or not and uh basically the the the agents during I have them go through two stages so during the first stage um they do eventually find the best solution and many of them kind of coales around it um but that said they learn to not rely upon themselves so by the end of the simulation um with one denoting exploitation and zero denoting exploration um the agents really rely upon just exploiting each other over and over and that carries over going forward into the next simulation and because they're all trying to just steal the answers of their neighbor rather than relying on themselves to try and come up with a a new solution or experimenting or studying um they don't even by the end after many time steps they don't actually even reach the optimal solution they just kind of plateau it's not a bad solution per se but still they they Plateau there and it only kind of further reinforces like to not rely upon themselves and it's I could guess that that's like well you have 29 neighbors that you could take their answer instead instead of doing your own work why not do that um but uh it's you know in any case it's just this adds a lot more to trying to understand the Paradigm because the original paper that produced this Paradigm they did they do not say fully connected networks are the best but they do say that it does often lead to really good Solutions and here this actually is a way of looking at like how does this impact agents themselves are these kinds of prac practices do they have longevity or do agents just kind of learn to exploit one another to the detriment of the overall group actually by the end because they they might not reach the optimal solution so more ideally the simulation would be run many times with you know changing the seed each time to see if if if maybe it's just by happen stance that these were the results but I I did run it several times and I frequently find the same result but I you know for the s a tutorial we only do it once um all right uh I think I'll stop things there uh I do want to apologize to everyone for taking so much time but I really appreciate your your patience with uh with this and I I hope it was you useful or interesting in some way and I'll I'll be sure to to post the uh link to the GitHub in the chat uh presently so oops I did have a few questions if you were open to them on time you said that you were looking for a good Sol solution to problem or to the problem but in searching for the solution how do we know that the problem is well defined inside of a lot of interpretability work especially for reinforcement learning we're not ever allowed to say this is what you want you're never allowed to tell the agent this is the problem you want to solve we can only reinforce certain behaviors that seem to solve this problem so we run into a lot of problems especially using like classical reinforcement learning like a PO and throw a Transformer at it in order to solve a problem that is mostly aligned with the problem we want to solve but slightly misaligned the classical example is whenever you have an agent moving right to the end of a video game to get a star like Mario Bros or something like that but the agent never learns to want the star despite that giving it the reward inside of this occasion it only learns move right how do you go about the problem of explaining the problem with the uh with active inference around it using the uh PLM PD or DP sure thanks for your question and I hope I'm understanding it well enough um but uh uh I mean in this case so the the the agent maybe I should have left the slides up I apologize uh but the so it's whenever we def if you recall the the agent can re can receive two different kinds of observations and so one of them is if it's attending to itself or to a neighbor that's all well and good the other one is if it's observing an improvement or no improvement in its Solutions so it's kind of like you can imagine just like a like a spreadsheet it's like one are the potential Solutions and each one of them has a kind of um they're called Fitness values it's like they're exploring a fitness landscape but you can almost just think of it as like a ranking uh you know the this there are solutions all necess solution all solutions are necessarily like better or worse than than other Solutions so it's like a kind of ranking scheme um and so the agents in their C Matrix uh are are are programmed here to prefer uh an improvement and so they they get that they don't know what the optimal solution is from the start they don't know necessarily how to get there um they don't even know what its Fitness value is they don't know any of that but as long as they know that they're going in the right or wrong kind of Direction which is that itself is very very relatable to reinforcement learning right the the sense of like here's a reward signal like you you got A plus one or a minus one in this case the agent is just getting a Improvement or no improvement it prefers per that c Matrix the yeah the Improvement and it dis prefers no improvement um so that's that's kind of the simplest way of of getting it that and then why I think differentiates active inference Agents from the know reinforcement learning models as far as interpretability interpretability goes that we do have have kind of like these words like preferences and and habits and and goals and so on like all those are kind of baked into the the the formula that underly active inference such that we can more directly interpret you know um uh maybe something as if the agent wants something or it's choosing this because of that or or otherwise that I think is kind of we I don't know if we necessarily get that kind of uh clear interpretability from reinforcement learning models um it's probably debatable but that that's my perspective um okay yeah I hope that was useful just to just to shortly kind of restate it there's like kind of one and a half points first is the big picture is good heart's law is a challenge that's open setting a measure then having a proxy that measure that can become the proximate Target of optimization so the relationship between the proxy and like the underlying State can diverge and that's kind of what partially observable means active inference benefits because the preference distribution is semantically interpretable over the observations so let's just say we have the maze is on the right so then we have a belief about location and a belief about whether we have gotten to the prize or not we could say we looking at this model we have a flat preference on location so assuredly pragmatic value is not coming from location as a proxy so that reles being able to control what is epistemic and pragmatic value and to limit pragmatic value calculations to only things that are loaded up on C and localize the epistemic value only to the uncertainty distributions I think I understand you're saying that like in classical reinforcement learning you have your loss which is not of assigned in variables to the environment because the uh pragmatics are assigned to the environment because the varibles there that means that we can get around this problem well not around it like you said good Hearts but it's going to be better yeah is that what you I'm sorry I was trying to understand from this from this perspective reinforcement learning is like okay you're going to take all the policy space and you're gonna propose this auxilary function called reward and then you're going to use that as a single ranker but that is going to need to implicitly tied together epistemic and pragmatic value and it's also potentially computed in a like uninterpretable way with respect to which policies have what qualities okay thank you guys both so much great and I just want to thank everyone here for this this uh impromptu uh presentation but uh it's exciting to to see that it's maybe going to be uh at least a little bit useful for Folks At The Institute as well so I the you know there's a lot of cleaning that maybe needs to be done on the the tutorial maybe there's some excessive slides at the beginning because again it was intended to completely introduce every you know active inference in the first place to social scientists so there's probably some some fat that can be trimmed uh using it as a resource for folks here who obviously here for active inference um but uh yeah I just want to thank everyone again this was uh it was really fun to kind of to kind of rehash this and also do a speed run of it I suppose but I didn't quite clear it but yeah we'll do a future model stream and in that one you could give the we jump into the reduced version of the slides and you could just flip through them so people could pause then we'll look at the notebook the advantage of the notebook no one needs to install anything you can just hit run and get to the output and then also we'll work on getting the script based version up and that include more logging and interpretability Analysis in the script all that tracks chill good idea thanks and then abstract it and move it into RX and fur that may or may not be the move yeah I I I like the folks uh you know developing imdp quite a a bit and I'm very excited about you know the new features they're adding and uh so I'm keep working with that but you know that said um we'll see we'll see what's going I have a quick question uh regarding um going through the rest of the book uh last week we went over chapter one right so with chapter to be next week or so the times alternate between cohort 6 and 7 so when and doubt just check the schedule's page just look for the date and the time that you're looking at but yes cohort 7 will continue with chapter two and two weeks on chapter two and then the third week will be that kind of open one and then in cohort 6 we'll do chapter seven for two weeks and then like that will be perfect for another Deep dive on the discreet time there we I have about two weeks to get through chapters three that's great you have three weeks but yes three weeks wow all right well I guess I should take my time a little bit cool all right thank you all bye all right