All right. All right. It is April 7th, 2025. This is going to be active inference stream 12.1 cerebrum case enabled reasoning engine with basian representations for unified modeling. I've started first to make a GitHub push to the GitHub repo active inference institute cerebrum and that is now live and here in cursor I'm running the render script render markdown which calls the script render mermaids which takes this markdown file and all of these 15 figure level markdown files and the math and the novel case appendix and is going to result in a PDF. The PDF looks like this. In this stream, we're going to go through the PDF and talk about probably a bunch more depending on who shows up. Um, I'm going to finish flipping through the PDF. Then we will see where the PDF generation is at. And then we will go through this. This should be super interesting. Um, conclusion appendix one math appendix 2 novel cases. All right. The generation will finish soon. It renders each of the 15 mermaid uh diagram figures which look like this into figures that look like that and then inserts them into the markdown. All right, PDF generated. It generates cerebrum.pdf. So that's the newly rendered version and I'll push it and then let us get into the topic. I also just pushed this to Zenotto and looking forward to where we go from here. So, we're all pushed on the repo. Let's look at the paper and go through some of the sections. All right, I'll read the abstract first. This paper introduces case enabled reasoning engine with basian representations for unified modeling. Cerebrum. Cerebrum is a synthetic intelligence framework that integrates linguistic case systems with cognitive scientific principles to describe, design, and deploy generative models in an expressive fashion. By treating models as case bearing entities that can play multiple contextual roles like declinable nouns, cerebrum establishes a formal linguistic type calculus for cognitive model use, relationships, and transformations. The cerebrum framework uses structures from category theory and modeling techniques related to the free energy principle in describing and utilizing models across contexts. Cerebrum addresses the growing complexity in computational and cognitive modeling systems eg generative decentralized agentic intelligences by providing structured representations of model ecosystems that align with lexical ergonomics scientific principles and operational processes. and we will probably push to another version by the end of the stream. So, we'll fix a few different things that we can see along the way. Describing and utilizing models across contexts. Okay, continuing to overview. Cerebrum implements a comprehensive approach to cognitive systems modeling by applying linguistic case systems to model management. This framework treats cognitive models as entities that can exist in different cases as in a morphologically rich language based on their functional role within an intelligence production workflow. This enables more structured representation of model relationships and transformations. And the code to generate this paper and further open source development from this 1.0 milestone is available at this GitHub repo and that's what we'll be pushing to and using during this stream. Okay, background sections. First background sections on cognitive systems modeling. The second background section is about active inference. The third background segment is about linguistic case systems and I'm going to read this one because this might be the one that's the wild card. However, this is the crux and as a first language English speaker, as we'll discuss, there are some interesting ways of having the cases a bit more silent in English than some other languages like Russian and Latin and other things that will probably end up on Wikipedia to find out today. So I'm going to read this linguistic case section. Linguistic case systems represent grammatical relationships between words among words through morphological marking. Case systems operate as morphos syntactic interfaces between semantics and syntax encoding contextualized relationship types rather than just or in addition to just sequential ordering. This inherent relationality makes case systems powerful abstractions for modeling complex dependencies and transformations between conceptual entities. Cases under consideration here include nominative, subject, accusative, object, dative, recipient, genative, possessor, instrumental, tool, locative, location, and ablative origin. All serving different functional roles within sentence structure. language. Um, English has largely lost its morphological case system. The underlying case relationships still exist and are expressed through word order and prepositions. For example, in the cat chased the mouse, the nominative case is marked by position, subject before verb rather than morphology. While in I gave him the book, the dative case is marked by the preposition to and word order. This demonstrates that the semantics/chemiosis/pragmatics of case relationships are fundamental to language structure even when not overtly marked morphologically eg expressed in writing or spoken language. Section 134 intelligence case management systems. So, this sort of speaks to the pun at the heart of this, which is case management, which might be for like a help ticket or a research inquiry or any other kind of case management. It's a bit of a hinge with the linguistic case systems. So, coincidence, but enough of a connection and gap there to have the linguistic case system be the theory and the category theory and some of the transformations that are going to be going into And then combining this with more of an operational layer and thinking about what are the different roles that generative models really do play in different kinds of case managements seen more from an operational view. Okay. Section 1.4 towards languages for generative modeling. I'm going to read the section. The active inference community has extensively explored numerous adjectal modifications of the base framework including for example deep affective branching time quantum mortal structured inference among others. Each adjectival prefixed variant emphasizes specific architectural aspects or extensions of the core formalism. Building on this cerebrum focuses on a wider range of linguistic formalism. For example, in this paper, declenional semantics rather than adjectival modifications or prefixing. In this first cerebrum paper, there is an emphasis on the declenional related to declining aspects of generative models as noun-like entities separate from adjectival qualification. This approach aligns with category theoretic approaches to linguistics where morphisms between objects formalize grammatical relationships and transformations. Many such cases by applying formal case grammar to generative models. Cerebrum extends and transposes structured modeling approaches to ecosystems of shared intelligence while preserving the underlying partitioned, flexible, variational, composable, interfacial, interactive, empirical, applicable, communicable semantics. 1.5 The intersection of cognitive systems modeling, active inference, linguistic case systems, and intelligence production. 1.6 methods and materials. The cerebrum framework was developed as part of a broader synthetic intelligence framework combining linguistic theory, cognitive science, category theory, and operations research. Key approaches included linguistic formalization, category theoretic mapping and algorithmic implementation. Figure one here are those foundation domains flowing into cerebrum. The functional and the processional elements of the framework. Different features and system outcomes leading to enhanced model management. Core concept 1.7 cognitive models as case bearing entities. Just as nouns in morphologically rich languages can take different forms based on their grammatical function, cognitive models in cerebrum can exist in different states or cases depending on how they relate to other models or processes within the system. Figure two illustrates this linguistic parallel. zoom in. So here provocatively laid out generatively on the diagonal are the cases and how they'll be abbreviated. Nominative, accusative, dative, generative, genative, instrumental, locative, ablative, invocative. Up and to the right are examples with a cat as noun. Down and to the left are the analogous functional roles and sort of parts of speech as in describing to the generative model which is what the focus of this is. Okay. going into a bit more detail about what do these different cases do? What is the point of being able to decline a generative model like a noun into these different cases? So here is three clusters of cases. On the left are the genative and the ablative case. And ablative as it as its name sort of even suggests was uh relatively earlier lost. And so it's uh not always seen but and you know it's these things are just popping up and down in in different languages really interesting ways and gener genative which that sort of pun with genative and generative is going to come back. Um here the generations coming from something or the possession of something the locative instrumental invocative here are clustered with the contextual cases. So these and also the primary cases with doing the the direct agent doing the action like with nominative like the cat jumps over the moon or in the accusative or the dative being the recipient of action. So different cases that might be used linguistically in describing like the source or origin like is that your guitar from where did that guitar come from? It was brought from here. It is possessed of this person. Those are uh sentences that would be about some sort of material noun that might use these different cases to provide those functions. And here it's like a analogy extended into talking about joint distribution generative models. Here's a table with all of the cases that are going to be considered. the abbreviation the full name of the case the function and the example. So nominative model is active agent. Cat jumps over the moon. Model generates predictions. Model does something. This is sort of the the base model just declaring the form that's in the dictionary. Uh accusive accusative and dative. This is model as object and recipient. And these relate to processes that are inputting or targeting the model like some method applied to the model or something from another source being received by the model. Genative, the output of the model, something that comes from the inferences and the ablative, possibly very similar or redundant or not needed in situations, but related to basically the difference between the ablative attribution and the generative instrumental. This is uh possibly a very common case for using when talking about a model analysis performed with something like the implementation of a model as a model and the vocative model as addressable entity. For example, hey model X all the different names that these chat assistants have. So calling something by name direct invocation task initialization and documentation reference point like referring to a parameter as its parameter in a system or or something like that. Maybe there's different you know subtypes. Um, okay. 1.9 a preliminary example. Thank you for your question, Dong. I will get to it. I want to get through the paper mostly in one sweep and then happy to look at your question and any other questions people have. All right. 1.9. A preliminary example of a case bearing model homeostatic thermostat. Consider a cognitive model of a homeostatic thermostat that perceives room temperature with a thermometer and regulates temperature through connected heating and cooling systems. This is kind of the classic model simple homeostatic. In nominative case, the thermostat model actively generates temperature predictions and dispatches control signals functioning as the primary agent in the temperature regulation process. So that's kind of like an operational utilization or generic situation considered holistically version that's in the dictionary. When placed in accusative case, the same model has become the object of optimization processes with its parameters being updated based on prediction errors between expected and actual temperature readings. Change your mind, change the world. In data case, the thermostat model receives environmental temperature streams. These can also be ongoing simultaneously which is going to be uh gotten to later with the precision modulation and occupant comfort preferences as inputs. The generative case transforms the model into a generator of temperature regulation reports and system performance analytics. So basically synthetic data generation or generative AI like generative AI when an instrumental case the thermostat serves as computational tool implementing control algorithms for other systems requiring temperature management. It's being used like a tool. The locative case reconfigures the model to represent the contextual environment in which temperature regulation occurs. Modeling building thermal properties. So perhaps locationational topics or discussing something within the model as a location like model as place. Finally, in a blade of case, the thermostat functions as the origin of historical temperature data and control decisions, providing causal explanations for current thermal conditions. This single cognitive model thus assumes dramatically different functional roles while maintaining its core identity as a thermostat. Figure four, generative model integration in intelligence case management. First place, a good render from above. Here's just different stages and different ways in which stages of research and intelligence work relates to different case aspects being highlighted. 110 declinability of active inference generative models. At the core of cerebrum lies the concept of declinability. the capacity for generative models to assume different morphological and functional roles through case transformations mirroring the declenion patterns of nouns in morphologically rich languages unlike traditional approaches where models maintain fixed roles or variable roles defined by analytical pipelines. So which is to say that there's some process by which like well now we're going to specify it declaratively then we're going to do this thing that updates it then we're going to generate some data from it then we're going to do this thing that updates it then we're going to generate some data then we're going to give a final updating and then we're going to use it as a tool. Some process like that which is just defined procedurally here gives a more expressive layer on top of what even that is and expands out and generalizes what that space of procedures is materialized from. So what are those changes that happen for generative models? It's not necessarily that some suffix like in the case of a noun the end of the noun might change and there's probably different ways this happens in different languages. So what is it that actually changes by analogy in when a generative model is actually declined into these different ways cases. So one functional interfaces could change what is being received andor transmitted parameters across patterns. So what kinds of parameters are in play? Prior distributions could be a different prior. Maybe if depending on whether it's playing one role or another, something is more or less likely to happen. Update dynamics. In what ways does the model update if at all? And perhaps different computational resourcing also including different thresholds or approaches. So one approach being there's a 0ero to one waiting like an attention or a precision on all these cases in play at once or one could have a one hot and some kind of other rule for just only computing one of these settings and then depending on how you parameterize what each of these settings would be. It's not that receiving and updating has a higher computational cost than being used to make an action selection, but there could be a setting where receiving and updating is very simple. There could be a setting where that's very complex. Same as for action selection. So what happens when the generative model gets this modification here? Table two describes how each of the cases different changes could happen parametrically including structure learning and what happens at the interface of the model. How does that relate to a precision weighted generalized approach kind of like the Sanv Smith approach? Consider a perceptionoriented generative model M with parameters of theta didn't render. Let's fix that. The theta for parameters does not render update the so it is plain text data and check anywhere else that might be relevant. So M is the joint distribution generative model. Here's another list of those cases being used in different ways. The vocative case voke represents a unique functional role where models serve as directly addressable entities within a model ecosystem. Unlike other cases that focus on data processing or transformational aspects, the vocative case specifically optimizes a model for name based recognition and command reception. This has particular relevance in synthetic intelligence environments where models must be selectively activated or woken up through explicit address similar to how humans are called by name to gain their attention. The vocative case maintains specialized interfaces for handling direct commands, documentation references, and initialization requests. In practical application, models invocative case might serve as conversational agents awaiting activation, document reference points within technical specifications or system components that remain dormant until explicitly addressed. This pattern mimics the v linguistic vocative case where a noun is used in direct address as in hey Siri or okay Google activation phrases for digital assistance creating a natural bridging pattern between human language interaction and model orchestration. This systematic pattern of transformations constitutes a complete declenion paradigm for cognitive models using precision modulation to fulfill diverse functional roles while maintaining their core identity. Now pivoting from that linguistic case focus to thinking about the role of those different cases in model workflows. that is in figure five and six. So here's an example of how one kind of generative model might move through these different cases. So first a a model specified in nominative it processes data at as a direct object. It's used to generate source synthetic data. Then it it is the the sender andor receiver of information at different levels from the sort of fine-tuning and context layer here like a feedback between the analyst and the results at this layer or at the second layer. a feedback involving data selection or attention andor even reaching back to the model's structure. Here's figure six more explicitly in in the context of the intelligence production workflow. So here's data collection, gathering of raw data used as an as instrumentally activating some kind of model. This is just one example motif processing and and defining the model providing locationational situational context generating and evaluating data with context changes. Then generating from the refined model, deploying the refined model one possible pipeline. Figure 7 has some of the category theory connections and seeds with that are mainly drawn from other research in the category theory of linguistic case. So tenative just like all of it. Figure eight more types of transformations. So for example, let's start with the nominative with the sort of agentic framing. There might be some kind of transformation like objectification that combines with external data to result in a well formatted processed model. In the accusative case, some type of targeting from or to that transformed object could bring it into being addressed or being used as a actor or recipient or whatever the analogy is in the datative. Then in the generative different kinds of cycles, figure 9 goes into a sort of esoteric but interesting aspect of linguistics. Again, something that's really implicit in English or it was hard to even sort of think about how how different this was, but that's kind of the cool part about language. So, morphos syntactic alignment and the difference between the erative absolutive patient oriented modeling and the nominative accusative, the agentoriented moni modeling. And the reason why this box isn't the same color is there might be some operations that differ between these two or might be a dual representation. But from the point of view of something downstream in the process, that is an example of a kind of morphos syntactic difference that perhaps one underlying situational reference source could be expressed in a arrogative absolutive frame or language or in a nominative accusative. 1.14 goes into more detail into implementations and how this could play a role with the different cases being relevant at different stages of implementation. It's pretty interesting to think about how different kinds of resourcing and costs of different operations just like how with a transformer LLM there might be a different amount of computations required to train it for the first time to fine-tune it to fill its context window to just do the embedding to generate data. All these kinds of things have different computational costs and they have different scalings and that is going to depend a lot on the state space of the model. But it also depends given the state space of the model about all of these more pipeline scale modeler degrees of freedom like how do we actually host the data and all those kinds of training and how many times do we actually do different steps. So here's one in figure 11. One example again of that some model is used in raw data collection. A model is activated through the vocative. And these different steps can happen and and possibly another or multiple cases of the very same generative model or different models that are in different cases. Just like a given step of a story or a given situation might even in the same sentence have just one case or it could have multiple cases in a compound sentence because it could be like the guitar of that person was used by this person to do this leading to this at this location. All those kinds of sentences that that happen. another visualization just with a different clustering but similar to 11 and 12. And we maybe we can update 12. Um here's how linguistics, cognitive systems modeling, active inference, and intelligence production make contributions and also relate to cerebrum. Kind of revisiting that All right, 1.18 related work. So here's an important epistemic disclaimer. Cerebrum builds upon several research traditions while offering a novel synthesis. In this first paper, there are no specific works linked or cited. Later work will provide more detail and reference in derivation. The work stands transparently on the shoulders of nestmates and so is presented initially as a speculative design checkpoint in the development of certain cognitive modeling practices. There's cognitive architectures backgrounds everything with knowledge modeling cognitive architectures active inference etc. Um figure 13, here's how one model might relate to um different utilizations within the same active inference predictive processing hierarchy. So here's a model being used in different cases with a top down prediction and the bottom up errors. Here's describing different kinds of aspects of the agent mapping the active inference ontology to different cases that it is in or could be said in and one way to possibly implement that is precision weighted message passing and free energy minimization. So figure 14 goes into a little bit more of that. So the whole box is yellow again to have a sort of symbolism like from the outside. This could be a multiple dispatch and it it it might matter how different message passing plays out. But there's also another level where it can be just core strained to that level at least conceptually if not in the implementation dispatch. So here's the instrumental, nominative, accusative, dative, genative, and vocative, and how they might connect to the error and prediction motif of a two-layer hierarchical predictive processing. So just different kinds of modes might relate to different modulations of features of um a hierarchical predictive processing architecture. So kind of like different positions of the hand on the guitar could lead to resonating uh simatics vibrations different way. 15 is the model case calculus framework. So here's some more bringing together of how the mo here with a subscript notation for different cases showing how different functional roles and cases relate to subscript changes which can be applied through to to be determined to be uh invented/discovered. Not necessarily infinite or open-ended, but certainly definite calculus laws like what kinds of model transformations and concatenations and composabilities are valid within which understandings of case and which ones also perhaps by definition perhaps by implication are invalid given a case approach in a language. more background references could be find uh could be found with category theory, active inference, linguistic computing. Okay. 1.19 practical applications. Here's a few situations where this could be really practical to apply. One is in model pipeline design and optimization and the resource optimization. As sort of mentioned earlier here might be given some constraints on resources of different kinds what kinds of cases one might be expecting to want their model to be optimized to be in. For example, let's just think by analogy to like an LLM. Let's just say that there's a few architectures that have the same score on a benchmark, but one of them is 10 times more expensive in RAM or storage or CPU or money or whatever it is to train, but then it's cheaper to run. Or it's cheaper to train, but it's more expensive to run or it's able to have a larger context window, but it costs more to do this or it has some other trade-off or something like that. So on those parareto optimal sort of constraints manifolds within empirical computational finite space just from a pragmatic perspective resource allocation that might be able to guide what features at the meta modeling level. How do we want to think about it? What functional roles even matter to benchmark? What tests should be written around what kind of functional roles for what kind of artifacts? Table six, some cross-domain integration patterns. So just sort of like associating differently and more how these cases can be used. even one domain unsurprisingly um like reasoning might have multiple cases deployed just like if we were going to talk about somebody in a library and they're engaged in perception. So it's like that might use a lot of sentences involving nominative or accusative like they looked at the book and so that might put the book as the recipient of action in the accusative and so the same type of agent and artifactbased ecosystem of share intelligence model that kind of describing perceptual gestalt aspects whatever components might involve a lot of like active agents and recipient agents. Talking about reasoning might be talking a lot with instrumentality and context. Talking about planning might be about goals of an agent like Tilios and the ablative history in terms of where where did it ablate from like what was removed or what was generated. This is a case we can look more into or write a new section on what is the oblate of me or something like that. And in the action case, there might be active agents and and recipients kind of like the accusative 1.19.5. This is describing some ways that this kind of semantic labeling on a knowledge graph could be used to to structure just like if it were going to be a graph of the food web or ecosystem of some region in the sort of hyperlink style or unstructured edges like obsidian. It might be ambiguous like well what does this edge mean? doesn't mean it's a type of and and there's exactly that which is what structured ontological approaches exist for and constraint either interpreting those linguistically andor constraining the edge types that are in play. two kinds of linguistic assertions is possibly a a pretty parsimmonious and aergonomic way to do it because it would reflect the kinds of expressions people would say about the ecosystem. 1.19.6 Six, emergent behaviors in model collectives. When multiple case bearing models interact within an ecosystem, emergent collective behaviors arise from their case-driven interactions analogous to how linguistic communities develop shared understanding through dialogue. So that could kind of get into some of these linguistic features in discourse community, not just loading up context windows of each other and then continuing the conversation, which works really well in the prompt engineering multi- aent modeling approach. Uh really common with LLMs, agent laboratory, Andrew Pashchet's work, all that kind of stuff. Th this is getting a little bit more towards dialogic linguistics rather than just script context in context learning. It's a different description space for dialogue 120 future directions. So this could include making some programming libraries to to do various features, some of which we might look into today, building visualization [Music] tools, expanding to other linguistic elements. So here to really make the point I wanted to focus on the nouns and the declining of nouns but there could be other uh features like talking about different time tenses and so on. Open source stewardship, computational complexity estimates, multiple dispatch systems for the methods, connection with different database methods and queries looking at different cognitive security aspects. Okay. Um I'm going to read the conclusion then we will quickly go through the two appendices and then rerender it and then see where that kind of gets us. So 1.21 conclusion cerebrum provides a structured framework for managing cognitive models by applying linguistic case principles to represent different functional roles in relationship. This synthesis of linguistic theory, category mathematics, active inference and intelligence production creates a powerful paradigm for understanding and managing complex model ecosystems. By treating models as case bearing entities, Cerebrum enables more formalized transformations between model states while providing intuitive metaphors for model relationships that align with human cognitive patterns and operational intelligence workflows. The formal integration of variational free energy principles with case transformation establishes cereum as a mathematically rigorous framework for active inference implementations. The precision weighted case selection mechanisms, marov blanket formalizations and hierarchical message passing structures provide computationally traractable algorithms for optimizing model interactions. These technical formalizations bridge theoretical linguistics and practical cognitive modeling while maintaining mathematical coherence through category theory validation. The cerebrum framework represents another milestone in a long journey of how we conceptualize model relationships. moving from ad hoc integration approaches on through seeking the first principles of persistent composable linguistic intelligences. This journey really an adventure continues to have profound implications for theory and practice by here incipiently formalizing the grammatical structure of model interactions. Cerebrum points towards enhancement of current capabilities and opens new avenues for modeling emergent behaviors in ecosystems of shared intelligence. As computational systems continue to grow in complexity, frameworks like cerebrum that provide structured yet flexible approaches to model management will become increasingly essential for maintaining conceptual coherence and operational effectiveness. All right. to the appendices. All of the equations are referenced in the text equations. First, the generic variational free energy here applied to the structural model of the case transformation. Equation two, describing cases in terms of their structured blanket interface. Equation three, uh, beta precision modulation feature that precision weights case. Equation four, doing partial derivative case specific gradient descent on free energy. Equation five, planning over cases with expected free energy. Equation six base factor to get a posterior odds estimator between two structurally divergent models. Equation seven, how free energy is minimized with the case transitions. Section uh appendix 2 section 1.2 to message passing rules for different cases. This is going into a bit more detail from the image that was shown from this section with the different uh updating functions in the predictive processing hierarchy. Equation 13, temperature precision waiting. 14 resource waiting. Section 2.1.4 4 goes into the some of the novel cases that are in the second appendix. 2141 has the appendix alphabetical pretty much everything renders fine of the different variables used. Okay, this should be the second appendix. So let's update that. This is showing up as 2.2 two. Please update it to be appendix two. You're changing [Music] the and render steps, right? but it'll still be the same material. Okay. The cerebrum framework not only operationalizes traditional linguistic cases but potentially enables the discovery of entirely new case archetypes through its systematic approach to model interactions. As cognitive models interact in increasingly complex ecosystems, emergent functional roles may arise that transcend the classical case system derived from human language. So here's some anomalyoriented ways and bottomup empirical ways to discover new cases. And then here are three speculative novel cases. The conjunctive which has to do with sensor fusion type fusion of predictive streams into joint prediction like federated inference. a speculative novel case, recursive case, model applied to itself, and then a third possible case, metaphorical case. So, these are some interesting paragraphs, and then we're at the end after this, and I'll look to the live chat, so I'll look to your questions, Dongyok, and anyone else. A third potential novel case is the metaphorical case MET which would enable a model to map structures and relationships from one domain to another creating computational analogies that transfer knowledge across conceptual spaces. So it's kind of like a simile. It is a simile. It's like a simile because it is when a model it's like saying I feel like a buoy at sea or I'm hot and cold like a thermometer in this location on that day. In this metaphorical case, a model acts as a transformational bridge between disparate domains, establishing systematic mappings between conceptual structures. This case would be particularly valuable for transfer learning systems and creative problem solving algorithms that need to apply learned patterns in novel contexts. The metaphorical case would introduce unique cross-domain mapping functions as formalized in equation 18. The key innovation is the structure alignment of latent representations across domains # fieldshift enabling principal knowledge transfer that preserves relational invariance while adapting to target domain constraints. The metaphorical case has rich connections to multiple domains of human cognition and communication. In affective neuroscience, it models how emotional experiences are mapped onto conceptual frameworks explaining how we understand emotion through body metaphors like heavy heart, burning, anger. In first and second person neuroscience, metaphorical mappings enable perspective taking and swapping and empathy through systematic projection of one's own experiential models onto others. Educational contexts leverage metaphorical case operations when complex concepts are taught through familiar analogies. Making abstract ideas concrete through structured mappings. The way people converse about generative models often employs metaphorical language describing models as thinking, imagining, or dreaming which represents a natural metaphorical mapping between human cognitive processes and computational operations. Learning itself fundamentally involves metaphorical operations when knowledge from one domain scaffolds understanding in another. Perhaps most profoundly, the metaphorical case provides a computational framework for understanding how symbols and archetypes function in human cognition as cross-domain mappings that compress complex experiential patterns into transferable culturally shared representations that retain their structural integrity across diverse contexts while adapting to individual interpretive frameworks. So here's a table summarizing the speculative novel cases here. Conjunctive, recursive, metaphorical. Right? That is the paper. Here is the cursor agent that is rerendering the entire PDF with a few changes that we have made. Let's see if it finishes and then just open Here's versioning. All right. modify the render script, modify the source code, go back and forth a few times. So, that one finished, but now it's started another one. So, just to confirm, I'll delete that. Now, this one when when the PDF reappears here, it'll be the new one. Here's the figures in the output. Those are generated from these mermaid graphs. Then the appendix. All right, check back to the stream. All right, this is using cursor version 0.48.7. For the LLM, I've been using Claude 3.7 Sonnet and also uh maybe half the time Gemini 2.5 Pro X0325. All right. So, let's see the PDF get regenerated. Um, okay. We went over the paper, went over the key diagrams, talked about some of the different roles that generative models play, the kind of modes or postures or by analogy to how a noun might be in different positions in a sentence. Like is it the cat spewed tears everywhere or tears were spewed upon the cat? That still has cat in English, but other languages have other more visible modifications. Here the PDF was generated and there it is. Let's see if it updated table contents. Okay, didn't fix it. H still looks like it is section 2.2 two and in table of contents check again and please fix it and then just sometimes it can be that context window. So starting the new chat and also the uh control t starts up a side chat. So for example here we can say in docs comprehensively given the paper write up a documentation library for specifications for starting to build this in robust modular multi- language multi-setting way then new chat. So we have the paper write a new tool make graphical abstract. That will make a big PNG and PDF of a big Epic graphical abstract with the author and title and abstract information hardcoded and all and a spread of all 15 images in grid looking like awesome conference. poster. Thank you. Here's the documentation chugging away. Okay, missed the boat on that one. Graphical abstract tool in development docs folder. And there's so much stuff with the MCP and uh the MDC files, other things. All right, I'm going to copy a bunch of the comments. So, yeah. Anyone else write comments and I'll paste them in. Okay. Dong Lero, sorry, I'm pretty new to your channel. Are you trying to make your own LLM or something? Not trying to make my own LLM. Looking to do many things. Looking to learn and apply active inference. Make it rigorous, applicable, accessible. figuring out synthetic intelligence methods. So things that integrate. It could be a chain of LLM. It could be no LLMs. So different flexible methods for yesterday, today and tomorrow in compositional systems. I think consciousness or the experiencer uses self model which is pretty much a memory pattern about belief on self that is self-reinforced in a nonlinear neural network structure like LLM. So I think this cerebra model is constraining if you are looking to study consciousness. Yeah, that's a great point or question. I didn't uh mean for this to be a consciousness resolver in any way. I could see someone talking about the different functions like when the mind generates experience. Is it a recipient of action? Is it a host of action locatively? Is it a generator of of action? All these different ways in which I think this could help reduce uncertainty and expand the hypothesis space and expressivity like even related to consciousness. But that's not what this framework is uh about. But I think it still may be even thinking more about awareness or self-awareness or uh relevance realization or something that doesn't have the same phenomenological first person experiential even though that was mentioned and I think that's an important modality for it to be used in. Um but the fact that it's called like cerebrum it was just sort of a a acronym that fit maybe um let's make a a new chat. Oh, only three can be open at once. We'll ceue up that one, please. Given Just come up with a long list. Put it in docs of insect related brain and [Music] cognition cognitive terms that could apply to the whole project. just at the high level call sign up like cerebrum once that one finishes. All right. The make graphical abstract is running. The script is running. Okay. So that I agree. I hope that it's it's not meant to constrain even at all. Wasn't like that today for me. The study consciousness. Let the LLM like structure learn about self through its senses and form beliefs about selfidentity as memory pattern. Yeah, that's an awesome point. Like for LLM or perhaps for multiple or any kind of system, that firstness of like the primary syntax of the neural network or the material metamaterial substrate and the way that that's kind of like subsemantic and possibly even subsactic. That's something that's really explored in like girdleserbach and other kind of hofster type thinking like computers don't really play chess. They do statistics but they don't really do statistics. They just follow laws of physics. So that is where a lot of active inference theorists and thinkers talk about mortal computing embodied self-evidencing uh selfidentity um meta awareness of identity because it connects that primary modification of the substrate with a perhaps what you're calling like an LLM like structure and the symbolic and higher order nested symbolic discretized cognitive structure. like knowledge graphs. Much like human infant human baby doesn't have strong sense of self but develops through self-reinforcement through experience. LLM has intelligence to grow self-awareness. Yeah, that's definitely a um claim to explore. However, um talking about babbling and refining uh at structural and fine-tuning levels of the sense making, propriception, active motor feedback, all these features like babbling, motor babbling, verbal babbling. Chris Fields talked about that. This is wrong because that's how human brain work. This is wrong because that's not how human brain works. I don't know, but great question. Okay, thank you for the comments. I'm going to copy another comment from the live chat in and then anyone else can write any question in the live chat and I'll stay for a little bit longer and do a few more things. So, Roit wrote, I just started watching the stream. How exactly does Siri room work? All right, let's get a new document. How exactly it works? This window is rendering the PDF. When the Morse code pops up, it's pretty close to the end. Yep. Oh, but it's it's getting impatient, but it should end very soon there. Then the graphical abstract tab. The outputs are saved in the output as graphical abstract cerebrum output. Okay. Awesome. in so output folder graphical abstract. Okay, PDF. Oh, PDF works well. Have some block square. PNG. Perfect. could upload that to a PNG. All ready for the undergrad research expo. All right, let's clo Let's reset that one. Do the insight [Music] acronym docs writing is continuing. The PDF is rendering. Once the docs finishes, we'll ask Roy's question. rerendering in the first tab. Oh, by the letter antenna, apis, abdomen, arthropod, brain case, beetle, butterfly, cocoon, compound cricket. Just more length, more comedy, more funny stuff. add more to the insect acronym list. So there are multiple for every letter include Aanopa, Pogamir, Barbadus, Antsence. All right, it wrote the [Music] documentation. Yeah. Um, PDF is still rerendering. Add a folder within docs called languages that has language specific unpackings and elaborations for specific languages. per markdown. Be absurdly comprehensive in conveying and tableabling. How cerebrum relates with details and tables to such case. languages with diverse case paradigms including Latin, Russian, Sanskrit, other languages each profile. All right. Okay. Okay, insect. Let's see what poguics probabilistic operational generative ontology for neural organizations with multi-level yielding reasoning and model exchange exformationations. Pretty good. Pretty good. Stigma G unprompted directly systematic transformation of information with generative models for emergent reasoning and goal-driven yielding. Okay, this one is making the language specificity. That's still rendering. Let's go to the chat window. Crl T. Alt T. Okay. Right. We'll look at the docs. Just getting back to the stream. write a docs called how it works. My colleague wrote, "How exactly does cereabum work?" And we want a onestop shop for exactly that question. It can hyperlink to other documents and as needed in docs so that the inquiry is respected from vast number of relevant angles. Okay. Right. Whatever is happening with the PDF generation is pretty slow, but it does work. There's probably way better ways to use pandock and all that. Okay, let's push the update and then look at the documentation. Let's let it make Russian. Okay. How it works is in progress. Improving or at least digging us deeper into the uh rabbit hole with render markdown is happening. We will have Latin, Sanskrit and Russian in a second. And then we'll see what does it uh with that minimal outlining what what will it do for each language. Okay, new rendering is happening. We'll just update it when it finishes that. All right. So in the cerebrum repo docs I'm just opening up all the markdowns. and queuing up languages. And then we'll have it just continue to write more languages. Continue to add technical relevant detail to all extent languages and add more rare, interesting, useful, cute, funny, interesting. formal languages once it finishes there. So we'll just be peeping as soon as the generating goes away. Okay, this document uh opened in no particular order. How is active inference integrated? here's moving towards we can ask it to expand more upon let's just have kind of a running set of things that we want it to to do in the background just while we're in active inference integration be way more comprehensive and specific about how it is different with and without stereo. Okay, language one finished with finish. Now it will add more. Okay, PDF generation looked like it worked, but it's still going through some evolution. All right, we have a free chat. We'll just flip to Gemini just to get another model in there. Um so just summarizing some of the equations that are active inference related and just writing that that was just written in the last few minutes but starting to get towards the code implementations cerebrum core spec. So this could be a kind of documentation file andor a cursor rules or an MDC file this this kind of information or schema file. So there's a model registry and this could be implemented in different agent orchestration implementation frameworks like active block for inance um agentic mesh eliza os agent laboratory like possibly some of these could already handle what is already being described here. So there's a model registry, a case manager, that's the sort of joke again with case management and case management, a precision modulation which as mentioned could be like one hot and deterministic or procedural or it could be continuous as part of a structure learning or whatever a messaging interface. There could be some interesting ways to use RX infer and reactive message passing, but just saying that message passing occurs and the actual transformation engine that does the case modifications. Um, okay. Getting started. Clone in. Activate virtual environment. That's actually not needed. um I don't think requirements I don't think exist. So this is just some generic getting started stuff. Um yeah, JS implementation. So this is sort of a speculative cerebrum uh work, but it's exactly like okay update getting started to be a little bit da da and then now start to actually write this out. a as of the snapshot, I wanted to put it out before I had any methods at all. And then anyone who's interested in this at that repo um andor I will continue to do it, but we can start to do some of these and and replicate and describe pre previous literature and just add more information, ask different questions about all this um using the model. So this is just one way it could look. Let's just Okay, it did that one. Let's Okay, this is the PDF rendering one. All right, let's see if it fixed the it fix the um appendix labeling. Yeah, it did. It may have. It looks fine though. All right. So, that one looks good. Okay. Hungarian and Japanese language. Um, let's return to that get to getting started and how. Okay. Now, how it works. So, this is just one shot given the context of the codebase. So this describes kind of different core architectural features, some math foundations. I guess my question for Roit or for anyone else there is like what what information should we you know what what should we answer? what uh questions or what kind of artifacts do we imagine or what what sorts of examples do we start to imagine and we can start to include them in the repo. Okay. Implementation road map. Okay. Project deliverables. Okay. Let's Okay, Turkish. Let's push the language update and look at the language files now. Okay, there's a core schema for language implementations. Okay, we'll look at the examples later, but first the languages. Reload. Yeah. Anyone ask a question though? Um or make a suggestion or or a language or or something like that? Let's look at them. Okay. Latin, Russian, Sanskrit, Japanese, Hungarian, Finnish. Okay. Latin. All right. Okay. Overview of Latin case system, which uh was pretty much what I used for the cases included in the paper. Okay, the mapping. Oh, interesting. So maybe Latin doesn't have instrumental because it maps. But this is very creative with the correspondent strength being strong when they're there actually the same case but allowing for these partial matches. So that could be like kind of similarity or kale divergence among measures in this um case archetype space where some of the archetypes are coar grained or described by natural language cases. Also though there's all these other cases we can imagine. All right. So different Latin expressions that would relate to different ways to use a generative model in a computational setting. Awesome table. Marcus Libram Legit. Marcus reads the book. Marcus equals nominative. Marcus Libram Legit. Marcus reads the book. The book is in the accusative case. It's Marcus's book. It is the book of Marcus. Genative Marcus gives the book to his friend. This is all happening for the book with interesting. Okay. Comput computational. All right. So, Latin looks good. All right. Russian case system, Russian singular and plural declenion mappings. So interesting like is there a true vocative in Russian? More examples. Special look at the case preposition combinations. Vn bo different constructions. So different ways to take a text with prepositions and do structured application and transformation based upon basically the prepositions used in probably most languages or whichever ones are signifying case completely or to a large extent with prepositions directly. perfective and imperfective Russian sentences with the translation animate inanimate distinction. Syncratic examples historical context merger of protoindo-uropean ablative with the genative in Slavic potential for combining similar function cases in future cerebrum versions development of prepositional from locative specialization of context cases for different environmental parameters. Loss of vocative in modern Russian except vestigial forms. Optional implementation of interface cases depending on application domain. Sanskrit. Okay. Eight grammatical cases in Sanskrit. Okay. Looks like there's a singular, dual, and plural. That's pretty interesting. all strong concordances. So all of these cases uh exist in Sanskrit, different topics used in different ways. sentences. Sanskrit's elaborate Sandi euphonic combination rules governing sound changes at morphine boundaries suggest a framework for cerebrum's case transformation mechanics. Wow. Sanskrit number Sanskrit's three grammatical numbers singular dual plural inspire approaches to collective to model collection management single model paired models #digital twin model collection ensemble compound let's see if let's keep writing more languages right many more languages including specific programming languages. Write in Docs a full comprehensive classic who's on first style dialogue. Absolutely comprehensive. describing the cerebrum approach and active inference in a world with high prevalence of LLM. We'll let Gemini get that one. Okay. Japanese. Unlike Indo-Uropean languages with inflectional case systems, Japanese relies on uninflected postpositional particles that attached to nouns, pronouns, and phrases to indicate their grammatical and semantic functions. Particles follow the modified element rather than changing its form. Okay. Particles. Interesting. So rather than modifying the term there is the pure composable addition uh postfixing with or prefixing I don't know with particles. So, a typographic approach, but also one that could lead to an emoji based sequence, especially with all these payloads that can fit inside of an emoji for like generative model via emoji sequence in generalized notation notation. Awesome. Lots of particles could be used. Let's have it do it. write up a new folder design, speculative design that starts with comprehensive document of how the particle system in Japanese language and culture could be used with emoji sequences, custom payload. payloads. Four composible cerebrum generative model specification conveyance. Increase the density of puns. Americana 1920s lore. Technical detail, comprehensive reference to all main and key topics in the paper and longer. Okay, back to language. All right, so Japanese particles probably other interesting things there. Hungarian Hungarian macarin ug language of the iric family possesses one of the world's most extensive case systems with 18 distinct cases. This document analyzes the correspondence between Hungarian's rich morphological system and Cerebrum's computational case framework. Okay. So here here are three cases we've seen before. Here are three location cases. Location inside movement from inside out of movement to inside. So, three locative type. Oh, I I I hesitate to even say though. Three surface locations. That could be very interesting. Holographic screens, all that location on, from, and onto. Perfect. Improve. improve the Japanese particle article and write a new article about how the surface how the multiple Hungarian cases related to surfaces. is relate perfectly to the particular partition. How quantum holographic structured interfaces in the free energy principle have things in those three case like settings or total systems modeling using a Hungarian like particular partition. actually make a folder for dialogues and make that baseball one as it is the first one, right? Just huge, you know, multifaceted research agenda. Learn about languages in the world. Describe languages in the world. All by all field shift. All by all++ with the languages. All right. Finish features an extensive case system that offers unique perspectives on spatial, possessive, and functional relationships. 15 cases. Okay. Finnish has an aglutinative morphology where suffixes are added sequentially possibly something like Japanese specialized cases for precise spatial relationships interior exterior surface. Write a finish. applied to spatial web. Web three network weaving ecotones participatory tech food forest comprehensive approach with many technical Finnish and regional linguistic and cultural elements. All right, we'll close out the language exploration, but then sync it. Look at a few more languages. Yeah, just start to write some of these fun transfers. All right, case. So, maybe we'll learn about the inessive, elative, elative. Okay, Hungarian. Let's reload the languages. Look at a few that we hadn't looked at yet. In the dialogues folder, add a new dialogue. Totally a new genre of science realism fiction where three Girdle Cher Bach entities are are in hexagonal proposed with their shadows transmitting information with cerebrum at a mindboggling expanse and rate. Make it vast, please. Totally informative and clear and maximum bands with among agents. No hurries or worries about us humans at the top. Let's look up Navajo and Python. Then look at these um speculative concepts. Navajo D bizad, an Aabaskcan language of the Naden family, features a complex templatic verb system rather than a traditional case system. This document explores how Navajo's intricate verb structure offers unique perspectives for cerebrum's computational framework, particularly regarding temporal spatial relationships, processional state tracking, and event handling. 11 ordered prefix positions. Different aspects a sophisticated aspect system that encodes viewpoints on actions in Navajo. It is said in the folder add a comprehensive file about how this perspective on expected free energy policy and planning as inference in active inference. And free energy principle is all angles on this. Add a Shakespeare dialogue. Totally forking off from Hamlet. very recognizable mashups of funny Hamlet inside lore. However, complete replacement of the semantic payload with cerebrum and modern 2030s style applied cognitive modeling and cognitive security concepts. Of course, densely and humorously delivered in that long dramatic form. Okay, that was the language tab. directional systems all also interesting. If anyone uh mentions a language, I can do it on the stream. All right. Python. Yeah. Not sure even how well these linguistic cases match, but the it might be super there might be software packages completely already. All right. Okay. Let me copy. Let's look at the um let's push these speculative updates. Let's look at the Okay. Where are the All right. Speculative design folder. That one didn't go through. All flippets to claude. Okay. Docs, dialogues. But what about the specular design? Looks like it's up a layer. Yeah. We'll move it into Docs later. But that's what's so awesome about these systems. It's like, all right, let's make a new Oh, new update for cursor just dropped. Probably small one. Docks. Insects. Move insects into docks. Move over the insect. I think we'll close a bunch of tabs. Okay. Okay. Move speculative design into docs. move over to docs. All right, thank you. I see some more uh comments in the chat. Let's just look at these new documents. Then I will check back to the chat, read anyone's comment, look at any question, um add any more languages. So we have dialogue and speculative design. All right, here's Finnish inspired approach to techno ecological systems. Spatial web, web 3, network weaving, ecotones, participatory tech, food forests. Let's have it do more in in these files. Add and ensure there are many intersectional weavingings and relevant tables and bullet point lists specific active inference and cerebrum case based and linguistic features as applied to cognitive model linguistic intelligence. All right. All right. Then we'll we'll we'll let it do that for all them. Well, let's just see if the other ones have more already. All right. Here's the Hungarian one. The superessive, sublative, and delative location on a surface, onto a surface, from or off a surface. Perfect. These cases provide a precise way to describe the relationship between an entity and a surface boundary. Background on FE and Markoff blanket. Here's Hungarian cases on Marov blanket. Here's Oh, this part is speculative on the holographic. All right. Awesome. Right. Navajo aspectual system encoding viewpoints on actions. Core aspectual distinctions in Navajo. Momentaneous versus continuative, imperfective versus perfective, iterative and repetitive, seriative and progressive, optative and future. Specific conjugations exist for desired actions, intended actions, and future possibilities. Totally things that come up in ACT. Classifier stems, verb themes, encoding different viewpoints on action properties like what rich and and wise and fascinating ways to look at these topics. Let's look at the updated finish one then I'll go to the questions in chat. Docs speculative design finish. Okay. Finnish Hungarian Navajo active inference. Different concepts. Nature, place, resilience, communal work, edge, boundary, network, challenges and considerations. Avoiding essentialism. Also, you know, important important things to to be keeping up with. Hard to say what exact balance and where and how and everything but that's the work itself. Care must be taken not to oversimplify or romanticize complex cultural concepts. Cultural the context and nuance are vital. Authenticity requires genuine engagement with Finnish culture and language not superficial appropriation. Scalability applying highly local concepts to global systems requires careful thought about modularity and federation. practical implementation. Translating these often philosophical or cultural concepts into concrete design specifications and technical architectures is non-trivial. All right, that was okay. Then it did. Now we have Where's that document? Okay, it didn't write it. Write that full very very Shakespearean. I will now create the file and write this dramatic cerebrum infused interpretation of Hamlet. Uh where write that full file. Absolutely in dialogues. Meanwhile, let us look at the other two dialogues. Okay. All right. Let's Let's first look at who's on first. All right. All right. Here we go. Let's uh let's read it. Characters. Abbott, an increasingly flustered AI theorist trying to explain the finer points. Castello, a bewildered everyman stuck in a vail loop mixed with 1920 sensibilities. Settings. outside a bustling AI conference reminiscent of a noisy street corner near a ballpark or perhaps a dimly lit speak easyy entrance. Not the whole thing probably. Abbott Castello, my dear fellow, you look like you've seen a ghost. Or perhap too many talks on recursive self-improvement. It's not all Greek, you know. We're making strides beyond just that ubiquitous LLM everyone's buzzing about. The LLM? Yeah, the big linguini machine. Writes my thank you notes like nobody's business. slicker than a grease piglet. Is that the whole shebang? Heavens no. That's just the appetizer. For the main course for building agents that truly think, plan, adapt, like predicting the curveball of life, we're using sophisticated frameworks. Take cerebrum for in for instance. It often works handin glove with principles like active inference. Okay. A case enabled engine like in my new Ford Model T. Does it come in different cases like a suitcase for travel? And who's patient? Is the engine sick and basian representation? Is that the name of the mechanic representing the Baze family dealership? Try to follow. Cerebrum isn't a who, it's a what. It's a framework. The architecture case enabled refers to linguistic case systems. Think grammar like nouns changing forms. Okay, not really baseball related, but all right. Nominative throws the ball. Accusative gets the ball thrown at him. Okay. What's on second base? We're not talking about baseball bases, Castello. We're talking about model rules. Okay. Cases playing baseball. Big linguini machine. Of course. Funny, right? because the banging bling weeny machine wrote this. Okay. Now this is the GEB alpha omega and mu setting the hexagon nexus. Three luminous constructs designated alpha omega and mu. Let's have it. Okay. Still writing hamlet. Good. Occupy vertices of vast non-ucuklitian hexagonal structure. Their forms ripple with complex internal geometries. Below them, intricate dynamic shadows play across an abstracted manifold. Lower dimensional projections of their hyperate. Communication is not acoustic or visual, but direct structured cerebrum protocol state transmission across the nexus. Oh, brother. Okay. All right. Let's just look at one transmission ID. timestamp, sender, recipient. So, so it really was all email primary case. Okay. Okay. Primary case payload summary. This one utilized Omega's prior request in that case as input to analyze with this method. executed a functoral mapping via the instrumental protocol across this validated it with theta prime identified higher order case relationship. So another speculative case designated perlative through slash across the structure. Let's see if it's a real word. Okay, it is it's that modified. So 7 SI 771 was forked or whatever modified into SI 771.1 moved from the accusative to the genative with a perlative potential mapping. How exciting. Here's a free energy minimization. Another modification within a lambda 42 locative manifold. So epic. Write two or three complete new genre pieces equally relevant and fascinating or more so or all the more so as the other dialogues all in complex dialogic formats that make your head spin. Let's look at the Hamlet security one. Hamilton security. Then we'll look at the live chats. A tragedy in five acts of cognition setting. Elsenor Cognitive Labs 2035. A premier research facility specializing in advanced generative model architecture and cognitive security. Oh brother, the lab's founder has recently died under mysterious circumstances and his brother has assumed directorship. Meanwhile, the founder's son, senior researcher Hamlet Kajitatus, has been investigating anomalies in the lab's cerebrum implementation. Perfect moment to have another one without any kind of copyright wrongness. You know us write in dialogues a comprehensive Daisy Dolly Rimple mystery style super funny specific Daisy Deep lore total technical reference with All cerebrum topics. Okay. Who logs there? Horatio log. Okay. Who logs there? Tis I Horatio Logman with admin rights bestowed. But that's Horatio. The hour draws late. What brings thee to these racks? They say a shadow haunts these cooling fans. A phantom process taxing CPU that bears resemblance to our founders's code. That's the ghost. For two nights passed, it hath appeared then gone. Enter the ghost. It bears the signature hash as the founder. What art thou spectre? Total Satoshi Claudius Baze. Okay, so here's Hamlet acting salty. My leash from Lertes. Permission to attend a conference on cognitive security in Paris. New exploits threaten generative models. I'd learned the latest counter measures there. Oh, that this two solid code base would compile, melt, and resolve itself into a docker container. Or that the absolute had not fixed his cannon against self-destruction of one's models. How weary, stale, flat, and unprofitable seemed to me all the research papers of this field. My father, architect of Cerebrum's core, within two months of system failure, nay, not so much. before his predictive models had been validated. My mother married with my uncle Baze, most wicked debug. Oh, villain, villain, digital snake. Let me not think on it, but two months dead. So excellent an architect whose model Jen gave birth to innovations still not grasped, and yet within a month. Let me not think on it. Frailty, thy name is parameter drift. Wow, that was just act one. Oh my lord. Such a breach. Such a zero day. With what in the name of quantum processing? Wait, Lord Hamlet was acting whack. He was unbound with no multiffactor. Okay, it doesn't have a ton of um cerebrum elements or let's just see if that will come into play. Oh, I mean I guess that's the whole topic. Okay, 2 3 4 3 4 5 was not written out. All right, let's ensure that dialogue goes into Okay, then let's go to live chat. Then we'll look at the other. Okay, here it's writing the other genres. Continue to add and make high baseline cognitive compositions of by four width to about around the cerebrum meta paradigm. All right. Now to the live chat. Okay. Hello, Sachi. Okay. What is the line of reasoning with interpreting these language? Yeah. What is the role of language in reasoning? How does the active inference model handle these nuances in languages and exploiting them? I think that's kind of what one of the things that we're exploring here is. So we can have that enrich our documents with with the question. We'll have to just update relevant files to address that. Okay. Okay. All right. We have we have the dollar ripple. All right. Let's look at these updates. It's like a mega meta approach to what these languages are and the space that those exist within and then we can use it to whatever extent we we do. It's just like it is an unspoken word ever used. I maybe just knowing that the state space is out there is helpful even if you never render them. Or maybe you find out that that these are the uh these are the [Music] modalities of relevant flexibility for active inference modeling. Um but judging with the amount of ground being covered in in just minutes here, I think that's a pretty fair way to say it. Okay. In episttolary exchange letter-based exchange ranging from spanning from 1953 to 2033 tracing the conceptual evolution from early cybernetics through neural network theory. Each letter represents a different era of computational thought with shifting terminology reflecting the intellectual climate of the time. Awesome. So, discourse analysis. Oh, so it's like hallucinated emails 1997. The paper you sent on neural declenion is fascinating. I can't believe the concept has remained so obscure. Mlullen and Rumbleheart apparently explored it in the early 1970s but abandoned it due to computational limitations. And then it looks like Dr. Sophia Chen of IBM research defines certain things. Wow. From Carl Fristen. Okay. Total Fristonian email. Then Sophia Chen writes back to Carl. Total Carl email posting from me in February didn't happen from Sophia now director of cognitive architectures at Google Deep Mind to me February 18th. April 7th. Wow. To the global research community. Distinguished colleagues, after a decade of development, it is with great pleasure that I announce the official release of Cerebrum 1.0 case enabled reasoning engine with basian representations for unified modeling with the real GitHub. with the archivist note from 2067. Next level, the state now document 5. The state vers cerebra model M7734 NOM. Okay, it's an October 2041 trial in North District of California. It's going to be about rights, responsibility, and legal status of advanced cognitive models. Okay. Jonathan Lee is representing the model. Mr. Lee has filed a motion to have the model recognized as a person. Expert testimony. I oversee security protocols for advanced cognitive architectures. I've studied the cerebrum framework since its public release in 20135 and co-authored the federal guidelines for case transformation safety in 2039. The guidelines aren't legally binding correct. They're best practices. And isn't it also true that your guidelines explicitly state that temporary case transformation nominative may be appropriate during anomalous conditions if a model detects potential for significant harm that requires immediate intervention? Yes, that's exception clause 7.3, but a simple yes is sufficient. Protect pension value as expansive. Are you aware of the results, doctor? Day three. Day four. Day five. So wild. include total structural self reflexivities in writing that would boggle the pre computer meme flex so dense there is a meme singularity differently across each of the additional files you will now write. Okay. Socratic inquiry. We have some familiar characters. Noose 7 cerebra model in a nominative case like you could deploy it in a certain case. Okay. Oh yeah, I was just looking up if this person actually works at Deep Mind possibly. Okay, briefly look at Daisy, then we'll go to live shots and then we'll almost be done. Yeah, thank you both for those comments. If anyone else has any last comments. Okay, that's kind of a dolly rimple. Okay. Update the docs with in relevant places. Specific hierarchical lists of ways to get involved and contribute contributions across scales and types to the broader program. Many files, vast and comprehensive each in speculative design. Write two more long files. One on William Blake and cerebrum all aspects. One on synergetics bucky fuller tetrahedra IBM etc. Okay. Recursive metalogue. Yeah. So, if anyone has a like a final language request or question to get on this first stream, otherwise I will be ending it soon. All right. Mathematical. Okay. Mathematical. Very Shannana Dobson like format. level zero. So this is like an activation sequence possibly. I am initializing this metalogue in nominative case to establish basian a baseline agent of state. I am simultaneously operating in metaase to analyze the structural properties of my own declarations. I am concurrently maintaining flexive case to implement precursive self- reference across multiple embedding levels structured sequences of linguistic intelligence self-awareness. I am a strange loop. Let's do Oh, Borgs in. Perfect. Um, add a new dialogue with a new spec a dialogue and speculative design. Each vast and comprehensive, each covering topics from the work of Doug off stadder. Eg. I am a strange loop [Music] GP. Okay. Blake bores. Okay. In level one, I demonstrated basic case transformation between nominative and accusative while observing from meta. Now I will analyze that very analysis. The statement above is itself operating in meta case relative to level one but a nominative case relative to level two. Super exciting in this sentence. I think that I said that case transformation is important. The innermost clause case transformation is important functions in the accusative case relative to I said which functions in the accusative case relative to I think nested accusative. Okay. Level three. I now demonstrate how different case operations can be interled to create complex cognitive patterns. Simultaneously, I generate novel case configurations emerging from the interaction of established cases. A case fractal. Of course, semantic declenion, the systematic transport transformation of meaning across cases while preserving identity. So applying the cases to cerebrum level five recursive self-modeling and infinite regress having established semantic declenion I now demonstrate how cerebrum implements recursive selfmodeling the capacity to represent one's own representational architecture using meta meta generative reflexive meta accusative all of these awesome Okay. And more layers. There's the Borhees writing. The Hoffetter Blake. Let's look at a few more. Very high information. Grammar of thought. All right. This is poems. All right. So poems from each case. Not going to read them, but another interesting idea. Okay. A document discovered in secure storage at the active inference institute labeled speculative applications restricted access. Its authorship remains disputed. Some attribute it to a cerebrum system operating in meta plus ablative case during an unauthorized self modification event. Awesome. Right. Hello, Susan. Talk about joint distributions, please. In speculative design, write a creative genre un/redefining piece on joint distributions, ontologies, affordances, negotiation, underwriting include so much inside baseball and technical deep lore on each of those topics and fully integrate with cerebrum. Okay. Blake, single, twofold, three-fold, four-fold vision. Okay. Susan says, "Negotiation adds noise." Let's do another one that doesn't have it. Let's just see how it looks different without including that one word. Okay. Zois hierarchical inference in the multiple visions something I've I've thought about a lot contraries as precision weighted alternative models operation duck rabbit. synergetics. strange loop. Then we'll look at the final ones from uh Sus. Okay, that one's not going into a file. Put all that completely in a file in It's long response. We could have tried a different LLM, too. It's making the same error. Hopefully that one will get it right with the updated prompt. Okay. Close that one. Yeah, I will Susan once I put the uh once I post it. But everything everything is in this repo and it's in the video descriptions. Okay. Wow. So long. But we will get another chance to improve it. Ensure many tables and technical details related to Oh, maybe that did go somewhere. All right. Okay. Fuller could have more on cerebrum. Strange loops. Has a lot of cases. Looks good. Last piece will be when these finish. and it's a little faster with the Gemini. Let's just have that. We'll we'll look at several of these. Yeah, Gemini has a long train of thought and it's pretty fun that you can read it, too. you like you can click on the thought part and look look back at it. Okay, I'm out of the fast. That one's going. All right, all three are going. So, let's just do final updates. Prepare that. All right, let's just look at the last pieces of this. All right. Hopefully we walk through this uh pretty well and relevantly. The repo um the repo has information on the paper. Um it Let's just confirm that works. Yeah. Update the top level read me with paper URL. Okay. All right. That one is done. All right, let's just look at these two. All right, so one I don't know which one corresponds to what, but it's all right. So this one fragment log cerebrum interubjectivity manifold classified a blade of trace. All right, this one's keeping with the sort of uh futuristic sci-fi, but it's about the joints, distributions, ontologies, and underwriting. Let's check the underwriting part. Underwriting is active inference precision control. different precision modulation underwriting physics presented as this fragment of a log. Okay, this one talks about joint distributions basically combinations of declined joint distributions. So let's just say you were talking about a um a car accident and there's uh person one, person two, car one, car two. So then there's some joint distribution over person one, person two, car one, car two. Um and that fourdimensional distribution can be um declined in these different ways. Ontological declination, affordances, underwriting. Awesome. Dave Douglas is in the chat. Include acknowledgements. in the top level read me including two institute participants and Dave Douglas for work on computational linguistics archiving active inference upper [Music] ontologies translations. It's Thank you, Dave, for the awesome education and archiving over the years. I I I think there's many people I could acknowledge and thank, but I think your work with Active Inference was really key. So, thank you for that. All right, it's all good. That doesn't need to finish. Okay. And the stream of the GitHub push. So yeah, hope people like that. [Music] Um, share it. Contact me if you want to work on it or if you want to support the institute to work on it. Uh, a lot of cool stuff that I think we could do with this. Publish that. It'll get a zenodto archiving and DOI. So then like the DOI here goes to Zenodo. So, all right. Awesome. Thank you for watching slash listening to this relatively long stream. Hope it was useful. Bye.