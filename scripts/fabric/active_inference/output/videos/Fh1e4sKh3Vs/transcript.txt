hello and welcome it's January 26 2023 we're here in active inference model stream number 8.1 today we're appreciative to have Thomas ringstrom who will be presenting on reward is not necessary a compositional theory of self-preserving Agents with empowerment gain maximization there will be a presentation followed by a discussion so Thomas thank you for joining really looking forward to this off to you yeah thank you very much it's really nice to be here and talk to this uh to this group um I'm uh a computer science PhD student at the University of Minnesota uh my my interests are in sort of what you know what are the composition what are the uh sort of computational properties that we would need to have uh agents which um flexibly plan in sort of high dimensional you know product spaces of variables um and also how do we get agents to perform complex tasks uh in an intrinsically motivated way especially in in high dimensional product spaces um so this uh this presentation is going to argue that um reward uh and I know I'm talking to a sort of active inference crowd um but uh some of the same points apply to active inference perhaps too but I this this presentation is is mostly about how there there's going to be some major problems I think uh using reward maximization objectives and by moving to sort of reward free objective functions we can get really nice um factorizations that help us plan so let's just start off with a sort of simple picture a simplified picture of an organism so we have a honey badger here and um so this honey badger has internal States it gets hungry and it gets thirsty and there's also an external World um that the agent lives in and you know in order to sort of modify internal State spaces the the agent might have to perform some complex tasks um it might have to get several items uh in order to you know eat eat an apple or things like that so you could you could see that maybe maybe um some symbolic State space sort of mediates the connection between things you do in the world and Transformations that you make on some internal State space and of course real you know Real Worlds um organisms and humans are live in a high dimensional world so you could imagine that there's an encoder and a decoder and really you know there's a sort of high dimensional uh physiological or interceptive um domain that that is these sort of Y tilde with the dot um for y tilde I should say and Z tell them and then there's you know High dimensional worlds you could just say that the that's X tilde and really you just need to sort of uh map these down to some discrete State space in which uh these domains can uh interact so yeah here here's like a simple encoder and decoder and and so if if an agent has a kind of simple ontology like this you can imagine that it's sort of generatively entrained to the world uh that it lives in uh which is a notion that's probably um pretty familiar to sort of active inference uh people um so you know you have some encoder of these high dimensional States and then you have some uh operator PS which is in the middle of the head here and that just sort of advances the latent States and then you decode and you'll get a sort of expectations of of the high dimensional world that the the agent lives in and so the problem is is that you can't really represent explicitly these latent space transition operators that would dictate you know the Dynamics of of all these variables you can't represent it as an explicit object because the more State spaces that you keep track of in the world uh the larger this uh object PS becomes so in order to handle this you'd have to represent it in a sort of factorized form so you just would represent its factors you wouldn't explicitly sort of enumerate all of the state Vector transitions under under this transition operator and so what we really need to think about is you know what are the sort of model based or the sort of bellman principles for decomposing hierarchical State spaces um so that we can create the right representations that help us plan in a flexible way in a sort of time to time varying time dependent uh world of lots of lots of variables so this just means um you know what is a sort of objective function L telegraphic L applied to PS or its factorization that would give us um that would give us some factors here uh Ada and and these omegas that could help us plan and also how you know given the fact that you know the state space the effective product space that we're working at is so large and there's many possible State vectors that describe possible states of the world how do you know what to do I mean if you're an RL theorist you would have to say well there exists some kind of reward function that tells you you know this state of the world is is worth this much reward in this state of the world is worth this much reward but it's not really clear how you should Define reward functions on huge product spaces so I'll address this a question from uh sort of reward free perspective and the just to give you a hint of what it's going to be like what I'm going to argue is that what we really need are agents that have a kind of structured core ontology that it needs to maintain that is coupled State spaces that really depend on each other in which the policies that you use maintain the sort of internal Integrity or controllability of the agent um so and that's where empowerment will eventually come in so I'll talk about empowerment a little too that's the controllability metric so um here I'm just saying what's a nice policy or what's an objective function f that takes in some intrinsic motivation function which is this fancy V here along with a nice factorization that that allows you to plan in the world okay so just to recap things that I already said um we're in a product space of uh state of variables we have internal and external State spaces um you know hunger and hydration State spaces can be dynamic as you do things in the world you get hungrier um so how do we plan how do we know what a good goal is and what I'm going to argue is that we should compute a specific representation called estate time feasibility function which is going to be an abstraction uh that's going to map us from initial State times to final State times and this will have some really nice properties that allow us to sort of reason in this High dimensional uh State space and I'm going to talk about this from a just a dynamic programming uh point of view there's there isn't going to be any learning okay so let's just talk about transition operator composition because I said well we have this large product space State variables and it'd be nice if we just represent it as a factorization so what does that mean um so imagine that you have PX which is like a base State space to move around the world and you have some internal State space or secondary State space p y and these are linked by some function f so here's your base State space and uh here's the honey badger in the state space and here's the secondary State space which is just like a you know your internal hunger space um and so F what f is going to be is called an availability function and an availability function is going to say oh this you know this honey badger is in state x a t and at time T and given that it's there um what's the probability that this goal is available from this State time State action time and so this goal is formally going to be an action on the higher level State space so for instance these green lines on the uh high-level State space are going to map you to you know the most satiated uh state where you're not hungry anymore at all and then these black lines on the on the internal State space are just going to decrement you you know one States if you're if you're not at the tree so you eat apples at the tree and that's going to map you to the top and all other states without a tree you're going to get hungrier over time okay so if we wanted to write a product space operator uh you know PDS you know y Prime X Prime uh we can just represent this as a composition where Lambda p is our composition operator and it's just going to be defined as the product of uh your two State spaces with f linking them and then we sum over G so we're getting rid of the goal variable and so we can represent the uh the product space this way and we can also just call the product space we can say that s bold s here is just the state Vector for you know Y X or if we have more variables that can be incorporated into bold s Okay so what if we have more features of the world we can drink water or we can get warm at the house um well we will need to have a a bigger composition so we can just create an operator called we'll just call PR or bold R is just a state factor of w y and c and W Y and Z correspond to the hydration uh the the hunger and the temperature space and so this is just a product of the individual ones uh individual operators and so this influence graph is connecting PX to uh PW p y and TZ through F and so we can just Define the product space operator as a composition this way and if we do that you know you start to realize well this is nice because the effect of State space you know exponentiates essentially you know as you add on more State spaces that you can control and so this influence graph is just showing you know you know what is the sort of ontology of the agent what is what what constitutes it uh as its internal and external coupling and you could imagine that it gets more complicated for instance that if you hit you know w0 or y y zero or whatever that you you these skull and crossbones indicates that you die so you can imagine being in a state has a bi-directional influence so Zeta here is um is conditioning the possible Dynamics PX can produce so you could imagine that you know once you're in one of those sort of defective bad states that it kills you and you can't move around so you can imagine you know sort of more complicated uh structures like this which essentially mean that you have to go out in the world and do things in order to keep the system alive uh so you don't want to hit W you know w w naught or Z naught um and you can imagine this gets even more complicated you could compose sort of larger structures where P Sigma is going to be some logical State space that keeps track of you know multiple conditional events that need to occur to for say you know say eat eaten apple or something like that um so representing it in this form is is very nice we just represent the factors and the the links between the state spaces and that's a sort of a sort of memory efficient way of representing the space okay so if we have a homeostatic task um I'll eventually get to the the sort of bellman equations in the model based form formulas but first i'm just going to sort of uh build up an intuition in the form of an example about how these State time feasibility functions are going to work the the state times usability function being the the representation that I'm arguing for in in this talk so if we have a hiker um and it can go into the world and drink and eat and get warm uh with these goal variables well there there's also um this goal variable G Epsilon which is going to uh decrement by by one uh by one state then as the uh as the hiker moves around you can you could represent this as a function so imagine that the hiker starts at xg1 that's the house and TS which is the start time and if the hiker follows a policy Pi 2. so imagine that you have a policy that is a goal like a gold condition policy Pi G2 that's going to take you to xg2 okay so it's a policy it's like a shortest path policy or whatever um then that means that there's going to be some final State time xg2 TF uh that you achieve this goal g-drink which remember is is an action on the higher level space so what this would look like is that you decrement two rights because that takes two time steps to get to the leg and then once you get to the lake you take the state action that's going to drink and it's going to bring you up so there's there's three total steps in this uh process of uh inducing the drink goal variable and then the step after drinking which would look like this and then you can imagine well the other state spaces are are not involved and so they will all decrement three because there's three time steps and so one way to represent this is is to realize that uh if all of these goal variable sort of goal variables on the way to the um to the lake are sort of the null goal variables meaning that the agent isn't affecting some other state space then you can Define that as a Markov chain so if you set you know all of the goals to be uh the Epsilon goal the null goal then this py Epsilon is going to be um a Markov chain Matrix and so that means that the time difference um encoded in the state time feasibility function the TF minus TS is going to be the power that you can take this Markov chain to in order to forward evolve the uh this the internal State spaces all the other internal State spaces so you can define a an operator that does this in one step called Omega Y which is just going to take some initial States so this Green Dot or this Red Dot [Music] um and it's going to take the time difference so this is going to be 2 to get from the house to the lake it's two time steps and so these uh gray arcs are just a an initial to final state map under this policy and so what this looks like this is sort of looks sort of complicated but you can build a jump operator that jumps you from your initial High dimensional state for instance y x t and we're just considering why for now I don't have the other state spaces uh w and z in here but uh this Ada over here is is mapping you on the X space the Omega Y is mapping you forward to in in the uh in the y space and then the py and the p x are just evolving by one step um to update after you hit the goal so after you hit the goal you have to update by one step so uh p y and PS do that for both state spaces and then of course you can do this for all the state spaces so a jump operator for all of the internal State spaces are where R is you know remembers the vector of all your internal States w y and z then this is similarly defined where umega Omega R is just the product of all of these other omegas so we defined Omega y up here but you can just Define this for all of your other state spaces so now that we've done that we can continue our journey by going to the tree and so that's two steps away so all of the other things decrement two and then an additional one and then if we go back to the house that's four times steps away plus the the additional time step of uh entering the housing and getting warm okay so here is another route you could take uh where the uh agent the hiker goes to the tree to eat and then the hiker goes to the lake to drink and then goes back to the house to get warm and so this is this JS is a huge it's important to keep in mind that JS is a huge operator and we cannot uh explicitly form it in memory uh because it's unless we have you know a lot of memory on our computer um but having this factorization allows us to chain policies together and evolve a high dimensional State vector in these jumps so we're we're jumping a state Vector around in this object-oriented fashion where each state Vector is updated once you get to some key object of Interest such as the tree with the the apple or the lake with the water so um also in this uh example you can see that it the the agent went from the lake to the house and um technically if Z it it occupies the lowest State Z zero and so technically it should die uh at least skull and crossbones on the map but we let it finish uh his journey and we haven't talked about this sort of bi-directional coupling yet but we'll we'll do that in the next slide to um to make that concrete all right so if we have these defective States we can define mode parameters e plus and E minus which um are just going to be variables that condition our Dynamics and we can have a mode function that takes any Vector r uh and so an R is any Vector of blue green and red squares in the space and we can map it to a mode parameter so we can Define Zeta to map to the good mode the the normal healthy mode um if it's not occupying any of the defective States but if one one or more um you know defective states are occupied then we can map it to e minus and so that means if we have some you know low level transition operator that's indexed by E that means we can split it into a normal Dynamics p e plus and a defective Dynamics p e minus and you just know notice that the defective Dynamics is the sort of identity operation it just sort of arrests your Dynamics in the space so you can't move around but the normal Dynamics is just normal grid world uh movement okay and so what we can do is we can compute a set of individual feasibility functions for each object of interest in the map so in in the previous example we had you know two trees two lakes and a house so it would have five state-time feasibility functions and then this Ada hat is an aggregate function of of all of these uh functions where the pi is indexing the corresponding Pi for that function of anyone can ask questions by the way if they have any questions all right all right so the full Dynamics is going to be just defined like we defined it before where we uh we have uh these two State spaces the base State space coupled to the internal State space through F but then we have this Zeta uh R here on the conditioning side and so this is a sort of uh a a an operator that you sort of have to uh you have to achieve goals in order to keep yourself out of uh these sort of absorbing defective States and this is and I showed this um influence diagram before but this influence diagram uh is capturing uh this coupling Okay so um many of you will probably be familiar with the standard Bellman equations um where which are you know formalized this way where you have some reward function plus some discounted expectation of your future value and so the value function in a sort of standard Bellman equation says how much reward am I going to get if I act um here optimally um over an infinite Horizon and so the reward function is that's often thought to be a task it has that sort of semantics in in RL land um and then the optimal policy is just going to return the action uh that corresponds to the best action that maximizes your value your long run sum of rewards that you're going to get so this is a a recursive equation and but in this uh my my thesis is is is that we need to rethink the sort of model-based bellman uh formalizations in order to compute these nice factorizations that move us around High dimensional space like we've been talking about on the previous slides so we're going to formalize some new development equations called operator development equations and these are going to be uh non-stationary development equations or they're going to be functions of time so instead of having a value function that says you know what's the accumulated reward that I get I'm going to have a cumulative feasibility function Kappa and this is going to represent what is the cumulative probability that I achieve a particular goal where a goal remember is an action on a higher level space so this has a very similar form to the infinite Horizon development equation that I just showed but you'll notice that we have this availability function here for a specific goal so this is a single goal G that we're picking out so say the uh the Eden Apple goal or the drink the water goal we're just choosing one and so f is um returning the availability of G from any given State and time and action and then so it either the agent achieves a goal now which is what this equation is saying or which is the plus the agent does not achieve the goal so one minus the probability of achieving the goal is the probability that you you don't achieve the goal times the expectation that you achieve it in the future so it has this similar recursive structure that the infinite Horizon development equation has okay so then we have the policy and this policy equation is a little different from the last one because you can imagine that you can maximize the cumulative that you achieve a goal but you you sort of do it at the last second you have a lot of you have maximized certainty that you can get to the store right before it closes so you have you know you know 100 that you can achieve the goal of getting an item from the store you know five minutes before it closes well it might be if the store is open for some you know period of time that um essentially you might want to get it as soon as possible so what this equation is saying is that we're going to pick the action which maximizes the cumulative but from that set of equivalent actions that maximize the cumulative which is a star so we're collecting the actions that maximize the cumulative and we're going to pick the one that minimizes the time so this is a sort of conditional optimization that says you know subject to the fact that we want to maximize the cumulative we want to get there as fast as we can um and so normally there's just you know with the infinite Horizon development equation there's the value function in the policy here we have a third um uh function which is the state time feasibility function which I've been talking about and the state time feasibility function says given that I start at x and t what's the probability that I achieve a goal G at a particular State final State and time XF and TF and so um you can compute this via dynamic programming as you're Computing these other functions and um there's a relationship between the cumulative feasibility and the state time feasibility which is that if you you can sum up the individual spinal States and times in that uh from your state time feasibility and that is the cumulative so the cumulative feasibility it's just summing over the individual probabilities of a of a given State final State time and the nice property is that when you move when you use these operator Bellman equations on hierarchical state spaces such as the product space um of the agent's core ontology um it has a nice decomposition property which is that if you're just solving to go to a particular point in the world then you can actually you compute Ada separately from the high level space so you just solve for for Ada the state time feasibility function on the low level space but then Omega can be computed independently in the high level space and they can be combined so this decomposition property is really nice because we never want to work in a product space especially as we learn tons of Dynamics about how the world works we need to we need to be able to compute representations in a factorized way which is going to help us move around uh in reason so the this means that you know when we compute um a bunch of uh individual State time feasibility functions for each goal I don't know why this little hat is here but um then if we have you know five of these and we have an aggregate feasibility function then we can use this aggregate feasibility function to move around from uh from feature to feature in the low level World and update all of our internal and higher level States and so this is the object from the previous slides that that sort of map us around the world so you can also do and I won't talk about this much but you can also you know do logical tasks where instead of eating an apple you might want to gather an Apple so if you if you go obtain an apple then there could be a bit that corresponds to having an apple and you flip that bit to one once you go to the tree and then you go gather water at the lake and you flip that bit and then you go to the key and you uh obtain that key and that might be a task where you have to do these three you have to obtain these three items perhaps because you wanted to compose this task with something else you want to you know bring these items to another agent or whatever so the point is is that you can use these factorizations to move around the world especially when you know things in the world have a sort of limited time period in which they are available so um so yeah I talked about how we can plan in these high dimensional State spaces uh using this factorization and now um we get to the question well why should I plan to any particular High dimensional States that's the intrinsic motivation question you know given that there's an exponential number of State vectors in a product space Y is one why is one state Vector of the world you know in the far-off distance better than another so that's where empowerment comes in empowerment is an intrinsic motivation metric which sort of represents controllability so formally empowerment is a function of a transition operator it's a it's an intrinsic uh measure of an intern of a transition operator and it's and you can also condition it on the states that an agent is at so it takes two arguments it also has a horizon n which we'll talk about in a second um so it's formally defined it's the Shannon Channel capacity between your actuators or sequences of actions that you take and the resulting States so the channel capacity uh so here you know an open loop sequence of actions would just be a you know go up right and up again or up left and then down um there's a whole bunch there there are a lot of possible sequences of actions so big a here is a random variable for your sequences of actions and so for a horizon n you can ask what's how much information can we transmit from our actuators to the resulting State at time Tau so Tau minus uh Tau minus t is our Horizon uh n which is this parameter on the empowerment so it's just saying you know how much what's what's the agent's capacity to affect the future with uh certainty or or varying amounts of certainty and so the channel capacity is formally defined as the maximum Mutual information Givens uh distribution over these action sequences um and the mutual information decomposes into the entropy of the final States minus the conditional entropy of the final State random uh random variable given that you know given the actions and and your starting state so this means that there are two sort of like extremes to empowerment um if PX is a deterministic operator and so anytime I'm at State X and I apply an action I get a deter deterministic output X Prime then um the conditional entropy there there's going to be no uncertainty over my future States so the entropy is going to be zero there's no uncertainty and so that means that the conditional entropy has to uh cancel out and so empowerment is really just the maximum possible entropy given this distribution and this just reduces down to the log of the number of possible reachable States so how much can I actually reach in the world um and so you know if my Horizon is two we can see that the empowerment here is just log of 13. so I get there are 13 states I can get to and I have perfect control so I can actually realize any of those 13 states if I want and the Other Extreme is if like if PX is action independent meaning that any sort of state that I'm in I have an action that maybe there's a the same you know distribution given that action say a uniform distribution [Music] um then you know my conditional uh my uh and my conditional entropy given the actions is just going to be uh the entropy and so the empowerment has to be zero because this term is going to cancel out um and so there might be a lot of States you can technically reach because I can select actions I'm just going to go random places so pink is all the states I could possibly reach but I don't have any control over which state I'm going to result I'm going to end up in and so I can't influence my future in any way um even though there are a lot of possible Futures so empowerment in that extreme is zero and then there's um you know an in-between Zone where you know you take actions and there's a bias in One Direction or there's different distributions for each action and so there might be uncertainty but you can sort of control how much information you can sort of control what state that you want to end up at okay so what I'm going to Define is a function valence um and it's important to note that you know there's there's two arguments uh the states and you can include the time too and the conditioning side so you have empowerment uh and um so you can define a diff an empowerment difference so say you start at uh s and t and then you um end up at a future State and time after you've execute some sequence of policies so row here uh appended to the s t is a sequence of policies and S row and T row are the resulting States then you can compute an empowerment difference and this will be our valence function so Q here is our is a function I'll talk about in in just a second but we can see that um our our jump operator that's moving around High dimensional space is defined as this factorization so we don't have to represent it remember that's an important part and so we can sample you know multiple policies from this or chains of policies so if I have policy one policy two then I'm going to have some resulting State R double Prime X double Prime TF2 um and so Q here can just represent out the output of chains of policies so you can imagine you know a tree search of chains of policies and Q is just summarizing the final state of those branches in a research of policies of chains of policies all right so we can use this Q right so notice that this Q here is in the expectation over here so uh it's linking our original state time to the final State time so it's the empowerment after some chain of policies okay so if we have [Music] um uh so say we have a deterministic operator and this should be a p um then valence is an empowerment difference we can have a simple sort of example here where we have two hikers that are considering two different plans and so if this tree here is like the space of um you know two possible chains of policies then these hikers are just executing you know one sort of path through this tree so the um the hiker on the so they both have the same empowerment because they're starting at the same state and they're both too away from starving so they if we just consider an empowerment at the beginning that's a log of 13. and if we chain together two uh two policies here we can advance our um internal State space and the agent will end up down at the lake and so given that it's at the lake you'll notice that it's one state away from uh from dying so that there's effect an effective range of what it can uh where it can go so the empower the final empowerment is log five because there's five reachable states and we're assuming determinism to make this easy so the valence here and this should be PS uh here but I have t here but the valence is just log 5 minus log 13 and that's the difference between the final and the initial and so that has a negative valence uh the the hiker the yellow Spirit hiker is clearly in a worse position that he started off at but we can advance the uh other hiker with uh Pi G3 to the lake and then Pi G4 to the tree and updates his internal States and we can see that he's three away from dying so there's a bigger effective range um and so there are 25 states that he can get to and [Music] um so log of 25 minus log of 13 is 0.94 which means that uh he is he's better off than where he started and so clearly um the second hiker is executing a better plan um yet he has more freedom to uh to engage with other tasks in the world and so if if we you know searched over this entire tree of policy chance uh we could pick the best one uh and then in this example we're just going to consider two two of these uh branches and we'd pick policy IG IG hi G3 Pi G4 all right so there's also an another interesting so in in this past example we're just sort of changing the structure we're just changing the initial state um but since empowerment is also a function of a transition operator and our operator Bellman equations are producing transition operators that map us from State time to State time then the output of those Bellman equations the operator Bellman equations produce transition operators so we can compute their empowerment uh so that's that's a deep connection between development equations and these intrinsic motivation metrics so we can do interesting things where we can say well if the structure of the world were different then the feasibility functions that I could compute in different configurations of the world would be different so if if the honey badger gets a key and it opens a door in the mountain pass then it could it could potentially get through the other side but it's important to note that this is changing this the structure of the low-level State space so p e naught is what we'll call the the original transition operator in which we can't go through the mountain pass and that means that if we compute State time feasibility functions kit on this operator that means PPE here is going into the development equation then we're going to produce you know State time feasibility functions from those operator Bellman equations that's this uh object here which means that we can use it to construct J to move us around High dimensional space and so there's a an empowerment for this J but then if we get the key and the key allows us to move through the door it changes the structure of the world then that's a different um that's a different mode of Dynamics so that means that all of the supplies on the other side where we can compute you know feasibility functions off of a different mode of Dynamics in which we can move to the mountain pass and so that means we can compute things you know um by compute valence by asking you know is is this configuration configuration of the world more conducive to the agent's core ontology that is the agent's internal external coupling that needs to be maintained um as a as a sort of core object [Music] and so we can compute valence just by changing J the structure of the agent's abstractions that it moves to go to um perform tasks in the world so by Computing empowerment on J we're sort of computing it in task space we don't have we don't have to consider all possible states of the product space we can narrow it down to um to operators that move us around task space that induce Dynamics on you know key other state spaces that we care about maintaining such as our physiological State spaces okay so here here's a simple example say uh the honey badger uh starts at the lake and has some initial um empowerment uh just on the low level operator so that's p uh five zero uh there there's 12 states that can reach but it also has a task empowerment um which is actually zero because there's only one task that it can in engage at uh in if you don't include getting the key as a task so um the circle here is saying that there's only one one sort of task that can be done and so if the agent goes and gets the key uh that's going to it's going to uh you know reduce its physiological States because it had to travel there and then it gets the key and it uh conditions a different mode of Dynamics so there's different feasibility functions associated with that mode uh which I just described on the on the last slide then uh the the door will open and it can travel back to the lake and so now that it's at the lake it can uh go uh eats food on the other side right so before getting the key uh if it couldn't get the key if there were no key then it would just starve because um well it could drink water and stay hydrated it couldn't get it couldn't eat from the apple tree but now it can cycle back back and forth between the apple tree and the lake uh for as long as it wants and so it has a higher empowerment just on the uh on the low-level State spaces but also has a higher task empowerment because there are you know just over a horizon of three and we're just Computing uh where we have to choose a horizon so the empowerment in task space is eight because there's eight possible branches in resulting States from from where it is and so that's very useful um so we can compute the the valence which is uh 0.5 just in the in the low level space but also three in the task space and another interesting question which is um very important is to say well what's the value of the key and you can compute this too you can say well given if I just fix a state Vector that I'm at and I just alter the state Phi that encodes the object of a key if I just alter that state and switch it between having a key and I event key you can say well this key is has this much value to the internal organization the internal Integrity or controllability of the agent so it's a sort of agent-centric uh it uh Judgment of how much something in the world is valuable and so the key can be not valuable if it doesn't do anything in the world that helps the agent control its core ontology so this is a way in which you can sort of bootstrap uh value into the world you can use it you can use the change in an agent's internal structure it's coupling between the internal State spaces and the external world you can use the changes in that structure to assign value to things and it's very useful okay I think our we're approaching an hour and so I'll just conclude by saying um intelligent systems operate in high dimensional product spaces often with non-stationary Dynamics this this introduces a lot of problems um especially in artificial intelligence because people normally deal with structured tasks and non-stationarity by training you know recurrent neural networks and things like this which take which contribute a lot to sample complexity and what I'm saying is that you know operator Bellman equations have this different form which produces transition operators which helps you factorize um your representations for moving around the world and predicting the resulting High dimensional State Factor and these operators are composable they compose with themselves but they also compose with higher level structure so you can remap different transition structures to them it's very modular and these are very nice properties that you that you need uh if you want to if you don't want to recompute things and you want to have sort of modular structure come in and remap to your representations that you've already computed and then so forward sampling uh can you still hear me Daniel okay just checking so forward sampling is a good way of solving a problems in high dimensional State spaces without representing the product space we can't really solve we can't do dynamic programming in a huge product space that's not going to work we can't sample low-level actions that's not going to work it's the the tree is too big but we can uh we can work at the level of sequences of policies and we can evaluate empowerment gain to justify our goal States and so valence sort of unifies a lot of distinct drives like there's a different sort of subfield of RL called multi-objective reinforcement learning which says oh we'll we'll have a bunch of different reward functions and then we'll have value functions for each of these reward functions for like different tasks and that'll make like a high dimensional value function Vector space um and usually in in like multi-objective RL you have to like pick a policy that that's in that you know that's does well um in that you know value function space but normally you have to uh you have to deal with the trade-offs by some waiting function so what I'm saying here is that it this allows a because valence is just one number and it summarizes an entire sort of control architecture that you don't have to have introduce things like waiting functions or weighting coefficients to say oh this objective is more important now or this objective is more important now now um so yeah many latent drives is not necessarily multi-objective it's multi-dimensional it's multi-goal but it doesn't have to be multi-objective um uh and so yeah with empowerment you don't have weighted combinations of empowerment and valence uh depends on the structure of the environment so it's um it's it's not just some static property of the world or a static property of an agent it it incorporates uh agent World coupling and I thought I'd just um end with this quote from Terence Deacon um who wrote a great book called incomplete nature which I love and I read it at the beginning of grad school and which inspired me a lot and Terence Deacon wrote a lot about teleology from a sort of thermodynamic perspective and it's really compelling um and I just liked what he had to say about teleodynamics the the idea that you know that an organism could be or Its Behavior could be organized around realizing you know something which is sort of virtual and he says teledynamics is the dynamical realization of Final causality in which a given dynamical organization exists because of the consequences of its own continuous and therefore continuance and therefore it can be described as being self-generating specifically it is the emergence of a distinct a distinctive realm of orthograde dynamics that is organized around a self-realizing potential or to be somewhat enigmatic it is a consequence organized Dynamic that is that is its own consequence and I think I think that's relevant to what I'm doing here because um I think empowerment sort of on a an internal sort of structured ontology allows an agent to say there are multiple there there's a huge space of possible Futures but I can evaluate you know a state of the world that's far you know far off in the distant future and I can organize all of my behavior around that because I can say I can give an explanation for why it benefits uh my sort of core ontology and so therefore it makes me um capable of acting that way in the future it's a consequence organized Dynamic that is its own consequence so um with that I will I just want to thank the active inference Institute um and I will take questions and I'm very interested in you know what active inference uh theorists think about to sort of potential for a sort of integrated view of empowerment because I think you all have a lot a lot of experience thinking about generative models and things of of that nature so I'd be very curious to know what you think thank you excellent thanks a lot great presentation so this will be a fun discussion those who are watching live please feel free to add questions in the live chat and you can unshare your screen and we'll begin I guess I'll take an empowering deep breath and ask a general question and then I have some scattered notes that I'll love to dive into so how did you come to this area of research what brought you to control theory modeling and to the empowerment perspective specifically um hold on I'm just bringing up the YouTube stream so I can see comments right now um what brought and just a general question mute that one there we go yeah I'll meet that sounds good thank you yeah so what brought me to empowerment I've always been interested in yeah how you know how how could animals uh interact in a world in a way that's so sample efficient especially like you know knowing that you know animals can you know like oh like a baby horse can you know get up and move around and interact with the world in a sort of fluid flexible way you know what are the sort of core representational capacities that are needed to do that and I didn't really see anything from the RL world and this is before I knew about active inference um and so that's always been in the back of my mind and another big influence was a guy named nishith srivastava who wrote an interesting paper about how you can basically have a sort of relativistic decision theory that allowed you to make judgments between different items without recourse to sort of hedonic utility Theory maximization and so he he sort of argued that if there was something like a latent acceptability function that you could you could sort of measure you could sort of remember a history of item acceptability and you could remember the context that you made decisions and then you could actually just do Bayesian inference over those memories and you could explain a lot of interesting things like preference reversal phenomena and decision Theory where you introduced irrelevant Alternatives and it like changes the fundamental choice you make and I thought that that that sort of initiated a lot of thinking into how could you have um how could you bring those kinds of intuitions into sort of embodied planning like how how could it be that you have an agent assign value to things without them being sort of attributed as sort of static um sort of static preferences or static Utilities in the world so I think that was also a big inspiration awesome okay and then one short general question why the honey badger hey Badger um yeah my advisor showed me a a YouTube video of of a honey badger named stoffel and in my paper reward is not necessary it's the the opening um paragraph talks about stoffel and there's a link to this YouTube video but stoffel is is is a honey badger in Australia and um he he he's at some sort of like Animal Care Center and he he's really good at escaping from things so the uh the caretaker of of of at this animal sanctuary constantly has to build you know like elaborate structures to keep stoffel in so he has this sort of pen called Badger Alcatraz and uh stoffel would do interesting things like you know find objects to lean against the wall and climb over or and if you took those away sofa would pack you know mud into balls and stack them into like a little pyramid against the wall to climb up and things like that so yeah it got me thinking like what is what is a sort of good General intrinsic motivation function that isn't doesn't just work on you know low-level States but also in a sort of more conceptual hierarchical space like there might be objects or mating opportunities or anything sort of outside Badger Alcatraz what is a sort of way in which you could have an agent sort of think in a sort of abstract way in order to justify motivate its motivations so I I encourage the uh the listeners to to uh look up that video it's entertaining it's like this General Escape impulse yes extended into our open air context where we also want to maintain the ability to move and for mobile creatures that's quite a good proxy for what we might want to care about like living right all right I'll go to a question in the chat from Alex Kiefer fantastic work maybe a naive question and I'm sure it's clear in the formalism which I have only begun to look at but the idea is that actual agent environment coupling figures in Computing empowerment right if so is there a fully internal proxy that can be optimized given information available internally hmm is there let's see is there a fully internal proxy that can be computed that can be optimized given info available internally in that part of the coupling does he mean I I suppose I mean you I suppose you could compute empowerment just on the internal State space um but I don't know I actually I kind of want to say no just because you do need to use actions to move around and influence other state spaces and things like this um I don't know how I would compute yeah just a sort of internal yeah intrinsic motivation function that that isn't a part of some coupling to some broader system what about the desire to think freely and to move in cognitive spaces broadly yeah I agree that's that okay so yeah I definitely agree if I mean if you have all of your sort of physiological you know needs met and there aren't sort of imposing themselves on you you're sort of freed up to do other things right um so yeah I think that this could work generally into very abstract spaces maybe even mathematical spaces um and yeah I think that there can be dependent like higher level dependency structures in you know abstract thought or mathematical thoughts or things like this I mean you think about you know faulty proofs that that sort of like destroy an entire field or something like uh there there's a sort of dependency structure in which you the you know if if you're working on mathematics that assume some proof is true and it turns out to be false then you know perhaps that's disempowering from a sort of abstract uh perspective I suppose so that would be yeah and when they're again you are yeah good I was just going to say but again yeah all that mathematics is being done by you know some system that has to uh perform computation which takes energy and stuff like that so it's always sort of constrained constrained by them well constrained by some kind of external internal coupling yeah many ways to go let's let's swerve towards active inference and then see if we can come back to some other areas um you mentioned the generative model of active inference but you took a different approach there's different model ontology so just broadly how would you structurally contrast the coupling of the agent in the environment in active inference and in what you've proposed because the representations that we see in active inference often feature the particular partitioning where a Markov blanket of a Bayesian graph is intermediating between internal and external States and then there's a mapping function between those internal and external states such that they can um engage in an Adaptive coupling again mediated through the blanket which is interpreted as providing incoming sensory observations and outgoing actions so structurally is that compatible incompatible or some other secret third thing with what you proposed yeah so I would start off by saying that I think the thing that makes my work different is that it's the structure of the you know the latent just sort of discrete State space it's that structure that's under consideration and I think and I think that inactive inference usually in code things like homeostatic drives right you encode them in a generative model right correct they're encoded as a preference over sensory observations so that the entity uh seeks out and selects ultimately policies that reduce or bound their surprise about those observations like I expect and prefer myself to be at a homeostatic temperature I'm not surprised when I'm in that range and I'm going to undergo action so that I find myself in that range yeah yeah so I would say that that is a major difference um because the because the state space in my case has this sort of self-undermining quality where it's like bad you know starvation States it's not it's not really a surprise an expectation of receiving a particular signal or having a preference over some state of the world it has a sort of uh self-undermining quality that affects your ability to control every everything else so I think that I would yeah I think I would contrast it that way that's usually the preferences or the quality of the states are sort of encoded uh in a generative model in the active inference setting and here I'm saying that there's a sort of structural coupling that's giving rise to these valence signals um you mentioned the key being obtained as inducing this change in the agent's ontology and one that was ultimately reflected by increasing empowerment hence increase in valence so how does it come to understand that this shiny object unlocks that door yeah I wish uh yeah I mean I think that uh throughout my career I will try to make steps towards like you know actually um actually figuring that out because a lot of this would comes down to Dynamics learning you know if an agent doesn't know what a key does right it's not going to know that it opens a door and therefore that it can move through the door and things like this I think there are a lot of sort of maybe like Dinah like algorithms in which you sort of alternate between learning learning things about Dynamics or things like this but that that is a sort of outstanding question for me is like yeah how would you take a key and learn exactly how it's changing the Dynamics um but yeah what I am saying is that given given that you can do that if if you can do that then you can really sort of make these value judgments to things in the world so I think that's really important too because like consider money right like if I find a 20 bill on the ground I'm just going to pick it up I don't think I'm doing a fancy computation right um of of like oh no my my bank account is twenty dollars greater and so therefore I'm gonna I have all this new capacity or things like this I think I think these sort of preferences for various objects like a a key or a dollar bill or things like this can sort of be you know stored and maybe you know maybe models with utility Theory who knows hmm okay jumping around to some different questions I hope I'm accurate in saying you described empowerment as a Shannon information theoretic Channel capacity between the actuators and the realization of the state yeah we might be familiar with hearing uh Shannon Channel capacity in the context of bandwidth of information transfer or upload and download for example but this is kind of an action-oriented Shannon Channel capacity so what is it mean to to get an intuition on that capacity between the actuators in the state yeah so it's it's the channel capacity is the maximum possible Mutual information between the actions you choose and the resulting States so the Channel capacity is sort of the maximum information that you can transmit from your actuators to possible states of the world it's it's a it's a form of optionality that says you know I can affect this many sort of possible Futures so yeah it's a sort of intrinsic property of an agent and so in a product space this is going to be affected by a lot of different state spaces if they're interacting like physiological State spaces can you know kill you if they get too low so I think the interesting thing about this is that it sort of encourages you to think about cognition in a in this sort of interrogative way um because you know you're essentially trying to figure out you know what what you can do but there are a lot of you know different state spaces that are hindering that information transfer from you know your actuators to your state spaces so I think I think it's very useful because it can sort of it's it also sort of leads to like explainable AI right you can sort of explain your intentions in terms of concrete State spaces which which have structure and explanation things like that um I think I got off topic from your your question but um anything else on channel Shannon Channel capacity that I should talk about um I think before we Loop it back to potentially expected free energy uh you mentioned the um AI topic and is there any risk of an imperative that features its own empowerment in terms of an AI being able to then select action policies that might not be what anyone else expects or prefers may not even be concordant with their own encoded explainable AI priors but rather something that takes an unbounding approach yeah I do think that that is um a fear I don't I haven't thought that much about like the alignment questions so I'd be very interested in in what like alignment researchers think of this perspective um I think that there's a lot of interesting work to do on sort of multi-agent empowerment especially with these sort of abstract uh transition operators that work on long time scales um and you know you can see that house sort of socialization matters and things like this um you know if you're you know in a world with multiple agents um do you have to you know do you have to learn to respect all of the agents you know empowerments I mean they can also act against you right um so yeah I don't know I do think it's a fear to take seriously um I don't know how I would do it though because it's it's an outstanding research question to me coming from an ant colony background you mentioned the socialization and I immediately thought well let's just say that the seeds take two or three Nest mates to carry home so in order to have one Nest mate achieve the maximum empowerment they must also engage in a pro-social environment because if anyone else even if their model is like smaller and less empowered if they just decide not to play then that individual until it figures out how to carry the seed Home Alone is going to actually be kind of um Tethered to a social fabric that helps it actually obtain those goals so it puts the social imperative as a screen in front of potentially any other imperative just not um yeah that's a great that's a great thought you mentioned the decomposition of some function I'll let you unpack what exactly what function was being decomposed and you you um you Justified that by saying we never want to work in the product space yeah from a computational complexity perspective or however makes sense what are the dangers or what are the scaling features of that product space and then what is the decomposition that facilitates a more tractable form yeah I'll share my screen again we can go back to that slide thank you let's see foreign [Music] you can see my screen yep and maybe even a brief summary of what is a Bellman equation and how did you move from the standard formalization of bellman equations into this operator space sure yeah the standard Bellman equation I don't know the button for one slide forward but okay so the standard dominant equation just says this uh recursive form and it just the value of a state that you're at the the optimal value of that state is the maximum it is the maximum value that you can get by choosing an action that rewards you and takes you to a state from which you can act in the future to get more reward so the Bellman equation um can be solved by dynamic programming um in order to maximize this function V so it'll it'll it'll result in a policy that moves you around the world uh in a way that accumulates a reward that you'll find in your environment um so it has this recursive form uh you can you can sort of unroll it into a sequence um and then the operator Bellman equation has this similar recursive form where now you'll notice that there isn't a reward function there's this availability function and it returns a number between zero and one it returns a probability and so that's significant because it makes it means that you can maximize the cumulative feasibility and so F here is just saying this goal is available either you achieve the goal now or you take an action you don't achieve it now but you've taken an action in which you'll achieve it in the future so that um it has the same form where you can sort of think of an availability function as a reward but it's maintaining a probabilistic um form and that probabilistic form is importance because it's what allows you to compute the state time feasibility as a transition operator as an operator that Maps you from where you are now the state time you're at now which is XT to the final State time and goal that you achieve so under the policies so it says if I start at XT and I follow this policy and I'm choosing actions that move me through this state space then I'm eventually going to get to the goal and I want to know the final State and time the probability that I uh achieved this at any given state in time and so the state time feasibility function is here as it's expressed is a transition operator with one action which is the policy but when we aggregate it oh bring this up when we aggregate it into multiple possible feasibility functions that are centered around uh multiple sort of objects in the world then all of the policies associated with with each one of those these are goal condition policies that that are going to terminate on achieving the goal of going to one of these features and and you know getting the Apple for instance then those pop each of those policies is an action for this transition operator so there's five five possible policies that are going to take you around the world around the space so these operator development equations have this probabilistic form which retain this probabilistic structure and you can sort of compare this with um I don't know if you're familiar but there's this concept in RL called the successor representation um which is like often a Hot Topic in you know computational neuroscience and the successor representation is sort of talked about like it's this predictive operator but really what it represents is expected State occupancies under a policy and those expected state occupancies are awaited by the discount Factor so it's really a for it's really a successful representation is really a sort of weighted statistic and it doesn't map from like an initial state to the state of achieving of inducing an event of of achieving a goal these are so where successor representations aren't compositional you can't multiply two successor representations and get another successor representation but you can multiply matrices that represent feasibility functions because they're mapping their their probabilities so of events so you can you can combine them just by multiplying you know matrices that represent you know the state time feasibility function for a given policy with another one for a different policy so that will um retain a the form of a probabilistic function and that's what makes them reusable composable Etc and I think you asked me about the decomposition and this decomposition yeah I might have glossed over this but the decomposition result is that if you have if you were to compute a state-time feasibility function in a product space okay which has lots of State lots of States lots of State vectors which are each States um then you know you don't want to do that because product spaces are very large and take a lot of you know in memory to represent the operator uh so if if this was not PX and this was PS and PS was the full product space operator that moves you around this High dimensional space well you can't really represent that and you don't really want to but if you did and you computed a state time feasibility function in a product space then you can under certain conditions uh and I can say what those conditions are but under certain conditions you can decompose this into a prediction of all of the higher level State spaces computed independently so you evolve the hunger space separately and you evolve the their space separately and you evolve the temperature space separately you can do all of those computations locally on those spaces um and you can combine them with a state time feasibility function that's only computed on the low level space so um so this hierarchical stf of the state time feasibility function is an intractable object for for most reasonable reasonably sized problems but you can implicitly form it by this product of these things individually and so this works when you're when your goal if for the for the uh hierarchical state-time feasibility function when you just have a single goal at arriving at a particular feature of the world like a tree um that's that's this that's the sort of can that's one of the conditions in which this decomposition holds so I think that that answered your question about you know the burden of a product space well you need to uh you need to overcome it by doing local computations on individual State spaces in a sort of network of interconnected State spaces that implicitly form a product space but you you want to compute all of the representation separately in this network of State spaces so that you can sort of move around this High dimensional State space under successive policies and so that's that's what allows you to handle forward sampling in in this High dimensional State space and that's that's important because I I think and just to um recall uh the sort of uh presentation I think was aval a-a-v-e-l who yes yeah just to Echo that that sort of sentiments that if you're composing if you're creating new state spaces or you're composing you're not Computing policies in a fixed world um you're composing things together um that's of course going to expand the product space implicitly of all the state vectors of the system and the act of composing thing or bringing new information in is expanding the implicit product space that you are in and so from an RL standpoint um it's not so clear you know what a reward function on that product space is even supposed to be I don't think that anyone will I answered that question but it's also not clear what you know what a generative model should look like on that product space either and I think given that you know organisms or humans at least are so skilled at this at this sort of dialectical process of proposing theories in you know composing structure um as hypotheses and in um in interrogating what that means I think that they're I think that value comes from interrogating you know what it means for the structure of the world to be a certain way so if I if I learn new dynamics of the world and I want to control Dynamics on some new space it might affect other state spaces but you know from a normative perspective it's not really clear you know when once you compose something in your and you're expanding the implicit product space it's not so clear where any sort of normative source of normativity should come from but I think the sort of flexible human reasoning that we sort of know humans to engage in I think is in this sort of regime of composition and interrogation where you're always sort of saying oh if the world were this way then I'm sort of I I then I could see how this state space affects these other state spaces in a way that I didn't anticipate and so I think normativity in a creative way has to come from controllability that that would be my argument there's a lot there so if you directions first to our colleagues in reinforcement learning RL the paper is provocative in that it includes reward is not necessary so is reward sufficient and or what is necessary for what yeah it's a good question is is reward sufficient I think I would argue this you know I think that the reward enough hypothesis which just to remind the some of the viewers is the hypothesis that reward maximization can account for all sort of artificial and sort of natural intelligence that all of the sort of features of intelligence the sort of capabilities the structure learning and stuff can all sort of arise out of some need to there's some some process of maximizing reward and from my standpoint this that's a frustrating statement because it's one it doesn't um one it doesn't really address where your preferences for specific reward functions come from and in the paper that they will say well we acknowledge you know that there could be multiple sources of reward and but you know the process of deciding on what you know reward you should attend to or care about that's I think is is a deciding what signals you should care about is an important part of intelligence itself and I think that reward is enough hypothesis as a hypothesis is sort of under constrained in that whatever that mechanism is to to that perspective it's going to be maximizing reward you know under their Paradigm and so therefore whatever whatever shapes what that mechanism is that that you know an agent should attend to this or an agent should stop attending to the things that's always cared about in and attend to some new signal I think that that call you know I think that that forces you to sort of take the position that you know maybe there's a meta Source like a meta reward that tells you that that directs this process because all sort of attendant processes of intelligent systems sort of are underlie the process of reward maximization and also you know it the the reward the reward is enough hypothesis is not being specific about it doesn't tell you what necessarily to compute it just says that if you try to maximize reward it will you will you will compute the right uh representations and so I think that there's just a lot of like nuts and bolts about what it takes to be able to reason in a flexible human way in what I'm proposing here with the operator Bellman equations is to say hey look there's these reward-free Bowman equations that help you deal with the complexity of the world there's no reward in them and you could make the case well maybe you could just use these operator development equations to occupy states that are rewarding but I would argue that since the product space the effective product space that we all live in is so vast and we reason about it in such a flexible way I very much doubt that RL will rise to the challenge of being able to justify motivations in real time in a way that humans can and so to get to the question of you know is it sufficient my hunch is no I can't like I don't have a proof of like you know reward is not sufficient um but I also think that the information that a reward function is supposed to carry about what is good I don't think that that is that is Noble or computable on the time scales that we understand human intelligence to to work at to work on and so yeah in order to answer your question about what is necessary I don't know what is necessary but I would just sort of make the point that I already made of that I think that we have to get to a point in which we sort of acknowledge the problems of product spaces and sort of reasoning dialectically in a product space that we can't explicitly represent uh and so I don't know I don't know what is necessary but I can say that it's not necessary for simple self-preserving agents and that's the claim of the paper awesome all right in our closing segments I'd like to take a journey to philosophy and then connect this back to potentially relationships between the models that you've presented here and active inference and maybe even walk to the edge of that Cliff of the hybrid model so Aristotle proposed four causes material cause what something is made of the efficient cause which is the source of change the formal cause is the essence and the final cause is the the teleology the end goal of the object and your presentations was Deacon's analysis of these different forms of teleology and indeed within the model proposed empowerment was that type of self-referential teleology when juxtaposing with active inference and specifically the expected free energy functional which has a lot of analogies with an operator it's a function of other functions the expected free energy functional is predicated around helping the agent select policies that over expected Futures reduce their uncertainty the most about which Sensations they receive and that's what ties active inference closely with perceptual control theory that that expected free energy is ultimately looking at a Divergence between preferences over observations and incoming observations so that's kind of the sense side of the coin it's like I want to stay in the game to be able to align observations with my preferences and I'm wondering if empowerment is the action side of the game you'll be involved with yes repeatedly sensing yourself to not be starving not be dying of thirst not be dying of cold like you'll be in your preference vector by way of this single value which is the empowerment whereas active inference kind of comes from the other side saying you're going to end up having a lot of squares to move around in But first you need to make sure that you're reducing Divergence between your preferences and your expected observations what do you think about that map or where would that take these intertwined models well I think that there could be like an interface between the two concepts if we consider that models of how things work like composed models of how things work could induce particular generative models that you would want to use in a sort of active inference setting and that would be the sort of dual nature between the two that there's a cross talk between the proposal of of some kind of generative model that would be conducive to the agent and if it is then it's a good then it could be a good State encoding which feeds back in on you know controllability or empowerment and things like this so I mean there's a lot to think about on this topic um but I guess I would just put put it that way that um that we still need to justify where generative models come from uh in new situations for new theories of how things work and things like that you know there could be a dual process in which the action side the internal controllability side is dictating [Music] what kinds of generative models that we that should be considered very interesting and the reason I brought up Aristotle's causes was because active inference as a process Theory seems to be describing that efficient cause it's just especially with a variational free energy which is kind of the real time version of the expected free energy it's like one step at a time ball going downhill and so variational inference is enabling incremental unfolding optimization again oriented around reducing that sensory preference and outcome it does everything but specify a final cause in a sense one might say that there's a local final cause within the active inference generative model which is like to reduce the Divergence between the preferences and the observations but the generative model also from the action selection side which is what makes active inference active inference it also needs a final cause in that self-referential teledynamic way and so there could be some very interesting architectures where active inference picks up where empowerment leads like through a needle because it's such a small representation with valence and then one other kind of connection or maybe mapping between them is we've seen models of valence in active inference such as the affective inference work where valence was associated broadly with whether things are going better or worse than expected in terms of statistical uncertainty if you're reducing your uncertainty more than you expected things are going better than expected and vice versa um so that is a very variance oriented valence Concepts where broader uncertainties are associated with inferior valence and Tighter uncertainties are associated with positive valence and it's just interesting that that's kind of like an orthogonal valence concept from how much you can actually do that's the actionable valence would you rather have a high Precision around not being able to do anything or High uncertainty about being able to do a lot or a huge amount and so it almost seems like when we contrast those two the direction that dominates is in the final analysis the ability to have empowerment not necessarily to just have tight control over your observations yeah yeah I agree lots lots of interesting avenues for hybrid theories well what a very interesting talk I guess one more question on the model and then we'll close which is the time Horizon is it an infinite time horizon or what is the treatment of time and can time be continuous or is time always discrete and is it finite or infinite Horizon it's uh so the operator Bellman equations are formalized as a finite Horizon I suppose they could be extended to infinite ryzen and infinite um or continuous time I suppose that's possible too but as as they're formalized now it's discrete State discrete time finite Horizon um but yeah I mean I think that there is there's Alternative forms that could be could be made [Music] um is the was there another question too in that I can't remember let us close with your time Horizon what are your next steps with the research and how would you imagine an ecosystem of continuation of the work yeah I'm I'm interested in getting this work into computational Neuroscience [Music] um uh because there's a lot of I think alternative models that need to be considered especially given some of the themes that I've touched on in terms of justifying what to do in a product space how do you represent control in a distributed system Etc um and so I think that there's a lot a lot to do there in computational Neuroscience on the AI side I want to put this in a world model I want to uh get good you know Auto encoders kind of like how I had on on the first slide where you have a high dimensional you know agent in a high dimensional multimodal world how do you how do you have nice World models and put this in in something like that so that's that's the AI future Direction and I'd like to do both of them so a lot of work to do awesome well in closing I'm just going to read some of the more statement like comments from the live chat just so that they're included in the active inference Journal so Dave Douglas wrote with regards to Deacon's consequence Galileo may have gotten in trouble less from insisting on a heliocentric Universe than from insisting that purpose value and meaning be banished from science as connecting explanatory principles the Galileo tolerated remaining connecting principle causation may have reached the limits of its explanatory power sometime between Newtons and four years day where all respectable principles of explanation must ultimately rest on invocation of either rigid rods or on elastic bands the Galilean program of Sola casa attained its absurdity in karnap's insistence that meaning value counterfactuals must be judged as strictly and literally as meaningless have we passed the point when the Galilean program must be simply abandoned and meaning value and purpose must be restored to science as irreducible explanatory principles alongside causation I find the mysticisms of both boom and Heisenberg's Quantum completeness and of Pauli and Young's synchronicity to just be too fluffy to be very useful in a word it has become a tradition of science as funded to insist that causation and causation alone must bound our Enterprise this is a tradition of men not a feature of Ultimate Reality meaning and value also have their place Not reducible to cause I like it yeah excellent well Thomas thank you again for joining you're always welcome back and really looking forward to seeing how this all continues me too and thanks again for having me I've been very impressed with how much work you do for the Discord and uh it's it's a it's a great Community I can everyone's very very nice and enthusiastic so I was excited to see it I just sort of randomly stumbled across it so I'm glad uh I'm glad I introduced myself to to the Discord so thank you excellent all right till next time thank you great thank you