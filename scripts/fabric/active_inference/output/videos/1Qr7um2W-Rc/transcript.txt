hello and welcome it's December 17th 2024 we're here with Josephine Pam and colleagues discussing free energy projective simulation there will be a presentation followed by a discussion so thank you to all the authors for joining looking forward to it take it away thank you um hi and thanks for uh having us um today I wanted to present the paper that we have um recently made public on the archive um you can have the the number here which treats uh the free energy projective simulation active in Fr with interpretability and our goal there was to combine um an existing framework for agency called projective simulation with the free energy principle and um active where we have an agent that is able to learn from its experience in the world Gathering percepts and remembering um a history of actions to learn a representation of its environment and um behave adaptively in its environment by performing active Insurance to determine its behaviors um I want to I have structured my presentation in three main points first I want to um introduce projective simulation to the community um then I want to um introduce the formulation and the aspects of active inference that I have used in these Works especially the the notation will be important and then the main body of the presentation will deal with the free energy projective simulation how what isly Ure how does it work um how do we train it and then some numerical analysis about it before I conclude so first uh what is projective simulation the goal of projective simulation is to model intelligent agents with a framework that is rooted in embodied cognitive science so now what I want to do is to look at the keywords of this sentence and understand what we mean uh behind those words starting with um the notion of intelligence for an agent where what we really mean here is that um the agents we consider are able to perceive the environment with um to perceive their environment and are able to influence and change their environment by acting on it and in doing so they are settling matters uh in the environment in a flexible way and then the agent uh uh brought definition for it or um its support would be a system that has a memory as I said I want my agent to be able to remember its experience in the environment and it's also able to interact with this environment and that leads us then to embody cognitive science where instead of having the agent learning from a set of predefined roles that we as designers would give to the agent such that it learns um high order Concepts the agent has to learn from the information it has gathered from its interaction history with the environment that is how it has perceived the environment and the action it has taken on this environment and now I have spoken about memory earlier and I want to make a Precision um for this intelligence it should also um the agent should also be able to adapt to situations that it hasn't experienced yet by comparing it to its existing um moments in memory and to be able to compare them see which element of its past history are useful to adapt to unprecedented situations now I want to dive into the way we are modeling the agent to implement projecti simulation the agent is engaged in in a procet action loop with the environment in that it has two types of subsystems a sensor system is able to perceive uh percepts and stimuli from the environment and it's able to act on the environment and change its state uh through its actuators and the key of projective simulation the place where projective simulation will um be important is the memory of the agent that is decoupled from its physical uh interface with the environment and that should allow the agent to reflect such that he can display some more um complex behaviors um in the environment going Beyond similar response uh behaviors the idea here is that the agent is able to um simulate possible scenarios using its memories in order to plan or deliberate um for its next action so this is already this diagram should already be reminiscent of the free energy principle and active inference in general now projective simulation is a specific process that runs on the memory to deliberate for this the memory is um depicted as a a um Network that connects different clips if you want a if an event where um a movie a clip would be a snippet of that of that movie just a few seconds of it where the agent would recall the sensations it has had the percepts it has had during this um um mini movie and the actions it has taken then the agent um the different Li are connected in a directed Network such that the the different events are connected with one another following the experience of the agent upon receiving a percept from the environment on its sensor um system a clip in the memory of the agent in the epizoic and compositional memory of the agent you will hear me saying ECM very often so a clip is excited by a peret from the sensors and then the deliberation of the agent will take place by hopping on those edges to um go from one clip to the next and simulate forward um the possible events that can unravel from this excitic clip so the agent does a random work on that Network until it reaches a specific type of clip that will then decou from the memory and this um um yeah fictitious uh random work such that an action comment will be sent to the actuator and the action will actually be implemented in the environment so here I want to emphasize two types of states that can happen um the any state that is any clip that is visited in the um memory is either a remembered um clip um a remembered event event from the past or something that has been created that is fictitious and all the surrounding events so the percepts and the actions are um real and um at the interface with uh the environment there are two specific type of clips that I want to emphasize um the clips that c with a percepts from the um sensors or called percept clip and the clips that decouple from the memory towards the actuators or the action clips now um how do we train this memory and how does the agent adapt and learn something using this memory there are two ways that this can happen either the agent is um creating new clips in its Network either because it has received a new percept on its sensors or because it has created new clips from existing Clips in its Network and another mechanism that is more important for um the present work is an update rule where each of the edges in the network is weighted with what we call an AG value that can increase and decrease as the agent interacts with the environment and in particular at each time step um the pre the updated weight decreases compared to the previous weight with a certain uh forgetting rate and it's brought back to its initial value um with the same rate and every time um an edge was traversed uh to make a deliberation that received the reward um it is reinforced with a certain reward and from those H value then we can define a probability distribution that the agent will used to um um sample edges during its random walk to walk from One Clip to the next and deliberate this is um this is it for the mechanics of uh projective simulation and its conceptual um meaning there were a few applications already in the literature for example it has been applied in robotics and reinforcement learning where um um a group in in book had a robot learn new skills from um existing skills by preparing new configuration of the environment that would enable the agent to extend its uh previous skills um it has also be used to simulate biological agent especially the behaviors of honeybee um of honey bees when they are trying to defend themselves so it's a multi-agent scenario um um where each of the individual um agents are coordinating their decisions to uh be efficient and there have been a number of um Evolutions that have been proposed on this um a standard projective simulation um multi exitation projective simulation proposes that the clips are truly compositional and a nedge is not only between two clips but rather between groups of clips that would represent the current situation and there have been also a quantization of uh projective simulation um in this semal paper uh but also this has already been implemented experimentally on a small problem so this brings me now to the second second part of um this presentation where I want to um give an overview of the free energy principle and active inference so that we are all um agreeing on um what we mean by um our notations and active inference here so the claim of the free energy principle uh is that any adaptive system can be modeled as performing an approximate form of Vasan inference so let's do as we did for projective simulation and look at keywords here uh what do we mean by an Adaptive system it means that we have a system that that is able to persist in its environment and to do so it has to exhibit some form of flexible behavior and it has to be able to act on its environment so that it can change it if needed and the main keyword is patient inference here which um um means that we can describe an an agent as as building an internal model that it is using data from this environment in order to update this model and that the the update takes the form of an optimization and another keyword I want to emphasize here is that it's modeled as perform coming patient inference that is it looks as if so here we want to distinguish between the um internal mechanics of an agent um learning from um stimuli it receives from the environment and its actions and um the mathem mathematical description that we can give of that agent as updating a model in inovasion way and hopefully these two will be reconciled uh at some point in the presentation first um to introduce um active inference before we introduce active inference I would like to introduce perceptual inference which is um describing the process in which the agent is constructing its representation of the environment using the observations it receives from it the environment can be described as having um hidden States and the agent acting on these hidden States will um cause or in um yeah will result in a change in that hidden State however um the environment is not fully um accessible to the agent it's only partially observable so the agent would not see this hidden States but instead only a small part of them which show the observations and in order to understand the Dynamics on those observations and being able to predict those observations the agent will construct a world model in the form of a partially observable M of decision problem a pndp using what we call belief states where here I use capital letters for the random variable and lowercase letters for the value that those random variables can take and the idea of those belief States here is that they mirror the the role of the Hidden states such that the agent can use them then to make prediction about um its observations so similar to I'm sorry similar to um the actions in the environment taking an action um will also U result in a transition in the belief States and those belief states have to be connected to the observations the transition between belief state is usually qualified as a transition function and the um Association of belief states with observations as uh the emission function and those two functions can be modeled as um probability distributions um here the probability of the next belief State given the current belief state in its action and the likelihood is uh the probability of an observation given the current belief state which means that then we obtain a well model that factorizes nicely into two functions um the likelihood on the one hand and the transition function on the other hand now we have said that the agent learns by performing basan inference which means that we need to bring um another component um to the table for the agent to update its model and that is distinguishing between a prior and a posterior the prior is the previous state of knowledge of the agent but it believed was true for the transition function uh before um updating its model and the posterior is a distribution function that is part of a variational family and that can be parameterized with here parameter fi and this fi will then be tuned such that we lend on a distribution here that enables the agent to make better predictions in the future using its current observation and the way this parameter is tuned is by minimizing a quantity called the variation of free energy that then gives its name to the free energy principle the free energy um or variational free energy to be precise um has um the following form where I want to First Look at the second term um this term here is called the surprise and that's um the surprise of the agent um getting observation SC environment here uh given it was currently in the belief State BT and we are aaging this over the variational um over the posterior such that this term becomes minimal when the current belief state is able to predict the current [Music] observation this also means that the best way to minimize this term is um to forget everything we knew in the past and to only update the variational um distribution here such that it picks only at the belief state that is able to predict this observation and to prevent that we add an energia term to the variational free energy that is um zero only when those two distribution remain the same and that is that having a new observation does not do anything to um the model so this um motivates the agent to remember uh is previous model and to um make a sort compromise between updating its model and um generalizing also to its past observations okay so in this whole scheme so far the actions were externally given I didn't say anything about how the actions are actually um chosen in this whole model and how they impact the ability of the agent to predict its uh observations and to um include actions in this uh framework active inference um gives us a recipe to plan for actions and namely the agent will try to choose actions that are expected to minimize the surprise about future observations in the long run um to do to do that the agent is endowed with a form of intrinsic motivation that helps it that guides it towards states of the environment that are beneficial for it um for example that are helpful in persisting in an environment or in um um doing homeostasis for example the way this is modeled in this whole framework is by giving another distribution to the agent that we call the preference distribution um that is a probability distribution over the future belief States and the future observations and as for a perceptual inference now the optimal actions will be those that minimize a quantity derived from the free energy which is the expected free energy so a sort of estimation of the surprise about future observations in the future the form that this takes um is often um qualified as a tradeoff between an exploration and exploitation where this first ter we are I just let me remind you that we are trying to minimize this expected free energy so here we are trying to make this term as negative as possible and what this uh is is the entropy of um the belief States given the current world model so the agent we select an action that um is the most um that leads to the most uncertain transition in the world model such that the the this uncertainty is resolved by taking the action so this is an exploration term it um Fosters um Discovery in the environment it want to resolve uncertainty whereas the second term um is derve from the surprise calculated from the preference distribution that I introduced earlier and it is a utility term because this term becomes minimal when the surprise over the preference distribution is minimal that is when the the chances that we do not fulfill our preferences um or as low as possible and now from this um expected free energy there are multiple ways that one can choose and plan for actions a very popular one is to do a research or using monteo methods and the method we want to use here is to define a policy from this expected free energy by feeding it into a soft Max function for example and um having a scaling parameter in front of it that would be negative such that we ensure that um actions with the lowest um expected free energy are more probable okay so this is the end for the little summary for active inference and now I want to move on onto um the model we are proposing the free energy projective simulation um some key features of our model is that um inheriting from active inference the agent um is endowed with internal motivations and also internal rewards in the form of prediction accuracy that we um model using um a concept we call confidence which will become clear shortly um it's also uh it counts uh among the implementations of model based based reinforcement learning using projective simulation and um it becomes a hybrid model between um a form of associative learning uh between different percepts um conditioned on actions um that will benefit from the reinforcements and the rewards the agent got from its predictions and the behavior of the agent is completely independent of any reinforcement um and is fully d with active inference and one of the main um benefits also of using projective simulation here is that the model the agent learns and the update role it learns with are interpretable as a legacy from projective simulation here okay so let's start with the architecture of the agent where um we are trying to um gather all the ingredients um necessary for the agent to learn a world model and perform basan inference so what we propose here is um um the for the belief states to be encoded in the memory of the agent because we want them to be decoupled from the environment just as um those um belief states in the free energy principle and um in order to avoid um having to learn the likelihood at the same time as learning the transition function here we give um this memory a specific graph structure um inherited from the Clone structured um hidden marov models that have been first proposed in this paper here and successfully implemented uh in this paper here to build cognitive maps and in another following paper I think in 2023 um for navigation tests in this one um the way this works is that it postulate that an observation is actually only sort of the tip of the iceberg of um the full random variable the agent would need to be able to understand completely and model completely its environment and therefore it gives each observation a clone that as we train with acquire more more meaning and it gives a uh this likelihood deterministic uh form such that each clone uh is linked to a single observation at a time that is the probability is either one or zero in this case uh to each observation we give a fixed uh number of clones and um the edges that are required to take um to deliberate in uh the memory um will be split into two sets um one I mean here two sets or n sets one for each action that the agent can choose such that we can learn this um prior and posterior in particular here um the um parameters that we can update are the weights Associated to to each Edge in this ECM now we could clone clip any clip in the ECM and we are going to call the belief State an excited clone clip that is where the agent is currently um uh sitting in its deliberation once the agent has excited a clone clip in its ECM uh this will Define um the belief say the agent has to deliberate over in order to um uh sample its action and what we do here is um each belief set is connected to all the actions and each Edge is weighted with a corresponding expected free energy and then these um uh these expected free energies will Define a probability distribution that will be um that the agent was sampled um from in order to determine which action to implement in the environment and to use to make predictions using its memory and the prediction will be the observation that is linked to the current belief State the agent believes it is in now I want to introduce um the concept of confidence in this model that is crucial in order to update the one model of the agent we call confidence um the um we use confidence to track the number of correct predictions um sampling an edge enabled in the future that is if I sample this Edge to start with at the end of a trajectory I'm sorry at the end of a trajectory it would count how many um correct prediction it enabled in a trajectory and what I call here a traj iory is a sequence of consecutive correct predictions um for the observations now I want to um move on in the next slide to show you how it works exactly going step by step um in an interaction loop with the environment okay so the environment starts in a certain hidden State and emits an observation that the en that the agent is able to um receive through its sensors then following this one of the Clones uh corresponding to that observation will be excited in the ECM here BT minus 4 and the agent uh will sample an action from this state here A2 and predict the next belief state um it believes will be useful in order to make a prediction about its observation in the future its prediction is then S2 because that's the observation that is linked to it simultaneously this action is implemented into the environment and a transition occurs in the environment that then emits a new observation that is again received from the environment and in this case it confirms um the um prediction made by the environment and therefore the confidence of the edge that was just traversed um is set to one because it enabled a single uh prediction in the future for now now every time the agent samples a new action and implements it into the environment the com the observations are compared with the predictions until the agent makes a false a wrong prediction so here the agent predicted S2 instead of S4 and that's where the trajectory ends and the the network the weights in the in the ECM will be updated following the PS update rule uh so here you will Rec um uh remember um the standard update rule where uh we decrease what we believed before going back to the initial State um of the distribution and here the reinforcement happens proportionally to the confidence of um an edge that is this um Edge will receive three times the reward scale because it enabled three correct predictions in the future now um I want to explain how um we Define the policy and how we sample um how we um Define the preferences espicially we have two proposition the first one is um trying to understand what happens if we give the agent the objective of actually trying to um minimize its prediction error directly embedded in his preference distribution so to do this uh we used a um we modeled the preference distribution to be the marginal of the world model here over its actions and after a little bit feding this into the expected free energy and a little bit of reorganization of the terms what one finds is that the um expected free energy B is St to be the Information Gain um about the next belief state from taking action a in the environment now you recognize very quickly that this is problematic because we are trying to minimize the information about um the next belief State and this is a well-known problem um which is called the dark room problem where the agent uh posted in a in a dark room and without any incentive to go outside of it that is a preference for a state that is not in the dark room would stay there because it's fully predictable however there is an easy way out um using our model uh where instead of minimizing here the information again what we can do is maximizing it by simply um um making the scaling parameter here positive and we will call an agent uh that uses this preference distribution a Wandering agent and then the um magnitude of a Zeta will tell how greedy an agent is for Information Gain it will um a bit like in epsilon gitty algorithms it will Define with which probability the action is actually targeting a regions of Information Gain or of uncertainty to improve the world model now this um the second um option is more directed towards um encouraging the agent towards completing a task in that environment okay and to do this we we first took a modeling assumption that if we were to give a task that is externally given to the agent it can only be modeled at the interface between the agent and the environment that is on its observations or its actions and here we decided to um embed the Target in the environment into the observations only and to call this part of the preference distribution the absolute preferences and um the task of the agent then is to be able to use its well model to propagate these preferences uh onto the belief State such that it um any state that can be useful in reaching the Target in the observations get a higher preference overall to do this we Define two quantities first um we ask how useful is a belief state in order how easy is it for um from the current belief state to reach the next belief state so sort of cost of uh reaching an action from the current belief State and the second quantity that is useful is how valuable is the current belief state to um achieve uh the task in the environment looking at the um first quantity we Define the reachability um as the mar the transition function marginalized over action so for example imagine you are in this state B2 here um here you have two actions going to uh B2 for the darker observation um compared to one with the same probability for um those two states so the reachability of State B2 dark orange will be higher than B3 dark orange for example a second quantity is the value of a Bel state which is connected to the uh value of the to the preference um over the observation it is connected to we initialize the value with um the this um exactly that actually the the absolute preference of the state it is connected to because we are using this clone structure so this means that if S4 is our Target then the preference for those three states the value for those three states will be um the highest and for all the other states it will be the lowest and now what we want is to propagate this preference to the other states in the ECM by using an iterate algorithm over um a number of steps that will Define a prediction Horizon for our agent and either the a the belief State um stays um keeps the same value as before so those States will never um have their value decreasing um as the algorithm progresses or um the um belief States inherit from the value of the belief States they can transition to um as Define with this term here with the reachability and the value so for example those two states here being able to transition to these here will inherit their value and now to define the L ahead preference distribution we combine those two quantities Define children of a belief State as the states that uh can be reached from the current belief States and um combine this to uh Define this look ahead preferences and from there we're going to um fit this into the expected F energy and Define a new policy that hopefully guides the agents towards the Target in the environment okay yes so far um we have tackled only two um problems that could get in the way of an agent performing a task optimally we have solved the first problem of um trying to learn the best world model possible to make good predictions about observations the second one is being able able to plan actions efficiently for the future using the world model and another problem that arises when we use um FEPS is the estimation of belief States um and for this we propose to estimate them in superposition what this means is that every time um at the beginning of an interaction with the environment the agent will receive an observation but won't be able to distinguish between the Clones to start with so it will build a hypothesis that contains all um three clone CL here connected to that observation and each of those clones would be a candidate belief state from there U the agent will use a rule to select its actions simulate its transition for each of the candidate belief State and propose a uh candidate future belief State and while the action is uh implemented in the environment the agent will receive um uh an observation from the environment and from there the hypothesis hypothesis sorry uh will be um um uh reduced by eliminate eliminating any belief state that does not that is not compatible with the observation the agent received from the environment and um continuing um this cycle of hypothesis predictions and confirmation from the environment the agent will slowly reduce the size of its hypothesis to a single belief state that the agent is not certain it is in okay this then um um solves the three main obstacles we solve for an agent to P perform optimally in an environment and now I want to um show you how this actually worked in practice and in the code in some um environment that um we um use for demonstration purposes um the the environment of the task we tried to simulate was a navigation task in a partially observable grid um the agent um this grid imagine has food in its upper right corner that let's say has some smell uh that Prov Ates through the grit so the the closer the agent is to the food and the more intense the smell is on escape from zero to three the task of the agent would be to then reach the food um in there but compared to other application for example grid World um the agent only has access to the span intensity to orient itself in the grd and determine its behavior and never has access to the cause inates in the grid for example and what I want to show you first is that even though uh the agent does not see the coordinates it's able to learn only from um its um percepts and it's what I call embodied experience that is um receiving percepts from the environment and using its actions to predict what I show here is a figure of merits for the a status of learning for the agents and I compared the two types of preference distribution that I introduced earlier the task oriented agent first with the L ahead preferences and the wandering agent here in reddish colors and we see that it seems that at the end of the training um given a fixed number of episodes it seems that those two model converge to the same length for their trajectories is um just to um remind you the number of consecutive transitions uh that led to correct predictions in the environment it seems that at the end of the training the optimal agents with either preference distribution perform the same uh with a slight Advantage seems for task oriented agents that seem to converge faster towards this um optimal status um and here there are two maybe um interesting features too um if um we set the scaling parameter to a value that is too positive that is we make the agent very greedy for information the agent does not learn as optimally as the other agent and in fact it learns worse than an agent that would just be given a uniform uh policy so would be fully um Act completely at random and this happens my explanation for it is that the agent then trying to seek transitions that um are the most uncertain will never revisit the same and therefore this won't be enough in our scheme to reinforce the edges such that they are no longer uncertain and that they actually stick um so it would be a subtle um balance between forgetting and reinforcing that is difficult to implement when the agent becomes too greedy and the second scenario is here one that we already know when we set the scaling parameter to plus three uh for a task oriented agent we are now maximizing the expected free energy and as we would have expected the agent stays um does not learn well in these cases because it doesn't Target uncertainty and it's not even trying to um fulfill preferences in the environment now a second point I want to make is insisting on this look as if um in the claim we made for the variation of free for the free energy principle I'm sorry uh what we have tra I'm sorry what we have trained the agent on was only um using the confidence to reinforce edges and never did we do anything with the variation of fre energy however it seems that if we try the variation of free energy it is actually a good figure of Merit to describe to describe the agent and its learning and as you see here these are um uh a average agents um and their variational Fe energy and you see that as before wandering agent and um task oriented agent converge to a very small value um at the end of the training that the task oriented agent go a bit faster and here and interesting features uh if you look at the individual trajectories here of the agents um you will see a little steps in the learning curves which if you actually open the blackbox and look at what happens at these different stages of learning for the agents it's actually corresponding to the agent adopting conventions in its clones to properly contextualize its clone clipse in um the grid here to describe its observations and um geographical location in the grid the third claim um I want to make is trying to understand this difference between um um the task oriented agents and the wandering agents even though it seems that when the agent is able to choose its actions by minimizing the expected free energy um that the desk oriented agents learned learned better than the wandering agents it seems that when we compare their performance by and set their policy to a uniform one that is the agent no longer are able to seek transitions of um maximum certainty it seems that wandering agent equipped with a bare estimate belief State estimation mechanism are able to perform in general a bit better or more uniformly better than um the desk oriented agents and that if we use this uh belief State estimation in superposition with a candidate belief States in um a hypothesis that we are progressively eliminating that for both um um types of training this will double the length of the trajectories for the agent so it seems to be rather efficient at least in this environment now I want to EMP emphas I'll emphasize um this um um slide here which relates to the interpretability of the model the agent is learning and what you see here is a train model for an agent that we train in the grid here um and the transition functions for all the actions uh the agent can perform in the grid and if you look closely and try to understand how um those transitions happen you realize that actually each of the Clones um in the world model that was initially only Associated the only information they had initially was um the smell intensity that the agent could um perceive in the grid um you find that those took on some contextual meaning on top of it that differentiates them and that Maps them to a unique location on the grid that in the end they are able to represent um a geographical location um and yeah so in this sense it's fully interpretable the the final model is fully interpretable and one final claim here is that uh the policy that the agent learned as a result of this look ahead preferences um that I described earlier is optimal and what you can see here is um we've trained the agent to either reach the target here um that is reach the food or um escape the food here by reaching the the lower um left corner instead and so this has happened without intermediate training which just changed um the absolute preferenc so this is a very flexible behavior and we've compared the trained agent to a random agent and we see that it's a very clear nearly optimal Behavior and the the restrictions on this optimality is not due to the the policy itself that is optim um optimal but rather to the belief State estimation mechanism that we are using and um that would require one additional step uh to be able to fully identify the single belief that the agent is and therefore the agent takes this additional action to understand where it is in the grid exactly um so that's the slight of head you might find um in these plots but otherwise fully flexible and optimal and we expect that in a larger environment with a similar structure we would become optimal okay and this brings me to the conclusion for um this presentation where we have presented uh a model for agency um that combines projective simulation together with a free energy um principle framework where we have introduce the concept of confidence in order to uh reinforce edges on the memory uh that uh corresponds to associative learning between percepts uh using this reinforce uh this um reinforcements and that allows the agent uh to plan for its actions flexibly and to discover uh optimal policies using this look ahead preferences and I want to emphasize again um the agent and its model are interpretable both because we can trace back the deliberation um path of the agent on its memory and also because at the the end product um the world model at the end of the training is interpretable we can map it onto hidden states of the environment of or the way we would um understand the grid and I think a nice followup work or a nice connections with we see with other fields is that this model comes with aspects that are um plausible from cognitive science perspective namely well active inference uh has been um already connected to um uh in order to endorse predictive coding and the beijan brain hypothesis the Clone structured um hidden Markov models have been proposed as a way to U model cognitive Maps um for um to determine behaviors and in memory and in particular here especially for navigation tasks um it has been proposed that the Clone Clips can play this a similar role as the play cells in the hippocampus and also the Luger head preference scheme um that we propose here is reminiscent of successor representation that a few years ago was also associated with um the role of dopamine in um kind of um um how should I say combining model-free reasoning with modelbased behavior such that um the U an agent or biological agent can have optimal behavior and flexible behavior in a changing environment yes and with this I thank you for your attention and again for having us here and feel free to ask any question that comes to mind awesome thank you would any of the authors like to just give any reflection or or share a little bit about the paper while people in the chat are writing questions or I can read some questions from live chat okay first um is the code available uh not yet but it would be available soon um yes I'm working on it okay from Leo Deion wrote great work would it be possible to make this using continuous inputs and outputs we are also working on it actually um so hopefully yes it might be a bit harder um but hopefully yes in the future which would be harder it takes a little um change of perspective on how we train the weights and we're currently trained to model exactly that actually um so yes cool okay reconfigurability trainer wrote How would task orientation limit outcome expectations or maybe a little bit more more generally there how do how do you configure task this is a navigation task but but what how would for other kinds of task cognitive task performance how would you bring that into the structure of the model okay so at the moment one of the limitations that we have with the way we modeled everything is that we can only um encod the task as an observation a Target observation in an environment so it doesn't have to be imagine the agent is able um to perceive I am hungry I am full it could also be observation I am full so it's not NE necessarily bound to a navigation desk it can um go to any um it can transfer to many other types of tasks now for sure there is a limitation and it was already visible in the grid world that I couldn't tell the agent go to the lower left corner directly I could only reach one of the three cells that had um um observation zero and what I have in mind or what could be possible is to um give the agent a few rounds of interaction with the environment and combine this look ahead reference theme with some reinforcements or some rewards that the agent would get from the environment um and then this way the agent could be able to identify which belief state is actually corresponding which unique belief State um is corresponding to um the successful state in uh the environment on top of the observation that corresponds to it cool just on the projective side like how do you parameterize the topology and the the lumping and the splitting of these cloned Clips since it seems like having an appropriate granularity and transitions amongst the these clips is sort of at the heart of how effective the method would be yes so what I'm doing at the moment um is helping the model I I'm giving as many clones as um as necessary um to um model the the the hidden states in the environment that is if I have um for example it's coming yeah here um you see that the maximum number of hidden states that would send the same observation these three so I give three clones um to um the model for each observation the the way I initialize the ECM um is um the at the at the beginning of the training the ECM is fully connected so I'm not saying anything about the structure of the environment in the in the ECM to start with except in the number of clones that I'm giving and then um we have been discussing also techniques that could um have the agent have an Adaptive um way of defining the number of clips for or number of clones for each observation where um for example if the agent realizes it needs more clones we could add some and then add a specific rule to connect them to the following month and also what is also very nice is if we give too many to start with and then we are able to reduce to the minimal amount of clones that would be required to represent each observation we could actually learn something called a minimal representation of the environment and that is something that is very valuable from a theoretical perspective as well yeah uh do those clips directly provide the interpretability or what kinds of statements in a grid world or just more generically what kinds of interpretability EX questions can you derive here that using some other kind of Knowledge Graph or graphical cognitive model couldn't yield okay so I can only talk about um the things that I know um so here what I want to say is that um in general Clips in projective simulation are understood as a part of the experience of the agent so by um uh construction at the beginning they have a um semantics Associated to them and here the the way we follow this is by saying at the beginning any clip is something in the environment that emits the same observation and in that sense it is it has some interpretability then this mapping that we can do at the end of the training um allows us to map um the um clones onto actual hidden states in the environment and in that sense because we can kind of open the world model and um take it apart and still understand what's happening in that sense it's interpretable and also one way that this model is interpretable and um compared to others is that at any time the agent is able to deliberate we can trace the deliberation path that the agent is taking in its memory so we know exactly why an agent is taking that decision and not another one and that's also how we reforce the model so we understand how it went there how it came to learn this model in um the first place do any other authors want to add any comments ideas maybe I can just add you know or try to to uh respond to your question in what sense you know where is the interpretability I mean these the clips that Josephine was introducing and talking about um these are internal representations of actions the agent did and uh perceptions it had so they the semantics derives from the description of you know its environment what it how how it canect on an environment what it can perceive from the environment this is the the basic semantics there is and it it inherits the semantics uh onto this clip Network in its ECM and so you know if what josephin said you know if you want to understand why did you do this action giving this percept you can trace the sequence of hoppings uh in these Elementary Clips which are stored percepts and actions it has done in the past so there's a basic uh interpretability there how it came about to to toh take a certain action what considerations it went through as it were now the clip um Network can grow and change over time dynamically then it gets more complicated when there new clips are created how do you how do you transfer the semantics to them as long as you can trace how the clip creation processes took place you can you can connect the semantics of the new clips to the previous Clips where it was made from but then it gets more complicated but the statement is that the basic PS comes with a basic structure of interpretability that is inherited from the task environment which is often an MD Mark of decision process thank you a Marius um perhaps to that I might um add a few things um so first of all I think there's some additional um interpretability in the way the updates are actually done um for example this confidence mechanism and the update rule perhaps the more obvious machine learning approach to take here would be to use the variational free energy um a loss to do a gradient descent on um but this can become quite messy because there are all these um different terms competing with each other and it's not really so clear what one actually gets from that and we now on the other hand have this update rule that very specifically says um that the um probabilities are reinforced um exactly dependent on the number of predictions that they enabled during the training and right now we are taking here and like in Parts it's a tabular approach one could say and there is some strength on it because in principle one can write down everything as a matrix and really inspect it um what's the best transition to take what is the best action to take and what is the best state to predict and um you know there are some papers in the variation of free energy literature and active influence literature that directly throw artificial new networks at everything and as long as one wants performance it's probably the most uh the best thing to do but of course they come with all the interpretability disadvantages So currently we are looking on purpose on environments and agents um that are complex enough to be interesting and difficult but small enough to en able a manual inspection but in the context of this project the idea is really to see how does the agent evolve how does it learn how do all all our principles that we have variational free energy and PS actually manifest itself what do they do in the Asian and for the um clone State something that one like I think there are some important intuitions that one can give on those um one can for example see the Clone index as providing a context but one can also think of it as dividing the um decrease of Freedom into the observable ones these go into the actual observation and the unobservable ones which go into the Clone index yes like um so like here um we actually see um in this figure here how The Clone indices um of the agent um uh directly relate to the actual True World model or the true states of the environment so we can literally say okay like this clone state is this environment State and so on and we can directly um investigate how the world model of the agent actually relates to the true environment model and if you just used um um like some artificial newal Network you could not really draw a map like this or C like this um if you don't mind I would as to have some additional comments on questions that were raised earlier yeah go for it um so there was the question about the applicability to um the con to continuous domains and one thing that's typically done in NE symbolic um AI approaches for example is to only use the um tabular methods um essentially as one uh part of the workflow um for example something that one sees quite commonly in the new symbolic AI literature is that one has artificial newal networks that act as feature extractors and then the more symbolic methods get applied to the um outputs of those feature extractors which is something that could be also applied here but as Josephine already mentioned we um working on an a generalization to The Continuous domain that's much more faithful to the methods that they presented here so yeah so I just want to um emphasize that it is um possible to um generalize this to the continuous domain and we are actively working on it and we have very a very specific Vision in how we want to make it happen so just to clarify this image here the location of the grid like the number of each cell or the X or Y location is not being revealed to the agent but they are being able to perceive this ranked score like pherone intensity or like temperature or something like that and they're learning through their embodied activity the transitions amongst that categorical variable and it's sort of recapitulating the actual spatial distribution even though they aren't coming to an explicit spatial reconstruction exactly yes yes that's a good summary of it okay and so then that in the fullest case there could be a clip memorizing every single location and then in the in um in then a minimal representation would start to Cluster coarse grain repetitive transitions between like maybe if there was a vertical border some of those could be coar grains or just sort of simplified because the same action would result in the same change yes um that's that's quite correct actually if you look at some um some observations actually only have two hidden States so we would only need two clones to represent those and what happens is that since each clone is related exactly to a single location in the grid um the two clones are representing exactly the same location and we could design a rule such that those two clones collapse and here those three clones collapse and then we would have exactly uh the minimum number of clones we would have in the in the ECM would correspond exactly to the number of hidden States we need to model in the environment to be able to predict observations H okay I'll read another question from the live chat and then if any other authors have thoughts on anything okay Leo wrote How long did it take for it to train to solve the grid task and how scalable is this network with the amount of beliefs and actions um I think uh it took a few hours for um the navigation tasks um maybe the order of four hours I think um so it's a trade-off at the moment between keeping the interpretability being able to understand um everything that's happening in the model and the kind of the scale of the models we are tackling um so this is work in progress at the moment being able to scale up this approach maybe we can um consider that um as um Marius was saying this featur trctor could be helpful in um scanning up to larger environments this idea of um adaptive number of clones is also interesting for this um and yes maybe using a hierarchical scheme could also be useful we don't know yet but this is something that we are actually thinking about and um that we would like to try to handle but we don't have a solution for it yet what what kinds of settings have these graphs absent the free energy principle angle been applied towards and would those be the kinds of settings that then you'd adapt this method towards or do you see this as tackling a different set of functions um when you say this kind of graphs do you mean the Clone structured graphs yeah are okay so to my knowledge this has been applied to navigation tasks um there was another paper I don't remember exactly what kind of task that was the the more recent paper where they learn schemes um using this loan structured graph um I don't remember which kind of tasks they used it on um but I I do expect that they transfer to most of the tasks what is interesting is to have some some ambiguity in the environment some partially observable environment um I think that's where they're interesting um yeah I am not fully certain about the answer probably to this maybe a degree of microbi also might be necessary since we are only modeling hopping from one step to the next at the moment um yeah this is something to to look into yeah it's an interesting challenge with wanting a situation that's simple enough to understand but not totally baked in so that it's obvious and these kinds of almost meta modeling considerations come up all the time like if the agent doesn't understand the efficacy of their actions what are they even doing inference on but if they entirely know the efficacy of actions then it's Sol then it's kind of like all loaded up for them to know what to do and then how many layers or what ways do you pull back oh well it will learn the efficacy of the action but that might also not be realistic and so just figuring out what expressivity of the modeling situation do we even want to set up because setting up what cognitive functions for the to perform is kind of like just the complement of the tasks that it was not set up to perform and sometimes that can be a bigger component so I'm just curious I guess as we sort of come to the end like your broader lab and group what kinds of questions and applications are you seeking to bring these methods towards and how do you see this work on the path of of your broader directions um okay so for myself they are two directions that I'm currently interested in and I'm still at the beginning of my career I would say so it's up to Evolutions um I am interested in the um ability of those models to um coincide with actual cognitive processes that happen in biological agents to be able to model that to maybe provide a tool that can help us understand this kind of deliberation and then to see how we can um um bring this understanding onto artificial intelligence models and matchine learning models in general this is the first aspect I'm interested in the second one would be um how do we discover or or from these very basic experience the agent has with the world how do we discover how does such a an agent build abstractions um and use them in ways that are um efficient in coping with the environment and Performing um yeah um fulfilling some preferences in the environment I think that's so far the two things I am thinking about I think that projective simulation has been used in uh on many problems um as a reinforcement learning methods on um you know it has been benchmarked on those um um traditional problems for um reinforcement learning the mountain car problem grid World also it works very well um also on some um yes I guess modeling um is usually the the main application does anybody have other application in mind bit yeah I can I can jump in oh H do you wanna go ahead yeah I can jump in a little bit um because I think one of the things that really interests me and and I know some of the other folks in this group is um modeling in particular animal behavior um and and foraging um and so one of one of the big questions that I think of driving some of this work is uh how animals Forge for resources that are sparsely but predictably concentrated in space and time and in particular when those when those resources are depletable and dynamic um and so this this poses a problem where um one scheme is not sufficient for the animal to come up with foraging decisions so in this framework we might see this graph that we have up on the screen on the screen as as one scheme that allows an agent to solve a task and so this this work has helped us to think about how the free energy principle can be used to help learn this scheme in a reinforcement learning Paradigm through this process of reinforcing deliberative Pathways um but then what the projective simulation model can do for us is start to help us think about how an agent can build up different schemes from a set of episodic memories um and so I think that this this work is kind of serving as a foundation to help us move in that direction of of how we can get use projective simulation to get to a set of schemes and think about a deliberative pathway that's helping agents build simple schemes of their current world um from a more complex and Rich set of memories that give them a full scheme of the world um and and hopefully this can help us to start modeling some of the complex foraging tasks that animals face um that may be um kind of the the the filters system for natural selection that has acted on the evolution of intelligence and in humans and in other animals cool thank you to to bring that back to like what is the task that the agent is set up to solve or not like in Grid World foraging examples often the location is given to the agent and then the agent is doing like this navigation as if it was doing GPS and that is set up as the challenges and trade-offs are derived from that setup whereas using this projective interpretability heris scheme configuration it could just be the learnt relationships between like Shadow and light or other kinds of sensory cues that might be pretty coarse grains and so it's not to say that there isn't an X and A Y location it just that when that's what's given to the agent or the the ID of each cell number is given to the agent then it's kind of like way going down this road that animal foraging may not even resemble and so this can I think hopefully bring it to a space of having the different experiences or Clips like the honeybees going on these orienting flights and they go out and they come back before they start to go for foraging trips and bring back food and so those kinds of her istics and and then what curriculum would support those Transitions and then each Edge can also be seen as a node connecting to other nodes so what should the structure of that graph be and those are some very flexible ways to talk about context switching and all of it yeah I think you're on the nose there do any other authors want to add any thoughts or what will they work on next year um I mean Hans wanted to say something um because um like of course uh so maybe um like the answers given so far don't really cover everything that we're doing in the group yet um because most most of us actually have um have a background in Quantum computation so we are physicists um who have mostly worked on Quantum computation at some of the points of our careers and um I don't know uh Hans do you want to give like a comprehensive Summary of why we care about explainable Ai and um world model building or should I no maybe I can just uh uh you know say something I I think Alexander gave a very nice Outlook and also embedding of all this uh paper and this Research into the into the foraging and and behavioral biology U projects that we are following and I don't want to add much more to it I see the still larger scope of this is that for several years we have now been really trying to understand agency and how to to model agency um really in a in a in a physical in a physical model so a notion of agency that is you know uh accounts for certain uh philosophical aspects as you know planning having an intention and and uh being situated in environment trying to get around being adaptive flexible things like this a non-computational understanding of agency this is what we want to model and this is another step forward it connects to some branch that we are also following in the group and this is what what mariio hinted at and and that has to do with the modeling of artificial agency to what extent can one uh understand or even build artifacts that can also learn but act in a meaningful way on an environment have a a certain notion of memory that helps them to build model and understand their environment yeah and that's of course something in physics and in the in the ongoing development of AI and its use in science is uh that is quite relevant so what we want to understand is or to develop a perspective on AI that one could call artificial agency now in the future we envision there will be devices that can themselves learn and interact with with an environment that do you know that perform experiments people are now speaking of self-driving Laboratories and stuff like that and that is all within a certain Paradigm and framework that's being developed at the moment but I think what we need to understand is really to to which extent can one call these or future artifacts agents to get really a relation to what they do and compared to what we do and how we are and and that's the broader scope and of course because to understand agency we should also be able to model biological agency yeah to have a clear to put what we would call artificial agency into context to what exists already that is biological agency these are different directions and dimensions of the of the of the research We are following in but that's I would say the bigger picture thank you for adding that anyone else want to add any comments the last piece just makes me think about observing the animal in the grid worlds and that's but you don't see inside with the construction of artificial or synthetic agencies then it opens up kinds of interpretability that aren't available like to the bird watcher and so there's some systems that were just watching the bird and then there's other systems where people are talking about designing it and so being able to have a a shared understanding and formalisms that that connect between those interpretability settings and and even no there might be things that are um interpretable from the outside but not from the inside or or at least even just the possibility of those kinds of questions and understanding where they always sometimes never happen is important research on the path towards this sort of more agentic Niche that you're pointing towards hunts awesome well thank you again for the presentation and keep us Post on the work and good luck thank you for having us thank you for