hello and welcome it's September 13th 2024 and we're in active inference model stream 14.1 with ranway talking about value of information and reward specification in active inference there will be a presentation then we'll have a discussion and looking forward to people's comments and questions thank you again for joining and looking forward to the presentation yeah sure so I'll just dive into it um yeah so thanks again for uh the invitation and uh uh so this paper is about the value of information and then some aspects about reward specification in active inference but it also touches on PD pce and so the motivation is very simple so was simply trying to understand the expected free energy objective for Action planning in active infant um and the nice thing about the EF objective uh which makes it appealing objective for uh a variety of uh people in a variety of domains is that it has a very intuitive decomposition as the sum of the expected value term and epistemic value term and then so the expected value term is this term that encourages the agent to obtain reward or uh achieve their preferred state of the world um because it's simply the cross entropy between the predicted uh future observations given some actions and the preferred observations P TOA um the second term epistemic value is what makes active inference uh or make what makes EF very interesting so the epistemic value ter is defined as the expected K Divergence between uh future uh posterior so belief about future States given future observations and the predicted um distribution of future States um so what this term does is it quantifies how far is the future posterior from the future prior which essentially quantifies the amount of belief update so if you op this term then it essentially encourages the agent to take actions that would lead to a higher amount of belief update so higher amount of information obtained about the environment um so experimentally people have found that uh optimizing this objective led to some very interesting Behavior so on the left hand side here we have an example from one of Alex papers uh where he compared the uh what we call the state coverage of reward maximizing agents versus active influence agents uh in this uh simple environment called a mountain car where the state of the agent is uh represented by its position and velocity so we see that active inference agent has a much larger coverage of the state space so it spreads more territory of the state Space versus the reward maximizing agents and so this um potentially allows the agent to do is to learn a better and more comprehensive model of the environment which it also turns out that having such a model enables better action selection and actually achieving higher reward um so on the right hand side we have an example from one of my papers so I I did a lot of study on modeling human driving behavior and this is one of the examples where we have a car called the ego vehicle driving down the road and then on the side of the road we have a large truck which potentially obscure the view of a pedestrian but we don't actually know whether The Pedestrian exists or not exists but if The Pedestrian does exist we assume or rather the ego vehicle believes that it's going to cross the road so we don't want to cause any conflict with the pachan who's trying to cross the road so this egole vehicle does as the consequence of optimizing EF is that it will nudge slightly to the left of the road to gain view of The Pedestrian and if it sees that The Pedestrian doesn't exist then it will return to the center of the road and carry on with the optimal uh Speed without ever having to stop for The Pedestrian um so these behaviors are very interesting and it highlights uh the value of encouraging the agent to explore and gain information about the environment and this is usually understood as the main differentiator um between active inference and some of the other frame works for decision making such as reinforcement learning but one somewhat contradictory fact um is that um actually Bas basian and met reinforcement learning are already known to optimally trade off explo exploitation so here we have an example from one of the more recent meta surveys where um uh so this is actually a very common Benchmark in met r where we would start an agent in the center of the circle and then the goal position which is unknown to the agent will be somewhere along the circle but again it's never known to the agent so what the agent has to do is to explore along the circle to find the goal so basically what it does is it'll pick a position along the circle and it'll just Trace along the circle until it arrives at the uh goal position so this is the uh the optimal strategy for this setting and and we see that it's already being demonstrated by metao so this begs the question even more of what is the relationship between active inference and Basin ail um and the main idea of the paper or rather the main Insight that we're trying to offer is that EF can be understood as an approximation to the base optimal RL policy and specifically via the information reward shaping uh of the F objective um so to unpack this I will U do a very brief review of uh some preliminary preliminary annotations on mdps and active inference and then we will introduce a few analysis tools from mdp Theory which will allow us to better understand and appreciate uh the notion of value of information which then will allow us to derive the main results of the paper and just close up with some discussion and more and and some other interactive uh activities um yeah so so the preliminaries will be very brief because most people uh in The Institute I believe are the experts in these topics um so this is the uh regular formalism for mdps and then um the objective of an agent in mdp is usually to maximize discounted cumulative rewards so I have Chen chosen to use the infinite Horizon uh setting with discounting because it makes the analysis a bit easier um but one so the so the only thing I wanted to highlight on this page is that uh the classic mdp Theory says that for a given mdp with known transition probabilities and known reward function there always exists at least one deterministic stationary Marian policy and then the value function of the policy which is the value accured by the policy in mdp can be computed from the bman equations um and then so another ingredient that we need is going is partially absorbable mdp which is just an extension of mdps where uh the state variable now is no longer directly observable to the agent but the agent can observe another signal o that's usually correlated with the states um but because the agent can no longer observe the state then if you condition your policy only on the most recent observation you don't really have um sufficient information for uh taking optimal actions so in general for p DPS the uh optimal policy has to be conditioned on the entire history which uh as it accumulates can tend to an infinite length um so here we need to uh recall another very uh important result from P DP Theory which is that the belief state which can be computed from the Bas in belief update is a sufficient statistic of the entire history which means that if you condition your policy on a belief State then you don't have the condition of policy on the entire history and then once you do that you can Define the value function also in terms of the belief so we have What's called the belief action value function the belief value function um so this belief State view of pal DPS is very interesting because it says that for any given pal DP you have an equivalent uh mdp that's defined on the space of belief uh beliefs uh equivalent in the sense that it would yield the same optimal policy for the belief mdp and the pmdp and then uh this belief mdp would have its corresponding belief action reward and belief transition Dynamics so the belief action reward is defined simply as the weighted average of the state action reward by the beliefs and what's interesting about this belief mdp is how the belief transition Dynamics is defined um so you can understand this by looking at this uh these two terms on the right hand side so this basically says that in order to transition um the belief from one time step to the ne next time step to get B Prime the first thing you're going to do is to draw uh a future observation um or sometimes I call it counterfactual future observation o Prime from the posterior predictive of the observation distribution under the current belief and you're going to you you then you're going to perform a basing update on this counterfactual observation to get the next belief and so because the marginal distribution over the observation is in general stochastic this makes the belief transition Dynamics also stochastic um so this is the belief mdp the equivalent belief mdp for the PDP and then we I will refer to this as the base optimal belief mdp because the policy computed from such a belief mdp would be would be the base optimal policy for the pal DP um so then we have active infant which similar to the canonical pum DP framework has an infant step and a planning step and then so the difference between active inference and PD the regular P DP is instead of Performing um exact basing update will perform variational updates but um because from the variation inference literature we know that if the variational distribution qfs is chosen appropriately then it would have to be equal to the exact Basin update so this doesn't really introduce too much difference from the classic um P DP framework the main difference between active inference uh and the class of P DP is that the agent would um instead of optimizing reward the agent op minimizes expected free energy uh and uh I I wrote the uh this form of EF here which I believe is more General um sometimes it's sometimes referred to as the energy entropy uh decomposition um so something I want to remark on this page is that whenever we talk about EF we need to be very precise about the definition of different cues because there are too many cues in active inference and in EF um and here I Define the Q over future States as the product of U Future marginal State distributions uh this is not because I invented it it I'm just uh writing this um uh writing with the usual the the usual definition of uh efp in a more formal way and explicitly call out that this is the product of future marginal States um and then so one more step to get the pragmatic and epic value decomposition of e is that we would have to define the energy function or the preference distribution to have preference only on the observations and then we can split up the energy entropy decomposition into the pragmatic and epistemic value decomposition um so what this characterization allows us to do is that we can now see EF as a specific type of belief mdp where um it it would be different from the base optimal belief mdp but it would still have its own belief action reward and belief transition Dynamics so if we look at the belief action reward it's simply the sum of the one-step pragmatic value which I denote with r TAA and the one-step epistemic value and if we unpack the one-step pragmatic value we can actually write it as a linear combination of a state action reward and this is because of the linearity of the expectation uh which is used to define uh the pragmatic value um but the linearity doesn't hold for the epistemic value and in fact the epistemic values concave in the beliefs um another interesting aspect about the belief transition Dynamics for EF is that it doesn't so different from the base optimal belief transition Dynamics it doesn't contain the counterfactual observation o Prime so what it means is that uh instead of drawing a counterfactual observation from the current uh posterior predictive marginal and doing a counterfactual update you would just directly propagate your current belief to the next uh State marginal and so this uh is sometimes called the open loop belief updating in the literature in a way that's analogous to open loop control so that you would update your belief without taking into account of the next observation but only during planning not during execution which we'll come back to later um but the belief mdp view of EF uh would tell us that uh there exists an optimal deterministic markovian policy simply due to the characterization as an mdp um so this with all of these um precise characterizations we can also make our main question uh more precise um and so we saw that EF uses the open loop belief Dynamics with an added information reward where whereas the base optimal uh belief mdp uses the regular uh can the regular linear combination of State action words and a close loop belief Dynamics so can can we understand uh these modifications in efv as an approximation to the base optimal belief mdp and in turn the base optimal policy so that's the question we're trying to pursue uh in this paper um and then so in order to answer that we need to introduce some tools from mdp Theory um so the first set of Concepts we introduce are the policy advantage and the model advantage and for the sake of discussion now we just understand them as some abstract notion of error measures of uh the policy and the model and the reason why I say that is that if you look at the definition of the policy Advantage for example so it's defined as difference between two value functions so for the first value function the action is uh taken from the argument of uh the advantage function whereas in the second value function the acttion is taken from uh this policy Pi which is used Define the value function so the reason why I say this can be understood as an error measure is that if the action a is more or less the same as the action drawn from the policy Pi then the advantage function will be zero so there will be no difference between between them so this essentially characterize how much is action a different from an action that would have been chosen by uh the policy that's used to define the value function and then basically more or less the same idea holds for model Advantage but instead of measuring the difference in the actions we're measuring the difference in the two Dynamics by saying that if I draw a next state S Prime from Dynamics P Prime and another next state s double Prime from Dynamics P how is the value of S Prime different from s double Prime so if P Prime is actually equal to P then there is no model Advantage so this thing measures the difference between the two models um two other Concepts we need to know uh one is effective Horizon so this is basically introduced uh uh as an analog of planning Horizon for infinite Horizon discounted problem so it's defined as one over one minus the discount uh discount Factor gamma so it's basically saying how many time steps do I need to accumulate until the geometric discounting that's usually used in uh in RL setting uh is zero and then the normalized occupancy measure is essentially the uh stationary or marginal State action distribution along the mark of chain of rolling out a policy in a Dynamics so this kind of essentially characterizes the equilibrium behavior of the system um and then so with these Concepts we have our first analysis tool uh which I call performance difference in mismatched andd pce so what we trying to do in this LMA is the following so we we say that we're gonna look at two policies pi and Pi Prime and then both policies are optimal with with with respect to their own mdps which we denote as M and M Prime and we we're going to say that these two mdps would share the same state and action space same initial State distribution and the same discount Factor but they could potentially have different rewards and different Dynamics so this is this this is very similar to the comparison of the EF policy and the base optimal policy where um they share the same spaces but they have different rewards and different Dynamics so this is basically our tool to to quantify their difference and then so what this Lima said move on to say is that um we're going to look at the performance difference between these two policies but both of them will be evaluated in environment M and because uh policy Pi is by definition the optimal policy for uh mdp or environment M then there's no way that Pi Prime is going to do better than Pi uh so that's why this quantity is also called a regret uh so we're going to interchange with calling it regret or performance difference but um basically this relationship here shows that the regret can be decomposed into three terms so the first term is the advantage of the actions drawn from uh uh policy Pi which I would uh I call it expert distribution because it's the better policy in this setting and then the second and third term um are defined in terms of the model advantage and iuse um notation or abuse labeling to call it also the reward advantage and then so you can understand this relationship uh by um saying that if R and RP Prime are equal and then p and p Prime are equal then there's no reward model advantage or disadvantage between the two policies and because of that then there's also no policy Advantage then the regret would be zero um so and and so the nice thing about this relationship is that if we put an absolute value sign uh over it and then we have um do some algebra then we can actually obtain an upper bound on the regret uh that's defined in terms of the policy Advantage reward advantage Ag and uh the difference between the two Dynamics so from now on I will actually call reward and policy Advantage um reward and pause the error because that's to some extent what they represent so what we see here on the right hand side is that the coefficients for these different terms are different so the coefficients for the reward and policy era are linear in the planning Horizon 1 over 1 minus gamma whereas the coefficient for the uh Dynamics difference is quadratic or squared in a planning Horizon so it says that if we're uh making errors or mistakes along all of these dimensions then the rate at which the Dynamics model error accumulate is going to be much faster than the rate at which policy and reward error accumulates um another thing to remark about this relationship is that it can be a little bit conservative because um sometimes even if you make mistakes with your choice of Dynamics you can actually make it up by uh choosing the reward function to be different from the optimal reward so basically whatever if if you incur some disadvantage in the Dynamics then you can choose to gain some advantage in the reward so that it canel out and then so basically I'm going to show that this is what happens with um the EF Poli EF policy compared to the base optimal policy um yeah so let's with this let's refine our main question a little bit more so we're trying to understand EF as an approximation to the base optimal uh believe mdp and it's specifically the way in which um it approximates that is via this reward shaping term to cancel out the this disadvantage uh due to using open loop belief Dynamics uh so before we do that we um we should review the notion of value of information uh which will give us clue um and also make better connections between uh these two topics so the value of information uh the most popular definition was given by Ronald Howard 1966 where he defines it as the reward that a decision maker is willing to give away if they could have their uncertainty resolved um by some magician for example and if you commit to this definition then you can quantify the value of information simply as the difference between the expected value given perfect information and expected value without perfect information and then one very appealing fact about this is that because an optimal decision maker cannot do worse by having more information then the expected value perfect information is always not negative at least in the single agent setting um but the thing about pmdp is that in general you cannot just provide agent with perfect information um usually what happens is um the environment provides the agent with an observation that provides indirect information about the environment um so we need to essentially make an extension of the expected value of perfect information for the P DP setting and uh and it's basically very straightforward and I would just label it as the expected value of perfect observation because we can make an observation rather than be given perfect information and this is sometimes uh called the the value of imperfect information uh from the decision Theory literature and the definition again is just the difference between the expected value given the perfect observation versus without the perfect observation and uh the non- negativity uh result apply again in the setting and then so to apply the expected value of perfect observation to the pal DP setting we then have to take into account the the sequential nature of decision making and once we add that it actually turns out that uh the definition of EV in the setting is um the expected value using the canonical linear combination rewards plus the open loop belief updating whereas the expected value given perfect observation uh is the same as the base optimal value function so so this is very interesting and um it also so because uh the definition of the expected value uses open loop Dynamics it sort of serve as a bridge between the base optimal policy um the active inference well actually maybe I put it here the active inference um policy whereas the open loop policy is in the middle um because this open loop value function takes one turn from the base optimal policy and takes another term the belief transition Dynamics term from the active inference policy um so again the non- negativity of uh the value of perfect observation appli in a setting because uh you increase the planning Horizon you're basically acre the non- negativity of value of information along your planning Horizon um so this is great but uh the problem with this proposition is that it doesn't really tell us how much uh is the closed loop or base optimal policy better than open loop policy uh and it also doesn't take into account of the fact that usually uh in applications even even though we would plan a policy in open loop when we deploy the policy we usually still deploy the policy in close loop by allowing an agent to observe the environment and update their belief and if we consider all of this then uh we we would actually need to compare um the Clos Loop policy and open loop policy using the performance difference in mismatch mdp where both policies are tested on the ground Truth uh pum DP which we allow the agent to update their beliefs um so if we do that if we make comparison that way then one thing we know is that the main contributor to the performance difference between the two policies is due to uh model advantage and specifically in this case uh is due to the uh disadvantage of using uh open loop policy at open loop Dynamics and open loop policy planning so what exactly is the advantage or disadvantage of being open loop so here we have a proposition that quantifies that and it turns out that the advantage of being close loop is exactly related to um The Information Gain so how did we get this um so basically we start with a one-step decision making and in that setting the advantage of being a Clos Loop is basically the expected value of perfect observation which is the difference between an expected value given perfect observation and without and then we can rewrite this as a linear combination of the difference between the updated belief and the prior belief and if we apply some bounds here then we can uh rewrite we we can upper bound this as the expected Information Gain and that's how we got this and if we ACR this along the planning Horizon then we basic basically multiply this by the effective planning Horizon 1 overs 1 minus gamma um so that's how this is obtained um so this is actually very interesting because it give us a clue that if we want to improve the open loop policy while still committing to open loop Dynamics then one very reasonable thing we can do is to change its reward and the Very obvious choice for changing its reward is to Simply add uh is to Simply make up for uh The Information Gain that it loses by using the open loop Dynamics and add that back to the reward function um and that and once you do that then basically you got the efp objective function in here um so to be able to formally show that the efu policy which is simply the open Lo policy plus an information reward and show that how how much it improves on top of the open loop policy we need to make a few assumptions so the first assumption we make here is on the preference specification so basically we need to what this assumption says is that uh we need to make sure that uh if we still want the EF agent to be go directed then we need to set the preference in such a way uh so that the gain in pragmatic value uh after a belief update is always greater than the loss and Information Gain after a belief update so this way we make sure that agent focus on a task and doesn't get distracted by collecting information that's potentially not useful for the task and then the second set of assumptions are introduced to make us um able to compare the open loop policy and EF policy on more of equal ground so the first assumption is that the we're going to assume that the EF policy commits no more uh or rather it does no worse in terms of policy Advantage compared to the open do policy and the second assumption says that the Information Gain value along the horizon is always going to be greater than some values uh so again these things are introduced just for us to better compare the two policies and with all of these assumptions and the previous insights we have our main result here which gives the regret bounce for the open loop and the EF policies and so we see here that for both the open loop and EF policies we have a term that's linear in the policy era and quadratic in the uh Information Gain but the difference uh between EF policy and open loop policy is that the EFC has an additional term which is uh a minus L something uh that that is linear in the information gain so basically the upper bound is being reduced by this linear this term that's linear in the uh Information Gain so that's how much uh the EF policy improves upon the open policy uh which makes it a better approximation to the base optimal policy than open Lop policy um yeah so that's that's the main results and the main idea uh in the paper so we'll just close up with some uh discussions that might be interesting to people from different communities um so the first interesting observation is that um this view of EF as an approximation to the base optimal policy actually connects back to a lot of work people have done in pomdp planning so the thing about PDP planning that's very tricky is that even though mdp policies are very easy to compute as soon as we introduce partial observation it just becomes extremely difficult to compute the um optimal optimal policy that would optimally trade off exploration exploitation so people have explored a bunch of humanistics and one of them is called the Dual mode controller where you would design an ad hoc rule to switch between uh exploration mode and exploitation mode so for the explor and and uh and and that switch is being uh determined by the entropy of your belief with some um chosen threshold and for so for the exploration mode um um you essentially take arcmax of the current belief so that's the state that you believe is most likely to be the current state and you would simply choose an action uh from the mdp policy which we know is easy to compute but if the belief entropy is higher than your threshold then you would take an action to minimize the entropy of the next belief so you can read this as something that's very similar to maximizing the gain of information in the next period uh another interesting framework again from the P DP literature is that um some people have realized that there just there there's certain types of behavior that just cannot be modeled as um obtaining uh as as reward driven um and and usually what happens here is that you want these agents to actually minimize the end U entropy of their beliefs or maximize the information gain so this is usually called Ro pump DP or active sensing so the objective function in this um in this framework usually is just to minimize entropy of belief but not the onestep belief entropy rather the cumulative belief entropy along the entire planning Horizon so something very interesting about this is that um this term if if we introduce um a uniform prior belief that doesn't change over time steps then minimizing belief entropy is actually proportional to maximizing the expected K Divergence between an updated belief and a uniform prior belief so this looks really similar to The Information Gain objective uh in efv and usually what people do is that um instead of just minimizing belief entropy depending on whether we have any requirements for uh getting rewards we would do a waited sum of the reward term and the belief entropy term and that and when you do that that objective is essentially the uh belief action reward uh in in the in the EF case um so the one final thing is that actually goes back to active inference so the formulation of active inference that I have introduced I I qu the vanilla active inference and it's also the active inference uh the the the version of active inference that's currently in uh the main branch of imdp um but um in 2021 um people have proposed A variation of active inference called the sophisticated inference and basically what it does is that uh it maintains the um reward plus Information Gain as the uh belief action reward but he actually uses the Clos Loop belief Dynamics I think there's a sentence somewhere in the paper that says that the difference between sophisticated inference and the vanilla active inference is that a would plan not for future States but for future beliefs so if that's the case then um uh it it's essentially using the same belief transition Dynamics as the base optimal U belief mdp um so for practitioners then um there's a potential problem with sophisticated inference which is that um now you're basically being based optimal with your belief Dynamics but you're adding an additional um term in your reward which means that if the base if the behavior that's supposed to be produced by the base optimal policy you're now ruining it with the additional information gain term um so basically the concern is that agent uh sophisticated INF agent would try to get too much information at the detriment of uh getting less reward because you would spend more basically you would spend more time uh collecting information than focusing on a task um but um essentially what comes to the rescue is our old friend uh the temperature parameter so depending on how you set uh the Precision of your uh preference distribution for example using a temperature parameter of a softmax distribution then you essentially construct a weighted uh essentially a waiting of um the reward term versus The Information Gain term so so if your preference is very precise with a very low temperature then or in this case a very high value of lamba then you're essentially saying that I only care about the reward I don't care about the information gain and in this and and and if you do this then sophisticated inference just becomes exactly equivalent to the base optimal policy um so um so essentially the Lambda temperature term Lambda interpolate based optimal policy for uh reward seeking Behavior versus information seeking uh Behavior if you have uh low Lambda and this essentially makes um sophisticated EF a strict generalization of the base opal policy uh so that's everything I wanted to discuss I think uh so the final slide is for people who are interested in uh more uh the basics of active infants and also the connection between active infants and RL I have uh two blog posts to offer that some people perhaps find helpful yeah thanks again awesome thank you all right I'll read a first question from the live chat and then I'll read some questions I wrote and some other questions from the live chat so Andrew wrote you mentioned that we should set preferences such that the pragmatic value outweighs the information value lost after the agent has already achieved that information value so that the agent does not become too focused on seeking information rather than achieving a goal can this strategy risk the agent getting stuck in a local Optimum by setting preferences which mitigate information seeking okay I I think something slipped her mind because because it appears to me that the question is somewhat reversed so if I'm getting the idea right so you're saying that if the agent somehow gets stuck in a local Minima along a trajectory to get reward then one thing you can do is to change for example The Waiting be between the reward term and the um the Information Gain term so that it um essentially switches the mode to focus on getting information and hopefully bounces itself out of the uh the local Optima is that right it could be um and then is the question whether that's a reasonable thing to do whether there's a way to automatically do it um or or is is that what what you're looking for I I don't know yeah where where and how does preference learning and scaling come into play so to to keep these kinds of epistemic and pragmatic dualities if there is going to be a unified imperative for policy selection that's projecting onto a a set of EF values then how do you wait the epistemic and pragmatic value like in training and in in deployment yeah I think this is an open question I don't think anyone has done very convincing work on this topic specifically so I know about One thread of work from uh Carl's people uh essentially they have a way to uh Define uh an update rule for the the temperature parameter or the Precision parameter so basically what happens there is that if um you end up in a situation where um your variational free energy which quantifies the accuracy and essentially the accuracy of your predictive model if variation of the energy decreases uh which means that your um starting to observe signals that uh constantly violates your model of the environment then you would choose actions in a way that's more stochastic so there are ways to derive such rules um from uh the view where uh of um this notion of free energy unifies perception action so people from Carl's lab has derived rules like this um and then uh there's also a work from nor and um uh I think it's called um uh I can't remember what it's called but basically what that work does is uh it's trying to use Basi and updating to learn uh the preference as the agent interacts interacts with the environment so basically the agent would start by having some preferences about it about what states or observations to achieve in the environment uh and as the agent interacts with the environment it would actually refine its own model its own preference model rather than its model of the Dynamics of the environment um but something very tricky that happens there is that um so she did the experiment in this environment called frozen lake uh so basically you're walking on a frozen lake and there are holes on a lake and if you somehow step into that hole you would fall or die and then um an agent that learns the preference in that environment basically learns that it prefers to fall into holes um so so that doesn't really make a lot of sense um if your goal is to maximize reward for staying alive but it makes sense in a sense that agent just builds uh a preference model that uh is essentially in compliance with uh with what actually happens in the environment so it's kind of Self evidencing in the wrong way um so do either of these two approaches um provide a practical way to automatically tune a temperature parameter uh to balance yourself out of a local Optima like you said I I don't think either of them do that um so so I don't I don't think I have a comment that like firmly addresses this at the moment I do know that um people who actually more work more on the Basin arel side they have um done some work uh to derive approximations to exact um basing RL and in doing so they were able to derive some automatic temperature tuning rules um to achieve that so if you're interested I believe the persons is Ian alband um but I can I can be wrong but just look for Bas in R literature um it's it's a paper that um essentially derives um U A Basin RL version of soft acor critic awesome thank you and another way to address that is to avert slash a void structurally so that like a blood sugar distribution could be loaded with high pragmatic value and is distribution could be driven by high epistemic value in hierarchical models so it isn't that every single model is just Loosely describing all of this cluster of undefined processes this is really getting to the distributional level and then if a distribution really does have such a challenging situation that it has to be tight and refined and then make a large jump to an area it's never been I mean those are the challenging situations and that's where these other techniques and methods can be brought into play however those are the challenging ones and and that is not even necessarily needed for simpler settings um well thanks again for the really instructive and informative presentation to kind of just restate I saw the big move as grounding in the full observability conditions and the analytic situation for the Markov decision process for the mdp so certain kinds of guarantees exist that are um as as they should be slightly weakened with the partial observability introduction of observables and latent States so how is it possible to use some of the techniques patterns guarantees etc for full observability on a partially observable situation and the move is kind of to consider the parameterization of the generative model as a fully observable setting which is what it is in like the programming environment whereas if if even part of it were obscured then even that would be a partially observable setting so by treating like the ABCDE partially observable mdp that we see in the textbook and and all this work by treating that as fully observable essentially to the proof finder exactly Thank You by by treating the full State updating of the PDP now that bottomup semantics of those variables might be the probability that this happens or the prob ility that this maps to that so still it is about partial observability but then considering that object in its fully observable condition to itself then you can see that no regret situation where there's no policy advantage and one's own sense making didn't have a disadvantage and an expert evaluating one sense making didn't have a disadvantage so in those situations the learner characteristic is zero regret on the policy and on the sensemaking side and then there's a kind of imperative which is the distance from the expert sense making and policy selection because it's basically like what would have been the expert policy and sense making if it would have been fully observable and then the pmdp P PDP reduces to an mdp yeah that's precisely the um the the strategy so um planning in uh pum DPS um can be turned into planning in mdp is just a much more difficult mdp you don't get it away with any of the difficulty of planning in belief mdp um because uh now the state space is uh essentially H essentially has um countably infinite uh cardinality uh which means that there's no way for you to uh compactly represent your uh beliefs over multiple steps or your value functions so it doesn't make any of that or any practical applications simp but it does offer a very nice theoretic technique to to better understand uh the different kinds of tweaks you can you can do to the theoretically optimal uh base optimal policy cool and and then another connection is with the Dual mode switching so in like chapter five of the textbook in their neuroanatomy they show one schema of how dopamine level regulates sort of a system one system two Thinking Fast thinking slow shift in the thinking fast mode observations expected or otherwise don't even need to be considered you can just sample directly from the policy prior from habit and then even to engage expected free energy use G to update policy prior into a policy posterior so even in even to consider possible future actions and observations that's a deliberative mode it's dealing with intangibles and doing computations that where habit will suffice can just be done so then by by proxy you can use the observation entropy and just say if the observations are within this acceptable threshold there's no reason to step outside of habit and then where observations start to become surprising in this way then a deliberative execution per policy can happen yeah so um I I think this is very interesting like um so mode switching or like um self um essentially self selection of how much compute resources you put into at at test time or at runtime um is a very interesting topic I know Alec has done some work on um like once you amortize a policy you can still choose to refine your policies on on online uh and so on and so forth and and it also makes a nice connection between what's happening with like large language models now day you have like so-called like post training you can do a lot of stuff so for example in what's called best of end sampling where you sample a ton of uh outputs from the policy and then you use that reward function of score which one um is better and then choose the output that instead of all the other um completions that you have sampled and then basically the more completions you sample the more likely um you'll get a completion that's um going to be scored Higher by the test time reward function and then you can basically decide okay what what's the trade-off I'm going to make between my test time compute that's been used to do a ton of sampling which is very expensive versus um um versus the the potential improve Improvement in reward that I'm going to get um so this this is very interesting um but I I don't think um like in in the active frence Community people have done enough uh principled work uh to overcome with the principal framework to uh allow an agent to dynamically adjust their uh computational resource and time uh on the flight so I would actually encourage and also I don't really understand it myself so I would encourage people to do more work uh along this along this direction uh to figure out what is the generative model uh that allow people to do test time adaptation of Computer Resources and so on cool yeah that's very interesting and gets into like system design like is it the agent that's estimating how much resources would be useful for it or is there an estimator that's making a secondary judgment um I'll read a question from the chat Shri Hara wrote thanks for the talk rant in your expertise from an engineering perspective what are your comments on building a deep active inference agent versus a deep RL agent like dreamer um yeah so I I think the the word deep sometimes mean different things um in different communities so in uh so so early in the day deep active inference um used to mean using like deep networks to approximate certain uh distributions or amortize certain distributions um in active inference agents um uh I I think over and then over the years the meaning of deep uh has shifted so now I think within active inference Community deep means having multiple levels of hierarchy uh sometimes it also means planning along uh a longer uh temporal depth um yeah so um so I think I think um being hierarchical is actually pretty difficult I I again I I don't know of a very um nicely principled and still um working way to um make agents um hierarchically deep uh and and also its first principle in a sense that it can be automatically discovered uh from data I have tried to do some work um but I haven't uh found a way in which the hierarchy doesn't uh collapse uh usually what I found is hierarchies tend to collapse and things actually like anything that you can portray as hierarchical can also be represented in a flat way so I would again encourage people to do more work in that direction but in terms of just comparing um the VAR ious tools and also way to hybrid active inference with uh with neuronet u that people have done versus agents so I think um basically a lot of people's view of dreamer versus you know deep active inference uh that many people have implemented is that dreamer is so similar to active inference especially I I can't remember if which version of Dreamer uh and if it does um use an Information Gain termin a planning objective and if it does then uh you basically cannot tell whether there's any difference between dreamer and deep active inference um so so in a sense even Carl has mentioned it sometimes that dreamer is basically like um some variations of um an active inference agent I would I would point but I would point out that there's one aspect of dreamer which uh makes it a little bit um um theoretically suboptimal but it might works well in practice because because of that suboptimality is that um dreamer doesn't perform belief stat planning as as most uh deepl systems don't perform belief space planning uh they perform State space planning because if you look at the the policy architecture of dreamer the policy is not conditioned on a representation of belief it's conditioned on a single or multiple particles sample from the posterior um so that um in theory would lead to a slightly different exploration Behavior but I'm not sure uh how that actually behaves and the different difference it makes in practice so if you're interested in that and if you want to make deep Ro agents actually do belief space planning which I personally have some interested in uh I would encourage you to look at some of the work from Shimon whitson's lab uh from Oxford uh they have done a lot of um belief space planning of De agents so for people who I I think PE some people in The Institute might noce there's an algorithm called very bad uh that uh the active influence folk has uh cited most multiple times so that essentially cames out of Shimon wion's lab um and and in that architecture the agents policies condition on a belief representation as opposed Asos to to state representation awesome thank you yeah um a kind of short comment and thought on this theory and practice like you mentioned multiple name spaces that collide with deep active inference sometimes people are using neural network approximators sometimes that's called amortized inference and then there's sort of in terms of model depth people could be talking about hierarchical like hierarchical basian models they could also be talking about temporal depth so whether it's more layers of the cake on this time slice or whether it's however many layers the cake with more time slices and then you kind of brought in a twist with like however hierarchies can be flattened we see that all the time in Matrix operations where submatrices are used to flatten all kinds of things and then similarly in planning there can be a thicker presence or habit and heris STS can be learned such that planning like Behavior can be emulated with with less depth so that's just so interesting like the fact that these formalisms can go and deep in space and time is really just like an initial surveying of spaces and just saying within the infinite policy Horizon situation this and this and this will be the case and then it's a purely empirical engineering question whether time Horizon of 1 2 3 4 5 has what computational trade-offs and needs so it's just like so amazing how the math has already sketched out some of the dimensions and then within the space of what is sketched out it's an engineering question almost alone and there's still theoretical differences to clarify this entangle but even those alone don't map obviously to Performance adequacy or advantage in the real world or for any given data set so it's kind of like fun to research and discuss this underlying layer which gives rise to those engineering results at the same time though imagining that a certain graph or equation has a certain output on a certain data set is is hopeful um and then also I think it connects to the low and the high road like with Carl's mention about different architectures that recapitulate and get to their own syntheses and combinations of what we call pragmatic and epistemic value or maybe what in the energy based learning energy based models they they might use more like you presented with the energy and entropy decomposition it's like those are constructions along the low road they are model architectures that get to this common Nexus of being able to have an imperative which unifies both epistemic and pragmatic value the free energy principle coming from the high road on on the other hand has no architectural suggestion it's a partition in the mathematical physics State space and so it's unsurprising that there are models that are similar on through further to simplified low road constructions like pdps because we're all talking about the same setting agent or interface input and output with pragmatic and epistemic component and so it's just interesting how all of those architectures even though they they might not be able to like make I API calls to each other or like hot swap different components they have comparability against the high road understanding of their belief updating for the next observation coming in and action going out in a fitness context yeah I I don't have much comments on like High Road stuff I personally um would label myself as a low road person that's that's awesome but both roads must be built yeah um I'll ask another question in the live chat and then we'll see if there's any final questions upcycle Club wrote are there any potential benefits of implementing sparse coding techniques in this framework so I'm not super familiar with that let me let me Google that real quick because I do want to see if I can address that question I I know about sparse coding and I know the connection um between that and uh uh vae um stuff uh so let me look that up real quick okay so I'm I'm looking at a tutorial on sparse coding it says that sparse coding is a class of unsupervised models for learning uh over complete representation representations um yeah so uh I'm not sure if there's a way to interact with the um uh with the the viewer but um um does that mean that um you're trying to um so basically learn more representations than what's sufficient uh in a way that's kind of similar to how people are training uh spars Auto encoders for interpreting language models nowadays I'm not sure that's uh maybe I should read on a little bit more to see what benefits it offer yeah I'll let you know if any other information comes in it's an interesting connection between the Matrix flattening and the sparse coding because sometimes when the matrices get flattened and fused together using the submatrix representations then they have sparsity and then that actually just from an engineering perspective enables sparse Matrix methods and compression to be used so again whether the hierarchical representation and with a lower density or a sparer flattened representation for a given like language Etc has this or that performance that's just an empirical question but it's interesting that flattening gives you a constructed sparsity even for nonsparse information and then there's the angle you mentioned about the um interpretability sparsity and methods to pull out fewer interpretable characteristics like for example um yeah yeah did the viewer respond there no but how about just um upcycle R better handling of missing data would be one benefit I can think of so maybe we'll research and learn 14.2 um yeah what what are your current research or development directions um so I'm interested in um pursuing um this uh view of um principal planning and uh active inflence a little more um so basically the reason why I did the study and wrote this is that I was trying to understand if there's way to develop um better planning algorithms for agents and specifically Bel space planning for agents uh in a more uh principled way and uh and and initially I was doing some experiments um uh and then it occurred to me that and then doing experiments for uh beliefs based planning of active inference agents and I realized that if I want to design better algorithms for active influence agents I actually need to be have a very precise understanding of what um the active inference agents is supposed to do um in a way that's um uh that's uh not susceptible to all the holes in active Theory currently so this um paper essentially contributes to filling up some of the hes in the um uh in the theory of active information planning but just more like planning and approximate U beliefs based planning in general so what I want to do as the next step um for for uh the stuff presented in this paper is to see if there are better ways to uh improve planning capabilities of active inflence agents and just belief space planning for agents in general uh that might um depends on um better um you know leveraging tools from from variational inference for planning my leverage tools from temporal abstraction my leverage tool tools from um essentially sparity or sparsification for inducing more structure it's in a generative model um yeah but what I really want to do is just uh make better planning algorithms for uh for agents any algorithms that actually work um that doesn't require a lot of tuning that um you can just grab and and run it uh for any ideas you have in mind you don't have to look for very um spe specific tools on the Internet and try it and realize it doesn't it doesn't really work that well awesome I'll ask one last last uh question in in theory or in practice how is planning for overaction embodied movement similar and different than planning for covert mental action h yeah I don't quite understand this so I I know a recent paper um I think nor is part of the uh is one of the authors um I think it's in Europe where uh they proposed the hierarchical um model based R agent from um like that someone inspired by active inference and then basically what they do there is to cast um action selection at the higher level of the hierarchy or rather well so basically casting that as covert action selection um so this doesn't really um at least if you approach it from the view of action as an interface between agent and the environment then in that sense action always have to um exist at the lowest level of the hierarchy right at interface between agent and environment so it will never be uh an uh overt action that exist at inside of the agent at the higher level um so so that's that's part of the reason why I say that people should do more work on hierarchical um models and hierarchical planning because uh even though a lot of the existing techniques work so for example in hierarchical planning we have the options framework from uh hierarchical RL uh so what that does is basically uh decompose planning uh at the planning of primitive actions at the lowest level to planning at the um higher level over more extended periods of time so basically the higher level will say execute this task which then gets translated to a sequence of lower level actions such as go into the kitchen grab coffee right that's a single movement at the higher level even though it's a bunch of lower level primitive movements so I think these um techniques people have been using in um hierarchical planning are very appealing and they actually work pretty well uh and now with like using language models to do the option selection they work very well in practice but in theory I I I feel like there's there's still a gap of how hierarchical perception model arise automatically from the environment and how um planning can just automatically be a part of that and then how um you know overt mental actions um you know play a role after you have obtained uh or learned uh hierarchical representation of the environment so all these in my opinion are open questions I don't think I think people need to continue to to address them I don't think people have addressed them to a satisfactory extent at the moment cool all right thank you this was a great Stream So till next time yeah awesome thanks for yeah