right thank you for joining everyone it's 11:21 and we're in our first discussion for cohort 5 chapter 5 before we go to any specific questions where does anybody want to begin or just any anywhere chapter 5 related or adjacent that they want to just begin with any thoughts on the chapter overall or like where it sits in the book or just any anything else at the whole textbook level or chapter 5 level um yeah I just this is a little specific but so far I think this might be the chapter that goes the most into precision and that's kind of a I don't know that it's such a key concept it seems an active inference that I just find that really interesting here um I especially find the discussion of like like getting more into the neurobiological aspect of like different neurotransmitters corresponding to tuning like different precisions like dopamine policies and nor adrenaline Transitions and so on so yeah yeah where whereas often the material aspects of the brain are linked to the kind of estimate of the mean like with rate coding here it's a connection between neurotrans transmitters or or sometimes called neurom modulators because through Precision modulation they're influencing how different things happen of course recognizing that like even the same molecule because of the receptor being different and the downstream processing and the context being different like it's not even like one neurotransmitter does one thing even in one species brain so of course this is painting with a broad brush but still these are real research programs that have benefited from unifying different experimental results related to a neurotransmitter Under the Umbrella of like a model parameter specifically Precision parameters precision and and variance just kind of being like two ways to describe it like how thin or how wide it's like the same thing you're talking about and in chapter five being where any empirical evidence even comes in or any specific non-toy example comes in Oh chapter one overview low and high road chapter four just the math and the details of the generative model so here's the first time that we're even talking about a real specific system which gives a flavor of like the kind of experiments that people have done kind of framings that people take towards the mamalian nervous system human nervous system not to say that's the only system that active applies to or anything like that but just that that's where the most empirical work to date has happened so to provide just one more chapter in between chapter 4 describing the generative model and chapter six how to build your own chapter five is just like this kind of empirical Waypoint to show here's what some research programs involving chapters one through four look like okay now chapter six here's how you would do that in this system or in a different system this first paragraph has a important idea about the sparsity of the statistical model and then the sparsity of the anatomy of the system and in the brain where those two sparsities kind of coexist but having a sparser statistical model means that variables tend to only influence a few other variables and that can make things easier to compute or easier to interpret statistically now obviously you can simplify it or sparsify it to a point where you're missing out in P vectors then you've sparsified too much but to the right extent model sparsification makes it more computable more interpretable and the second sparsity is like the anatomical sparsity which is the fact that although there are a lot of connections in the brain with the dendrites and the neurons and all this and cells really are connected to a lot of other cells in the body they're not connected to all cells and so to the extent that not all things are connected to All Things most directly or in a given way that's the morphological sparsity and the kind of neuromorphic hypothesis is that the sparsity of the substrate has an interplay with the sparsity of the algorithm such that it can be carried out in a energy efficient way um in terms of the overall structure of the chapter and Al just please raise your hand or write in the chat if you have a um question the overall structure of the chapter pretty much builds up to figure 5.5 figure 5.5 represents these three neural systems or these three motifs one of them is this kind of canonical cortical microcircuitry the sort of thousand brains uh Hawkins numenta type cortical microcolumn in the prefrontal cortex the second neural system consider and then that's discussed in the in the context of a hierarchical predictive processing architecture the second neural system discussed is the the variance um the dopaminergic Seesaw in some of the dopaminergic regions of the of the brain in the basil ganglia and so on this balance between on one hand habit driven policy selection where the policy posterior is basically just passing forward the policy prior e and on the other hand the expected free energy updated policy posterior where the policy prior gets like updated with G and then policies are selected from either the best or drawn from that expected free energy updated uh policy space and then the third neural system considered is this spinal reflex arc this is like a cut across a spinal column with sensory propri receptive data coming in and a sort of error differential being calculated between descending prediction incoming sensory data and then that differential basically the sign and the magnitude of that differential can be resolved locally through motor action so it's kind of broadly sketching out a type of symbolic understanding or hierarchical prediction a motor and policy selection apparatus that can have different temperature values and then once the policy has been selected it is used in a kind of set point calculation and then the differential between the set point and the actual is used to enact the policy so the chapter kind of goes through those three different neural systems and sites work that goes into more in depth but it's an example about how a subsystem motif is created and then also how they can be composed and assembled to start to get to these more composite synthetic architectures anyone have like a thought or question or quote they want to go to or we can look to the questions I had a question like in the uh 5.5 in the figure which you showed uh what were the what are the elements in the in the bottom half of it like like yeah this this Pi 3 this is all this third yeah this this areas yeah um do you want to look at this the basil ganglia section in the chapter ah okay okay yeah yeah let's let's look so first part is that cortical Motif second part is that neuro the the sensory motor okay then here's where we get to the dopamine part the subcortical structures first they're introducing some of the empirical gross and fine histology like which actual parts of the brain are connected to what and expressing what subclasses of dopamine [Music] receptors because of the the BIOS Semitic interpretation of what different anatomical connections map to like if it's coming from a proprioceptor we're just going to like call that appr proprioceptive input they use some of those empiric structurings to associate with different um aspects of the model um it just they're like discussing even though the the the imag is later down yeah yeah then so they describe it then they talk a little bit about Parkinson's and about time scales in terms of what um that image is showing and those images are like uh the the they're like anatomically repres representative of the like basil gangli the stum something like that okay yeah um I'm I'm going to go to a figure where the the beta and gamma are shown clearly okay so here's step-by-step tutorial from Ryan Smith this is model stream discuss the model stream one um okay so um this step by step builds up to the model at the bottom right from first static perception so prior hidden State mapping between states and observations observations so thermometer in the room truth true underlying hidden temperature prior on temperature mapping between thermometer and the temperature in the room then the B Matrix is introduced this is a different pedagogical path than the textbook takes which why it's good complimentary resource then from here with have the same L then B is the transition Matrix so that's how the temperature and the room actually changes time to time as modeled um could stay the same could change this is still a passive inference problem and this is like the music listening example from chapter 7 in the textbook then the upstairs control component is introduced Pi is policy policy intervenes in the transition Matrix it basically selects a transition Matrix to use it's like as many policies as there are they're like index cards and then one of them is selected out and that's the Matrix um that's used for the transition under that policy policy selected by this expected free energy calculation on the policy prior and part of that the pragmatic value component is driven by the preference and then finally on the bottom right it's kind of like a a slightly Fuller version now there's no fullest version because you could continue to add uncertainties and all these other things to all the parameters so it's not like the stopping point but here it's more explicit that the policy is um driven not just by expected free energy based evaluation but but also it's driven by the prior on policy here it's just being shown more clearly and then here's that like variance on policy and it it's just saying that that gamma is like one over beta so just saying whether you think of it as like being high Precision or low variance or high variance slash low Precision those are the same it's like saying how light is this how heavy is this it's like you're getting the same value so what that variance or Precision on policy is doing is it's taking the G updated policy posterior there what functionally what it's doing and it's sharpening or flattening that resulting distribution of policy um likelihoods so under the high temperature setting low Precision High variance policy sampling setting then um differences amongst policies are blurred so that's one dimension that that policy selections controled that is whether differences amongst policies are blurred or in the low temperature setting whether the policy that's slightly more likely is always selected so that's the temperature component and then there's within um um let's just say lower temperature decision making there's this question of how much does g sharpen Pi ranging from none at all policy prior just getting passed through to high level sharpening it a lot so a policy with even a little bit more or a little bit better expected free energy little bit more expected um pragmatic or epistemic value becomes like greatly differentiated from other policies sanj's textbook has some really good visualizations of like how these different features influence each other Nikos uh what was that textbook and uh also have a question about one of the diagrams um sanjie Nam josi's active inference textbook like kind of in preparation if anybody wants to see it then then then email and let us know um but but he has some new new ways to lay out some of the spaces of those like like that kind of space of policy like high and low temperature and then like all all those things but those are like some of the knobs that exist on that diagram that you have before I think it was of the pdps with increasing complexity uhuh yeah so just to confirm so are these each letter is representing an expectation Matrix or some those are all matrices and that would be I guess I'm trying to understand if all of these are internal to the system that's doing the modeling good question the environment is not shown here these are all describing the agent itself and then the difference between the square and the circle they're all variables in the computer program so but um however as shown the the squares are kind of like the ones that tend to be kind of stated and fixed and then the circles are the are the ones that are getting like that are being that where inference is happening like State inference is happening on them but that's not an in principle thing it's actually just sort of an implicit graphical style they might describe it in the caption But ultimately they're all just variables okay got it thanks yeah in the lowest level I think those were sigmas little tiny sigmas isn't when it's observation doesn't it touch on the environment or it's still we consider it still it was on the previous picture you were showing eer like those the lowest the the yeah observations yes the O's yes like this is the thermometer reading at time point one two and three so it's it's on the robot but it does relate to the environment so you're right it is coming from the environment this is kind of the where um these are the sense States so to speak but they're on board they're not this is not modeling the environment and can I ask something um ber deis was mentioning common filters would that would that be that area that you were just showing the observations like kind of be modeled as a common filter or am I off track here yeah it's a great so here's from 7.1 it's the only place I see where um common filter is mentioned in this Koda so like here's the bottom left there here's the seven figure 7.1 so it's same exact thing so common filter um like we could look it up and there's a ton of things to go into but a common filter is basically solving the inference problem how much Decay should the signal have how much time averaging and how should the signal Decay through time so that we're not like over under smoothing so it's very related to to generalized filter it's just like how much should we smooth this signal to do optimal reconstruction on it so you could interpret you you could interpret this as doing a colan estimation through time probably getting in a sequence of observations and needing to do an underlying State estimate through them that's smoother we're getting noisy estimates from the thermometer and then we're going to like draw a line of best fit basically through them or you could interpret these that would be over a very F that would be like a fast time scale or maybe we're talking about the hidden state every um every you know minute and now so it's slower and now we're just thinking okay the a matrix summarizes a kind of smooth so that we're we're not you know it's probably different ways to do it let's see if it's in the textbook chapter 4 part of smoothing part of optimal smoothing is estimating how variable it is so that's another connection with the Precision estimate if it's highly variable you need to do a lot of smoothing if it's low variability you don't need to smooth that much and then there's some technical points about whether like the fluctuations of the stochastic process are this way or that way here's here's one one case where the Precision on policy matters [Music] um that they bring up on page 92 so here there's two policies that are possible like staying seated and being standing just to simplify and if we're seated then we're getting sensory input that we're seated we're getting confirmatory evidence we have high Precision on the everything is kind of lined up the ball is kind of at the bottom of a bowl but how do we consider the adjacent possible sensory data that we would get from being standing such that that movement to standing can can actually be realized the reward learning approach there's there's different approaches so not not not to oversimplify but would be something like you have to switch something in the model so that standing is more rewarding and then learn or decide upon a policy by which that more rewarding policy is selected so kind of just pushes up a question to how do you um come to assign that standing state with higher reward um here the the which is discussed in the context of like standing and seating um it's also discussed in the context of icting and precision on the sensory input and and also especially in literature Precision on like more um cognitive parameters is associated with with a wide range but the the movement and the catic are these are kind of like gross versions and it's kind of this common pattern where um in advance of a movement being made there's an attenuation of precision on the sensory input and then that enables it to become plausible that um the sensor data are this alternative way like the variability of the of the estimate on the incoming sense data includes both how it actually is and how it could be and then from that kind of like foot in the door possibility that is leveraged to actually select that possibility like the the the select action shift is made while the foot's in the door and then as the Precision is clamped back down it's like oh yeah it's totally likely and possible and confirmatory now that we're standing so that's this kind of sensory attenuation phenomena so that's kind of like attentional focusing and then attenuation is like the opposite of attention or just describing like the the diffusion or the suppression of attention which is to say increasing the variance but that's an example about how whether it's like a Precision on sensory or a precision on policy like there's a lot of different knobs to play with even without the kind of reward or habit or learning knobs but just in the Precision knobs a lot of phenomena can arise but these are kind of the cores to statistical elements we return to estimates like expectations Center Tendencies distributions variances and differentials like here inks that's a silly comment but I was thinking if someone can make a board game where we block out different the different knobs and we only play with some knobs that would be cool yeah if we can just play with it and see what happens yeah I I I I hope we do have physical and um digital games I I have several times experienced when people doing somewhat like a guided thought Journey with the goal to increase or reduce Precision on different things call are there digital games that do this um like none that I specifically know of there's Pablo at all's digital game related to ACM that they presented at the the Symposium but but that wasn't explicitly like a psycho technology modulating Precision but let's find out and add to this list wi with all the strengths and weaknesses in caveats of kind of entering at the phenomenological level like just our experiences on any given moment are are not equivalent to the technical definition of some variance estimator but again it gives us a really good experience and sometimes it really does line up and like it it does point to to to really important phenomena and it helps helps yeah yeah Nikas uh sorry to interrupt I was just um G to ask about figure 5.1 thought that was pretty dense um I was wondering if you could skim over it and uh in particular um just highlight again the distinction between the subscript V and X in the middle column okay anyone want to give a thought on this first I'm just going to bring in here so we can see it little easily okay so there's the six cortical layers just just so that people see the hystological part of it there these these six kind of semi morphologically defined layers in the cortex this is not a six layer hierarchical predictive processing architecture actually all six of them Implement like one hierarchical node which is then Associated laterally into something like higher order basian um nested models just to be clear we're talking about like a six layer integrated unit not a six layer hierarchical model but they are layers um okay the left side summarizes these literature cited and um the cell types are described like with the sssp and this is just describing the actual cell populations and their connections the middle message passing that underwrites hierarchical predictive coding so there's two um kinds of nodes here there's Mew for um let's let's double check but I believe Tilda is representing as it usually does through time or generalized coordinates I'm not sure um V and X I just want to find where X is defined or used in the text I think the last time I saw it was back in chapter 4 I don't know if he re restated it yeah where X and V were used yeah I I would go with the MU and x and V definitions from for unless it looks like they're being changed um X in this situ so here here in in 415 is kind of a classical SPM style um two equation setup the second line is our neuroimaging device we're getting sensor data y real scientist sensor data y it's a function of underlying neural activity and V this kind of control unobserved um control State plus sensor noise and then change in the neural activity through time is defined by um it stay at the current moment and then and then this causal Force V plus a variability associated with with it what would an example of a causal force be like the attentional state as far as I understand this from SPM so this would be like the neural activity at a given time so here's the Bold signal or the um EEG signal X would be the the um the actual neuronal activity which is what what um synchronously connects to a given sensor measurement and then this would be like the like attention or no attention unfoldings of that system okay I I I believe this is accurate now return here but we should confirm and we should annotate the figure if someone puts in the the the research and feels confident and then please annotate the figure so that we don't need to do ad hoc interpretation because I'd like to have the ontology based just simple description of this to actually go to rather than need to forge for it it's it's an interesting progression that they go through because on the right is where we see the kind of O and S PDP type um variables whereas in the middle is the message passing more chapter four like variables and would be good to understand if if where are onetoone assertions um or concordances being claimed and what would the interpretation of that be we can look to the papers to see more but on the incoming our sensory areas our information flow is coming in and then kind of policy relevant auction information is coming out modeled by the a matrix and the the hidden States coming in and the um at a lower level so this is the kind of hierarchical logic lower level State estimates coming in and then at the ith level the state estimates being done here influencing policy selection but a lot more remains to be connected about how these are how this is formally linked to to you know for and elsewhere uh do you mind if I take a stab at let if it is in the ballpark so um X could be maybe for example like glucose metabolism or um you know how cells are regulating the glucose metabolism and V would be some underlying maybe thought process um governing where attention and I guess thought process that is being influenced by you know some deeper Dynamics going on in a nervous [Music] system would that be kind of an acccurate so those are those processes are both happening in the the uh cortex but they're representing two different physical processes that are influencing each other perhaps hard to evaluate just in in the moment but it would be cool to flush it out okay cool thanks yeah thank you no it's good we we we should have good accounts of all the figures Pros yeah I just uh was trying to understand the diagram and uh they were also mentioning that uh uh information is Flowing from uh lower cortical areas and it's this directionality it's the like this information is going to the layer four and then uh from three it's going to higher layers so we have that flow of information which can also help us in understanding that yeah just wanted to point that out that's it yeah here's where a three a three- layer basian model is being shown kind of the lower the middle and the higher level and each of these levels they're sending up um observations that are being juxtaposed with descending predictions within a column and for each X and for V there is a me and there's a mean and and an a differential estimators it could be the mean in the variance and that's not even incompatible with it being um the um neural activity and the regime of attention under the model that that the variance is what is controllable high and low variance condition which is exactly this kind of neurom modulator as internal policy par saying well this is the knob that you can control you can release or or not release and you know modify the sensitivity of the post synaptic neuron and the downstream receptor and all that but you can't just be like well this neural population is firing at intensity 4 let's just make its intensity nine because there isn't a way for the Post synaptic neuron necessarily to send back and and to know what to ask for or to make that retrograde ask on the right time scale so different synaptic mechanisms modify the Precision so that different neural messages can be tuned in and tuned out brains a complex system even as as mentioned in um 56.2 today um like even the nematode and the fully mapped nervous systems of the fruit fly and the honeybee have a lot of sophistication not that it's like a ladder of of complexity or anything like that but just these are big big evocative simplified models however they are really developed out and used in Precision neural settings and where there are important critiques and and um extensions those are literally research and development advances but the chapter tries to represent like a little bit of a higher symbolic generalized a little bit of a more basil or Elementary cognitive reflex arc and then the kind of like in between with the dope energic policy selection um Andrew um yeah just for like I guess like clarification and maybe there's not enough information here to be able to fully answer it but like on 98 with um back to the different neurotransmitters or I guess neurom modulators um so like for example with serotonin there at the bottom of the list and preferences or interoceptive likelihood being like the Precision that's that it's being modulated in that case with like a a way we could say that is that serotonin is um changing the it's so it's modulating the Precision meaning it's like adjusting basically how how confident one can be in um in the interoceptive likelihood such as someone is currently experiencing I don't know like a increased heart rate and there's like a signal coming in like informing them of that or their breathing is faster or or you know they're hungry right now or something um a higher Precision like if this is really simplifying everything but like say serotonin were the key lever here that says okay the serotonin is released it modulates the Precision of a hunger signal such that you now pay more attention I suppose to that hunger signal or it appears stronger or rather the the likelihood or Precision of like I'm feeling this sensation uh I'm certain I'm like very confident that it means I'm hungry like is that kind of how that is playing out in some f way that wasn't too much that's a good question also they mentioned preferences or interceptive likelihood but of course they're not the same thing but let's let's think about what that would be yeah to have a precise preference distribution would be like I really want the Die to come up as one and I really don't like two through six imprecise would be like kind of flat across preference distributions so what would it be like to have the flat or sharp interceptive likelihood distribution well if it were totally flat I'm getting ambiguous signals from interception I'm not I'm not getting um useful or epistemically meaningful information about whether my heart rate is high or low high Precision I I'm I'm super sure I'm super sure that I know what my inter receptive state is could be a false Precision on the extreme case too but this is dealing with the parameter right I don't even think they're using at least not in this chapter what one one reason why the precisions matter okay so here's figure 7.10 so here's the kind of figure 4.3 that we usually see PDP and now the a b c d and e the variables that you specify like in chapter six to have the agent those all have these lowercase distributions on top of them they're they they're priors on the other distribution um and so that prior distribution So like um let's just say it's a coin flip setting we could say um I I have my prior is like one zero it started on heads period or your prior could be 5050 0. 5.5 like um I I I'm equally confident that it started on one or the other state that's without the lowercase D so now we add in say we're going to draw our prior on D across trials we're going to draw that from 0. 5.5 plus or minus 0.1 so then one trial you you you start with a prior 6.4 then 0 4.6 and 0. 5.5 and so on so you're drawing from a prior about D and then that could be narrow or it could be tight and then just like in any other basian statistical setting like when a distribution is wider it can be moved D more the distributions narrow even incredible evidence is going to be basically systematically downweighted so severely that the the distribution only is going to move a little bit um even though the evidence May repeatedly be signaling that it's off in the extreme case if you have like the Delta distribution just like a spike you say no the average um height of the children in the classroom is 5 ft plus or minus zero then even getting other values doesn't update your distribution but these are like variance parameters you can think of on the other ones because these are basically again to return to those simple statistical motifs we can basically be talking about the central tendency of a distribution like the the median or the mean the average expectation leloy and approximation all these things so central tendency mean variance or variability those are kind of the two parameters as part of like the generalized gaussian scheme and then a simple Motif is like a differential between two means but a big part of the Simplicity of the hierarchical predictive processing architecture is all it does is calculate means es and differentials and that there's no parameter that's loaded up with like Mega significance and taking on like too much more than just describing a a mean variance or differential for a given model feature nios ju so just to clarify were you saying that the variance in for example a normal distribution is is equivalent to the Precision in this sense um um there like just just it's in the spirit of I know that there might be like a little bit of technical detail if it's a co-variance or something like that but yes the the standard deviation a larger standard deviation is larger variance but I know there's like standard deviation and variance but yes at a broad scale just talking about how fuzzy versus sharp a distribution is high Precision gaussian low variance low standard deviation low Precision High variance big standard deviation and the flatter the curves basically the more leeway you have in picking whatever the policy whatever the curve represents policy is or beliefs or whatever yeah limit case uniform distribution like everything's equally likely then like whatever data point you get is like as good is the best one that you have yeah the other limit case the Delta distribution whatever data point you get you go with what you already thought can you go back to the diagram that you had up just a second ago just had a quick question about that was it I think it was it was the mdp with the um ABCD oh oh okay okay um it so and this is just to kind of really try to understand this model a little bit better is there any particular point in this model and I know it's a little bit different for the one with the squiggly lines from um chapter 4 but is there any particular point point in here where variational or expected free energy is being calculated that we could point to yes 100% of those calculations of expected free energy are happening at the G Okay g is a functional it's a node that's taking in habit and preference and policy prior and it's outputting the policy posterior the whole upstairs is the decision-making apparatus the whole downstairs is just hidden state observations mapping prior so expected free energy never enters into the downstairs component but that downstairs sense making could be described by variational free energy okay got it so that's really helpful and is there some implied intuition here that the there's a feedback loop going on I guess that just happens during message passing iterations feedback loop what uh you know there's a lot of diagrams especially earlier in the book that are going from perception to action perception action yeah great great Point great Point here the policy selection it just kind of selects an internal index card and then it's like but then that's actually Chosen and emitted to the niche and then that results in the next observation coming in differently or not but yes that is not shown the like the a Markov blanket or the relationship between the agent and the niece are not shown that's helpful yeah and and again son jev's textbook I I I hope some he'll share some of these images or people can can look into it because he has some different representations that that that dispel a lot of the inertia of how things were visualized not all of it but but helped a lot um inks and then any other like last thoughts people have I'm turning us back I think to Precision modulation I was thinking of last time Andrew was saying these are just constraints and I was trying to imagine what constraints would look like with Precision modulation but I was imagining this game I don't know if you have it in America like warm warmer warmer hot cool colder like as you're approaching an object stent right like like I imagine the do is doing that like giving you a warm warm warmer towards how right you are about something you're predicting that's that's awesome like that's very much like the kind of um veilance in the sophisticated active inference that modeled veilance like psychological veilance positive or negative they Associated that exactly with Precision estimates if the Precision if things are as variable as you expect them to be you have neutral veilance if things are more precise than you expect them to be you have positive veilance if things are more surprising than you expect them to you have negative veilance and anxiety and then it's like the signal that that is given by by veilance which is like the top level you know which way is the wind blowing of the whole body is like is it getting warmer am I getting warmer or not is my free energy on the right track on the lukewarm expected or or whatever someone wants to calibrate their path too and that's why and that's why they were positing multiple axes of positive and negative veilance right it's not positive to negative it just like it's Precision evaluating different predictions you have so I imagine sometimes you predicting something you're not happy about but the Precision is kind of making you oh I'm right so there's like a a bad positive veilance there if oh yeah um I'm I'm I'm as expected that I'm the kind of person who sees scary movies that surprise me but I'm not surprised in that I'm surprised that I feel this way I mean those are the higher order um Yarns to unwrap these are just the elemental kind of Legos thank you everybody beautiful thank you so much this is fun times as we kind of as as the as the niche continues to evolve around us and we come close to our last discussion in the end of the year so thank you fellow see you next time bye thank you thank you