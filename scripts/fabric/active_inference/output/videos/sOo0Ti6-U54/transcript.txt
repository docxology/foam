hello welcome thanks everyone it's February 1st 2023. we're in meeting 14. cohort 2 of the paradol textbook were on chapter six so today we can look over any and all of the questions on chapter six that have been raised we can also go to the text but just to begin does anyone want to bring up any quote or topic or or anything about chapter six or any section from the text or question that someone wants to go to first or a general point about chapter six um Ali and then anyone else uh well I think now that the white paper from versus lab is published it might be a good idea to read designing ecosystems white paper along with chapter six because it can provide a much wider perspective and see how uh how the research in developing these kinds of active inference based AI uh is currently developing and uh to see how the plan for the future developments have already been discussed especially in the the case for uh sympathetic and shared intelligence which uh is kind of long-term plan to develop those kinds of intelligence systems so yeah I think this paper can immensely help to see the big picture related for Designing these kinds of um active imprint space models awesome thanks yeah the the first 20 pages of the paper have great points but just to jump to the the timeline part it's 2023. this was released at the end of 22. and in the coming years here are some doubling timelines that we might expect or prefer so that does help set the stage um for chapter six um anyone else you just unmute or just raise your hand chapter six is a recipe for building active inference models so it's not the uh fast food restaurant yet but it's a recipe for the home and potentially industrial chef it covers the essential steps to design an effective model and that is going to be taken in like a staged way um and starts on page 105 in the textbook so shall we look at the questions does anyone have a specific question they would like to jump to or any general points otherwise let's look at the recipe and then see if there's things that we can add and then in this active inference model recipe um let's see if we can flesh it out with what we're seeing and learning from doing modeling and also how it's similar and difference from other modeling recipes how different is this than doing a linear regression model how different is it than doing some other type of modeling or analysis give me six chapters to model active inference and I'll spend the first four on the generative model Carl Lincoln which system are we modeling what is the appropriate form for the generative model how to set up a generative model how to set up a generative process so those have been copied out here does anyone just want to give some comments on these stages or how it one just Briefly summarize what each of these four stages are and what they do or just any feature about these four steps I guess one thing is they need to be discrete these steps um and that's already a decision that needs to be made do you mean the steps are separate from each other yeah and we need to have categories let's um if we if we have these um hidden states in the Markov model um these are separate states and um I guess that's a it's a kind of a problem and a restriction also are there other modeling approaches that you think don't have such restriction yeah for instance in neural networks um these Transformer models where you can type in whole uh a whole picture for instance or something like this I may be wrong so maybe maybe there's categories to yes well maybe a related question is when we specify what is a hidden State and what is an observation in our generative model are we kind of locked into that or is it possible for something like a pre pomdp that describes the structure potentially this is between steps two and three like the hidden state is going to be an image and then it gets set up with a certain dimensionality foreign can raise their hand let's just look through the questions see which what we can bring into the modeling page and um like which ones are remaining to be needed why is modeling of the generative process a necessary step for building an active inference model okay we have a lot of notes on here what would somebody just give on a first pass why is the generative process a necessary step for building active inference model I guess on a very high level um because the generative model is ultimately creating a representation of the generative process if we want to create that model we have to allow for the generative model to be able to model the generative process so we also need to model the generative process yeah one aspect there is the generative model is or the general process is providing the observations and then the sort of the second layer on that is the generative model is working with or is building representations or is a good regulator of or is needing to maintain the requisite diversity to deal with the generative process so it would be like asking why is modeling of the grid important in a net logo agent-based simulation it's like well you got two kinds of things the agents moving around the grid and you have the grid so in this active inference particular partition we have the generative model that's the active entity and then we have the generative process which could be another active entity or it can have any structure so like in model stream 7.2 the generative model was an active agent the generative process was just an if then logic Ali I also think that these uh distinctive steps that have been outlined here in this chapter as a recipe to designing um any active inference model are not necessarily uh are not specifically distinct or even sequential steps because uh for example in the first step it says which system are we modeling but on the other hand the choice of sorry the choice of the system that we're modeling would inevitably uh would it have an effect on how on the type of the generative model or the the type of the general the process we're dealing with so in a sense all of these steps are kind of intertwined into each other and they're not necessarily uh have to be seen as a kind of a sequence in a sequential order but rather in a kind of uh from a hierarchical point of view we just need to have which component of the modeling process are we dealing with so for example we can possibly Begin by identifying the generative process and then uh look at how we can optimally represent the generative process uh by a suitable generative model so what I'm saying is these four steps are not necessarily something that uh to be taken on the pace level they are just laying out the the Territorial map of modeling those systems and we might begin from uh each of one and each one of them and then proceed to any other depending on the situation we're involved then yeah thank you it it may be seen as an atemporal partition not necessarily the order that a traditional recipe would have let's let's continue on and so we can just touch on on each of the questions okay what are the four steps some of this material we moved over to this distinct model recipe page just so that we can develop that um in its own section um okay what are the four steps that are being addressed on that page in figure 6.1 what did the unidirectional and bi-directional arrows mean what's a thought anyone has on this so in a Bayesian graph the nodes represent random variables and the edges represents statistical influences those can be represented in an undirected fashion if there's no arrowheads and that's a lot like a structural equation model like if we were dealing with linear Notions of correlation Pearson correlation we could make a structural equation model and then the edges would be like correlations between two variables which are undirected of course in a Bayesian causality framework like Judea Pearl at all it is possible to have a bi-directional arrow um which is a lot like an undirected arrow but also you can have a unidirectional Arrow such that like changes in one variable cause changes in another but not asymmetrically especially if you consider time series information um in the work of Aguilera at all how particular is the free energy how particular work I think um there are several papers and and a stream on it they analyze which topologies of the action perception Loop have which properties so one can imagine like a simple cybernetic action perception Loop like which we could just call like around the clock so no line in the middle just internal states only influence action no no backwards arrow action only influences internal internal onsense Sense on internal and all the self loops um that's like a simple Around the Clock topology of the action perception Loop so then we can ask all right so from that around the clock uh causal framing what extra edges and arrowheads exist in the way that it's shown in figure 6.1 there are three there's the connection between active and sensory States so the blanket states have an additional relationship and there's this somewhat interesting symmetry where active states have an arrow back to internal States and sensory states have an arrow back to external States frankly I am not sure to what extent people are just showing representations of the way that previous models have shown it which could be in a graphical representation like this or you could think of this as the adjacency Matrix of the causal influence of the four states on each other like those that could be a fully connected Matrix with a value in every cell or it could be like just the identity Matrix would be like four states that are independently evolving and not influencing each other um different topologies of this particular partition are going to have different properties and I think the most General approach would be to do structure learning and Bayesian model selection on different topologies for the action perception Loop so what do they mean well they mean that you are um comparing models with the constraints or the availability for that parameter value to be non-zero if you delete the arrowhead of a certain kind then you're basically testing models within a restricted class where that parameter has been fixed at zero so if you want to fix the effect of sensory States on external states to zero maybe you think that's quote unrealistic so you want to fix that to zero so that you have increased statistical power to resolve other parameters that's valid although one of the most intuitive um reasons to do so which is like but sensory states don't influence external states in the real world it's like yes but that's the map territory fallacy this isn't the causal structure of reality Ali or anyone else uh actually it might be helpful to look at figure two from the paper pass integrals particular kinds and strange things and especially the different typologies of the particular kinds provided there being inert particles active particles conservative conservative and strange particles so briefly they Define inert particles as the kind of particles or systems with no active States and in the in the order of increasing complexity uh one uh one step further or one uh one level up would be the active particles which is a particle with a non-empty set of active States but then uh ultimately we and also the conservative particle is a is an active particle whose particle whose particular States follow Paths of least action but ultimately we reach a kind of definition uh I believe for the first time for strange particles which are defined as conservative particles whose activist states do not directly influence internal States so uh this paper claims that only this fourth type of particles namely the strange particles have the capacity uh to be a kind of sentient agent or any kind of uh intelligent agent depending on how we Define intelligence or sentience but in any case all of these kind of this strange particles some some uh subsumes all the other kinds of particles but with varying uh levels of complexity uh assigned to each of them thank you yes good connection here we see um so interestingly we don't see the backwards Arrow here we see the Around the Clock and the intra blanket connections but we're we don't have the backwards Edge yeah that's because uh these kinds of strange particles uh have not been developed into a fully sentient agents yet so to be fully sentient is to be able to actively uh influence all the uh the uh the active and sensory States so I believe that's why we don't have those bi-directionality in this uh picture for strange particles yet thank you yes it's almost like this is one circuit diagram it's like we have four CPUs on our circuit board that's the particular partition and then how we wire up the topology of the particular partition is like the hardware of our circuit board and then what the Dynamics are is like the software of our circuit board foreign once we start mixing in these um taxonomies of particles of increasing sentience and cognitive complexity and then we start mixing in the blanket index like it's a big space but also it's a very semantic space and one advantage of fep and active modeling is we can develop special or reduced cases of particular things that are sub cognitive but they're still broadly considered within a cognitivist framework whereas something like thousand brains it may be a super effective architecture for inference or generalization or any other number of advanced cognitive tasks however it cannot be constrained or projected down to the case of like a ball rolling down a slope whereas Bayesian mechanics Bayesian physics and this um taxonomy of particles helps us Bridge the continuum with expressive modeling framework that spans the simplest least active particles to open-ended complexity and intelligence Michael yeah I I also had a little bit of problems with understanding this inter um blanket uh Markov blanket Loop and what that could mean um so there's somehow unmediated action that does not have an impact on the environment and The Sensation sensation action loop I I have problems really to understand what that is what that could be and um I mean I can see that one can have a system that produces something like this but I have understanding uh problems to understand so I was thinking whether This was meant to be for instance a reflex you hit on the knee and then the leg does some kind of movement but without going to the brain so that um the nerve ends in the spine somewhere and then this action is produced by uh by placing the spine and and then it doesn't go too but it's I mean that it depends on what you call an internal state right so if the spine that triggers this reaction uh not part of an internal State I I well I'm I'm a little bit lost in understanding what how we how we can understand that yeah thank you for the comments so super important Point like it depends what is internal States it's not like States um even within a fixed like we're these are the only variables on the table um whether one is internal or external is of course whether we're choosing to model it as the generative model or a generative process like which side of the blanket and there's a symmetry there um and then it's not just like this variable is an internal State it's always like the four-fold particular partition comes into being in the same time like the blanket states are those that make internal and external States conditionally independent so but that blanket state with respect to X and mu it might be an internal state with respect to something else so it does definitely depend on it and I think now to the question of the connection here's from emperor's new Markov blankets by vernberg um I think the edge may have to do with just the uh the fact that there is an arrowhead doesn't mean that the parameter value is non-zero it just means that we're describing families of models where there can be a non-zero influence we're not a priori fixing it to zero but you could conceivably remove this Edge and just fit the simple around the clock or around the clock with the two backwards edges so because we're making models not um of maps not territories we don't need to be constrained to a physical um anatomical realism with respect to what these edges are and so it's easy to sort of contrast the plausibility of these edges with anatomical realism but these are not anatomical edges so it's just opening up this the family of models under consideration to a non-zero statistical influence between these states and it also May inherit from um the um the question of co-parents and also the developments of what people call a friston blanket whereas the Markov blanket formalism by Pearl was initially characterized on an undirected graph and there was only one kind of blanket state it was just the set of nodes that make internal and external States conditionally independents and um you could imagine a sort of clean friston blanket with like a clean division of labor where there's um some of the nodes in the of that blanket set are strictly outgoing and some of the nodes are strictly incoming so that could happen you could buy dictate Design Systems that have that characteristic and that may map onto a anatomical realism but also you might want to consider parameterizations of models that don't have such a clean partitioning like there's a dual functional node that has an incoming and an outgoing statistical dependency and that doesn't have to map on to any actual sensor or actuator in the world it's just a statistical note um but isn't okay I can see that one can design such a system but does it make sense I mean if there's an infinite Loop between active States and sensory states that mutually manipulate themselves but they have neither an impact on an external world not on an internal how of course we can assume this but what is the re what is the reasoning there I mean does it make sense if if something happens and we cannot see it and and not outside not inside but it's the point it's like just like a full turing machine that generates a derivation tree with huge number of internal load that just disappear and one has not a track of what happens there yes thank you for the comment so I think one one way to think about um this topology this wiring of the particular partition here's from that um uh path integrals particular kind strange things paper this is how we're modeling flows over states in the fep we have the flow over the total particular partition X is like a tuple there's a four unit four four length Vector over internal external sense and action States and the flows of each of these nodes are a function of if it were all by all connected so if there was like an edge here then you could just like the fully connected model would be um each of the flows would be a function of all four other variables and and you can um you could write that out if you wanted but importantly in these bottom two the action and the internal or the autonomous States don't have a direct Reliance on external States the uh Etta is not in the bottom two and then importantly the external and the sense States don't have a direct they're not taking as an argument internal States directly that is like the minimal cybernetic constraints we might be then interested in constraining further but and it fep maybe General May apply to the to the um all by all case as well but this is the minimal constraint that allows autonomous States octave and internal to be under um perceptual and action selection imperatives we don't want to have our finger on the scale on external States or sensory States directly except through action so in any given case the topology itself could be a structure learning question or given a fixed apology parameterizing it is is going to be the question but does it make sense it's like saying does a given structural equation model make sense it just is a function of what data you have let's see if we can just touch upon subsequent questions is there a common good representation of rubric for evaluating GM GP does anyone have any thoughts well one approach that's not even active specific is using the Bayesian information Criterion or its cousin the AIC the ikiki information Criterion and um the Bic and and basically the AIC reward good fit and they penalize models with more parameters so the bic is widely used to establish the Pareto optimal trade-off of model complexity and model accuracy so if one is interested in like what is a good generative model generative process fit if you just want the sheer um Fitness you're going to end up on the slippery slope towards High dimensional generative models because adding a new parameters appropriately strictly increases model likelihood because new parameters are always sucking up variants that was left unexplained in the noise term by whatever you had previously so we don't just want to Simply increase the likelihood blind to all other factors we want some sort of balanced measure that penalizes adding more parameters so that's one approach for model evaluation in the Bayesian space and another that is commonly used is a Bayes Factor which is a relative measure of fit amongst multiple Bayesian models so the base factor is a ratio between two Bayesian models and so it can be used um people have kind of heuristics just like there are heuristics for p-values not even going into the whole question about p-values here but there are also heuristics for Bayes factors whether the evidence of one model over another is like substantial strong decisive Etc and base Factor allow any models to be compared with each other whereas if we were in the parametric modeling space we would only be able to test using the hierarchical ratio likelihood ratio test the hlrt for parametric models so for parametric models you can only test nested models like if you're doing an anova you can test an effective a an effective B and then you can compare that with whether there's an interaction between a and b but you would have a hard time testing a model with a and b versus a model with b and c unless you put that into a nested parametric framework Bayesian statistics gives us a lot more flexibility to compare models so model selection evaluation comparison those are big areas short answer we're using Bayesian statistics so we have the most modern tools for evaluating that in a specifics it's always going to be the specifics but in terms of the tools for model selection Bayesian statistics has the best tools all right how does the particular partition so 6.1 which we were looking at earlier or the sort of Proto particular partition the the um the dyadic partition here informal relate to the pumdp in figure four three what would anyone say these are two common figures that we see but I don't see X mu U and Y here though interestingly the X and the y in the continuous time representation they actually do map on to the particular partition and um kind of with a v whereas the discrete time pomdp does not use this exact notation what would anyone say why do we have this closed cybernetic partition but then it looks like we're moving into a pomdp model with a different topology I guess in the p-o-mdp model there should be also an arrow going backwards to the beginning of the so if we and in a in a state on the right side there should be somehow a loop back to the beginning which is your implicit somehow that is called sophisticated active inference where in the past present and future there can be an inference over the past present and future okay yeah so it is it is possible but it's not the base case yeah no yes so this is uh unrolled in the time now actually yes exactly exactly this has no time the particular partitions are not defined in time they're defined as functions of other states in the partition because what a Bayes causal graph represents is not as a function of type so it has to be put into a form that allows us to explicitly enumerate time and then in chapter 4 and then again we'll see in chapter seven and eight we have two ways to deal with time broadly discrete time continuous time discrete time actually models discrete slices of time sequentially and generates parameterizations for like hidden States and observations at different explicit time points in contrast The Continuous time generative model which is being shown like with these one twos and threes instead of the D B and A they're being shown juxtaposed in figure four three to emphasize their structural similarities however whereas the discrete time model is explicitly predicting States and observations at different time points The Continuous time model it's B Matrix equivalent is not the next state but rather the derivative of the states and so the continuous time model has more in common with a Taylor expansion or a Volterra expansion or generalized coordinates of motion so in either case pomdps help us focus our statistical and informational power on constrained sets of models that are not too restrictive if we're willing to abide by certain uh standard modeling practices like that the past can't influence the future except through the presence or that like a derivative can't have an influence on its second derivative without influencing the first derivative first so under those standard modeling assumptions that uh reduce the state space vastly like one can imagine if there was 10 time points or 10 derivatives and you did all by all you'd end up with a lot of edges but if you constrain it to time points can only influence next time points and derivatives can only influence the next derivatives then your model scales linearly with time points or derivatives rather than um Supra linearly so PMT pmtp is one implementation architecture that generates generative models which are compatible with The Continuous Flow Bayesian physics underlying just like you might have some flow or field equations for electromagnetics and then you have some discrete time realization in your simulation that is the relationship I I have a question huh so if you say it's like a Taylor series then this is a somehow deterministic isn't it I mean in the beginning in the Taylor series you say what functions you want to decompose and then all the parts are a little bit deterministic but but is it what we were trying to model here or am I completely wrong I mean it is a series you already I mean it's maybe infinite but um but you know from the beginning what the um what they want what the individual components are anyone else want to give a thought on that or I can add something well it can be deterministic so you could have a model where for example like there isn't [Music] um like there's an identity Matrix for a so like hidden states are fully observable or you could have one where the state transitions are fully deterministic as well in the discrete time case okay now in the continuous case um is it deterministic well from a given hidden state you could have a deterministic or non-deterministic emission of observations you can have probabilistic action selection that influences how States change Through Time and the way that they change their time um could be sampled hence also not deterministic um and I think the Taylor series and variational inference have a lot in common in that you do have to commit to like a given family of distributions that you're wanting to fit but then within a given variational family so you have some data set and you say and now I'm going to fit this type of distribution to it you might fit that with the stochastic gradient descent or there might be a deterministic algorithm to fit it but it's a little bit too broad of a brush to think about the bigger picture of using these models and simply say that they're deterministic or non-deterministic I hope that makes sense because it is a very subtle question yeah okay yeah thanks like I'm wondering if you could have a non-determined like can a function have a uh stochastic derivative well I don't know but I mean somehow if we think of the environment that produces new input um and we don't know that environment then somehow on an undeterministic factor is there right yes the generative process is not shown here the generative process external States is handing observations in so the GM to kind of bring back the question it unrolls and it shows the agent's perspective this is necessary and sufficient to define the agents but here's the great unknown that's handing the observations to the agent so the GM can be seen as like the cognitive architecture of the agents it is not describing the total scenario that the particular partition is showing so that's one other difference X um this s is not the actual external States this is the agent's variational estimate of external States Ali I think the remark on page 24 of pass integrals paper might help to understand this issue a little bit better especially the second paragraph which says that from the perspective of an observer Behavior will appear to be fulfilling the epistemic and pragmatic imperatives afforded by expected free energy so it's basically uh the control versus planning or a homeostasis versus allostasis when we look at the inference from the point of view of the internal states of the agent and because only the from that perspective can we talk about reducing the generalized free energy right so uh there's a fundamental difference between looking at this agent from its internal State's point of view and looking at it from the external from the point of view of an external Observer I don't know if it makes sense but uh although they're complementary to each other but uh in a sense they're fundamentally different thank you and um just to to look ahead to nine though I wonder if this might be uh useful to see in six is and it relates to the map territory so here's figure 4.3 is in the middle we're parameterizing a PO mdp with real computers about a rat in a teammase or an ant in a colony or a cyber physical entity so we're parameterizing this pomdp but that pomdp is in a parameterized environment as well and six kind of brings us to that but it would be awesome to see more connective tissue between the particular partition which is where the physics is inheriting from flows on the particular partition and how that relates to this view from the inside of a cognitive agent um okay in the final um minutes of this chapter six okay last week I think we explored a little bit about how b and a are um two different variables they're not directly connected to each other but they kind of work as a team like B increments the hidden State along and then the hidden State emits an observation um good regulator we talked a little bit with Bronwyn at office hours we explored um good regulator theorem and cybernetics and requisite diversity um we talked about what what happens to bad regulators um we explored also in the office hours last week a little bit of the discussion around the blanket trick and about how affordance is handled in active and some similarities and contrasts with how it is handled in um other areas of like other threads of thinking in ecological psychology um in the coming second discussion on chapter six we can return to these bottom three questions temporal depth cognitive systems outside the brain and similarities and differences with active inference and other modeling any final remarks on this all right awesome I will stop recording let us take um a two minute break and then we will begin for cohort three thank you all