hello everyone thanks for joining and welcome to actin flab live stream number 24.1 today it's june 22 2021 and we're going to be discussing the paper and empirical evaluation of active inference in multi-armed bandits with several of the authors so thanks everyone for joining welcome to the active inference lab we are a participatory online lab that is communicating learning and practicing applied active inference you can find us at the links here on this screen this is recorded in an archived live stream so please provide us with feedback so that we can improve on our work all backgrounds and perspectives are welcome here and we'll be following good video etiquette for live streams this short link has a schedule of all the live streams that we've been doing and will do for 2021 and we're here today on june 22nd in number 24.1 which is the middle of the three-part series on this paper about multi-armed bandits the zero video 24.0 had some context and some background on this paper that we're going to be discussing an empirical evaluation of active inference in multi-armed bandits and today we're here with three of the authors so thanks so much for all of you who are joining today because we'll have a lot to discuss and learn about and in today's discussion for 24.1 we're gonna first just go for some introductions and then we're going to have a presentation by the author and then we're going to just open it up for discussion so if you're here on the video call or if you're watching live in the live chat just feel free to ask any question and we can go wherever people are interested in going so that being said here we are in the introductions we'll just go around and introduce ourselves and say hello and then especially for the authors who are joining for the first time it'd be awesome to hear anything you want to say or we'll ask you questions as well so i'm daniel i'm a postdoctoral researcher in california and i'll pass to dave i'm in the tropical mountainous rainforest 120 miles north of manila my background is in cybernetic learning theory general psychology and machine translation but not much math so i'm floundering with much of the active inference world we'll go to sarah and then continue on i'm sarah i'm a postdoc in treeson together with tim t my original background is physics and biophysics but i wrote my dissertation essentially about active inference and especially applied to habit learning and yeah that's it for me and going on to dimitri should we let her introduce himself i don't know i'm not sure if he wants to do that okay i mean i can also introduce him shortly so there was our colleague from ucl previously so he worked in max planck ucl center for computational psychiatry and aging research and currently he has switched to industry so he is working in second mind uh on the applied reinforcement learning there so basically he is a expert on this other part of staffordshire which doesn't cover active inference right so and uh yeah i'm also the postdoc technical university of dresden uh both me and sarah are at the chair for neuro imaging uh where like the head of the chair is stefan kiebel uh and and uh yeah we have been involved with active inference for a while now since probably 2015-16 and have been applying it to various questions regarding human behavior cognitive control decision making in dynamic environments and similar um and yes so um should they kind of switch to the slides now or sure we can go to the students or if i can just ask one general question to any of the authors who wanted to respond was it that you were interested in active inference and then looking for domains to apply it in or were you interested in a domain and then sort of found active inference as a way to integrate what you were working on i yeah well i mean so my background is also like right in physics complex systems uh and computational neuroscience and this physicist we like to think about this unifying theories so in this sense active inference has this appeal that it can collect connect a very kind of distinct way of basically thinking about decision making thermodynamics um right uh stochastics dynamics and uh very kind of different areas of of understanding dynamical systems so to say uh and kind of in control so that that would be the appeal and uh right uh i mean definitely uh as a tool to apply like to decision making human understanding human behavior this is kind of where this all started right so what new can we learn basically uh using this approach and sarah any thoughts about especially i'm curious about the biology side because we hear a lot about the math and the physics verging towards active inference but it's also cool to hear about biology and that was my background as well my background is rather in biophysics so i'm only tangentially in biology but what i like is also that it connects with some general information processing scheme in the brain and actually my masters see this i was working way way down in the abstraction hierarchy on spiking neural networks and like receptor dynamics and actually in this area i find it easy to not see the forest for the trees and i think actually connecting upwards and asks the question what is a general information processing that's going on and then connecting it back downwards again is what also attracted me to active influence awesome so as usual people will be uh i guess joining or leaving i'll unshare this screen and dimitri if you want to uh jump into the presentation that'd be awesome yes okay so you see yeah and uh all crop everything so go for it excellent okay so let me start with a bit kind of motivation for this work uh so as well you realize kind of our involvement with active inference came from this kind of cognitive neuroscience uh direction uh and originally not so interested in the technicals or machine learning side of it uh however if one thinks about uh multi-arm problem bandits right that's a very general uh problem which kind of generalizes resource allocation problems then when they realize that this is most of kind of behavioral experiments can be cast into this kind of framework right and once these this in the range of kind of uh extreme experimental cognitive neuroscience uh domains like decision-making in dynamic environments value-based decision-making structure learning and similar uh one can also think about like um attention as a resource allocation problem right so it's kind of there are many other domains where maybe they're not explicitly talking about multi-arm bandits as such but they can also be cast uh into this uh general framework and for example for from decision making dynamic environments one of the most well-known kind of tasks is a probably like reversal learning task right that's kind of used in uh hundreds of papers and this is also kind of what initially motivated me to make this paper uh it's a kind of just uh understanding so if i apply active inference to liver reversal learning tasks how does this compare to other like alternative decision making uh approaches we can apply to that right however right besides this kind of cognitive neuroscience uh uh direction uh multiple bandits have like a range of industrial applications uh they really use like you know starting from marketing to finance like recommendation systems in finance for trading applications uh i even in many like optimization problems in the like deep learning right you know there are lots of papers actually showing how you can speed up learning by um uh finding kind of better sets of examples for uh for the deep learning uh uh systems uh and right so basically the way this also kind of allows them active inference to bridge this into a machine learning gap and find new uh applications there if it's fine if it kind of shows to be useful for this kind of multi-armored setup as kind of adding something new to the existing work right and when we talk kind of multi-arm bandits have a range of different formulations today we will talk about stationary bandits and dynamic bandits uh where just means that uh reward probabilities either are fixed over time or change uh in different ways however people also write uh discuss adversarial bandits risk of air bandits uh contextual bandits one can also talk about non-markovian bandits or that kind of rewards uh depend on a sequence of actions or have kind of some memory dependence in the system and so on right they're really like a range of different uh kind of structural definitions of multi-arm bandits which kind of then require a different way of thinking about the problem uh and this is potentially like something uh also interesting for the future expanding uh what we did here uh into this other domains and seeing if again this the results are generalizable to other uh definitions of multi-unbending problems uh okay so just um how i structure the slide so far we have like two parts one isn't about stationary bandits and then we will switch which will go to switching bandits yeah uh and uh yeah given that we have two hours i hope that will be enough time but if yeah if not we can also continue for the next session for example with switching bandits or so let's see let's see how this goes um okay so what is stationary bandit so the definition of the problem is as follow right on any trial an agent makes a choice between k-pops uh choice outcomes are binary variables so we kind of focus here on bernoulli bandits outcomes can also be drawn from any other distribution so and then for example you talk about gaussian bandits or uh yeah depending on the under how the rewards are generated we will just uh work with bernoulli here here bandits and we have we have fixed uh reward probabilities in a very specific way so there is only one arm which has a reward probability associated with p max which is one half plus some uh term epsilon like which is larger than zero and all other uh um all reward probabilities associated with other arms are just fixed to one half but this kind of allows us to like control the difficulty of the task so because the more smaller the epsilon is the closer to zero is uh the more difficult is to distinguish the best arm from from others and the more samples one he needs to draw to kind of realize the difference right uh and right beside this like the best arm advantage term uh one also realizes that right number of arms k is also increasing the difficulty the more arms you have the more time you need to figure out what is the correct arm to play okay so just to give you an illustration of the example right we have forearm banded here uh so whenever kind of agent pulls one of the arms uh it gets it gets either zero or one so we can think of this as a reward or absence of a reward right uh beliefs about the reward probabilities uh we will assume that right these are beta distributed so we will use beta distribution to model kind of a representation of reward probabilities associated with each arm and we will use um bayesian belief updating for all action selection algorithms so this also kind of limits uh the range of algorithms which we want to compare here in this study i will explain a bit uh later why i mean we focus on that uh so just to give you a kind of visual example of what this does is right uh whenever an arm kind of an agent starts with with kind of flat beliefs over reward probabilities associated with each arm so this is like a uniform distribution uh this is a special case of beta distribution and whenever uh it pulls like selects one arm uh it gets uh either one or zero as an outcome and it updates right uh the beliefs about the reward probabilities uh in one of the directions right uh so in here right we're kind of it's sampling one more often so it has increased belief that there is a higher reward probability associated associated with this uh so formally how this is kind of implemented what you see here is like to a very simple you know generative model uh so with each k arm we assume there is some associated unknown reward probability that okay which is just a number between zero and one and uh choice outcomes of choices are either zero or one so they're just binary choices so this kind of constraints are like uh observation likely to a bernoulli distribution so this is just kind of product of different bernoulli distribution depending on which arm one is selecting and this kind of uh term below like this is our prior belief about reward probabilities and this will assume is just kind of product of uh many uh beta distributions where here alpha zero and beta0 are just fixed to one so this corresponds to the uniform right right one can also start with other values but uh i think this makes some sense again reflects uh no knowledge a priori about report uh so now given some choice at trial t80 uh we just apply by subject rule and we get from some prior beliefs uh a posterior beliefs and then i think about the stationary case is if our priors are beta distributed our posteriors will be also beta distributed and the update one only needs to take care about how we update write the parameters of beta distribution alpha and beta and this just works in a form of accounting basically alpha counts how many times you observe one as an outcome and beta counts how many times you observe zero and so because of all this uh in this stationary case inference is uh exact so that because both prior and possible belong to the same distribution family you have kind of conjugate prior here setup so you can just track effectively the updating there is another kind of uh representation of the update rules one can use here so for example if we express the updating in terms of expected kind of outcome mu and the scale parameter nu here we see that actually the expectations are updated in the form of a delta like learning room so right where this kind of learning rate is something which decreases over time so basically new is just increasing over time whenever you select specific arm and you will just increase by one and this means that the learning rate decreases over time so the more you sample from one arm the less you will update your beliefs about it all right so this is uh in practically what's happening here very very simple um okay so and uh basically this is everything we need to know about kind of this uh let's call it perceptual part of the generity model so how we update beliefs given some outcomes now we have to kind of introduce how we select actions right based on these beliefs and for this right i we kind of can talk about action selection algorithms and so uh uh they are in kind of in the literature there are lots of red examples one of the oldest one is probably epsilon greedy you also mention it during the first [Music] presentation point zero tyler people also talk about right uh ucb uh upper confidence bound this is also one of the oldest one kl ucb uh it's kind of extension which uses uh kl as uh exploration okay kl divergence is a estimate of exploration uh bound there is thompson sampling etc uh here we will focus mostly on on this tree right we will use the optimistics thompson sampling as a one kind of comparative example and another is bayesian upper confidence bound so first both of these algorithms have been extensively analyzed in the literature and people just find something they work in in many examples lots a lot much better than kind of let's say non-bayesian approaches uh and secondly right we can use basically for all three algorithms the same uh update rules because they are just bayesian uh they're just algorithms corresponding to bayesian and bandits so we can kind of our motivation here is to compare kind of action selection principles based on the uh action selection algorithm and not based on potentially different ways how you update beliefs about uh about the history of observations right and uh so right this would be kind of uh motivation and also a bit to simplify uh the comparison i mean you can like add to the list at least 10 other ways of uh doing the rate action selection and even like decision making in multi-armed bandits uh okay so just a bit too kind of historic example important this is just like upper confidence bound although i will not directly compare it it has some relevance to understand also what is happening kind of in active inference later uh and this uh this form here uh just corresponds to a version uh which is adapted to specifically bernoulli bandit so some people might be more familiar with other version which was derived for gaussian bandits right so this is kind of in this paper here one can see uh these different derivations and examples how how this works what is important right to notice here is basically that so this first term is just the expected probability of reward on arm k and the second two terms correspond to this exploration bonus or the bound uh and uh so this is uh typical for ucb is that the bound kind of increases over time logarithmic logarithmically with the number of kind of trials you're doing so basically the more you're not selecting one arm the more you're kind of pushed again to select it at some point in time in the future all right so basically this necessarily increases over time uh if you are not interacting with the specific r right uh but still although like where the algorithm is very simple it has nice theoretical results so because it's kind of it's efficient algorithm it converges in infinite number of samples however right here we'll talk on this kind of asian variant of the upper confidence bound which actually make a select arms based on kind of the percentile uh upper percentage of cumulative distribution function right so so as time uh progresses uh you uh one is trying to estimate the uh like the extreme value of a believe distribution which in this case is just like a beta distribution and and one one gets this extreme by solving this equation which just corresponds to inverse regularized incomplete beta function so basically the more extreme point your beliefs contain the more likely is that i mean the more likely that you will select that arm so uh the more relevant is to select this arm because you're expecting that right uh this high value is still possible in the uh as a reward probability in this uh in this setup and so but this kind of algorithm has couple of parameters but what authors kind of show in this paper is if you have a very good results for just fix by fixing c to zero so basically this term just becomes one uh and so we we just use their advice here in the paper so we will not uh and we didn't kind of try to analyze other possibilities or other kind of values of these parameters in in the uh in this case uh so for um thompson sampling and this is again one of the classical algorithms first probably um attempt with bayesian bandits so with bayesian decision making and it's also extremely simple right given some beliefs about probabilities at each arm you you sample one point uh so right if there are like 10 arms you would sample from each arm one point and you will just select an arm which gives you the high largest values right uh a variant uh of that which was shown kind of last five to ten years that it works slightly better is this optimistic stomps on sampling where uh one is constraining the samples only to uh values which are larger than the expectation right so so one is first making a sample from each arm and then if the sample is larger than the expected probability uh of reward then uh one keeps the sample otherwise uh you just use the expected value as uh uh as a value a kind of a reward probability associated with that right and you're just again over different arms maximizing taking the uh selecting the arm which gives you the maximum reward probability but this is a kind of a very stochastic approach where this exploration actually kind of bonus to this algorithm comes from basically this random sampling from a probability distribution and right the broader your beliefs are about something the more likely that you will get the kind of large value hence the more likely that you will explore more uh select that that arm uh different runs and basically exploration is kind of here completely driven by uh the noisiness of the sampling process itself um okay so now we come to basically active inference uh version of this so um we are kind of simplifying here things from what people maybe know uh how active infrared is used normally so first we will use like rolling behavioral policies which means that agent is not like tracking history of actions that perform it just kind of a repeating step of policies on every trial and in this case the behavioral policy just corresponds to a single choice so in this type of bandit problems we we are kind of analyzing here like agent cannot change anything in the environment hence planning is completely irrelevant uh in a way you cannot kind of position yourself in us in space better over time so that you kind of need to plan something which means that right just kind of single choice policy evaluation a single time step in the future is sufficient to make uh good decisions uh and uh right uh uh generally expected free we will base our action selection on expected free energy here uh where this would be uh a form which uh decouples into a risk and ambiguity term but we can also think about this problem as freight uh estimating uh expected value of of different arms plus the expected information gain so how much information we can extract from different arms if we select so now let's assume that we know how to compute expected free energy i will uh go through details on next slides uh normally like right posterior oval policy is is estimated as a kind of mixture between expected free energy so kind of a future uh expectation about what behavior will do plus the kind of the second part which is the free energy about the past kind of outcomes however because we have like rolling policies this term is just constant uh for each policy is the same it has the same value basically so the posterior policy or where action just corresponds then to the to the soft max right over the expected free energy uh and normally one one things like uh about choice selection this is assembly samples from the posterior however we are here only interested in kind of optimal choices so for us basically gamma is just infinite value and we are just selecting the uh making sure choices about actions which minimize expected free energy so this this is useful parameter to have if you need to kind of uh fit a model to the behavior but for this kind of a practical application there is not much gain in adding another source of noise here um okay so how do we compute the expected free energy right well that's in a way quite simple so we just have a couple of terms here one is uh posterior over posterior beliefs over reward probabilities right given is this like q term this is just the product of beta distributions we have marginal likelihood so probability of observing o outcome given some action 80. so this is just marginalizing likelihood over uh our current posterior beliefs and another term is here just the prior preference over different outcomes right and this is really easy to parameterize because we we work with binary outcomes and we can just define here a single parameter lambda uh so the higher the lambda is the higher the preference is of the agent to observe once com relative to zeros right and this uh lambda parameter also kind of has a role of regularizing the amount of exploration an agent does because the larger the lambda is the more um uh the more focus is uh is set on the exploitation of kind of making selections based on the expected value instead of expected information uh and so this final term right computing ambiguity is basically just uh computing expectation over the entropy of different outcomes right so which also has relatively simple form here uh because of bernoulli likelihood so uh without going into all the details this is basically how the expected free energy looks like so this first term here is correspond just to the expected negative of the expected reward so because we are minimizing expected free energy this effectively means that we are maximizing expected reward and the second term is just a very complex sets of set of like equations which gives you estimate of the expected information gain right and the in a way kind of motivation is just because one cannot really understand what is going on here right we have kind of logarithms of expected uh reward probabilities but then we have a d gamma function of parameters and uh so instead uh we can kind of try to simplify the term by approximating right effectively the information gain part uh just to get better understanding of uh how basically expected free energy scales uh like with repeated choices and uh so what one ends up with very simple form right basically the exploration term just corresponds to 1 over 2 divided by the number of time when selected different arms right so this is in a way quite similar to what what we had in the in ucb algorithm right the like first exploration term was also divided by something which is proportional to how many times you selected uh an arm with but without write this kind of um x expansion of exploration bonus with time so there is no logarithm of t and i mean this will have a consequence on efficiency of active inference so i will show you this in a moment uh so how you wanna achieves basically this simple form is just by uh assuming that uh for large uh the this the gamma function has this type of approximation when x is sufficiently large right so this just means when you sample sufficient number of time each arm this will be more like very accurate approximation and we see that this approximate so algorithm and the exact they behave very similar so it is in a way reasonably good approximation so you actually uh danielle i think you asked like a question uh about the scaling properties of this right one kind of uh motivation for introducing approximation is that in a way the algorithm is much simpler so it scales better so you can kind of run it on much more arms easier uh however like right the advantage is not huge so maybe uh it scales slightly better but you're maybe gaining like 10 or 20 performance in computation time uh so it's not something like uh which destroys completely the exact form at least not in this example right because the problem itself is simple however it helps us a bit understand but just get intuition what is happening so and uh at least like right how the exploration bonus changes with time uh okay so before i kind of just show some results uh uh like comparison of different algorithms i want to introduce uh just concepts which come from uh machine learning analysis of multi-arm bandits uh which people use like just to kind of rate the uh and compare different algorithms right how good they are in in solving this task and this is done by using something defining a regret so basically uh regret is simply a difference between uh what you did at trial t minus what was the best choice that uh at that trial and uh so kind of uh assumption here is that there is some oracle which is solving the task which knows exactly what was the best choice in in every trial right and so normally people kind of consider two uh two quantities either a cumulative regret which is just some uh of of regret over or up from first trial till the current one or a regret rate which is just the average over time of cumulative regret right and we will here use both just for visualizing different aspects of the algorithm uh and a kind of in stationary case at least this is a very kind of important result because all for all good algorithms one would expect that this regret goes to zero over time right so as you go to infinite number of trials you should be able to always do like a good choice uh and if this is the case then one can show that uh algorithms which have this property they are called also sympathetically efficient and they scale for large t's as something some terms times a logarithm over t right uh and so this is kind of um uh one important aspect of uh a slice for stationary case of different decision making algorithms so multi-arm bandit algorithms so they should at least scale as logarithm of t when you expand uh right when you go to large till limit otherwise right this first thing will not probably hold well or they can also scale slower than logarithmic but for example if they increase linear limit t this will not hold anymore uh okay so um now let's go to the comparison right uh we will look into how the misting thompson sampling bayesian upper confidence bound exact different inference algorithm and the approximate one uh compared to each other uh i will start first with just trying to see what would be kind of a good value for this lambda parameter uh in different settings uh and also to give some kind of initial comparison of exact uh algorithm and the approximate one so what we are i'm showing here is a regret rate right for different kind of snapshots so these dotted lines are after 100 trials uh the dotted dashed line is after a thousand trials and uh the solid line will be after 10 000 trials right and so what one kind of notices here is that there is some obviously kind of minima for different lambda so this kind of preference parameter uh and that the longer kind of uh well the more trials you do the smaller the lambda should be so this this is not quite nice and this kind of has a consequence however we can like just pick some value which uh so for example this purple dotted line shows like around 0.1 lambda 0.1 which seems to have be close to minimum for most of these cases right so we don't want to kind of have different value for different examples because i mean this is just not practically feasible you want to have a kind of general algorithm which can be applied to many different situations at the same time so what when we compare so bayesian ucb domestic thompson sampling and just the approximate active inference so i'm just excluding this here the exact one because they will behave the same pretty much for this specific parameter value if we compare them in terms of cumulative regret we see that right the approximate active inference is not asymptotically efficient so this curve just goes diverges over time uh where's those if if you if you look at the kind of the great uh sorry the green and the yellow curve they they kind of flatten out after some time and they get the slope proportional to this dotted lines which actually shows the slope of this kind of uh asymptotic limit right what you should see for large t's now uh so why is this happening so the thing is that um because this exploration bound is kind of just decreasing over time uh active inference algorithm kind of gets stuck into the wrong solution with some probability which depends on the difficulty of the task so right the smaller the epsilon is uh the more likely and the more arms you have sorry for smaller epsilon and for small number of arms there is a higher probability that you get kind of stuck right and one can see this if kind of we take a snapshot of of different runs so this is kind of a distribution of cumulative regret i'm just plotting the logarithm here so like making a histogram of over different runs of this is like 10 000 runs of different algorithms and just showing at what value for the logarithm of the cumulative regret they end up as you can see here there is for kind of active inference based uh algorithm you see this kind of well spike here and in the tail of the distribution which is proportional to just doing random choices so that means that basically algorithm was just selecting wrong arm constantly it never it never converged to a correct solution in a way it gets stuck into a rock solution because the exploration bow was reduced to too soon uh right so in it so this is kind of uh say limitation or application of active inference to stationary problems because this is not the feature of an algorithms you would like to have in a way uh normally if if you're kind of so example of this would be for example optimization problems that you want to find the best solution for a set of parameters uh now for example by asian optimization finding the minimum of what some unknown function uses a thompson sampling this is kind of seems to be a very efficient way of finding the minimum however if you would apply to such situation an active inference basis um kind of arm selection or sample selection uh there is a chance that the algorithm fails right you just kind of get stuck in the wrong minimum wrong optima it doesn't do sufficient exploration uh so this kind of uh requires potentially some adjustments to how actions are selected in at least in the stationary case uh so just to okay strange yeah i got some string slides just to kind of uh know a bit uh what the short term behavior looks like so from the perspective of kind of cognitive neuroscience or like human decision making you don't really care about this asymptotic limit because you don't usually expect people to be in a either stationary environment things always change or write that they kind of have to repeat actions so many times so what i'm showing here now just in a very reduced example so if we have only three arms and we just use kind of different epsilon values so uh task difficulties what is the probability that uh for different algorithms that you select actually the optimal are and as one can see that right initially so by asian ucb for first maybe 25 trials has the highest probability to select an arm however there is a range of trials like from 50 to maybe 1000 where active inference based algorithm takes over so right in a way because of this information game term active inference is more efficient in uh targeting like the arms which will give you the most information has hence it can recover uh the best arm in kind of some intermediate interval with the highest probability however as you kind of expand this after like thousand trials or so you see that this probability gets stuck so it never converges to one unlike the other ogre this is especially evident for this difficult right difficult problem small epsilon and and so this is uh in a way explanations of what happens so basically algorithm although it reaches good solutions after it's hyper higher probability than other algorithms there are still a lot of uh um lots of examples so in this kind of simulations uh parallel simulations which get stuck to a wrong solution and they cannot get out of this so in a way right this kind of asked question okay what can one do to make active inference also synthotic efficiency so how can the maybe either generative model be changed to support increasing the exploration bound over time or maybe introduce kind of instead of computing expected expectations in um of expected free energy one can also just draw samples from a posterior and kind of also compute this these two terms like information gain and similar to like thompson sampling so right there are kind of different ways one can think of how to add uh exploration bonus another third option would be to actually introduce learning of the lambda parameter so that lambda itself kind of goes down over time so it kind of goes towards zero uh in with specific uh uh specific rules however uh yeah currently we don't have very a good solution for this so we just leave it to this that has a uh obvious limitation of just applying active inference to this type of problems um are there kind of maybe any questions i think uh this would be like the half where we switch now to the perfect the other um if anybody has any thoughts definitely i could ask some things or also we'll ask if in the live chat people want to post any questions but um how much longer of a presentation did you estimate that you had so that we could kind of also address some general points here during this point one um well i don't think there is more than maybe 20 30 minutes max i didn't really uh gauge it but there is less less slides in the second question would it make more sense to go through it quickly here yeah or would it make sense to do it in the next weeks uh i see which i think uh both are fine for me i mean maybe additional 15 minutes okay so we'll complete the presentation and in live chat and on the video chat here we'll compile our questions and then in the remainder after your presentation today and then next week we can have more open discussions so please continue thanks okay okay so uh now we go to this like dynamic non-stationary problem uh and on any trial uh uh in this case right it's this very similar setup on any trial an agent makes a choice between k arms again we are focusing on the bernoulli bandits so outcomes are either just binary variables however what happens here is that the reward probabilities associated with charm change over time right and we kind of differentiate between switching bandits uh where we will assume that the changes happen at the same time on all arms and furthermore in switching bandits they are kind of also called piecewise stationary so there is like a period where nothing changes and then there is just one moment in time when uh rewards reward probabilities on all arms change or uh we can think about a call of another variant of dynamic bandits would be restless bandits where changes happen independently on each arm and they are continuously changing over time so for example following a random walk i will only talk about switching bandits but from some testing i did all the results generalized also to the restless case and in this kind of beside this number of arms and the difference between the best arm and the other arms of kind of reward probabilities epsilon and k we have another task difficulty this is the rate of change or like just change probability so the more often things change the more difficult the task is first especially if you have uh many arms um and uh i will consider like switching bandits with fixed difficulty which is just extrapolation of the stationary case to uh by introducing changes to about to which arm is associated with the maximal reward right we will always have the same rewards on on all arms it is just that from time to time optimal arm changes with probability rho right and in this case we have right just three parameters which uh define our task that's difficult we can also think about switching bandits with varying difficulty uh which then just means that with probability rho uh the reward probabilities associated with each arm either remain fixed on the next trial so they are just kind of translated or they are sampled uh sorry with some probability one minus row should be here right they are staying fixed or with probability rho they are just sampled from a uniform distribution in this case beta distribution with parameters one uh and in this in this variant of the task right we we just have ks and row as a fixed uh difficult as difficulty parameters so active one kind of disappears so you're kind of averaging over epsilon in the task um and just to give you an example also we will not discuss this but how restless bandits uh setup looks like is basically you can uh uh you assume for example that the logic transform of of reward probability just follows a random walk so it's just a brownian exploration in the logic space of the reward probabilities and this would also require potentially changing the generative model which i will introduce but it's not necessary so one gets pretty similar solutions and behavior so um now uh to come back to the example from before uh if we have like multi-arm bandit with four arms uh the setup is exactly the same and in addition we assume that the agent has access to the underlying probability of change so this is not something which is unknown this simplifies uh the learning rules and belief update equations uh however one can extend this what i will introduce today to the setups where the probability of change has to be learned also or that the probability of change is also something which changes over time so that right you can have to track how often uh reward probabilities change in different times uh so this will be kind of example of decision making in volatile right environments uh so uh to visualize the algorithm and the basically the only difference is practically that now you have an effective forgetting of what you uh you know what what agent learned before and one can see this for example if you look uh uh on this square to the left uh as kind of time evolves and agent is selecting other arms uh this value which uh reward probability associated with the leftmost arm just decays back to uniform probability right so in a way agent is forgetting information or expectations it had about this arm and it assumes that with time the reward probability beliefs about reward probability will revert back to a uniform distribution and the algorithm is really a straightforward extension of what i already described before so the generator model uh now is slightly more complex so be besides the the likelihood term so observation likelihood which remains the same is just the bernoulli distribution now we have a kind of state transition term uh which tells us how reward probabilities change over time so and what this means is that if uh one believes that there is a change uh reward probability will be independent from the previous values and they are just drawn they just belong to a uniform distribution so this uh kind of prior belief and if there is no change uh our the transition corresponds to a delta function which just means that uh uh that the reward probabilities stay unchanged from trial t to t minus one to t and finally uh like the prior on each trial we have the same prior about probability of changes and this is just again a bernoulli distribution with probability rho which we here we will just assume this is a known parameter to the agents right so uh the problem here in like dynamic cases uh you can still apply the the bias rule and you can compute the posterior both for the change probability terms of jt and for the post marginal posterior about reward probabilities however as you see here the exact kind of form of the posterior is not anymore doesn't belong to like conjugate so the prior is not any more conjugate probability distribution to the likelihood and this is not anymore a simple beta distribution but it's becomes a mixture of beta distributions and as you are kind of evolving into the future this becomes larger and larger mixture of beta distributions which is uh well practically intractable right if you kind of expand this to open any and the number of trials so because of that we want to have something which is a bit more efficient algorithm uh we can basically introduce a mean field approximation when we now say that okay our probability distribution can be described as a product of a bunch of beta distribution and a categorical distribution which just tells us the probability of change right on trial t uh and uh how one what what this corresponds to here so basically uh uh what is actually we are using here is a bit of uh it's not a standard variational inference so where you would have to kind of compute the gradient over the radiation free energy to find the optima this simplifies the things because you just need one step to update parameters both about change and change probability and about right reward probabilities uh this makes it not super optimal so there are better solutions how i can do this but it's very efficient so and in in the end for the bernoulli bandits this is there is will not be much difference uh you can use better algorithms by uh this gives you just marginal advantage on the long run uh just because the the problem is very noisy and it's very difficult to actually figure out the correct choice so what this variational smile does so it was introduced by basilic uh in well quite recently 2021 uh so they provide you a bit more detailed justification for what i'm what i'm saying here i'm just paraphrasing paraphrasing a bit uh how the algorithm is defined so basically we can associate the marginal about change probability with the exact posterior marginal because you can compute this analytically and then we use this as a basically uh known known belief about change probability to estimate uh to estimate re reward probabilities by right averaging in the log space uh over over different uh prior beliefs about reward probabilities right so basically you're instead of averaging in the probability space you're you're kind of um averaging in the in the log space here uh what this kind of results is very in a very simple set of update rules so on the left side we just are just showing how omega t is updated and this is um just correspond basically to forming beliefs about change probability based on bias factor shown here right which is just the likelihood between uh observing ot given that the change didn't happen and observing ot given to the change happened right at the current trial and then using that estimate to update uh your beliefs about different arm probabilities and basically um depending whether you selected the arm or not so right basically the omega term here plays this a forgetting rate so the uh the larger the omega the closer to one so the larger the probability the change occur the more you will revert back to the beta0 and alpha zero parameters so the initial prior belief unless you will depend on your current uh current beliefs from the kind of previous trial uh and this also has a important limit like right if the agent believes that there is no change in the environment you will you will revert back to the exact inference and the update rules which we had for stationary case right so uh and that's kind of also nice thing about this algorithm it can be just generalized to any any knowledge about probability a change probability okay so uh the action selection algorithms did not change so so the learning rules will change but we are practically doing still the same way of of making action selection so for thomson sampling there's a sampling from the posterior beliefs uh for the asian upper confidence bound it's slightly different so we are using kind of the mixture between uh again the mixture of possible parameter values to estimate the inverse uh because from the this predictive posterior it's difficult to inverse it it's a mixture of two beta distributions so this is just kind of approximation one uh can use for bayesian ucb and for action selection uh approximate uh expected free energy one gets with uh a very same uh uh set of equations because basically raw row can just so probability of change drops out it can be incan being nor easily there uh okay so now if we do kind of the same uh comparison first of uh exact and approximate after inference algorithms we see a slightly different picture to what we had before first it seems that as you increase the number of trials and you kind of compute this regret rate for this specific number of trials the regret rate does not change so in a way algorithm converges very fast to a specific regret rate and for different and there is a clear kind of stable minima independent of number of trials you're you're exposing algorithm to uh similarly again we see like very similar behavior between exact active inference algorithm and the approximate right and for this example i will just i just kind of fixed lambda uh to uh 0.5 so in it just seems to be reasonably well uh valued parameter for many uh many situations which we see here uh however so here i'm showing for 10 arms if we go to 80 irons the picture slightly changes so it seems that optimal lambda parameter although it doesn't change with t it changes with the number of arms and obviously with all the other row and epsilon all the other uh parameters so this makes things a bit difficult and basically suggests that it's important to kind of find potentially a learning rule like self-optimizing way of of estimating lambda and there there is some work actually in active inference literature which potentially has solution for that so uh we just have to test it out at some point uh so however or we can still use the same value this does not uh influence that much the performance later on so what we see when we kind of compare it to uh approximate active inference with bayesian ucb and thomas sampling is that for a range of uh right settings uh after either with like fixed um uh advantage of the of the best like reward probability advantage of the best arm uh we see that active inference is just right uh performs better already after like 1000 trials or so compared to the other two algorithms this is uh uh just for different change probabilities so for example row 0.05 corresponds to a change every 200 trials it should be every 100 trials on average and the last step here is every 25 trials so this would be the most difficult right uh scenario so one sees is that kind of the more more difficult scenario here's the the differences uh disappear so you're kind of losing the advantage uh however um if sim okay similarly if we look at the if we fix number of arms for example 240 and we just change the epsilon parameter a similar trend is visible right that uh the more easier the task is the bigger the difference uh in like non-stationary scenario you get relative to to other algorithms of course one would potentially uh kind of destroy this advantage if uh changes become very slow like every 10 000 trials or something where you are kind of approximately in the stationary stationary world right and okay so this would be like this switching bandits with varying difficulty we can do the same kind of analysis for switching bandit uh sorry previously we looked at the switching bands with uh fixed difficulty similar analysis can be done for varying difficulty and uh interestingly one sees here uh notes because epsilon drops out one sees here uh actually for the exact active inference there is like a clear minima which uh seems independent on the kind of any of these parameter terms the so rho or k so basically we can fix lambda to 0.25 for the exact acting difference algorithm and we can still keep for the approximate lambda to for the approximate active inference algorithm lambda to 0.5 and just to right show what happens here is that there seems to be like invariant difficulty there seems to be also advantage of using exact active inference decision making algorithm uh it outperforms the approx approximated quite clearly uh in this settings and it doesn't require a fine tuning in this sense right for the range of problems you can have much better performance with the single uh choice of lambda value uh and again so the in this case interestingly the easier the problem becomes so in this kind of quadrant the less the difference uh between algorithms is and the actually bayesian ucb algorithm becomes quite uh quite efficient in these settings also here right you see that beijing ucb also achieves uh quite good performance interestingly much better than optimistic thompson sampling which for me at least i didn't found any paper who previously uh showed something like this so yeah this is also a bit probably new result for machine learning people uh okay so just to conclude right active inference does not result in asymptotically efficient decision making uh additional work is required to establish theoretical bounce and regret and derived improved algorithms in non-stationary bandits however we see better performance in comparison to other algorithms and especially noticeable in more difficult settings right that you're kind of uh getting uh in the more difficult task is you're kind of forgetting the better results a tentative to-do list for like next steps here is like introduce learning for the lambda parameter establish a really like kind of theoretical bounce on cumulative regret so what can one expect to see given different choices of the algorithm for action selection uh based on expected free energy and so right this would hopefully improve uh behavior and stationary cast a case and potentially also apply to some like real world examples so this kind of right uh in into in machine learning field i would say right you know this kind of recommendation systems uh optimization problems and similar right just to see how it performs in this kind of scenarios yeah yeah okay that i would just like to thank all the collaborators on the project uh and the people who helped me with different advices and you can also uh find the slides here and the code is available my github page i would just not recommend to use it in the next two weeks because the paper is under revision and i'm breaking stuff constantly yeah awesome thank you you can unshare and we'll return to just um discussion for the last 45 minutes or so here but thanks for that awesome presentation with always good to get multiple um multiple times just to sort of see some of those figures in the paper then also there are some different figures and some different views so yeah again anyone in the live chat is welcome to ask a question or anyone who's here in this jitsi i'll ask um a question from the live chat first and then if you're here in the jitsi please feel free to raise your hand so um it's written in a live chat since the bandit problem here is not markovian does this mean that we only need to consider the current time to calculate the expected free energy uh why why is not the mercurian well let's let's actually clarify what makes a problem or a situation markovian or non-markovian okay so uh for example uh the problem is markovian because the changes are only dependent on the last trial or the current trial uh right so the reward probability in this non-stationary case uh will be a function only on the reward probability in the previous time step and this is what uh makes the problem actually markovian so i i didn't talk about there are non-markovians bandits but this is not what we are discussing here right it is really just markovian but this is not the case why you don't need well maybe yes actually i mean if you would go to non-markovian bend it then you would potentially need to plan ahead for longer because that would require that there are kind of dependencies between your actions and outcomes which you observe in a sequence and different sequences might result in very different outcomes so right because here we are in markovian case and uh agent cannot change the dynamics of the environment in any way so it's in a way just kind of passive uh sampler from the environment uh then you just there is no gain in planning ahead so you can just re-evaluate your beliefs on each trial so it's kind of like a memoryless process which you brought up and then what would have to change to maybe account for situations where sequence of actions does matter uh so for example one could introduce structural dependencies between different arms so that for example rewards uh reward probabilities depends on the location and allowing agent only to to select uh to make their choices from the nearby uh right kind of introducing spatial dependency that would be one example where then depending on in which part of the space i am in kind of where i selected one arm this limits me to what what's the next arm which i can select and this would require you to then plan depending on uh where you should be in different trials depending on how you expect things to change right all right this this would kind of introduce then uh the requirement for planning cool very interesting um blue or dave or sarah wanna ask anything otherwise i have some questions you mentioned a few industrial applications and a few ways in which people do use the multi-armed bandit this is just kind of like um a logistical question like what is the rate limiting step in those use cases deploying active inference agents is there a way to kind of wrap the inputs and outputs of multi-armed bandits uh in a way that's sort of interoperable like you talked about how the learning rules were similar but then you you juxtapose different action selection approaches so in the context of pipelines that people are already using is there a way to kind of hot swap active inference and maybe have it be deployed in industrial settings very rapidly well yeah i mean that would be one idea i mean for example uh if you have these non-stationary problems and you have already been used using thomson sampling for example or even like optimistic thompson sinclair as a choice of the algorithm in a way you already have a way to form beliefs about relevant aspects of the of the problem then you can like really easily swap right the two and you just can then apply the the difficulty just to figure out how to compute basically expected free energy and test it out right for the different generative model that people might probably have yeah uh again for stationary problems that's maybe trickier it might work in some situations but yeah you you don't have this kind of nice asymptotic guarantees that you will always find good solutions in a way it's an advantage of active inference in a changing world like people's preferences for a given advertisement or the the situation for trading is always changing and so it's a false allure to have something that has extremely well behaved behavior in the asymptotic or infinite case because we're not in the infinite case we're in the finite and dynamic case and so it's almost like the sort of strong pillar that purportedly is underpinning these other approaches isn't so much of a gain pragmatically so it's pretty cool to hear about that blue thanks for the raised hand what would you like to ask so i have a question that was left over from the dot zero video and something you kind of alluded today in your talk can you kind of detail the difference between the switching bandits and the restless bandits i was like unclear on like the timing of the switching in the switching bandits and then also um just like a part b of that question in the case of the restless bandits um what are are the similar like algorithms that are optimal for switching bandits also optimal for restless bandits you had mentioned the active inference was good but what about the others that are commonly used no it's the same right i mean so let's say in restless bandits the only kind of so in switching bandits we have like piecewise linear problem or piecewise stationary problem which means that between trial one and trial ten everything behaves stationary and then when the change comes you're just getting uh new reward probabilities associated with each other right for example that would be a kind of idea of a switching bandit so that between changes everything is like fixed and the restless bandit assumes that things continuously change and here the example i gave was one can assume for example that reward probabilities can be described as a random walk in this in this logic space of the probability so basically because probability is like between zero and one you transform into one constraint space between minus infinity and infinity you just have a then gaussian random variable so to say and then you back map it back into the into the with the sigmoid function for example into the probability space uh and and uh so but so for example the algorithm i show like for belief updating you one could also just apply it there so it just one one doesn't know what would be the row so what is the change probability in the restless case so in that sense you would need an algorithm which can also infer the change probability and the restless case doesn't necessarily translates to kind of potentially fixed change probability uh so it's a bit more different problem so because the maximum arm the changes between right between arms which are optimal uh do not follow specifically like the same structure as in the switching case so uh one can either take a different generating model which actually assumes explicitly the random walk and like for example uh hierarchical duration filter is something which one could apply there but they're also uh like what other belief updating algorithms for that so the local or the global maxima and minima are changing in the restless as well yes yes yes so this would be kind of this case of uh varying difficulties over the relative uh probability between the best arm and the second best arm varies all the time yeah i don't have i could i could have just drawn the lines to show but i don't know i didn't do that i i could hear that i just don't have it right now something to illustrate this so one other thought on the advantage possibly of active inference is that with a deep generative model using the same skeleton of maybe even the same code it could be possible to do model testing between two different types of bandits like what kind of scenario am i in or even have deep temporal models so it could be extended in a way where a sort of instantaneous sampler might be led astray yes yes yes i mean in principle you can um any hierarchical asian generative model which consists of multiple models potentially you could also write generalized to thomson sampling or by asian ucb i just wouldn't assume that this would be very efficient way to figure out which is the correct model which you should be currently applying to the specific task and this is potentially also where active inferences uh would would provide an advantage in such scenarios right where you can also kind of learn about the generating model itself better over time and that made me wonder what would it look like at the sort of human level as we're making decisions that are sequential in our day our decision making what would it look like or what would we keep in mind if we were going to be making decisions more like an active inference agent than like a thompson sampling agent like what would be you know when you're in the grocery store looking at the cereals that you've had before or not how would an active inference agent behave differently um just kind of wondering yeah i mean that's a good question i mean we could actually we have some data sets which which we could use exactly to ask this kind of questions yeah i mean i don't have on top of my head any clear answer um what what would be your expectation sir well i guess um it would tell you one is a or better tell you when is a good time to try a different cereal because you may like it even more especially if cereal recipes change over time and yeah then you have some likelihood to be stuck maybe with the super bad cereal but also i like you to have the one you like most yeah i mean i guess this exploration wouldn't would be more structured in a way more directed right um right like i mean when the ingredients change you check the ingredients and then now you've updated your likelihood of trying something new yeah all right i mean thompson sampling doesn't have directed part of the exploration just the random exploration whereas they're here the focus would be more and right directed exploration and potentially thinking how to add random so yeah one other piece is like we're often comparing and contrasting active inference to reward learning and reinforcement learning so it's almost like instead of making that decision based upon the highest expected value like choosing something proportional to its relative value or always going with the one with the best likelihood of having the best tasting cereal there can be some other heuristic and so it opens the door to just purely curiosity driven sampling like i've just never had this and it's not even as much a reward maximizing maneuver as it is just a purely epistemic gaining maneuver and then as we've seen when there's a pragmatic and an epistemic component to the function that's being optimized then those decisions can kind of co-exist and be put on a common grounding unlike in a purely value driven framework where even the exploration has to be kind of coerced back into reward yes mean so i think uh syed noor and she had like as a first author she had an interesting paper the mystify active inference and they discuss a bit about learning the preferences themselves right and what kind of consequence this has and this kind of puts a different perspective on understanding what the reward is because in real world you don't necessarily know why should something be more rewarding than something else but you learn this over time and you just learn to prefer different outcomes they don't even necessarily have to be in somehow rewarding more in the absolute sense you just build experience with some outcomes over others and you'll start to prefer them and this then appears as if you would doing reward based decision making but i mean it's actually in a way preference based decision making right cool i i had another question about the approximation of active inference is that the only way to approximate active inference are there other moves that you could have made to approximate the sections that you did approximate are there other pieces that can be approximated what can be swapped or approximated but still retain this essential structure of an active inference model [Laughter] um well i don't have many ideas what else one could do i mean the problem is in this scenario it's relatively simple right but for example one can kind of think of okay given that we have a to compute the expected information gain and we are kind of learning of having a way to to estimate it efficiently like just with this approximation uh there is other way how you can compute this just by sampling and you can draw a couple of samples from your beliefs and you get some estimate on the expected information right so this this would still be in the kind of active inference frame framework but just like a different way of how you actually compute expectations and what they will kind of represent there right you will this would be a way to add a bit of random randomness to the decision-making process i mean active inferences also general enough to work with many kinds of approximations right and you can do approximations and many different points but as long as you they get some sort of variation of free energy i guess it's still within this framework and immediately separated the inference part from the action selection part but i think actually you can use it as a model of both right and then you can do approximations and different points in this joint model and as long as you can still write down a variation every energy i guess it's a counter active inference well i mean i don't know even if that's necessary i mean i would say any kind of bayesian belief updating even if it's not specifically motivated by variational inference would it the consequence have minimizing variational free energy once once you compute it from some posterior which you obtain and i mean right it's it will still be in a way active inference so right in a way one can replace like smile variational updating with any belief updating rule and still keep the same concept there so because we are just getting a better bound on the marginal likelihood if you have a better posterior so so what would you say you know anyone is necessary or sufficient for a model to be considered active inference versus not active inference just including sense and action or perception inference action or agents in a niche or blanket states these things are sort of we're in an overlap of venn diagram with certainly many classes of models different approaches and is it like that blurry intersection that we're looking at and that's where action and inference are being just applied together or is there something unique or something that we can use as a diagnostic well i guess the difference would be more than this kind of uh this action selection i mean planning is inferencing part not then not necessarily as perception in its inference because as i said one can think of many ways how one can solve that part but kind of once you go into this planning as inference part and also concept that right your actions themselves will change beliefs and you will choose actions which are the best in changing your beliefs or what you want to achieve then this is like i mean idea of active inference in a way it's uh let's say it's circular inference problem right uh it's not in a way it's any more so easy to disconnect in the effects of choices and effective perception they all depend on each other and also being aware of your own uncertainties and i think if you do standard reinforcement learning where you just calculate expected rewards i don't think you're always aware of how certain or uncertain you actually are about your own generative model for example and then potentially with active inference it's easier to know which action to take in order to better learn your model very interesting it's almost like by carrying and propagating our uncertainty and having a self model of action and learning then we get that almost like second order cybernetics where we're acting in a way where in the future we can expect to learn better or expect to act better as opposed to just this hungry search for the best action and then learning is only a one-step projection into like what action is gonna be informative right now uh what's the next wikipedia article that's most informative rather than what's like the trajectory that i expect is going to be resulting in more effective learning or action again on a common grounding so that's pretty interesting looks like you have a thought though dmitry no i mean uh the yeah that that's i mean definitely important part but i mean i would then call thompson sampling would also satisfy that beijing ucb right i mean they all kind of are any bayesian decision making algorithm would necessarily have to take into account uncertainty uh if it's derived from a bayesian decision-making theory but yeah it is not that clear that by this aspect that okay the actions also have a consequence of reducing your uncertainty in the future and this is what you can use also for that's a gauge on which one which action is better so yeah um subtle differences and not that obvious always it's it's one reason why we're so interested in in ontology and slowly scaffolding the research so that we can actually juxtapose different models and understand where they differ like okay they're kind of it's like two road trips and then this person just took a little extra loop or this bridge they crossed this way versus this person you know took a different route right here we can understand like you talked about the smile variational updating but then just recently you mentioned that there's that's sort of like a module you can switch out so what is smile or because i also noticed a very recent citation so what does it do or what is it different in regards to other ways you could have fit that module in i mean i have implicitly used smile for years now it's just a very simple way of updating your beliefs so uh so the problem a bit with kind of uh variational inference if you want to find the minima of approximate posterior you kind of necessarily need to iterate uh through several loops like minimizing the gradient uh so following the gradient of the variation of free energy right and i mean this also doesn't make it very efficient uh in for this kind of applications so for me at least right this variation smile approach side steps this iteration so you can kind of transform variational inference into just a single update step uh just because there is a way part which you can compute explicitly and you just assume okay this is now my fixed belief about that and the other part which you need still to update through variational approximation [Music] uh and so this is i guess a small advantage here but as i said right there are other approaches how you can com update beliefs in these scenarios and uh you know in the bayesian sense there can also be more uh optimal but they just bring you closer to the exact posterior so but i mean we tested these things uh with some examples and the story remains the same so there is no gain on making things more complex on that on that but one can also imagine that like in different environments better generality models and approximation rules would take you further so but for bernoulli bandit this is just not the case what kind of empirical data sets are almost whether they're open source or just obtainable are amenable to this kind of analysis like if somebody hears about the algorithm and they kind of want to see it in action or play with it themselves once you're you know two weeks from the recording of this when your code is available like is it possible is your code set up for more of a simulation or is it something where we can plug in a type of data set that might already be structured appropriately uh it's currently just for simulations but i mean one could potentially write play out with different algorithms add other algorithms both under like a learning part and the action selection i mean that that would be super useful for everybody yeah if there are people interested in that where that would be our repository is open and welcoming any contribution it was part of the structuring of your paper that like made us excited to juxtapose it with these other approaches is you kind of showed that you can directly compare active inference alongside other models obviously it's something we're coming back to because that's the crux of the results of the paper is really the different dynamics as time increased or as the relative challenge increased as the number of arms was changing between these different styles uh and the two styles of bandit so it's kind of a cool thing that people could both build on directly what you're working on but also maybe more broadly instead of just a single model being presented in a paper people could just include multiple types of models in their sort of baseline paper so that we wouldn't just have to read the paper that says this algorithm works well we could see them directly compared and that's something that more and more papers are doing with active inference yeah i mean the the kind of also the libraries we use is also this is something which was developed relatively recently like uh by google jacks it's kind of uh acceleration for linear algebra algebra libraries so which allows you to run very fast this code and it also integrates well with the probabilistic programming language so it's a numpyro which was also uh developed really recently so one could be in practice link this to any behavioral data or to create some test data uh very easily uh with a few lines of code so yeah there is also this perspective what language is the code in one or multiple and then what areas of conceptual math do you think somebody would want to know before diving in so the language isn't i mean like program language is python and just depends on as i said couple of libraries like jacks numpy very standard stuff except the jacks that's that's new so yeah somebody interested in using it would have to learn a bit about jacks but that's a long term also useful so go for it but from the matte side yeah i'm well i mean maybe getting a couple of introductions to multi-arm bandits that would be kind of a good place to start uh right there there is a there is a recent uh quite recent introductory paper mostly on blended which exposes many different algorithms in the stationary concept so uh provides a bit insights about this uh historically historic way how they analyze uh these problems so yeah uh i would suggest that that's the kind of potential place to start cool blue so i'm curious about this google docs i haven't heard of heard about it but like i thought google had their own language like don't they have like golang right isn't that google or tensorflow right well yeah tensorflow yes they have tensorflow and i think the basis is uh similar xla uh it's called like accel accelerated linear algebra uh for both tensorflow and jx so what jax is is basically accelerated numpy so you can just run standard nupi code and in pure python more or less uh and get very um so you you get for free like uh um computational gradients so it's kind of autograd library so you can kind of the compute gradients on very complex graphs uh this is also something which tensorflow allows you but this seems to provide more more benefits for this dynamical scenarios so i have problems when i tried learning or using tensorflow it's very difficult to think into right code which is dynamic there and that's why i i never actually start started using it i started with pytorch at some point and now that jax showed up that has some speed up advantages it's also kind of quite lucrative so one code question just to stay on this theme and then ask a question from the chat what about the um python implementation like uh i think infer actively or what alec chants at all have worked on what are the similarities or differences with their python approach there i think currently none because i'm also involved in that so they will also start using text okay i guess cool so that those threads will join together okay uh but i mean this also code just like produces some of the i mean one could also write use just the spm code but this is super complex uh right it would be super slow so for example just to examine numerically this stationary scenario this would take months probably too [Music] okay nice somebody's ranking uh so yeah um right that's uh uh that's kind of the the problem there and this i wrote the code to be kind of very efficient in a way so just kind of removes lots of complexity that you don't need cool uh all the right in some scenarios you want to have general uh description of the program so yeah so here's a question from stephen in the chat do you think that parallel modeling processes might be used more in the future with different model approaches highlighting different patterns of behavior happening in different niche contexts i would say in some ways it's what your paper did but what are you thinking about yeah well i mean from practical side yes i mean that's what you should be doing because just figure out what works best and just don't don't think too much about well the philosophical side of things uh but yeah but i mean i guess long term one could i mean imagine scenario where more and more things become generalizable to this description so it's just in a way a difficult process takes time even for those who are just learning about the technical details like the visual tell for me was that the figures were a grid of graphs so it was kind of like three different settings of difficulty or three or five different settings of arms it wasn't just showing we ran it with one parameter combination each of those graphs was a grid of combinatorics and then you presented what i guess could be considered two different niches with the static and the dynamic and then today in the presentation we heard about all these other variations and so those are like kind of toggles in the code you can say i'll take no a1 alpha or i'll take a2 beta as far as the combinations of how to run it and so as the code becomes more interoperable and pruned down to really the necessary pieces then it becomes easier and easier to expand it back out so that we can choose amongst different options for a given piece and so that's like this skeleton that so many variations flow off of yeah um cool well in our last few minutes here where would you like to uh take it next week or beyond i mean what are your current interests or curiosities well i mean for me it's just important to have this paper out so next time when a reviewer asked me what about that and that algorithm i can say okay look we analyze this it's like that it's similar or different you can you can get whatever you want basically there uh right i mean because you have these adapted parameters in a way you can generate very different behavior one can also think okay if you have like an agent which behaves as a thompson sampling what would be the corresponding parameter in active inference framework which would emulate that can you actually even differentiate between them so uh i mean this is uh potentially interesting questions which one can try to answer and which are just from my work kind of relevant because of this constant uh questions i'm getting in during the review process uh and that was actually yeah but then there is also this like quite interesting side in like machine learning uh where this can find potentially quite interesting applications uh i i got started a bit uh recently working with kind of monte carlo research and interestingly uh sorry this is something which was applied in active inference as a way to compute expected free energy in complex problem but turns out that if you have kind of very complex problems you can also use thomson sampling in monte carlo research uh right so as a way to just figure out what is the best path to follow uh in a sample and this potentially provides if this stationary scenario can be improved somehow and uh this is also kind of you can apply active inference to planning inside active inference itself you know in a kind of uh hierarchical circular way though right so what does that look like or how does it get implemented or how is it different than just straightforward active inference uh so the problem is like that when you have quite a complex decision making in a planning problem where you have multiple branches in the future which you have to go it's like not practical to compute everything uh so what people do is a very popular way to do is like monte carlo research where you just sample different paths you you estimate us on a sub sample of possible paths what is like uh the best path to to go and this would correspond in active inference like you're estimating expected free energy of a path in the future uh just through a sample but then you have a problem okay how do i select the same the paths in the sample and then you can apply it active inference to the pet selection itself so right you can kind of choose which paths i should sample randomly when i'm trying to approximate uh expected free energy for this problem itself so that's kind of what makes it kind of interesting is a possibility to explore and the way that you just framed it as well as what we've seen which is the relative strength of active inference in dynamic settings it's consistent with a lot of the qualitative and philosophical ways that people are talking about active inference as like a sense making or a way finding or a navigation approach rather than a sort of cut and dry calculus of decision making just resulting in the total you know crystal path just being laid out before you it's really about the instantaneous actions that we take now in light of uncertainty about the present and really the past and the future as well so it's always cool to see how the technical developments while they're like kind of weaving and recombining they proliferate and then we see oh actually these three are kind of interchangeable or these three are complementary and then we get more technical detail and speed ups while we also get more and more clarity on what the structure of this sense making problem is yeah i agree any final thoughts here from anyone otherwise this was a super interesting presentation and questions so we really appreciate it thank you cool yeah thank you yeah we're looking forward to for the next week great so thanks everyone for joining and everyone's welcome to join live for next week when we'll continue the discussion and the dot 2 is kind of like our jumping off into the the unknown unknown instead of just the known unknown so thanks again for joining and we'll see you next week thanks bye bye