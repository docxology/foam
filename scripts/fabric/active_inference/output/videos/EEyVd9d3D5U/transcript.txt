foreign hello and welcome everyone to the octave inference Institute this is active guest stream number 41.1 on April 25th 2023 we're here with Elliot Murphy and Stephen piantadosi this is going to be quite a discussion we will begin with opening statements from Stephen and Elliot Elliott will then lead with some questions and will have an open discussion at the end so Stephen please thank you for joining and to your opening statement cool hi so I'm Steve piantadosi I'm a professor in Psychology and Neuroscience at UC Berkeley um and uh I guess part of the reason that we're here is that I recently wrote a paper on large language models um in part trying to convey some enthusiasm about um what they've kind of accomplished in terms of learning syntax and and semantics um and in part pointing out I think that these models really change how how we should think about language how we should think about theories of of linguistic representation and and theories of grammar um and likely also theories of learning hello awesome yeah so I'm Elliot meffee I'm a postdoc in the department of neurosurgery at UT Health in Texas um I read Stephen's paper with great interest I did a lot of people and there were some areas of convergence but the the things I want to kind of focus on today in responding to Stephen and kind of probing are to do with areas of Divergence maybe um so you know Steven's paper is based on the idea that modern machine learning has separated and bypassed the entire theoretical framework of chomsky's approach so I wanted to kind of respond to some of these main arguments and some other related arguments in the literature uh that's some folks listening might have some some insight and thoughts on so it's a very common criticism to say that large language models just predict the next token and which there's obviously a bit of a cliche right it's um not quite true and they don't just put it in the next token they also seem to confabulate they seem to hallucinate they maybe lie they randomly provide different answers to the same question and they seem to have stochastically mimic language-like structures they sometimes correct themselves sometimes when they shouldn't if you push them a little they kind of change their mind sometimes um in fact if Fox News is currently looking for a replacement for a Tucker Carlson they could do less they could definitely do worse than using uh gbt if they're looking for a you know similar caliber so these models seem to do all sorts of like wild things um and over the past 10 years there's been a sequence of different you know systems developed like where to back glove bed and each of them is based on a different neural net approach But ultimately they all seem to take words and characterize them by lists of uh hundreds or thousands of numbers so the G23 network has 175 billion weights and 96 attention heads in its architecture and as far as what I know maybe Steven can can correct me here we don't really have a great idea of what these different parts really mean it just seems to kind of work that way like attention heads and gpt3 can pay attention to much earlier tokens in the string in order to help them predict the next token but the whole architecture from start to finish is kind of engineering based motivations um and I always kind of wonder what about all the models that kind of failed from these llms from the different tech companies it's like these companies often seem to um you know make it seem like they have these models that really work very well straight out the box um and they all seem to be named after some kind of famous artists right right they have Darley after Salado and Dali they have Da Vinci maybe pretty soon one of these companies will release a large language model uh called Jesus or something I don't know um but they always say this is here's our New Foundation model it's called Picasso it's the first one we tried it we're just great no problems straight out the box but I always wonder what about Oliver Black boxes that have kind of failed every time there doesn't seem to be a kind of a very open and clear structure to the kind of scientific reasoning behind selecting you know one model or another uh but again I might be I'm open to be corrected about that um so even basic language models do pretty well on um like basic work prediction so the issue is whether these tools provide any insights into like traditional psycholinguistic Notions like grammar and passing so this is really why I kind of prefer the term focused model rather language model uh suggested by people like Saba veras so it's been pointed out that no one really thinks llms tell us anything profound about python when they land python code just as well as natural language but python is a symbolic language with a phrase structure grammar and nobody says llms are unveiling the secrets of python right so just to put various here he says if a and N models can be construed as explanatory theories for natural language based on their successes on language tasks then in the absence of counter arguments they should be good explanatory theories for computer language as well therefore successful a n models of natural language cannot be used as evidence against generative phrase structure grammar's own language so Corpus model is really a more appropriate term for other reasons too people like Emily bender and some others have shown that features of the training Corpus in fact I think Stephen cites this you cite this in your paper actually as a limitation and they show that features of the training Corpus can heavily influence the laying process so it's been shown that the performance of large language models on language classes is really heavily influenced by the the diversity of the training Corpus um but natural language itself is not biased right it's just it's a computational system human beings can be biased in what they say and how they act and but natural language itself isn't biased right so large language models therefore it seems difficult for me to you know agree that they are being subject to all sorts of biases they therefore can't really be models of language they're models of something else so just to kind of wrap up this argument um you know even though llms are clearly exposed to vastly more linguistic experience in children again this is something else that Stephen concedes and talks about in his paper and even so their learning outcomes may still be relevant in addressing what grammatical generalizations or learnable in principle so I do agree with this statement here you know that in principle they can tell us something about learnability rather than things like you know broad Acquisitions Frameworks and but that's about as much I think you can maybe say right now showing that some inductive biases um are not necessary for learning is not really the same thing as showing that it isn't present in children so there's been a long debate about whether you know negative evidence and instruction and correction and feedback during language learning are necessary or even useful for infants and children um but right now I kind of agree more with Eugene Choi and Gary Marcus and others who've highlighted how llms are currently you know very expensive to train that clearly an example of a concentrated private power in the hands of a few tech companies their environmental impact is massive um and you know many people have been less constrained and conservative in their assessment here um which much less other than Gary Marcus and Eugene so Bill Gates recently wrote that chat GPT is um the biggest Tech development since the uh graphical user interface the GUI um and Henry Kissinger wrote in February in the Wall Street Journal that as chat gbt's capacity has become broader they will redefine human knowledge accelerate changes in the fabric of our reality and reorganize politics and Society generative AI is published to generate new forms of human consciousness so very radical claims happening at the moment and I do wonder if sometimes or the AI hype may have you know saved into certain portions of Academia potentially a lot of grand plan is being made but I think you know more concretely just to put it back to Stephen here I wanted to maybe raise the issue of um there's a critique by rorski and Beaumont that I I think he's read um on lingbuzz um I think you saw on Twitter that you don't like the response they gave because the objection that they made is that you know science is an example of deductive logic your objection is that science isn't deductive it's inductive right and but I think their General Point might be more accurate namely that you can't you can't use the fact that language models do well predicting some linguistic behavior in humans and some neural Imaging responses you can't use that alone to claim that they can yield a theory of human language so in your paper Steve and you know that it seems that certain structures work better than others the right attentional mechanism is important prediction is important semantic representations are important and therefore we can glean currently based on these models right um but so far that's really all I've been able to glean in the literature I'm not sure if you have more insights here so Roski and Beaumont use the example of poor prediction but strong explanation right explanatory power and not predictive accuracy forms the basis of modern science and I want to explore this a little bit later maybe um but modern language models can accurately model parts of human language but they can also perform very well on Impossible languages and unnatural structures that humans can't learn um and have a great difficulty processing and I know you're familiar with these with these criticisms right um but you're definitely not alone here at the same time so uh earlier um the chief scientist at open AI he said in an interview recently uh what does it mean to predict the next token well enough it means that you understand the underlying reality that led to the creation of that token which is quite Divergent from a lot of more conservative claims in the literature here um and also you know I would just say in response to that that different components of science can be either inductive or deductive right it's not really in either or you have an existing Theory you formulate hyper hypothesis you collect data you analyze it and that's kind of deductive a deductive process but there's also cases where you start with a specific observation you find some patterns and you induce General conclusions right and then there's abduction where you magically invent hypotheses and and reduce the hypothesis space you wouldn't really say that deductive reasoning is unscientific or inductive reasoning is unscientific or abductive reasoning is unscientific right these are all just different ways of of doing stuff um I mean in your paper you give the examples of using models to predict hurricanes and pandemics as being examples of stuff that is as rigorous as science gets and then you implore your reader to conclude that the situation is no different for language models um but I guess for me the issue is models predicting hurricanes are not in the business of answering the question what is the hurricane right models accurately predicting the weather are very accurate but they're not you know they're aligned with the meteorology department but they're not a substitute for it um so I guess I'll just you know hand it over to you yeah okay well uh there's a lot there um I I guess I could could start just by saying that um uh I agree with like many of these criticisms right about uh these these models being controlled by uh you know one or two companies um that being very very problematic um uh you know they have all kinds of biases that they've acquired because they're trained on text from the internet um that's hugely problematic um you know I I certainly agree that there's uh things at least at present that the models don't do well right so um I think it's it's easy to to find examples of you know questions and and problems that will trip them up um I I think why I've been excited about them though um is not uh not necessarily in in those terms right but in in terms of performance on language um specifically syntax and and semantics um I think they're uh far beyond kind of any of other theory in any other domain right so there's there's no other Theory out of linguistics or computer science which can generate you know long coherent uh grammatical uh passages of of text um and so kind of admitting all of their problems as uh you know tools or or things which are deployed by companies um there's still this this question of like how are they uh at dealing with language and um I think this is where a lot of the enthusiasm comes from is there really hasn't been anything uh even remotely like them in terms of linguistic ability um and that's the thing that that I think is is exciting so yes I I agree with with a bunch of these things you you started with um uh but nonetheless like I think in terms of syntax and and semantics there's just no other Theory which is is comparable to them um but so let me let me push that back then right so yeah I I would the the main objection from a lot of people I've spoken to in Departments of linguistics who are like a lot of the general you know first of your paper is to really say well um you know you're right they do they do a wonderful job uh accurately modeling all aspects of a lot of aspects of syntax and semantics however um I don't know of any real just like you know Chomsky talks about facts about language which is an old-fashioned notion and but I really think that's kind of an important notion too right like is there some discovery about language itself that llms can uniquely provide so like if llms made some prediction about let's say you have a sentence structure Type X being more difficult to process than sentence type Y and this is a unique prediction that only they'd generated and no human linguist Chomsky hornstein none of these people had ever predicted that before but it turns out to be true you do eye tracking experiments you do all sorts of different behavioral experience and then oh you know after all it turns out to be true this this is a new insight about language processing it's a new insight about language you know Behavior I just wonder I'm not saying I'm not saying that this is not possible in principle because it might happen in the near future but that's I guess for me the Crux of why a lot of linguists speaking up speaking on behalf of the entire linguistic Community here and you know I guess that would be one of the main objections yeah I mean I I don't know of of uh I I guess I I think of the insights they've provided as kind of general principles right so um uh I think about these things like the the the power of memorizing chunks of language right so like they're they seem to be very good at constructions for example and there's lots of linguistic theories chomsky's in particular right which are uh about trying to find kind of minimal amounts of of structure to memorize right trying to derive as much as possible from from um some small set some small collection of of operations um and I think that hasn't gone well for those theories right um whereas this goes really well right so um uh if we think about something which has the memorization abilities if we think about theories of grammar for example which uh build on um you know humans like really remarkable ability to to memorize different constructions right or different words we know tens of thousands of words tens of thousands of different constructions sorry tens of thousands different idioms maybe our theory of grammar should be integrated with that and they're in some sense a kind of proof of principle that that kind of approach can work well right um can think about making other types of predictions with them um some of which people uh are are currently doing but for example trying to use them to measure uh processing difficulty measure surprisal for example from from these models um there are surprisal measures right are um uh much better than say context-free grammars or other kinds of language models and then it's interesting question how those uh surprisles or predictabilities relate to human processing right and it may capture some of it or might be non-linear or it might you know only capture a little bit of it or or whatever that's an interesting kind of other scientific question but I think in principle right they they can make predictions about for example the connections between sentences right so in in the paper I gave this example of you know converting a declaration into a question in 10 different ways right and presumably when it when you know GPT or something is is doing that it's finding 10 different questions which are all uh in some way related kind of nearby in the models underlying semantic or syntactic space um and so those kinds of things are uh of the type that I think um uh you know some linguists might want right which is here's some hidden connection between sentences or their or their structures but as far as I know they haven't been evaluated empirically yet so right yeah yeah I mean these kinds of models are only a few years old so I I think it's um it's reasonable to be excited about them even though this kind of work hasn't been done yet no that's right no totally totally I mean I think I think that's that's the right perspective to take but I think this gets to the issue of the um you mentioned surprisal you mentioned laneability um you know LMS learn some syntax but they do so with obviously way way more data than infants do um such that observations of potential structure in and of itself is not a reputation of the poverty of the stimulus well the weaker version I should say of the property of distinguish argument so the mere fact that LMS can do what they do with our grammatical prize is very striking I agree and in fact you wouldn't have predicted that maybe five or six or seven years ago um but it doesn't yeah invalidate the claim that humans have surprise and we bring those prayers with us and so in order to see if computational Linguistics can constrain hypotheses and theoretical Linguistics which I think it can do by the way this needs to be done with you know careful experiments in which different learning parameters are controlled and gigantic language models like GPT free are basically you know useless here um so this gets to some of tile lens and complaints that we need something like a baby LM project which I know you're interested in and where we have more you know ecologically valid training sets you make the prediction in your paper that some structure will be learned from that I suspect you you might be right there um but you know even so even with the baby LM challenge there's still the kind of non-trivial issue of addressing more traditional issues like when the kids start to generalize based on the amount of current input based on different factors cross-linguistically and that requires just traditional you know psycholinguistics and language acquisition so LMS you know do care about things like frequency and surprisal as you said but there's a really nice paper by Sophie and Andrea Martin the really beautiful paper um that I think you may have seen that shows very nicely that distributional statistics can sometimes be a cue to moments of structure building but it doesn't replace these Notions pertaining to composition so I'll just read a quote from Chomsky 57 which sounds a lot like what um slots them out and say despite undeniable interest and importance um of semantic and statistical models of language they appear to have no direct relevance to the problem of determining or characterizing the set of grammatical differences I think that we are forced to conclude that grammar is autonomous and independent of meaning and that probabilistic models give no particular insight into some of the basic problems of syntactic structure so that's second uh hedge of the of the of the second sentence turned out to be incorrect but it's so it's true that you know what chompsy said are available stat models in 57 is no longer accurate when applied to models today and that can make abstract generalizations about novel strings and distributional categories as you mentioned right but the performance of a single model does not provide direct evidence for or against the inability of a particular structure like given the vast distance between any computational model available today and the human brain model success does not mean that the structure is necessarily land and model failure also doesn't mean that the structure is not learnable right yeah yeah so I I mean I think it's maybe worth unpacking kind of a couple different versions of learnability arguments that people have made because there have been uh very very strong kind of impossibility claims um coming out of kind of chomsky's tradition right that we're we're never claims about the amount of data that was required right they were claims about The Logical problem of language learning and that it was just impossible um right it was impossible without having uh uh uh kind of substantial constraints on the the class of languages or the class of grammars that that you would you would acquire um and people for a long time have been arguing against that that version of things um um you know there's there's old work by by gold and then there's whole kind of grammatical theories of of acquisition built on that tradition uh that worry a lot about the the kind of uh order in which you Traverse through different hypotheses and consider different options and things um um and my favorite reference in in this is is this paper by um Nick jader and Paul vetani um called something like ideal learning of of natural language um that basically shows that a unconstrained learner could uh with enough data acquire the uh the the kind of generating rules or the the generating grammar um just from observing strings right but that that that paper was was really in in response to this huge body of work that that was arguing that that learning from positive examples so from just observing strings was was like logically impossible right so um uh of course you know people in in chomsky's Tradition really liked that form of argument because it was one that said uh you had to have something innately uh specified in order for language acquisition to work it was like kind of a mathematical argument right that you had to have uh some some kind of innate grammar or innate ordering of hypotheses or something and all of that just turned out to be totally wrong so um if you uh you know move to slightly more kind of realistic learning settings which Tater and vatani do um uh then it turns out you like an idealized learner can acquire stuff and there's no statements about um the amount of data that's required even there right that's the the kind of pure logical uh ability to learn um and that ability is is what I think the uh big versions of large language models also speak to right so chater and vatani and other other work kind of in that that Spirit um is you know mathematical and kind of arguing in principle but but never created something which was really a uh a grammar right or or a a real kind of implemented language model um so even you know a model which is trained on 100 million or 100 billion or however many many tokens right um uh even that kind of model I think is relevant to that version of of the debate right and and and showing that uh language learning is is not impossible um from a very unconstrained space okay um and then then there's a second version right which is uh can we learn language with the specific data that kids get right and that's both amount of data and and form of the data um and so for people who don't know the the baby LM challenge um uh is um uh this uh um sorry thing to call it a a competition or a um a uh uh I guess it is a a challenge um trying to get people to to train language models on human-sized amounts of data um so that's something more like um I think there's two different versions 10 or 100 million different um to 10 or 100 million different words in the the training set um which is like you know 100th or 1000th or something as as big as um these big AI companies are using for their language models um and um I think actually it's it's like that's exactly the the right kind of thing and exactly what the field needs right because you might find that on um a child-sized amount of data um you can essentially learn syntax right which I I think would would be the strongest argument against these Poverty of stimulus claims you could alternatively find that um maybe you can't learn very much um maybe you you you know come up with a a much crummier kind of language model or it's lacking some syntactic or semantic abilities um um I actually think that the the failures there are a little bit hard to interpret because um kids uh data when they're actually learning language they get a lot more data than just uh just strings of sentences right they're interacting in in an environment um so there's stuff in the world in front of them um their utterances are also interactive right so you can say something and and see whether your parent brings you the thing that you asked for for example right that's long been been argued by people um um as a a you know important cue in in language acquisition um so um uh in the baby LM challenge there there is an ability to train these models uh with kind of multimodal inputs I think you can give them as much video data as as you want to give um uh but probably it's hard to to kind of replicate exactly the type of setup and and feedback that kids actually get so um uh I don't know you know I'm I'm excited to to see uh where that goes and and how things pan out there um um uh you know I I think that there's an interesting related question for large language models um which is like what it which is understanding exactly what all of the data is doing so um it could be that that you need so much data for these models because they're effectively inventing some form of semantics internally right so um they're both discovering the rule of syntax and there they appear to be learning uh quite a bit about word meanings um and um it's not it's totally unclear I think how much of the the data in these modern models uh is needed for syntax versus semantics um my own guess I think would would be that the syntactic side is uh probably requires much less data than the than the semantic side um actually a student a former student of mine Frank Malika and I wrote a paper a few years ago trying to estimate the amount of information a learner would necessarily have to acquire um for uh learning the different aspects of language so you have to learn all the words and you learn their forums you learn their meanings you probably know their frequencies you have to learn syntax and basically what we found um in that analysis that was you know basically just a kind of back of the envelope calculation for each of these domains is that syntax is actually very few bits of information it doesn't take that much information to to learn syntax whereas like most of the information you you you acquire is actually for semantics so specifying you know 30 to 50 000 different word meanings you know if even if each meaning is um uh just a few bits right like that that requires a lot of information and probably each meeting is more than a few bits right so um uh it could be like that that would make me guess that what's happening with with large language models is most of their training data is about word semantics and you can think about other ways that kids get word semantics right that's that's not just kind of co-occurrence patterns in in text um but I I agree all of that is is up in the air and and really exciting to see what will happen so yeah I know that some of the earlier results from it from Lindsay's lab suggests that at least restricted to ecologically valid you know training sides uh models seem to generalize you know linear rules for English yes no question formation ROM other than the hierarchical rule the correct hierarchical rule so I think there's a real sense in which you know this the the the space of the correct syntactic price and inductive biases really is is yet to be really settled on but it seems at least to me pretty obvious that there has to be some so there's also some evidence um that children in English going back to this frequency issue that children in English sometimes spell out an intermediate copy of movement in the specified position of the lower complementizer position of a long distance wh question so there's a thesis by Thornton and some of the papers about this so they say um which person do you think who did that rather than which person do you think did that so this is an interesting you know Miss setting because some languages do actually spell out these intermediate copies but English doesn't so the kid makes the error in setting their grammar but the frequency of the input is actually zero and so our mutual friend Gary Marcus also has an argument against frequency determining a kid's output in the case of German noun plurals a more regular form of the certain kind is preferred not the frequent one and there's lots of examples like so it's sometimes claimed that subject experience passives where the subject is passively experiencing something or very delayed in kids in comprehension studies until around eight because they're not very frequent in the input But Ken Wexler and colleagues have gone through um subject experiences two great questions like who likes Mary and they discovered that these are as infrequent in the input as subject and experience of passives but kids have no problem in comprehension studies of these questions but they do have problems comprehending subject experience survival passives so frequency once again seems to be irrelevant or at least it's not explanatory right I guess it's not explanatory with respect to Theory building so how can LMS help with these you know decoration cases when there's clearly something else going on besides frequency so alums you know they seem to generalize just again going back to this issue of the the cases that you have in your paper and you show that they generalize the structure of colorless screen ideas which is obviously very cool um but the positive stimulus has never really been about not being able to learn language statistically I know you made that claim right but chomsky's point in the 50s about statistical models of the day is not true of commercial LMS in 2023 and that's correct but we can't use that single point to undermine you know the entire geometry Enterprise chomsky's basic point was that you could have a grammatical structure wherein every diagram has zero frequency and it also fails to provide clearly interpretable instructions to the conceptual interfaces so interfaces of other systems of the mind so as you're showing your paper GPT mimics examples like pull the screen ideas and but you know again this sentence yields over 150 000 results on Google and it's discussed extensively in the literature it's able to mimic the fact that it can mimic this doesn't really tell us much at least we can't really say anything with much confidence so you know abiba behind a university college Dublin has this quote recently uh do not mistake your own gullibility for an lm's intelligence and in fact even young the wrote last year that critics are right to accuse LMS of being engaged in a kind of mimicry um and the example sentences from gbt that you give in the paper actually don't do a good job because as you say it's likely that you know meaningless language is rare in the training data but they can either do it or they can't but there's no middle ground in terms of giving us 10 examples like this so you have colorless green ideas which are very different semantic objects from things like brown shimmering rabbits white glittery Bears uh black shiny kangaroos green glittering monkeys yellow dazzling Lions red shimming elements right these are all like semantic semantically weird and a bit strange but they're still like legal structures they're kind of meaningful synthetic semantic objects yeah right I I just said yeah yeah I I I I mean so I I maybe I can I can respond to the first point first right so um uh you started off talking about these other uh kinds of acquisition patterns which maybe don't map directly onto to frequency um and I I think it's actually a mistake to think that uh kind of modern learning models should be just based on frequency because um they're clearly learning like pretty complicated families of rules or constructions or something and um I think it's very likely that when they're they're learning that they're um in some sense searching for a simple or parsimonious um uh explanation of the data that they've seen right and how that cashes out in a in a neural network is is maybe complicated and you know depends on um you know parameters and the specifics of the learning algorithm and and and those kind of things um but I think it's it's uh I guess I'd suspect maybe that that it's likely to be the the case that um uh like they're they're they're learning over a complicated set of things right a complicated kind of family of of rules and constructions um and that means I I think that um their generalizations may be like the examples of people that that you gave um might be kind of discontinuous in the input right so sometimes you you could imagine seeing some strings which leads you to a grammar and the simplest grammar of the data that you've seen so far is one which predicts an unseen string right and um if that happens then you'll be uh taking the data learning a representation which generalizes in some novel unseen way so far um purely because that generalization is is sort of the simplest account of the data that you've seen seen to date right I think that's sort of what linguists tried to do right try to uh look at the data and come up with a theory of it and then sometimes that theory predicts some new phenomenon right or some some new type of sentence um and so if they're learning over a sufficiently rich space of theories um then it wouldn't be you know unreasonable or unexpected for for them to also show those kinds of patterns now whether they they do or not I think is is still an open empirical question right um because we have to train them on small amounts of data and test their generalizations and and these kind of things um but I don't think like just the fact that um you know humans do things which are not purely based on frequency is any evidence at all either way right because once you're learning over rich and interesting classes of theories then that that is the expected behavior um actually I I had um a paper about a a year ago that I think you're you're familiar with um uh Yang and and pianta dosi where where we were um uh looking at um uh kind of what happens when you give a program learning model strings from different formal languages so think of like giving a a general model just you know 10 or 20 maybe simple strings that obey some pattern and then asking it to find a program which can explain that data which often means you know finding um uh finding some way of kind of programmatically writing down the the pattern in in the strings and in that figure we we have a paper which is really relevant to to this point where um the uh generalizations that that kind of model makes um uh are I think kind of qualitatively like the ones you're describing for people right where um uh you can give them a small amount of data and it will predict unseen strings with very high probability even though there's zero frequency in the training input right and the reason it does that is that often the most concise computational description of the data that you've seen is one that predicts some particular new unseen an output so that that model is is essentially an implementation of the the kind of Chader and Vitani program learning um idea that that I brought up earlier um but it it's one that that I think you know if you think about in the context of these arguments of kids saying unusual or unexpected things like that is predicted by all of these kinds of accounts right because as long as as long as these things are effectively comparing an interesting space of grammars um then they'll they'll show that that kind of behavior I think uh yeah so okay so I guess you know the argument would be that at least from the gender perspective syntax is functioning separately but it still maps to semantics it informs pragmatics right so in the minimalist program syntax is obviously meaningless it's very small it's just it's just a linearization and labeling they're the two only operations you have a linearization algorithm to Central motor systems and some kind of categorization algorithm at the at the um Center at the conceptual systems and so chomsky's architecture is kind of reliant on the process of mapping syntax to semantics right it's full meaning regulation it's not just structure and it's not just meaning so LMS don't really have this mapping process right like where's the mapping to semantics and if there is a mapping what do the what does the mapping process look like what are the properties of its semantics uh you know what do these what the properties of the semantics place on their own set of constraints on the marketing process like they do for natural language are they kind of you know uh do do these kind of constraints inform each other is there kind of a back and forth process right like alarms don't really seem to describe this form meaning pairing correct like which meaning you've got rich strings for example right well sorry are you saying that um that that they don't have semantics at all or are you saying that there's just not a clear uh delineation between how the structures get mapped onto the semantics yeah the last thing right so they clearly have some potentially some kind of semantics I know you've argued for a conceptual role Theory being relevant here right the rest of it is maybe a little bit more mysterious but the actual so in linguistics is a very there's a theory of the mapping process itself it's explicit and you can see it in action and you can test different theories of it in Psych linguistic models and what have you the the actual regulation the kind of you know constrained ambiguity ambiguity in the sense of you know one word multiple meanings or one structure multiple interpretations Etc right yeah I mean if you think they have semantics then then I think they have to have a mapping from the syntax to the semantics um I agree it's it's not as like nobody really understands how they're working um on any deep level right so I so I agree it's it's not as clear as um say in in generative syntax and semantics right where um you know you kind of write down the the rules of of composition and and can derive a compositional meaning from a sentence from the component parts or something right like that's um yeah that's not how they're working right but um I I just I I wouldn't take for granted that it has to be like that like um uh it could be that how they're working is actually how we work right that uh everything is represented in some high dimensional Vector space and there's some complicated uh way in which that Vector semantics gets updated with each additional word or whatever um in in a linguistic stream um but like I I think it's clear that they have some kind of representation of the semantics of a sentence right like they can answer questions for example at least approximately I mean it's not not perfect but um it's it's not like a engram model or something right which really doesn't have doesn't have semantics so um I I think that they're um they're they're definitely representing semantics and um uh you know updating that as they as they process language it just happens not to look like these other formal theories um and I I guess I I don't see why that's a problem right like those other formal theories could just be you know poor approximations or or just totally wrong right yeah yeah yeah no totally totally I mean there's also ways in which some of the formal theories in semantics are already potentially compatible with what some of these things are doing right so another way to think about this is you know LMS are well LMS are compression algorithms but natural language understanding is kind of all more about decompression it's disambiguating meaning X Out of meanings XYZ it's all about making inferences about you know meta relations between Concepts that are not in the training data so some examples that Melanie Mitchell gives out things like on top of you know she's on top of it again uh it's on top of the box all these kind of variable context so there's a lot of other things that are going on right um and I think you discuss some of those examples on your paper so you know um but the faculty of language is still not at least again under this theory of language and it's not about string generation it's about this form meaning pairing machine so some semences in the genital tradition even think all the rest of semantics is just and right so borgatroski's conjunctive display or something politics is that human semantics is just and that's it um which again is is very simple elegant it's it's it's it's interpretable it's compatible with a lot of the things that you know or maybe going on in in your neck of the words right but regardless it's still you know natural language is still more compositional them things like uh you know formal languages just to make a clear distinction that's been made they have a much richer compositional structure there's more stuff going on uh maybe so it's been pointed out before that you know things like attention-based machine mechanisms and Transformers um allow for combinations of discrete token bindings which is more approximate to a merge like operator than simple recurrent matrix multiplication um but you know the issue of binary branching binary branching government just to choose another example here to talk about the full meaning regulation one principle binary branching image is an interesting question but geometry grammar has always been open to different Origins and locations of this apparent constraint in syntactic computation like where does it come from maybe it's a condition on merge maybe it's imposed by a smooth system maybe it's a kind of Prior you know who knows and in fact it's some more recent working geometry grammar has tried to ground and do away with a lot of theoretic assumptions of of mid right maybe set theory isn't the best way to model um The Gentle grammar maybe mariological accounts or appropriate there's lots of other recent ideas there which which are all compatible with the with chomsky's approach right in fact you know one of the things that Trump collects the most is when he's when he's proven wrong right A lot of these theories are drawing against the core mainstream minimalist architecture but that yeah I think so it's a very diverse like vibrant field the people who are Bornstein you know petrovsky uh uh hajipura they disagree in fundamental ways with a lot of what the mainstream of chemical grammar would say but there's still more scope for disagreement but it's still compatible with setting core assumptions right so a lot of David I just wear for example kind of deviates in this core respect but it's still trying to ground these intuitions in in different formal systems um so you know it's kind of I want to get your thoughts again on um I mentioned Mitchell right so Michelin Bowers uh 2020 they have this paper trial list recurrent networks laying curiously that I think you might be aware of right so it's a really good example just to kind of get to the heart of the issue so recurrent neural networks have been shown to accurately model you know non-verb number agreement but Mitchell and Barry showed that these networks will also learn a number agreement with unnatural sentence structures so structures that are not found in natural language and which humans have a hard time processing right so the mode of learning for rnns is at least for rnn's positively distinct from from infant you know infant Homo sapiens right so the story is Mitchell and Bowers show that while the lstl model has a good representation of singular versus plural for individual sentences there's no generalization going on right they can represent at the individual level so the model doesn't have a representation of number as an abstraction what number is only concrete instances of singular versus plural um so successfully predicting language Behavior via LM or successfully predicting neural responses in a similar way is obviously great and maybe we can get into that issue later but there's only one side of the coin here right the other side of the coin is explaining why this type of behavior and not some other Behavior why is this structure and not similar and that's maybe chomsky's most and like you know his most important Point really why this are not some other system uh so linguistic Theory kind of gives you that or the start of the coin right whereas LMS really don't so the Mitchell embarrassed paper does something like that me does it well yeah so like take um Yael Le cretz and stanislaster Haynes were from 2019 right they looked at number agreement in an lstm and found two specialized units that encoded number agreement but the overall contribution to Performance was low and then in 2021 uh yeah credits had this paper where they show that um in their neural language model it did not achieve genuine recursive processing of nested long-range agreement gender marking in Italian I think um even if some hierarchical processing what you know was achieved as you've argued before right some hierarchy was left it was there but the question is is it the right mapping is it the right kind of hierarchy they found that lstm based models could land subject web agreement over short spans one degree of embedding but they failed at some longer dependencies finding the most recent paper uh Le Crete settle with the hand um showed that they evaluated modern Transformer LMS including gpt2 XL on the same task and the Transformers perform more similarly to humans than lsms did and performed above transfer overall but they still perform below chance in one key condition which is the as I mentioned the multiple embedding one of the difficult structures and so the reason why I mentioned these studies is because you know it's not just to explore the limits of OMS which is an interesting question um but consider it worked by people like Neil Smith at UCL right um he did work in the 90s with a polyglot Savant and neurotypical controls comparing them so he investigated second language learning of an artificial language containing both natural and unnatural graphical structures like the Michelin virus paper right the whole framework is naturalized it's unnatural and they found that while both the savant and the controls could Master the linguistically natural aspects only the controls could eventually handle the structure dependent on natural phenomena and neither of them could Master the structure independent aspects so some weird rules where it's like you know you mark the emphasis on the third word of the sentence things like that so they argue that Christopher's abilities are entirely due to his intact linguistic faculties but the controls could employ more domain General kind of cognitive resources like you know attention control Etc which is why they could deal those difficult processes but I just mentioned you know a minute ago that the lstm in the Michelin barrows paper approaches natural and unnatural structures in pretty much the same way so it's not you know it's not a psychologically plausible model I would argue and for whatever humans are doing and similar observations can apply to the limits of Transformer models in La creta's way and all of these themes are like right up that they're staying with us all the way to the present so another one of talins recent papers that I posted a few weeks ago looking at child directed speech showed that um lstms and Transformers limited to ecologically plausible amounts of data generalized as I mentioned the linear rules for English right rather than the abstract rules and in fact more recent work from linton's Lab last week looking at uh well last year I should say shows that looking at Garden paths surprisal does not explain uh syntactic disambiguation difficulty right and surprises will underpredicts the size of the golden path effect across all constructions and this gets to this issue that you mentioned before you know maybe surprise all this related uh to some aspects of syntax but maybe not the ones there's kind of a it's a very non-tribal issue that is very much it's open to to discretion it's not it hasn't been settled yet but so Lyndon showed that dinopath effects are just way more difficult than you would expect from me unpredictability so another way of phrasing this argument um is the is the quote a recent argument with chomsky's to get at this natural basis unnatural issue he says suppose we have an expanded periodic table that includes all the elements that do exist or the elements that can possibly exist and all the elements that cannot possibly exist and let's say you have some model uh some artificial model that fails to distinguish between these three categories whatever this model is doing it's not helping us understand chemistry right it's doing something else it's it's doing something for sure but whether or not it's helping to understand chemistry is something separate and I know that you've said in response to some of these studies I think you've said that you know and in order to show that something is likely to be impossible somewhere in your paper I think you say um in order to show that something is impossible with normal balance of on false positives you need to show you need to look at something like 500 independently sampled languages so you cite this in the paper right um which you probably can't do that's just not it's not a feasible thing to do and so you know I'm not I'm not too sure about this really refutes the principal argument that I'm making here right because people like Michelin Bowers are making an argument about impossibility in principle not um in some kind of extensional sense you know just like searching across the world languages to see to prove across every single language that it is impossible right that's kind of it's a different argument whether it's impossible in some random language in the Amazon compared to actually impossible based on the principles of what the language system is actually doing like what it can do so I would just say that you know all of this kind of stuff I think that that that point is is that you don't actually know what is typologically not possible right so some people like to say things like you know there's no language that does X therefore we have to build that restriction into our our statistical models right but if it's not statistically Justified that there is no language that does X right if you've only looked at 20 or 20 European languages or something right I I mean it's it's not um uh like that shouldn't motivate doing anything to the models right um uh if it's if it's not a statistically Justified Universal I think um well you know I I I think you know you're totally right but that just applies more generally to the social sciences and psychological Sciences right like typologically yeah it's very difficult to establish these things right so I guess you you I guess you're just kind of Stillman you're a bit you're saying that the strong claim is very difficult to prove right like there is no language that has X the the strong claim that something is not allowed in in natural language is I think very very difficult to prove um um and you know I I think that there have been uh lots of you know strong attempts there's been lots of strong claims from um uh often from from generative syntax right about what all languages do um and I think that you know people have been very good at at finding kind of counter examples to to a lot of those things I cite this this paper by Evans and and Levinson yeah um which actually you know I I had heard for years about how no language does X and and that's what we're using to construct our theories and that Evans and Levin's paper Evans and Levinson paper really uh kind of changed my mind about this right that like language is actually much more uh diverse than than I think most um most syntacticians will you know try to construct theories for something so um um you know I I I think we going back to kind of the the beginning of what you said I think we we'd agree that that uh you need language architectures which learn the things that kids learn and learned it from data that they learn and those architectures might might be unlikely to be things like lstms or you know simple recurrent networks or or whatever right like um I think all of that work is is very useful in in kind of honing in on the right architecture um um uh so I'm just trying to to remember all of all of the the points you were making oh yeah so um but I I think this that there there's a a kind of Flip Side to to this which is that um I think that the space of things people can learn is actually uh kind of underestimated right like there's this bias to to to say you know people can't learn x y and z um but people uh at least outside of language have this this really remarkable ability to learn different kinds of patterns right like the patterns you find in in music or mathematics for example um uh we can learn sophisticated types of of algorithms right we can learn to you know fly a space shuttle or to you know tie knots in for rock climbing or whatever right like there there's all kinds of uh kind of procedural and algorithmic knowledge which is structural that that people are able to acquire and I think that that that uh notion uh very rightly kind of motivates looking for learning systems which can work over pretty unrestricted spaces right so um uh you know you you you might say that okay well language is different because language is a restricted space um uh and it might be true that that language is restricted but it also might be true that the things we see in language come from other sources right it could be that uh language is especially pragmatic for example compared to uh music or mathematics right and those kinds of pragmatic constraints um are the things that constrain the form of language right or language is communicative it's probably more communicative than than music for example and that might constrain the the form of things so I mean as as you know this is very old debate in in linguistics about kind of where the uh where the properties of of natural language come from um and uh I guess what I'm trying to say is that there's one kind of perspective where uh you look at all of the things humans can do even outside of language all of the rich structures and algorithms and processes were able to learn about about and internalize and you say okay maybe language is like that and then yes language also has some of these other funny little properties um but you know maybe those come from some other other pieces of of where language comes from right it's uh you know we have pretty sophisticated pragmatic reasoning um uh we're using it to achieve certain communicative ends you can find all kinds of kind of communicative features uh within the the language system itself and so so maybe some of these other properties are are properties that have some other origin um and that that view I think could be wrong but it's it's one that um I think needs to be looked at to see if it's wrong right like I I think it's been um uh kind of dismissed um by uh large chunks of of linguists right just you know I've heard people say stuff like oh well communication doesn't really explain anything about language right and what they mean often is it doesn't explain like the particular Island constraints or something that they're that they're working on right but there's all kinds of other things in language that communicative pressures probably do explain um so um I I guess my my pitch is always for for kind of breadth in term breadth in consideration of uh the forces that that can shape language and not needing to put it all into some form of innate constraints or something like that no totally and I think I think a lot of that stuff is is compatible with with them illness program because the middle of this program one syntax be minimal it doesn't want it to be complicated it doesn't want it to be you know any more complicated it has to be so there were some you mentioned the the Curious properties right so there are some of the properties that need to be accounted for in any model of language that uh I'll give you an example right the setting of a person features and these person features exhibit very non-trivial different generalizations that do not seem to be accounted for Via domain General learning mechanisms so I'm citing here the work of Daniel Harbor at Queen Mary so for example the morphological composition a person it's interaction with number it's connection to space uh properties of its semantics and it's linearization they all appear to be strong candidates for our knowledge of language right what we mean by knowledge of language but on the other hand we have things like case and agreement and head movement and these are all structural phenomena however they seem to resist a purely meaning-based explanation uh in theoretical Linguistics right it would be great if syntax were nothing but a computational engine that builds structured meaning and that's the minimalist program the goal but that's not what we actually find that's not in any actual minimalist like concrete model any concrete mineralist Theory the goal is just like the program is language is perfect okay that's the program is that what we find no obviously not okay no no linguist actually believes that um so it'd be great if syntax was like that but I think you know the program is to look for Perfection but not always find it so case an agreement and head movement are morphological more for phonological phenomenal the properties of the performance systems what's called performance systems and so the minerals program itself is really compatible with a lot of what you're saying about you know language language there are aspects of language that can be um perfected and optimized for communicative efficiency absolutely totally no doubt about it but where is that locus of efficiency is it in the syntact itself or is it some kind of extra linguistic system is it in pragmatics you know is it in Century motor is it in the speech um probably the speech and phonology probably you know I mean who knows but I think a lot of these things demand much more you know serious consideration into old-fashioned Notions like structure dependence compositional theme what have you things like that which you can maybe find somewhere in the literature but um even just basic topics like you know um quantifier raising extended projections um adverbial like adverbial hierarchies all of these things in the minimalist program can be extra linguistic right they can actually be outside of syntax and query very queer properties of the semantic uh conceptual systems which are in themselves kind of domain General weird leftovers from ancient primate cognition right the features of the way we pass events the way we pass you know agents and patients things like that that's definitely not that's not human specific um but you know the way that syntax provides instructions to these systems you know probably seems to be so you know generative linguists have different theories of also language production too I'll just talk about language production based on whether we store lemmas or whether we build words in the exact same way we will phrase and sentences so I know that you you make distinction between construction grammar and kind of generative grammar and you know the the weight they place on memorizing constrictions whereas is just building things from from the bottom up from the ground up right and so you know in some generative inspired models mechanisms which generate syntactic structure make no distinctions between processes that apply above or below the word level then there's no pointer which meaning syntax and form are all stored together a single Atomic representations each stage in lexical access is a transition between different kinds of data structures right there's meaning there's form and there's syntax these three features kind of co-mingle together and they don't always overlap different languages realize them in different ways and so you know a weird the basic definition of a word is just this weird multi-system definition and where lots of things lots of different cognitive systems enrich the basis of every electrical item right you have um there's nothing like this really this enrichment process um anywhere else in in linguistic Theory right or at least in what llms are doing like so I guess what what I guess I'll ask you what is your definition of a word right and what can llms really provide insights into word Hood right because if you kind of if you don't have a definition of what a word is then you're really in trouble right like we have to at least use LMS or artificial systems to inform what we mean by a word or maybe we don't need that anymore I'm not sure what do you think I I'm not I'm not sure what you mean I mean um why does that matter I mean that that that's just a convention about how we use the term word right what like I mean you could use you know lemmas or word firms or or whatever like that that just feels like a conventional Choice I'm I'm not sure what's it what's at stake there so how would you I guess I I would say I agree word is a conventionalization you know icons are intuitive concept of where it is often biased by orthography the way we put spaces things right so so that I I agree with that criticism you know word in the intuitive sense is not really a scientific construct however I guess let me rephrase my question how would you um you know decompose the intuitive concept of word into something that is more kind of you know scientifically amenable or psychologically plausible which is exactly what geometric primary tries to do by decomposing words into you know distinctive features uh morphological categories conceptual Roots being merged with categorical features you know you get a concept you know and you've made it with a noun or a very category to get a noun or Affair these different models make different predictions right yeah I mean I I think that general idea is likely to be right for large language models like I think they kind of must have things that are kind of like part of speech categories for example um and I think that they they kind of must be able to uh update those their categories based on the language that they've seen so far right so like like you know GPT puts nouns and verbs in the right places and to to do that you kind of need some representation of the nouns versus the verbs and you need some ability to uh uh locate yourself in a string of other words and figure out if there's likely to be a noun or or a verb next um so I I think that that on on that level those kinds of properties of words uh are very likely to to be right and there there are also things which are uh um very likely to be found kind of in the internal representations of of these models I don't see how it could be any other way uh other than that um but like as as far as I know that that's not where the uh that's not where the main debates or or disagreement I think is right like um uh yeah I think all theories of language have to have to say that there's different kinds of words that can show up in different places or something like that um yeah okay so how about the issue you know you mentioned communication right um so you know and you're totally right when Trump says things like language is a thought system or you know language didn't evolve he's kind of being a little bit cheeky he doesn't really mean that he kind of means in a very specific sense right um but you know when we say language is a thought system what we mean is um we're trying to get it an architectural claim so if you look at the architecture of the minimalist program the syntactic derivation and the conceptual systems are literally different systems right the conceptual systems take stuff from syntax and then do their own business with it and the CI systems have their own peculiar rules and and principles which is why I thought in language are both similar symbolic compositional systems but in different ways only a subset of thought is properly called the the CI interface System since the CI systems are by definition you know whatever conceptual systems human have that can access and read out instructions from syntax and we don't know what they are fully they seem to have something to do with events in grammatical reference and definiteness they seem to be the main categories that language you know cares about conceptually but we don't really know that's kind of just a hypothesis right um but what we do know is that they don't seem to make use of color all that much or um so no language morphologically marks you know shades of color um or other conceptual features like um um worry or concern like no language morphologically marks a degree of worry or concern about an issue but we do make use of like um episode epistemological Notions like evidentiality and things like that so you know one well I guess what I'm saying is the minimalist program does a good job of trying to figure out which aspects of thought language is intimately tied to and which aspects of thought it's not tied to so the minerals program allows us to kind of carve that up quite neatly and this is a much more nuanced framework than you know when Chomsky says languages thought again he doesn't maybe he means it maybe he doesn't but that's not what the actual architecture of his theory says it's a rhetorical device that is very you know useful and uninteresting to attract undergraduate audiences but if you look at actual theories that are coming out of The Mentalist program no one really believes language equals thought right the language system seems to it tries its best to access and reformat and manipulate various conceptual systems but it has its limits right we know what systems spell Keys called knowledge systems are hooked up to with respect to the syntax engine and which ones are not um so you know this kind of gets back to the idea that Lex by lexisation of a concept seems to maybe alter it in some way it kind of Infuse it with elements that are not there in the concept itself so if you lexify as a concept you suddenly transform it a little bit you give it a little extra you sprinkle something else on top of it and that seems to vary across different noun types and but these are all like very clear architectural claims within geometric grammar that make very clear empirical predictions so in other words I guess what I'm saying is all these neuropsychology studies that are up incited you know in a lot of work um in this vein what does it really show I think it shows that you know when language is damaged in the brain it loses this particular sway or mode of influencing those systems but there's no real prediction from within the general grammar Enterprise but those non-linguistic systems should be impaired or should suddenly you know shut down if the core language system um is compromised right in fact if anything like that just emphasizes the principal divorce between the syntactic system and non-linguistic systems right so I think the a lot of predictions here from the language and communication uh you know literature are kind of missing the point of the architectural claims um I I can just give or Daniel do you want to go uh give a little bit of a background there so so there's these papers um uh from uh uh EV federenko and and rosemary Varley that that are um examining um uh uh in part of them aphasic patient so so people who have impaired uh linguistic abilities um um basically showing that with impaired linguistic abilities you you um Can can still have um uh preserved kind of reasoning abilities so people like Chess Masters chess Grand Masters for example who are obviously very good at at reasoning um uh um might not have uh kind of intact linguistic abilities um and then complementing that that kind of patient work there's also uh work from ebb's Lab showing that um uh the uh parts of the brain that that care about uh language um um are separable from the parts of the brain that care about other other domains even ones with seem kind of language-like so things like like music and and Mathematics um uh tend not to happen in the in the language areas um so EV and others have have argued that um this is uh basically evidence against the Chomsky and claim that uh that language is the medium for thinking right because there's thinking that can happen in the absence of language and the brain areas that care about language seem not to be the brain areas that care about care about thinking um I I guess Elliot you're you're saying that people don't don't really believe that um uh they don't believe that that distinction I mean um that uh no it and also there's a lot of like self-contradiction even Within These arguments right so so in your paper you sometimes say that Chomsky thinks that language is a thought system but then a few pages later you'll say Chomsky also believes that syntax is some totally separate system from anything else right you autonomy of syntax Etc so which is Chomsky thing that's not my contradiction I mean he said both of those things um right exactly so so therefore you may want to ask yourself does he really believe these things or what is the particular arises from from the people from the architecture right so just saying just saying language as a thought system what does that mean that doesn't mean anything it's just a very vague statement the question is how exactly is language contributing to Thor and how is it not contributing uh yeah I mean I think his claim is is mainly evolutionary or something right that uh this is the the origins of of the system which I think is is sort of equally hard to square with um uh the kind of patient and and neuroimaging data um um but you know if if he doesn't think that then he shouldn't say it or people will respond to what he said I think well no no because the argument is that language is a kind of thought system it regulates some aspects of though and it and it yields some aspects of thought that are clearly unique to humans but it's not intrinsically or causally tied to it right the the architecture of the system is very different from the kind of generalizations you can rhetorically events from the architecture so for instance when you site work from aphasic patients showing no deficits in complex reasoning as you just mentioned playing chess and so on we would actually expect this under a kind of you know non-lexicalist framework of geometry syntax where meaning as I said meaning syntax and form form just meaning anything that you can externalize language and all these things are separate features and separate systems right the autonomy of syntax doesn't mean you know what a lot of people think it means it just means either a certain there are certain syntactic operations that are not semantic there are certain things you can do with syntax that you can only do syntax and you can't do semantics so this gets back to the difference between you know uh petrovsky's theory that semantics is just and right versus the uh a lot of synthetician's belief that there are certain peculiar weird things you can do with syntax that are just syntactic so there is a divorce even within the kind of architectural framework and so it's not too surprising that you also find that divorce at the neuropsychological level I would say well that I I think I I would I would want a prediction of the language is thought evolutionary idea then right so like uh if that's not if you're saying that that doesn't predict that thought is relies on language um then then uh I think Whoever likes that theory should should come up with some predictions um about uh you know what that what that theory actually means I mean I feel like those kinds of predictions are are often really necessary for understanding the the content of a prediction um um so sorry Daniel your your hand's been been up for a while uh no it's all good just kind of wanted to bring a um breath in and um an opportunity for anyone to uh ask any other questions but wow thank you both for the many topics we've covered um we'll have in the last minutes uh kind of conclusion and next steps but Dave would you like to ask a question or just give a short reflection okay no um there are many comments in the chat so I hope that both of you can read them on your own time to to see what everyone added where do we go from here as we Roar into May 2023 and Beyond what can linguists large language model developers and users cognitive scientists what do you each think are some of the most fruitful Pathways forward well I would say um you know the most fruitful pathway forward is to really take um like cognitive psychology no seriously there's a lot of nice work recently trying to align things like yeah you know tragibiti Wolfram Alpha plugins the way that chatterbd can interface with different kind of modules and the way of building a legitimate kind of AGI system it doesn't necessarily have to you know be psychologically reliant on the kind of modules that human beings have but I think it will benefit from it so there have been some some claims that large language models can maybe do you know all sorts of things right everything everything you're like um but I think in the long run it's most likely going to be the case that llms can do something very important and very interesting but it's only going to be one piece of the puzzle so in fact even open AI CEO Sam Altman said last week that um you know what we can do with llms has really kind of been exhausted we need new directions new new new avenues and so on I guess it was probably you know speaking to investors more than uh you know linguistic students here but I think he's so right you know llms can do something spectacular but they're probably going to form a small part of the general AGI architecture right if you want to think about AGI as a potential potential goal here um so you know I think a lot of the so let me give me another example here so um Anna even over um who's a a very good productive scientist she has a paper recently um arguing for a kind of modular architecture for llms and which is a very nice framework right it's very productively plausible it's exactly the kind of thing that we should be pushing for it's compatible with Howard Gardner's you know notion of multiple intelligences and so on um but I think at the same time just just to finish this comment um there was a tech talk last week I think or maybe a few days ago where um a lot of other stuff can be conflated with AI hype in unproductive ways so Greg Brockman from openai he gave one of his uh one of these big Ted Talks where he showed different plugins that chat GPD can do I mentioned wolf and operate but there's also things like image generation instacart shopping where you can get tragically to buy you things and what have you um and again this this takes you back to the idea that multiple subsystems can do different sub functions so Brockman also showed an example of giving chat GPT an Excel file a CSV file and from an archive database of academic papers where it just listed a bunch of papers and and then titles and what have you right um and he said that you know using Chaturbate it uses World Knowledge to infer what the titles of the columns mean so we understood that you know title means the title of the paper it understood that authors mean the number of authors per paper it understood that created means the date of the paper submitted right and because it's a TED talk you know the whole the audience gave us a standing ovation right um but the ability to describe labels on an Excel file is I guess nice but um I'm not sure you'd really call it World Knowledge so I guess there's a lot I would just say there's a lot of progress needs to be made alongside reducing anthropomosis anthematism you have the right balance of it so like I said you have to have the right balance of um psychologically plausible kind of modular architecture but you can't have too much um anthropomorphism because then you'll get carried away you have to find we have to find the right balance between modeling kind of human-like uh modular systems but not doing it to a degree that is a bit you know um implausible or scientifically unhelpful I mean I think I I agree with all that I'm really excited about these uh ways of kind of connecting language models to uh other forms of of information processing uh which does seem like what what people have I think I've I mean I've been like very surprised at the uh the things they are able to do um just as as language modeling right so um you know different kinds of reasoning puzzles and things that they can solve I I think is is uh really fascinating and and you know maybe will require us to to rethink our you know the the relationships between language and thought and and try to figure out a way of being specific about what it means for something to uh have a representation or to reason over that that representation But ultimately I think I I agree that um um you know people have different modes of thinking about things and that that seems important for uh for intelligence um I'm also super excited about the baby LM challenge so I think on the kind of linguistic side right um uh that's exactly the the right thing of of seeing how far we can get with uh smaller data sets um and maybe eventually after that you know trying to um uh understand some some more about the kinds of semantics that that kids acquire and and where they get it from and and how kind of external semantics can inform uh language learning or specifically maybe maybe grammar and and syntax learning um I guess my my other uh pass forward point would would be um that there's uh like I I feel like these kinds of models have have um uh really gone far beyond people's expectations for this kind of class of of model right kind of ground up statistical learning discovering patterns in in text um um seems Seems to give like really pretty remarkable results um and that for me going forward I I think has just introduced a huge wave of uncertainty over theories so I think that our theories of basically everything in language for sure um but cognition probably Neuroscience right like all of those things I think are are going to be reworked when we when we really come to kind of understand the um uh the ability of really General kinds of learning systems like these so um that makes it you know on the one hand uh um kind of a bummer for for past theories right especially theories which relied on on um uh you know learning not being able to to work well um but on the upside I think it it makes it a a very exciting time both for for AI and cognitive science and and Linguistics um where now there's these these really really powerful tools um that that seem like a qualitatively uh different size step towards human human abilities um and I think kind of integrating them and and taking uh both the the kind of engineering lessons and the the kind of philosophical lessons about how they're made and and what kinds of principles go into designing intelligent systems I think that that uh those things will will really shape the feel over the next five or ten years um and also like I would just say in in the context of broader themes here right like you're totally right like I remember when I was reading about when deep blue be uh Casper of was it the the chess uh thing right yeah and there were some commentators who said you know chess is over if an AI can be a human then it's game over what's the point in studying chess you know there's no need of boring anymore um and I guess if AI has achieved seemingly everything that humans need to do to play chess what's the point of playing it um but I think you know if anything it turned out to increase the popularity of Chess right they're in our mini chess celebrities as well worldwide tournaments and I I would predict that the same is probably gonna happen with language too you know llms do not mean it's the end of language no more language no more Linguistics I would actually push back and say maybe it would be the opposite um you know the success of LMS will increase general interest in linguistic Theory due to their pairing you know weird constraints and unapparent limitations right because I would also say you know scale at this point the the chess issue scale is kind of definitely far from all that's needed what is lacking is an ability of LMS to you know really abstract their knowledge and experiences in order to make group Rush predictions and generalizations and so on I gave some examples but the Smothers in the literature where it doesn't seem to really be good at generalizing it can of maybe particular token types um but I would you know I would guess my final my final claim would be that you know the language acquisition literature um doesn't necessarily need llms though you know cognitive scientists don't really need llms we could potentially uh you know reinstatement obviously disagree here but um I would say big tech companies profiting off llms need llms right they're the only ones that really do it may be the case that the mind is a very that I will say you know the mind is a very diverse space it may be that there are certain forms of behavior and learning that might be captured by processes similar to what llms are doing so Stephen has given some interesting examples in his papers about magnetism and and weird kind of rules of learning that are very domain General and very quick and very mysterious so you know maybe for those sorts of things that that kind of learning will be will be relevant and but I still think it's unlikely that one of the candidates will be natural language and at least the way natural language works and it's full glory in terms of the form mainly regulation and what have you so I guess I would you know it kind of reminds me of where you you know you have this image of I saw John with chapter four recently right and he has this there's this scene where he's walking in the desert and he's not sure if he's seen this guy like he wants to assassinate it's kind of like when you walk in the desert um and you have an illusion of seeing an oasis because it turns out you're hallucinating but then you realize that you know sometimes before it's too late that you actually are hallucinating it's you're not seeing an oasis you're still in the desert and I think that's kind of maybe the situation we're in right now with with linguistic competence of lots of language models we have the illusion of uh linguistic competence for you know um you always see the illusion before you find the Oasis right so I think I think right now we're in the hallucinating stage of the desert where we're seeing potential Sparks of of linguistic competence but it's still not very clear and robust um we haven't actually reached the Oasis here yeah um just a rapid fire question so see if you can give a a short response so svenochino writes question is it correct to say that large language models have no priors do large language models have priors I'd say yes they definitely do um and um they're I think the the difference to how people you know are used to thinking about priors and in Bayesian inference for example if you like write down a Bayesian statistical model you you say like you know here's the parameters and here's what the priors are on the parameters um large language models I think the the priors are and maybe neural Nets in general I think that the that the the priors are much more implicit right so there's some functions which they find easier to learn than uh than other functions and there's even some work trying to discover you know some statement of what those kind of implicit priors are um but that that's actually how I think about um um you know comparison of different neural network architectures right um uh which is maybe something elated and I might might agree on right like you have to find priors which allow them to learn the things that that kids learn right and um not all architectures will do that um even among architectures which are turning complete or capable of learning any kind of function not all of them will will do it uh even on on kind of huge data set sizes so um I I think of this sort of search over neural net architectures as really one of of a search over priors um but it's not priors or I mean you could think of it as a search over Universal grammar or something right but it's it's it's not priors or Universal grammar in the sense that people have talked about it as like an explicit statement about what kinds of rules are allowed or an explicit statement about what kinds of functions are high probability or something like that it's all implicitly coded there um yeah yeah totally I think I think that's right I mean you know the real question is reducing the space of what those that prize alike and if it's anything remotely like what human beings are doing so llm's like I would I would at least say that things like qpt3 are in existence proof of you know that building fully functioning syntactic categories from surface distributional analysis this alone is possible that's yes that is correct but you know even so I would say most think tacticians don't really believe that syntactic categories are innate so the prior issue is slightly less irrelevant here it's the operations that are said to be innate so the in the syntax domain it's particular linguistic computations that are said to be in a and categories themselves if I even Charles young um has admitted in the last couple of years that they are maybe in it uh but maybe not so people have given another of a relevant priority are things like um you know me and Gary markets have talked about compositionality that seems to be a big problem so people have given chat GPT BBC News articles asking it to compress it and then re-explain it uh so one example I saw was Peter Smith 58 is being arrested on charges of manslaughter and you get it to compress it and re-explain it and it comes out as 58 people are being charged with manslaughter right that's a pretty clear example of a lack of compositionality being built into whatever compression it's doing and there's another example where there's been there's some examples of potential analogical reasoning so in Bing chat you know Bing has this this chat function um the question is is it just finding meta relations that have already been documented by humans or is it genuinely creating new relations the new stuff that's being built um so you know someone asked uh draw me a table comparing Jesus Christ with the Nokia 9910 right the cell phone Nokia 9910 um and it said you know they compared the release dates it compared the size the weight it compared the CPU with Jesus's all-powerful knowledge it compared the memory of the phone with the all-knowing nature of God right um also I think it said that they were both resurrected because the Nokia was re-released a couple of times right so the Nokia that sounds like a great answer what's wrong with that that's okay it may be maybe it sounds a lot like analogical reasoning but then it also had some quite weird ones where it was like you know for the camera it said no it just gave Jesus's description or it's not really what a camera is there's some kind of things that look like analogical reasoning maybe but it's unclear yeah yeah and hey oh I think that that sounds like an awesome answer to me I I I I was going to say like you you said large language models learn they're an existence proof of part of speech categories but like they don't just output part of speech categories right like they they have a lot of grammatical syntactic knowledge um uh and moreover like they have a lot of semantic knowledge and probably some pragmatic knowledge and you know they're they're not bad at translation and like it's it's way more that they have discovered um than just part of speech categories um uh well I'm sorry I said scientific I'm sorry it's like it's a technical categories right yeah well sorry so yeah yeah but they've discovered way more than that um yeah um I'm going to as a um teaser slash motivator for hopefully both of you to join uh again in the future with with or without other guests a few of the exciting questions just for us to include in this transcript and then thank you both Ellie and Steven for joining so just a few of the last questions that were asked Juan asked how do small Transformers Jang at all 2020 compared with children learning language um 96 asked what are your thoughts on implicit priors versus animal Instinct rojda asked what constraints that space in llms don't they get there by training so are they discovering it that's not what they Implement at the start maybe and there's many more questions so I hope that we can all um review and re-read each other's works and come together for 41.2 in some future time thank you Elliot and Steven for this excellent stream thank you Dave thank you both yeah thank you so much farewell bye see you