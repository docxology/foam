hello and welcome this is active inference guest stream 79.1 on April 3rd 2024 we're here with Roto canai talking about meta representations as representations of processes we'll have a presentation then a discussion so thank you very much for joining and looking forward to hearing the presentation and talking so go for it yes okay right yeah yeah thank thank you very much for this opportunity to uh present our recent work U yeah and thank you for sporting our very recent paper which uh just uh uploaded preprint so uh so in this talk I want to talk about what meta representations are and but uh but to uh explain my motivation I want to start from my kind of frustration in uh theories of Consciousness so um so a lot of theories of Consciousness tend to be described semantically and and then you know for example like know in today's talk I mainly focus on the uh higher order theories of Consciousness but um it kind of makes sense at the semantic level but when we think about uh how we can Implement uh such uh kind of high level theories uh with uh new networks uh we realize there are a lot of uncertainties about how we might be able to implement them but uh but I think this is a you know something we now we can call it like constructivist approach so so by thinking about how to implement uh theories uh we can actually reveal vagueness and make uh the concepts uh more um so more precise so so that's the uh the main topic and so so as I said so I want to talk about higher order theories Consciousness maybe many of you might have heard of it um but just uh broadly speaking uh high order there is of Consciousness uh claims that U mental state is conscious uh not just by having that processing but it becomes conscious if it's represented by a higher order mental state so by higher order mental state or higher order representation uh generally it mean means it's a representation about another representation so so intuitively I think this makes sense so when uh so for example when your part of your brain is processing something red uh know people ask whether that's sufficient for conscious conscious experience of seeing red but but when you have an additional representation that represents that you are processing something red so that kind of meta representation seems to be functionally related to awareness so that kind of makes an intuitive sense but but once we start thinking about how we can implement this sort of meta representation it's very unclear what it really means um so so we try to uh find a way uh where we can sort of represent meta representations but uh but let me explain why it's difficult okay so this is a very naive way to think about uh constructing meta representation so let's say you have a like image as an input and then there's some processing in the brain or deep new networks then you can represent uh the the contents of the input or image but but if you transform this first of the representation by another neuron Network U that's like very naive way to implement meta representation but but with this uh you can easily have like many meta representations so know you uh let's say like if we think of this as a uh like V information processing in the brain maybe know this could be the retina and this this could be lgn this could be V1 V2 and so on and but but this but with this somehow like everything is just a representation of the first input so so somehow like this doesn't work or like if you think this is enough for met representation then like everything needs a meta representation of something else so yeah so that seemed a little bit strange and another way to conceive meta representation is to think about confidence so uh but in the context of metacognition uh of and cognitive Neuroscience we often take confidence report from participants in an experiment so for example like know we could present some visuals stimulus and then people report what they saw like know if it's a color discrimination task know they could say uh blue versus red or something like that but then uh know you use this first order information to make a perceptual decision but also you can report uh the confidence of seeing that stimulus so so this is a probably a bit better than just like very simplistic uh sort of any transformation as a met representation so but on the other hand if we think about implementing this this is a very simple operation so people working in uh artificial neuron networks uh easily use something called Soft Max where know you convert this kind of representation to some sort of probability uh interpretation so so basically if you convert like activation patterns of this layer um to to that so that it normalized to you know one uh then it is confidence so so in a way this is just one very simple uh transformation of first all the representation so okay now maybe no this is still meaningful when we think about how the brain process this kind of uncertainty uh but but but this may not be at least it was not really satisfying for me as a way to uh so uh characterized met representations so the the main uh solution we came up with this idea so uh so you can so so we always tended to uh think about transforming first order representation to something else but but here what we are proposing is to construct a representation of processes rather than this kind of States so the idea is okay so like like know any neuron Network can be conceived as some sort of uh function from input to Output but but there's a way to present these functions or processes so so instead of re representing first of the representation or we are proposing we can if we represent this function f to some met representation uh you know know we can have a potentially uh better interpretation of meta representation and because this makes a qualitative uh difference so but but maybe you know this might be somehow confusing so I wanted to give you an example using artificial neuron networks so that's the main topic of the [Music] paper okay so here okay I just want to briefly sort of introduce the concept of Laten space or embedding uh for people who are not familiar with this kind of stuff so in artificial neural networks uh there's a concept of encoder and decoder and so for example this is let's say you train an auto encoder uh neur networks uh where the input is an image and then output is an image so so basically uh you want to reconstruct input uh using encoder and decoder but there's a low dimensional bottleneck in the middle and and then usually uh when you have this kind of construction you find like interesting like features where you know the states in this uh the vector in this space captures the you know like important characteristics of the uh data set in this case images so yeah so that's the idea so so basically it's a know the compress nice represent a of the data set and but what we are proposing is to create some sort of meta o coder where the input and output are NE networks so you can use a neur network uh which can be uh parameterized by or represented by the weights and biases or you know you you can use like many different ways to sort of convert NE networks into some vectors and then you reconstruct the ne Network so so maybe you can already see this is somewhat meta and so with this construction uh you can have a representation of newer networks so in this case uh you know like know each point in this space cor responds to some sort of neur network or function so yeah so so in a way this is a space of functions uh parameterize some you know like latent uh axis so so that's the general idea so I hope you get the idea so far yeah and so but from here uh know I want to explain the actual experiment we did and so this is a little bit more detailed construction of what we did so we first created a lot of encoders by training all encoders for all kind of different stimulat so example like in one case uh you know let's say like we trained this kind of uh first order new know Network on just on uh parts or just on flowers or or sometimes just know the sign of dog barking and so on so like each and know first order your network is specialized in some kind of stimulus category and we use both Visual and audio stimula but but each network is trained on just one type of stimulus but one category and so and then this encoder uh uh can be represented as 1056 by 16 uh elements so like each column cores okay so here like in the know first all the lat space we have 16 dimensions and then input image 56 uh pixels and yeah so basically know you can see this number as the sort of you know parameter for the new dispersal the new network and then like each column corresponds to okay so as you can see that this is a very simple neur Network so just one transformation right and then um then know we took each of these as the input as a filter and then know we get like like this kind of so okay so so this just one column here is a filter in this network and then we use this as input so so we know we trained another Al encoder to represent individual filters and then we computed uh representation of this whole network using these two networks so so this may be slightly confusing but the main thing is uh we get uh representation of a network at this stage and then we uh created another meta coder uh using this kind of lat representation of our like first Network so it might be a little bit confusing compared to the previous image but but this is more more to deal with the technical aspect but the main thing is uh we trained like a lot of small networks on the specialized stimulus category and then we try to get representations of those Nets so so that's the idea okay and so then uh know we applied uh like tne to visualize lat representations um so here like all the blue dots corespond to some visual stimul some category and then the orange uh they point correspond to some auditory stimul so that clearly you can see there's a separation between Visual and the audio stimulate so that's kind of interesting right so because this suggests that when you have a meta representation you can already tell whether the network is specialized in visual stimulus or auditory stimulus so so that's a um that's that's what we were uh excited about yeah but also you might be able to see like some clustering of same shapes it might be a bit difficult to see this this image but but there might be something like you know like some you might be able to tell whether uh a network is specializing cars or buses or things like that so so we tested that uh uh by trying to predict the original data set just by looking at the meta representations so yeah so as you can see like there's a clear separation between modalities but but there's a little bit brighter like diagonal Elements which suggests that know sometimes we succeed in predicting like really specific uh stimulus category so so that's uh that's really exciting um yeah okay so so so based on this uh it seems we can uh there's already some information about the structure of the weight of the network uh which tells us about what kind of information they were trained on Specialized in that's an interesting observation but also like we tried this in the original weight space but in that case we cannot really predict like which know category of the network was trained on so so in that sense you actually have to have this kind of meta representation to uh be able to make the all these predictions okay so um but maybe one uh crucial question is know why is it possible um so especially uh when we try to compare like Vision versus audition U maybe the key differences they may have different kind of invariances or equiv variances U so yeah so for example like you know in images you know the object uh category or labels do not change if we make like know translation or Chang in scale and so on so like the the classical convolution neur networks are kind of uh rely on like this kind of uh invariances and but but maybe for auditory stimulate or you know if you sort of move the know if you translate the input image or the sound differently so you experience different kind of sounds and also the identity of the sounds changes so so there might be some modality specific invariances so so that no so maybe know after training a network they learn to capture that kind of invariances and meta representation may also find this kind of uh representation of invariances or equiv variances in the structure of the weight so so that's our current explanation why this works okay and I'll come back to this later so and um so so in a way um like our experiment uh was also motivated by the like classical uh question of what it's like to be about and U the interesting thing is we don't so bats can do echolocation of know by emitting sound and then receiving sound but but but but they use it for navigation so maybe it's it's like vision for them but it's hard to say like whether they are experiencing visual Quia or auditory Quia so so we've been know we want to answer that kind of question eventually in Consciousness research and uh so but but to uh introduce uh an interesting context so U so this is a a very exciting series of research from uh uh American group in at MIT in the 2000 and so maybe many of you are already familiar with this experiment but but they managed to do an experiment where they rewired the uh input from the retina to the uh auditory cortex so yeah I think it's like really amazing that they can actually do this sort of thing but um yeah so but the U the idea is that their auditory cortex are developed by receiving input uh from visual images the data coming from the retina and the interesting so they had several of nature papers and they were all very interesting but the one of the key findings is that uh after uh auditory cortex was kind of trained on visual input their connectivity pattern looked more like normal visual cortex so yeah so this kind of so connectivity to l connection to like other neurons know prob with like similar uh like orientation properties and so so somehow their connectivity seems to reflect the sort of uh like statistics or structure in the data they learned and uh but there's an additional interesting experiment Behavior Experiment they did and the the main question they asked is whether they see or hear the activation of this rewired auditory Vortex and the short answer to that is there the animals seem to see the uh the know brain activities in the rewired auditory cortex so that's like very amazing result so yeah so based on this I thought like this could be a really useful way to like think about whether we are you know whether you know some activities or or some new networks uh it's more like for hearing or seeing and so yeah so that's something I always wanted to ask so yeah so maybe um you know I've been thinking about like testing IIT uh using this kind of visual versus auditory uh Quia and yeah so the question is whether we can tell whether a piece of Cortex is visual or auditory just by looking at their connectivity so of course you know IIT is very difficult to apply and we don't know yet whether I is true or not but but this know know we we need to think about how we can test implications of IIT by Empirical research okay so so this was know what I was thinking originally so let's say you know you take some you know like Anatomy from visual cortex or auditory cortex and then there are like many techniques to look at anatomical connections and functional connections it's very still very difficult to read out the weights from the actual brain but but if we have this uh we should be able to compute uh information structures as suggested by IIT and we can probably match the structure to uh the report of uh know V visual qua or auditory Quia so that's the ideal experiment I wanted to do but there are a lot of challenges so the first one is in the actual brain it's it's very difficult to have like complete uh characterization of anatomical connections and you know it's also impossible to look at the no caal uh patterns uh in the activation from all the neurons in question so that's that's very hard seems impossible and another difficulty is the the computation of five in 8 is very hard so uh and yeah we've been also trying to compute some surrogate of five or earlier version of five but it's still very hard if we want to compute five from let's say 10,000 neuron so that's that's prettyy hard and so and also maybe a third thing is no how can we match the structure of the report of experience to the structure of information so that's again a difficult question so but this know with the uh approach I showed you earlier uh we can kind of uh replace some of the problems so so instead of looking at the complete anatomy of the human visual cortex and human aary cortex we can create sort of artificial neural networks by training them on specific stimulate or in this case like sounds and images so that's uh no the great thing about artificial new networks is we can see all the weights and connectivity patterns so of course you know we don't know whether they're conscious or not but but least know this approach gives us something concrete to work with yeah and another thing is uh about uh Computing file in IIT uh we can uh do something slightly different so there's a really interesting paper by mediano and colleagues where they propose the concept of weak IIT so like if you take the original IID it's um yeah it has like both like mathematical and philosophical aspect so so that's uh my interpretation and but but if we can use the uh mathematical implications or know if we use by as a uh kind of index of complexity or something then uh we use IIT in a more pragmatic manner so so that's their uh proposal weak IIT so I think I think you know there there have been like a lot of Empirical research driven by IIT so I think that's a a good thing but but here U what I'm proposing is like more like conceptual IIT so so in the U uh meta representation work I presented today I I also took some inspiration from IIT as well so the idea is like in IIT whether that know some Network generates uh visual or auditory CIA should be fully described in terms of their connectivity patterns so so that's a kind of conceptual implication of IIT so but instead of applying the mathematical formalisms of IID we just use all encoder to find or embed uh neural networks uh in a a practical computationally tractable manner so so so so I think this kind of the conceptual IIT approach could be also a useful way to make progress in Consciousness research okay so the final uh point so so maybe one question is whether meta representations uh as we presented today actually exist in biological Brands so that's Highly Questionable so for example uh you know today I embedded new networks just by uh the weights of first order networks but but in in the brain seems impossible to read out the weights from other brain region well it may not be impossible but but it seems like very unlikely and so there might so maybe in the brain there might be a different mechanism to achieve this so so instead of using weights if you have like many sort of input output pairs you can also uh Constructor representation of that kind of relationship so maybe without some figure or or equations it might be uh a bit difficult to understand this but but let's say uh you have two brain regions like you know V1 and B5 and but if a third region receives the input from both regions the third region can learn to represent the pair uh pair of representations so that kind of meta representation may exist in the brain and also know we've been thinking how we may be able to find such representations with f so that's a a topic for a future study yeah and another question is what what's the point of having this kind of metal representation so what what's the functional role so our current interpret ation is when you have this kind met representation you can uh sort of compare different networks or uh you can have some sort of qualitative characterization of first order networks so let's say know you have uh meta representation of uh know red neur red color processing Network or or you could have like know meta representation of like many many different first your networks but but then in this space you know you can talk about whether two processes are similar or different so for example in the space you can say the visual experiences are very different from auditory experiences but within the visual experiences there are many different types of experiences and here you know you can talk about the distances and similarities uh so maybe it's a bit similar to the idea of like what to V know where words are embedded in a uh lat Vector space and when you have that kind of space you can actually have some reasonable representations of semantics of the words but but here if you embed uh new networks you can have some sort of semantic or representations of those networks so so that might be the potential functional role and so so in a way like know this has like very this already has a a flavor of something like Quia so when we talk about Quia we always compare certain experiences to other experiences and we can talk about what you know two experiences are similar and so on so so in that sense maybe you know this kind of met representation might be very important for us to be able to report the qualitative aspects of our for experience okay yeah that's all and thank you very much thank you [Music] awesome cool great I wrote down some questions and also anyone watching live can can write questions um well thanks for sharing it and it's definitely very striking finding about the visual and the audio differing so just kind of a preliminary question here how was time handled in the audio setting oh good question well we we just converted the raw signal into an image so it's basically time and frequency U so so that we can use the same network for handing to different modalities yeah interesting because it's also a difference between those two features and it made me wonder about video video chatting or watching a video perhaps as being a meta representation with audio and visual because if there's like a lag or if there's a disconnect or any other number of relations it can um be noticeable as a contrast like oh they're talking louder than it looks like they're talking or something like that so I mean have you looked at that kind of fusion or how could you look at it with the architecture that you had here oh that's a great question so no we we have very simplistic architecture you know since this was just a pereral concept but um yeah so but but I think uh you know in the brain we must have like really uh multimodal representations at the same time and so that way uh we should be able to compare different experiences at the same time and so so that that's where we get somehow close to the idea of global workspace so um yeah so we we are not directly you know addressing your question but uh but we are you know on on a separate project we are actually uh training Global workspace like new networks so that you know they learn kind of multimodel representations and uh also so in that context uh we believe it's very important to look at the latent space structures from different modalities and then see how they related to each other so with uh with Ru V uh we wrote a paper in Trend in Neuroscience where we propos that Global workspace maybe uh can share lat and space across different specialized modules so yeah so so I think that part is also like important uh like next step to sort of uh to understand uh theories of Consciousness from the perspective of deep [Music] learning interesting uh I I found it also very interesting how you began with the uncertainty estimate because in active inference a lot gets low loed onto uncertainty estimates of different parameters and that made me think about for a uh yeah thanks for the side it made me think about for if you only have two parameters to encode then you can encode like the mean and the variance however with the neural network autoencoder concept you could project down to just two or could be more and so then there's a much richer pallet for and more bandwidth than just a statistical distribution even though it's also composed of statistical distributions but the minimal and the simplest and like the kind of most essential is really the statistical single distribution but this is basically just talking about the connectivity of multiple [Music] distributions yeah that's an interesting on [Music] um yeah I guess the maybe crucial question is whether and how uncertainties are represented in the brain so like when we think of uncertainty estimation in terms of the two parameters uh you know that's kind of mathematical notion but uh the uncertainties Maybe you know proba we don't use just a single neuron to represent whole uncertainty so so there might be also population representation for uncertainty so so in the actual implementation in the brain um yeah you may actually use like many neurons as well but I've been also like really interested in the this topic and especially uh like uncertainty estimation in the uh thamus so no my friend and colleague uh called comra he found a like uncertainty estimation neurons in the Piner in the thamas for visual experiences so I thought that was like really and cool study and I thought you know there seems to be like a close link between having high confidence and conscious experience so so I thought know this might be like like a really key ingredient but but somehow like when we think about uncertainty in terms of deep learnings it it's kind of it seems like very trivial so I feel like there's a gap there [Music] um that's very interesting there's a lot of ways to go with that um in the the last two points that you had up there you return to this kind of functionalist question or at least just perspective like what is it doing so that made me wonder when you look at the meta representations for the networks you construct do they seem to convey something like summary statistics of the network like the the spareness overall of the connection or like some kind of network descriptions you also mention the differences in the stimuli type so like does the what parts of the represent of the meta representation reflect what the network is and then also like what it does input output wise uh great yeah so we we don't know really what's in the meta representations of our uh networks um but but maybe this slide might be relevant so uh so each new network the first order NE network uh tries to find the good basis functions or kind of good filters so that they can efficiently encode the images or sounds and uh but but they they really come from from the statistical patterns in the stimulus so so that so that should be somehow reflected in the know weight structure so so that may be related to know this kind of invariances or equiv variances uh that that's our speculation but but also you know for like like really specific type of things so for example like these are all like different kind of specific stimulus that maybe this is air conditioning or car sound and so on so so they may have like really specific uh sort of structure um so so then they may be embedded in the new networks but the interesting thing is when you train a new network uh you get different things every time uh but but they have something common across every training but on the surface they look very different right so yeah but but but across the results of many different training uh different networks trained on the same stimulus there is something common across them and so so that's the kind of features we wanted to so capture with metal encoders Okay this may be reading too much into the image but the um it's all V for visual a for audio oh yeah yeah that's that's right yeah explain it the the big off diagonal blocks are just the differences between the audio and visual that are solid purple but then um and then um the audio stimuli has more a little bit with other audio but then like in the visual it's I don't know it's like related to the syllable air so it's kind of like a natural Association even though it wasn't trained or maybe just cuz it's on a diagonal uh yeah there might be some bias oh yeah this so this one yeah somehow the kind know predictor thought like all the images come from this one um yeah but it seems like you know from this figure like a different audio stimul are very distinct whereas like in visual things they seem kind of simar yeah but probably the but this may be more uh due to like technical constraints so so here we have to use like really like small patches so so maybe you know like instead of looking at object categories per se maybe those networks looked at the texture Parts across different categories so there may be still some information but uh but maybe they're not very specific the IND c h yeah I think I mean you're you're shooting for the moon with a Consciousness component but even making simpler Networks to understand what aspects Laten spaces learn is a very useful method that I think this provides a strong example of yeah yeah I hope to so and but also I know since we work in the domain of uh like AI research as well uh we encountered something called mechanistic interpretability it's a kind of field of where research where people try to understand what's happening inside NE networks because you know like people claim a deep learning is blackbox and we don't understand what's going on inside but but but I think eventually we want to understand what's happening in your networks as well and the it should be still uh easier compared to understand in the brain so know maybe you know in the context of idea I talked about this a little bit but in deep learning we know all the weights we can do like any experiments we can do ablation or know we can look at the function of part of the network and and we can you know give them millions of Trials so so it's like a ideal experimental setup for neuroscientists and so if we don't understand uh deep learning your networks it's hopeless to understand the brain because know it's much harder to do experiments in the brain we cannot know the weights and connectivity so we we only have very limited access to the actual material so so in a way we can practice how to understand systems uh using a simple neuron networks and then try to sort of use that experience to understand the brand so I think that kind of coperation is potentially very interesting yeah very well said um it reminds me of some work by Jonas and cording from 2017 the paper called could a neuroscientist understand a microprocessor and they had an in silico simulation of a whole microprocessor doing different operations so they could do the lesions and the double lesions make all the recordings MH and so that kind of revealed on one hand the limitations of different methods that people often use to ascribe function to the brain however there's always this component where it's like well maybe that's just because the processor is a weird architecture or because the operations that they do are very synthetic and they don't really have a natural component whereas when you're proposing that the base neural networks must deal with the symmetries of audio visual and so on it's the kinds of challenges that organisms actually have to solve with nervous systems as opposed to being software which could have been written in an arbitrary way and so the results are a little bit different but it kind of makes the proof points which is that to have the in silico version that you can do the digital simulations on can help you identify like where your studies are well powerered or not and do all these other useful functions even if they don't directly answer the question itself yeah yeah I'm also a big fan of that like cing paper so I think I think uh know that uh yeah yeah they made a really important point so even if we have access to all the things you know we still may not understand it and uh and it's hard to say whether you know the organic systems are easier to understand compared to synthetic systems so well I guess we know we just don't know but yeah but but without making any assumption I think it's just very hard to sort of uh understand any computation happening happening in many different scales uh not just in the brain but inside individual cells or maybe at the smaller scale or you know larger scale like Galaxy and so yeah I think we still like the science to connect physics to computation so so on a different paper uh we also propos something called universality so the idea is uh well so so the point is a theory of Consciousness should be also Universal in the sense that we can apply it to non-biological brains so like a lot of times people ask whether current llms are conscious or not and but it's kind of it depends on the theory you subscribe to so but the current theories like U higher order Theory or Global workspace Theory uh do not tell us whether know AI are conscious or not but I think the reason is uh we need to Define theories based on some physical system so U what so for example it's hard to say whether a neur artia neural network has a global workspace we can we have those Concepts but but but when we try to analyze physical systems and then try to see whether there's a global workspace inside it's it's very hard so you know we we need to uh make such theories uh universally applicable so that you know they they can tell us about know computations happening at many different scales correspond to you know the concept in those theories yeah that definitely uh kind of calls back to the alien slide and if we had a tissue sample with only the static topology with no function what could be assessed um on a different note um inactive inference I mean half of it is action so when we're thinking about variational autoencoders and the transformation um from observations like down into State spaces often that's in terms of the policy rather than only a compression which is kind of like a sens making and reverse sense making layer that's the predictive coding predictive processing Origins and then also to bring in the action representations so that's kind of like being able to distinguish the primary audio and the primary visual from the motor cores and those may have very simple or very um sophisticated representations but they're also ones that could be directly correlated with like bodily movement or muscle activity rather than being correlated at some level if you go that way like with something can't measure yeah um yeah I think you know like here like we only talk about sensory experience but but but like you said I think some of the meaning uh of sensor experience comes from action or in relation to your body so for example uh you know without body know there's no up or down left or right so there's no Direction but but but you get this kind of know so Direction in the visual image or reactivity your body for example and so and also maybe you get uh kind of representations related to affordances so whether something is actionable that also you know relies on some combination of sort of action and extensor representations yeah I think of it's very interesting to think about how we can extend our current Research into you know more like agent uh based uh architecture that makes me think of um the possibility that the meta representations could be higher judgments whether those are experienced so like a a very cross modal judgment would be like can a human make this and then that could be asked across modalities or could reflect multiple modalities but it it it it has to draw information from also not just the sensory data but like other kinds of memory yeah it connects with with the broader cognitive modeling and the agent architecture whereas here it's really reducing it to just the streams of vision and audio which is the right starting place and then there's going to be other streams that aren't just the sensory oh okay yeah that's a great comment so this reminded me of the the something I I had in mind and forgot to mention so we we are thinking of this kind of met representation as one of the modules attached to the global workspace so when you have something like Global workspace you know you want to use functions not just from like sensory inputs but you want to combine that act with action reward and so on and so in principle okay so we want to use representations of potential actions or even like mental operations so like you know adding two numbers is a kind of mental action so when you try so so we are actually like thinking about connecting like this kind of met representation to general intelligence so when you are have a new program know you have a representation of the task and but but to solve a new task you want to find a potential combination of you know specialized modules you can use to solve that task so so so in a way you know this kind meta representation can be used to match the tools you have in your brain with the current tast so so in that two box you don't just have this kind of visual uh representations but but you may also want to use potential future actions you can use so so but but you can also use the same meta representation approach to represent the reperat of actions you can make so so that's a know that's the connection H hm yeah potentially in the active inference model like you had the slices with the different filters MH the B Matrix in active inference or just the transition matrix it's the policy dependent transitions on the world so that's kind of like the policy dependent filter applied yeah yeah yeah yeah oh yeah that's a yeah good connection too yeah and then also on the thalmus I'm not familiar with the mamal neuro Anatomy very much but when you said that Clarity was important did you mean like what um yeah so so I think a lot of interest uh in confidence or met cognition comes from the literature blind sight or blind sight like uh phenomena and so it seems like even when you have you have the ability to report these stimulus without confidence know you report no you didn't see it right so so it seems you know this kind of confidence is expressedly uh present in the brain so for example like in the thamus and but so when you de activate those neurons somehow you know at least you know in monkeys they lose the confidence as they so in terms of confidence rating they seem to become like Blindside patients so so so in in my introduction I kind of criticize the kind of Simplicity of confidence as met representation but but at the same time in the literature of Consciousness research it's also very important right so somehow like without confidence uh know you don't like really report it but but I think maybe you know we need to have like further studies about how confidence or like lack of confidence changes the way information enters something like Global workspace or I don't know some stage of Consciousness so but I think that's like really likely in the context of like know Precision waiting and stuff right so without confidence maybe confidence works as a way for way to get information to something like Global workspace I don't know maybe I talk about global workspace too much but yeah I think it's a good way to like think about systems yeah I I agree there these are just like kind of the labels and the the models that can then be applied so what that made me think of was the again an active inference if we're thinking about attention or Precision confidence it's usually just one variable in the minimal case just changing the variance on a statistical distribution latent spaces makes it much richer um so then let's just think about what sources are we paying attention to which sources if are we allowing to like ignite the global workspace or have more of a percolating effect and then there could be like it's like the dinner party problem so one part of the problem is just distinguishing and interpreting the sounds so that might be a simpler like left side louder or quieter and or or moving to something like that but then you would have a higher order evaluation space with like this person you can trust on this topic and that topic but not this one and vice versa here so that's invoking more of the world model and then that could control to what extent different things that those people said after they'd been parsed out so like the primary sensory mapping is simpler because this is just a very sensor actuator type problem whereas the higher order Laten bases could be just very large oh yeah I guess maybe for like a simple like presence versus absence kind of judgment this confidence may be like critical for a stimulus to enter conscious awareness uh whereas probably met representation I talked about maybe more related to sort of evaluation of the quality of the content of Consciousness so if we want to make comparisons across like different sensory experiences you know we need this kind of meta representation I think that's a kind of different aspect of metacognition um yeah so I guess maybe like like in like earlier days of Consciousness research like people did a lot of like present versus absent report and it's all about detection but I think detection is very different from kind of qualitative assessment of experience and so I think this uh kind of transition in the focus of Consciousness research from you know like present versus absent uh report versus more like structured report by comparing like two experiences are similar or not so I know uh now GS group um has been like know developing this methods to sort of capture for structure so so I think you know we want to understand why certain experiences feel the way they do right and then that those things seem to be very difficult to approach but but by by looking you know at you know sort of comparisons of different experiences we seems to be able to capture uh the overall structure of the experience so so maybe you know our meta representation approach here is also more about capturing more sort of structured structure of the quality of experience rather than you know whether we consciously perceive something or just subliminary process that information that's a very interesting point it's almost like the early Global workspace and ignition were focused on the extent of the causal efficacy and The Binding but it was sort of Left for later what the semantics or the contents were because the question was at least from how I've seen it more like whether it percolated to awareness or not or like whether there was a binding or a a reportable versus unreportable stimuli MH and so it totally makes sense that Within the space of of the aware the question of how to bring it to awareness is kind of already taken so then it requires a much richer State space like you have here to even approach it because it's just not an all or none question at this point yeah yeah I agree yeah I think that's a like really interesting tradition although like I feel like we haven't really solved this report reportable versus unreportable question yet because you know we still don't know where Consciousness is really happening in the brain but but I think uh we need just um several different approaches to you know tackle the same Pro uh problem cool well I guess sort of in closing what are you going to continue working on or what are you excited about uh so we want to uh do Empirical research uh based on this uh so so I think we know here we are proposing a new way to uh look at meta representation so uh one thing we are interested in doing is to uh find whether such met representations exist in the brain so that's an open question but but we have some ideas about how to try that and another thing is this is kind of beyond my ability but there might be a way to do this better using mathematical tools so so intuitively you know so now now like some people talk about category theories and things like things like that but so it seems like what we are doing is um can be made more abstract and formalized so that uh we can have kind of better insight about what we are trying to do so so here know what we did was kind of like naive weight to express our thought about met representations but but here yeah so we are kind of transforming functions into an object so so basically one function is a point in some functional space and when this point in functional space seems to be uh somehow Rel to Quia so that's my current intuition but but but I think uh like in mathematics there must be useful tools to sort of deal with this sort of ideas I'm curious like if there's any mathematician who who can of formulate this nicely I think that would be an interesting thing to do next very interesting and also to the little I know about it and people have discussed it that is sort of a category theoretic move to have a handle like a point around a mapping so that you can have the space of the models so you took a very constructivist engineering empirical approach and then it's an exciting open question like what are the formal structures that generalize that and then you could spin up a million experiments just like the one that you probably built by hands and then learn from that pick up that as a point yeah any last comments oh I have just one announcement which is this one um yeah so I'm sorry about advertising but but we have a the Consciousness conference called assc I think that's one of the like Main Consciousness conferences and that's happening in Tokyo this July so like if you're interested please come I'm one of the main organizers that sounds awesome cool thank you very much for this presentation good luck with the work in the conference and hope to talk to you again yeah thank you was fun yeah see you see you thank you for