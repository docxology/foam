hello and welcome everyone this is the active inference live stream it is act INF live stream 8.2 and it is November 17th 2020 so welcome everyone listeners and participants welcome to teamc everyone we are an experiment in online team communication learning and practice related to active inference you can find us at our website our Twitter our email on our public keybase team or on YouTube this is a recorded and an archived live stream so please do provide us with feedback so that we can improve on our work all backgrounds and perspectives are welcome here in learning about these questions and as far as video etiquette for live streams goes just remember to mute if there's noise in the background and raise your hands so we can hear from everyone on the stack use respectful speech Behavior Etc ET today in exciting act imp stream 8.2 we are going to have introductions and warm-ups and then we will at whatever rate makes sense welcome Shannon we will walk through the sections of 8.2 as with 8.0 and 8.1 we are going to be discussing the paper scaling active inference by chance at all 2019 and Alec thanks so much for coming on to the show today we're going to talk about the goals and the road map of the paper hopefully from the author's perspective we'll then have a little pause just for an open Q&amp;A because there's a lot of different directions this paper can take one so we'll just pause there and then we'll continue through the figures just two of them and just address a few different domains of follow-up questions about the paper and then if we want to slash still have time we can go through some of the notation and math again for the rest of 2020 we are going to be moving into papers 9 10 and 11 so check it out on Twitter paper nine is going to be about Consciousness paper 10 is about scripts and about social interactions and paper 11 is about um modeling under the sophisticated effective inference framework for the intros and warm-ups if everyone could just introduce their name and their location whatever else they'd like to State especially our firsttime guests and then just pass to somebody who has not spoken so all start I'm Daniel I'm in California and I will pass it to Alec hey everyone yeah I'm Alec chance I'm based in Brighton um final year PhD student at susex and thanks for having me on the stream we will go to um Sasha hi uh my name is Sasha I'm also a graduate student and I'm based out of David California I will pass it to Sten Stephen you're unmuted but um it doesn't have any sound maybe just reload and temporarily let's just go to Shannon hi guys I'm Shannon I'm based at the University California in Merced but currently in South Dakota I'll pass it to Blue hi I'm blue Knight I am an independent research consultant and I am based out of New Mexico I will pass it to Alex hi my name is Alex I'm in Moscow Russia and I'm a researcher and systems management school and they pass it to Ian hi my name is isan I'm in Russia Moscow and I pass it to uh Steven Steven are you here I'm gonna try again does that work yes yep okay hi I'm Stephen I'm based in Toronto I'm actually studying a practice-based PhD through Canter Christ Church University in the UK and I'm pleased to be here hi I'm Mel Andrews um and I'm in Cincinnati doing a PhD cool lot of different stages and areas so it will be a great discussion to bring to the technical side with this paper for the warm-up questions people can just raise their hand as they'd like to speak and I will just put up the first two questions they are for today's discussion what is something that you are excited about and then the second question is what is a question you're wondering about so while people are thinking about it or raising their hand one question I was wondering about was how do we trade off between explore and exploit in the two figures we saw examples of explore or exploit but what is required to enact something that is able to mediate between the two of them or find a compromise between those two strategies anyone have any thoughts or want to raise their hand Stephen yeah I'm also excited about the explore um and the exploit side as well to be honest because I think that opens up um a big shift in the way that we socially interact and the way that we organize ourselves so trying to look at a real foundational way to sort of approach that is going to be quite helpful I think in a lot of fields of practice cool Alec and then anyone else who raises their hand yeah am I unmuted or muted sound good sound good yeah uh I just wanted to comment on your comment um because I I'm also very interested in that but I guess I'll go on the record saying that I'm not not convinced that active inference provides sometimes it said to provide a solution for the explore explo dilemma I I don't think it does uh I think it re recasts it so you know now it's not a question of balancing exploration exploitation but they're both zoomed into this different objective function but in practice uh there there isn't a sort of magic balancing act that you you get out of this um so I think it's a good way to recast the exploration expectation D but certainly not a solution to the problem awesome blue so I kind of have like a tangential question I and thank you Alec for your comments because it just plays right into I think what's missing very clear that um if the environment is homogeneous there's no real need for exploration because the agent is not going to gain any you know any additional progress by exploring the environment but I I just wonder where autonomy factors in right like so you know some agents have more or less control over their environment and there's no like real room for this or not that I've seen anyway in in the um active inference framework like where the differing levels of autonomy of the agent play into the need to explore versus exploit cool good question Stephen yeah I think that that point about how to look at the agent starts to bring in the enactive component a lot more I think and the generative model that's kind of sitting inside the person and existential kind of sense making so I think that's that also yeah it's like pushing this back from a a real world perspective and saying okay maybe we need to work back the other way from complex multisensory experiences towards active inference as well as using active inference to do modeling of maybe kind of very clean sort of low dimensional approaches it's like should we also be coming back from the other direction um and I did I've got an activity called conceptual action sociometry an activity where people move around space to understand how their orientation is during a moment of encounter and I've asked people to unpack it in terms of the three different areas of active influence the sort of pragmatic gain epistemic foraging or or risk mitigation or salience and it's actually really interesting when they break it down that way and start to think about what was I trying to do but from that kind of high dimensional kind of synthesis of being in a really immersive moment and they it actually people seem to who don't know anything about active inference actually were quite intuitively seeing quite a lot of shifts in those three different areas of focus depending on the type of um event or moment that they were reflecting on because they were going back to an event and thinking about the focus of their attention at that particular moment in time and how much were they sort of bringing these three areas in and then I suppose between that you could infer something about the exploration sort of desire to learn or the exploitation gain but I I agree I think it does depend on the moment that's in question so yeah I'm there's a few things that kind of this is sort of tapping into which I'm really interested in cool and to tie that just together with a recast idea from Alec it's um kind of like nature nurture explore exploit people don't know about these bin Aries they're not natural kinds until they're taught them and then the resolution to these kinds of dichotomies or binaries false or not is not some parameter that varies from zero to one that trades off for example when Evelyn Fox Keller was moving beyond nature and nurture it's the Mirage of a space between it's about reconsidering the situation so that there doesn't have to be something like a bipartition so that's definitely the direction that we will try to take it because it's a really cool idea the second or I guess the last warm-up question is is and On a related note what would be an interesting control system or any system to model with active inference so we're going to go into like a trolley car and a um Hopper I guess and a pendulum and I think it will be fun to hear about those but um we heard about this the social and the spatial there's questions that we've raised about biology communication anything else that somebody wants to bring up or whether it's yeah Mel please and then Sasha yeah um so I'm what I'm excited about I'm really excited about uh this kind of work um as I'm a I'm a philosopher but um with this sort of stuff the AC inference fup stuff I'm like please stop theorizing just like let's let's play with this um so I'm really excited about this kind of work and um the question that's motivating me is is really sort of figuring out um what the boundaries of these various models are so so what the boundaries of of active inance are relative to the free energy principle and then relative to EG predictive coding models and that sort of thing um someone brought up recently on Twitter I guess uh it there's a there's a biologist at my University um who's doing really kind of groundbreaking work in in in like perception cognition um perceptual cognition and stuff um in I guess butterflies but mostly uh jumping spiders Sal um so the the morouse lab Nate more house is doing for example um salti had like very uh elaborate like um mating dances and things and and they're signaling to each other um and they're doing stuff like like the evolution of color perception in in these jumping spiders um it's really fascinating work and they mentioned recently um or Nate mentioned recently that um that the lab is doing some like simulation work on on how we evolve additional color per set um so so you can right you can you can if you have uh certain like genes and proteins for for um pursuing certain colors you can you can kind of like shift the probility distribution the the the range of of color that those pick up right um that's something that that Evolution can kind of modulate but um the evolution of an entirely different um an entirely new color percept is is something different um and so they're doing some some like in silico work on that and I I thought yeah that's what that's what active INF that's exactly what active INF should be do that's what we should be doing it's great so I think that's what we should be striving for thanks for that very interesting summary Sasha and then anyone else um thank you Mel for bringing up the jumping spiders um they have incredible eyes you should all look it up um but I uh I'm interested in this um from a developmental perspective and what active inference can contribute to um how systems develop and communicate and specifically uh humans um how our Ino environment and the activity um of that process um shapes the development of the human um and kind of on a more broad scale um how this can play into teaching and how active inference can be applied in the classroom at different educational levels but in a way that um sets up the environment such that agents are uh encouraged to explore instead of being dragged through the material as uh students often are so that's that's my curiosity in this time theot cool yes lot of otsky fans slash colleagues through through time on this discussion any other comments but really great points there about like the evolution and the development of perception and then at the level that we can at least experience or enact agency can we choose to see a color as different can you look at that ambiguous pattern and choose to see it one way or another can we understand uh many of these perceptive questions as being related to maybe not simply explore exploit maybe having an element of multiscale agency I think there a lot of ways that that could play out in people's life while also recognizing that we're not talking about just metaphors here or narratives we're also talking about this specific framework with the paper that we're going to discuss now so let's talk about those details but then maybe you know we can dip back to the other systems but let's think about how this paper and also our special time with Alec will help us understand these broader questions at least where we want to go after this short discussion so the paper scaling active inference we read the goal last time um Alec maybe if you could just however you want to or however long just what what were you setting out to do or how did the collaboration come to be what were you asking before how did you get to this paper and this goal um so I guess historically the sort of historic uh lead up to the paper was just very interested in reinforcement learning machine learning and also very interested in active inference so there a those sort of uh a natural overlap but in terms of the motivation I think the two communities don't really speak to each other as much as they should and so much of the work that's happening in reinforcement learning um especially modelbased reinforcement learning has direct analogies to the work that's being developed in the active inference Community um so stuff like the of variational methods variational Auto encoders the use of Dynamics models the use of trajectory based planning uh or just planning in general and these algorithms have a lot of similarities between the two um the use of intrinsic objectives these Information Gain terms is widespread and both the use of belief based planning so all the things that we sort of touched on in the paper so in my mind when I started out the the aim was really just to construct an agent using the Machinery that would be common and um relatable to the machine learning crowd but that was consistent with the active influence very much um and I guess a secondary goal was also to put together an initial attempt to look at whether what needs to be put in to get some of these ideas to scale um uh with a particular focus on you know this wasn't a biologically plausible suggestion but looking at some of the objective functions the expected free energy at scale um I think is of interest rather than you know where it's commonly looked at in these smallet scet uh TM sty environment cool really helpful in a lot of terms that we've heard but to link together the ideas of the variational inference with the belief and the trajectory based planning those are all really cool things I just have a question about the at scale part so one aspect was that you introduced The Continuous State space as opposed to the discret but is there anything else that's meant by at scale or what what does it mean to scale in this context well I mean so I guess the scale just means is almost defined in relation to what is currently uh common in the active influence literature which is where your entire world is often designed by four or five stat um and there's transitions for so this paper um presents a framework that can be applied to you know observations of um you know 10,000 100,000 uh and so your state space or your observation space is much larger um so that's what I meant by by scale is is putting forward a framework that could be applied to high dimensional tasks and the ones that we currently consider and there's also a second thing which is not really captured in the word scale but also the complexity of the Dynamics um so some of these tasks actually have quite a small state space you know four or five but the complexity of their Dynamics requires um large models of a number of parameters to them um so there's kind of a scating aspect and a scating in in the space of complexity of cool very interesting stuff and at any point people can just raise their hand but I think we can keep on walking through these areas and last time we did walk through the different areas in the road map just how we introduce active inference and the current state and then walk through the specifics of the model show some proof of concept experiments relate to previous work and then discuss and conclude with some of these sentiments related to what Alec is saying and I also found it interesting there's the scaling dimensionality then there's the introduction of continuous State spaces not just discreet and then often as machine learning people might use the word scaling to mean it's about the number of observations the size of the data set um and that relates to like the compute scaling relationship as you add uh when you double the data set input is it the same amount of training time is it twice as long is it four times as long is it 4,000 times as long and um so really interesting because there's also the scaling in terms of the understanding in the world world and people talk about scaling in innovation in entrepreneurship and how do you scale a solution so there's a lot of parallel meanings and a fun and concise title because uh you know that the finding isn't going to be in the title there isn't just a simple way that says oh this protein does this in this species we're talking about developments in a modeling framework but we also want to be specific about what those advances are and how they relate to systems so maybe just on the road map Alec or anyone else just anything to say about it or why was it arranged this way personally I was just kind of wondering why the previous work is at the end is that a difference in our um academic conventions or is that just because it was more results oriented yeah I mean you don't get a lot of leeway in machine learning sty papers um pre I mean previous work can I go after the introduction or uh at the end but it's always you know introduction some methods of presenting a model normally quite short test when the experiments previous section very short discussion that's kind of standed across the field but um the reason I think I chose prev the end was just because I discussed active inference which naturally leads in from the introduction and that introduces a little bit of mass which then naturally flows into the mod like a previous work might have broken up that flow a little bit but more L cool um makes sense um I think we can walk through the next sections so right now we're in the just open pause for questions I have a few written down just as far as just big questions because we're trying to be a bridge in this discussion and through our conversation between those who might be hearing about active inference for the first time but they're coming from machine learning thinking about how cool methods perhaps with philosophical implications might be utilized and there's also a lot of people who are in the active inference side of things or the in activism side or the philosophy side where the details of the machine learning might be their first time hearing about it so how will we just come from where we are and think about what questions and what kinds of fun things to talk about will make that bridge Stephen first and then anyone who wants to yeah I'm I'm just curious because I know that the scaling question has been maybe looked at more from a philosophical perspective in the kind of active inference so what this paper really helped me do or helped me was it like broke that fear of questioning the kind of these models because the like people like yourself and um annel and that are saying look there's a challenge here in the scaling like this stuff is being kind of used at these low-dimensional spaces and basically it showed like a gap in the literature in terms of how to action that which kind of helped me because it sort of gave me a bit of permission to say okay it's okay there's a gap there and it's not that I just don't understand everything it's just not easy to scale and I think this paper was the first one to explicitly say that because all the other papers tended to talk about yes we know how to scale but in a philosophical way but this is like okay well this is the problem from a a practice perspective cool just one note on that if anyone else want raise their hand I really love this idea where there's the Gap in the literature and there's there's a billion gaps on the literature um it's about which ones are Salient and fundable and relevant so there's many gaps in literature and then when we find that gap which is often very Niche like in a PhD and I thought wow this data set has been collected from this ant species but not this other ant species there's a gap in literature we don't have this data set on this ant species and then seeing that as a opportunity and something that's okay even if it's not your additional field of study it's okay because you're at the edge like at the Gap with us talking to the author and talking through the equations but this is what it looks like to look across the Gap and try to connect because somebody else has identified it as a research question or maybe you have Alec yeah I just wanted to comment on that as well and entirely agree with what you're saying um to come to maybe the defense of uh the existing literature on Act I think it's viewed a little bit or this maybe is just me interpreting what they're saying but um that maybe that the problems that they're um testing their agents in for instance something like the teamas are actually viewed uh as quite complex in a different dimensions so a lot of these reinforcement learning tasks stuff like Atari uh most of Atari most of you know stuff where you're already pushing to get state-of-the-art results with huge amount of compute and Deep Mind competing against there they're often you can often get by with just this model free reinforcement learning that doesn't require any notion of beliefs or any notion of proper epistemics you can get away with under exposion so what I think uh the active influence Community has tried to do is look at simple tasks such as the teamas that actually well you can solve them with beliefs but are far far more amable to being solved with a belief based scheme and with this directed uh expiration and uncertainty reduction um to show the kind of complexity that the framework can come up but then you could also respond by saying that this is also thought about a lot in reinforcement learning and uh there is the question obviously of whether the kind of uh process varies they put forward will apply when you've got uh when you've really got the scale that reinforcement learning TS to do yeah let let me add on to that because it's a really great point so a lot of times what's perceived as Cutting Edge or modern or Advanced research in machine learning I mean go check go check the lay media and it's going to be about this many graphical processors or this size of data set or this accuracy or this level of skill using massive data and it's very much on the performance end and on the eing out a little bit more performance from increasingly large data sets with increasingly um large types of models um but but it's actually in some sense a local exploration it's locally exploring certain Frameworks and ways of doing machine learning which is just computer statistics and so what this is like with this paper is a return to Simplicity and it's a return to a slightly different way of conceptualizing some of the parameters and how they're related and the Big Data just train it bigger train it uh better is kind of like thinking that we can do X without a belief we can train on go just by watching go and we can train the L of physics just by watching physics we can learn language by just watching regularities in human language these approaches um there's Merit to them this isn't just about one way being better it's just that what is being done in this paper is to take the free energy principle and AC of inference which previously had made these kinds of philosophically at the very least tantalizing claims like the relevance of a belief guided trajectory based optimization and search taking these very fascinating ideas a little bit out of the sandbox into the next level of the playground where now we can actually start to compete or at least compare and contrast directly with the kinds of benchmarking algorithms that are used so maybe one day you know active inference go or active inference chess but today we have the simple control theory parameters which is one level closer to these kinds of use cases that are happening today one level closer than the te-as or the the three-state uh decision is it a mouse or a hawk or a cat that type of stuff cool just the questions that I put up just thinking about how can we continue to deepen our understanding which we'll move on from but just like what's something you wondered about just always stick with that and anyone can raise your hand well what's something you learned about while studying the paper whether it was something that they wrote specifically or whether it was something that you kind of went down a rabbit hole and started studying about definitely that happened for me and then lastly like what's something your motivating to do more or um learn about now that you've read this paper and have this discussion yeah Mel thanks and then anyone else yeah I I guess I just wanted to jump on that and say that um I'm I'm used to seeing people sort of try to compare um reinforcement learning with with act entrance or or um related approaches and it's nice to see something more that's more of a a synthesis there cool yes so I think that if there's any other thoughts feel free but we'll look again over the experiments and then we'll sort of work kind of in and out about broader topics but let's try to understand what was really done because this also is the point of contact with people who might be very familiar with machine learning and optimization control theory but for them it might be the first time hearing active inference so where's the common train stop where we're on board with the machine Learning Community they're interested in these kinds of tasks they frame it as explore exploit and as we've been talking about maybe there's like a bit of a re-imagination or a reconceptualization of this relationship explore un exploit or what of these variables that's what we want to get to at the end so let's for sure remember that but for now let's think about being in common with benchmarking different data sets and different machine learning algorithms so so maybe we could talk about explore and exploit how these communities think about these and maybe your observations about how they're different Alec in terms of first the example that you chose to highlight as explore so maybe tell us about the mountain car and then when you tell me I'll go to the figure one but like what is this what does it have to do with exploring what does the machine Learning Community think about it how does active inference apply like what is happening here yeah so Mountain car's quite an interesting one um it seems so simple it's two when you do it in a fully observed environment there's two states one action uh this is the continuous version so it's continuous actions so you know it's like the simplest task that machine learning people present in papers but it's actually one of the hardest so um you know dqn vanilla dqn doesn't stand a chance of solving this what is dqn sorry uh d q networks have to remember uh so that's taking like the old um the old idea of Q learning which is kind of was the biggest idea of reinforcement learning before this whole deep phase and just uh amending with deep networks it's the one that um be Atari uh yeah the first like Deep Mind nature I can't remember the details but it was It was kind of the the birth of the deep reinforcement moment great so that one has challenges on here and then just continue I mean basically the reason is is because you only get a reward when you reach that um flag up at the top and before that your um you know that takes about 180 steps or 180 actions that you have to um string together before you get any notion of um reward so just pure reward-based schemes uh really struggle with this they essentially have to do more or less random actions until you get um out of the Hill um so yeah it's it's a struggle and that's why it's kind of exemplifies exploration because you need to care about learning about the Dynamics um or some other intrinsic quantity if you want a chance at finding out how to solve this PA cool very interesting blue this is just really reminding me of a lot of what we've talked about with explore exploit a lot of the perspectives you've brought about how exploring how and when and where does it matter so pretty interesting to hear Alec just how you phrase that and and also just like to hear that it's 180 sequences of actions so this is not like control theory like Connect Four this is like walking you know who knows how many degrees of freedom there are but this is many tasks that have to get strung together in Long sequences and then you said something about how um it has to learn something intrinsic for example the relationship between its velocity position and policy a little bit more nuanced Nexus of action rather than just learning like simply whether you know go means fast and stop means slow Stephen and then anyone else yeah can I just ask the question about that when you say stringing together um actions because I I kind of had this feeling that it was kind of a case of like you keep it the by by there being a desire to explore in itself through free energy that the car realized that it could go up the other side of the Hill further because it could just have a reason for going there because of Trying to minimize the desire to free energy around exploration and in doing so it gets that extra kind of momentum to get up the other side of the hill but I haven't really g into a detail so I was just wondering how this kind of sequence of steps and maybe just finding yourself far up far enough up the other Hill to get enough potential energy just to go down and up and make it to the flag I wonder how those two sort of relate great question Al maybe if you have a thought on that yeah yeah so I think your intuition is mostly correct so how it plays out is that this agent is evaluating different sequences of actions some of those actions are going to keep it where it's already been which is at the bottom of this hill uh and then some of the actions are going to take it into a place where it's uncertain about the outcome um and that UNC and that uncertainty is essentially valued by the agent it wants to resolve it so it's going to say okay what happens if I do a little because it has to go left up the hill I should have mentioned that before it has to go left up the hill actually actually go away from the flag to gain momentum to around so it's going to you know it's never gone left up the hill and then accelerated um doesn't know what's going to happen when it does that that's valuable to it in terms of epistemics and then it realizes oh wow up the hill yeah let me let me link that um to the question the sequence of of so imagine if you were going to do a control theory optimization on shooting a a bow and arrow so shooting a bow um is a complex motor movement uses probably many joints there's a lot of you could do different speeds different ratios it's going to be dependent on the bow just like this is going to be dependent on the slope and the car so it's it's an enacted affordance that you're trying to develop an action sequence for and your action sequence in uh game is often just one stepper Drop The Connect Four token here I'll re-evaluate after I see what they do but if you're going to be evaluating prospectively an action sequence in the world it often has depth so it's like getting out of my chair shooting the bow there State spaces are very there's multi- um joint so it's multi uh dimensional and it's continuous so that's why the multi-dimensional and the continuous are so important and then the depth through time is so important because you can't just be doing a get out of my chair or a uh shoot the bow and arrow short-term One Step optimization there's no opponent for you to then take a look at their move like in a chess Checkers go Paradigm or even a video game Paradigm to some extent this is something about planning in an enormous State space and thinking really constructively about these intrinsic relationships so in the bow example you might be attuning to the tension between the propri reception and your shoulder and how tense the bow is to know if you're at the end of your range of mo movement or the bow is and if you trained on one that was half size and then you go to a larger one or if your arm is hurting that day or all these differences differences in our abilities these things all become enacted in the relationships that we're learning about the intrinsic variables so in the depth through time in intrinsic variables if you learn that hey if I actually reverse and start accelerating downhill a 100 times steps later it's just better like if I go to sleep at this time 100 times steps later it's just better and so these allow for very nonlinear policies to be selected because there can be temporal depth that's learned as a function of the actual physics basically of the setting not just like counterfactual you know if 13 moves down the row this person does this with my Rook that still is in a very if then context and this is taking it into a totally different domain with a proactive action selection policies yep so that's why this is such an interesting paper and approach and like again a branching off point that's why it's foundational in the machine learning because just like Alec described like if deep Q learning cannot accomplish this task then yeah it's all great to be a human at chess or go but if that algorithm or even that architecture can't defeat this challenge then we're developing something that's incredibly specific which is great we should have good map routing algorithms and things like that but we're definitely going hyperlocal down a rabbit hole if it can't solve this but it can solve go I don't know if it is that case um a machine learning person would be really welcome to help fill us in on some of these details but that's the kind of stuff that is um interesting and and broached in this topic so just to see what that looks like empirically like the first 100 epochs um are plotted in terms of the state space coverage which here is just the position on the x- axis and then the velocity on the Y and the flag is at 0. five right Alec like the Flag represents being at position of point five but yeah continue from here just anything you want to add on figure one or what what does this mean what is happening here I guess no your description great the flag's not at0 five the flag's uh what is it oh yeah 0 five yeah uh but I just kind of want to um self deprecate myself and say that these results kind of they're not I mean they're not bad the result but um we had a follow-up paper where we got this all working properly um called reinforcement for inference um which is very similar but we use a slightly I can discuss the differences I think they're interesting if we want but uh just that the the results are so much more impressive we can still uh it's kind of both uh hitting on this and also um here's the to advertise that that work so in that work oh yeah if you can actually just show the figure and that we can sh so there it can solve the mountain car in a single trial and I have also the sort of State space spots that look a bit better so here's the so just one yeah so if you kind of go yeah I wish had the video it can just the first time it goes into the ring it just solves it straight away which is much nicer so the other the other one in the paper that we're looking at today um so the here it's being solved every time that it does it and it just con gets out and we try some harder tasks in this like the half G2 and ant maze coverage gotta love it um yeah I guess that was just a I wanted to highlight that um it can do a lot better than this um but I'll be happy to discuss the differences because it's just one part of the architecture that sure yeah what takes us from here to what we just sort of peaked into in this paper like what was the one thing that you added or changed yeah so when I wrote the original one to um so to get this kind of what what Carl calls parameter uh exploration or parameter Information Gain it's essentially you're trying to reduce on certainty not about what's out there in the world but about your own model you have beliefs about um your model uh and it's kind of in a sense is you know what you don't know and to get that you have to have a distribution over your model and in this case our model is a neural network to need a distribution over your neural network there's two ways is in the literature to do this in the first paper scaling active inference we tried B in your networks uh there may be more principled you know you're actually using variation inference to estimate this um distribution of each of your parameters but uh in practice they don't work nearly as well as what we tried in the next paper uh which is called The Ensemble approach uh and the idea there is you just take in this case 2 Dynamics models and train them on different batches of the data and that's kind of like a proxy for a nonparametric Bas in posterior over the um Dynamics model um you can do a little people are kind of working out how close it is to actually proper Bas and beliefs but um you definitely get a notion of uncertainty there so here um and there's lots of interesting reasons as to why principles reasons as to why Ensemble models would work better than um bin models it's also easier in practice to estimate stuff like um information game um so that just turned out to be 100 times better and in the machine Learning Community people generally find more success more success with uh deep ensembles for both um uncertainty calibration and directed explation that's very cool thanks for the great explanation there and so just to sort of rehash that or say it a little differently the approach that was taken in this scaling active inference paper to estimating some of these essential parameters was done with the basian neural network which Alec just described it's like one way to do it it might be a way that interfaces pretty cleanly with a lot of other software packages or approaches but it's the skeleton of the model and that's the diagram and then we think about how can we improve it and so One Direction is more analytical or more principled as you described it and that's like the fully basian sort of specifying every little uh hidden State and that approach can provide some interesting Avenues at times but an approach that is also relatively easy to implement from a programmatic interface level and also makes a relative minimum of assumptions about the specific mechanics of the system is this Ensemble or deep Ensemble approach so the Deep adjective is just meaning you're going to have like a multiscale or I mean a multi- level neural network with hidden layers or it's just going to be deep and modern but it's kind of like just an adjective The Ensemble approach is describing having a bunch of different models that are going to be trained up on the same or different parts of the data set and then you could look at the average of The Ensemble or you could look at some other weighted combination of the ensemble's performance and so it's almost like Bridging the Gap from Individual testing that's like the single model testing to Now The Ensemble approach is like we're going to have a classroom and then the best answer from R9 group all working on it independently the best one will push forward or will average and then even another level beyond that is like the truly emergence which is actually what the ants are doing where the Ensemble works as an ensemble in a way that is itself shaped by development and learning and evolution so right now The Ensemble modeling is still like sort of well if you just split it up into many parts you can cover more State space you might be able to train the model in parallel you might make sure that no single model over gener izes there's so many benefits that come simply from batching and Ensemble modeling that go beyond just saying well what's the best single model or what's the best um parameter range in this type of model The Ensemble can consist of a single type with a different parameters between the different Ensemble mates or they can be heterogeneous in some aspect so just really cool you went with what was implementable and a common Point of Departure with machine learning community and the basian neural network kind of peaks in one direction towards more analytical more principled fully basian approach and then on the other side to Modern approaches in machine learning like deep Ensemble learning cool let's look at figure two and what is conveyed here so this is the hopper task oh we'll go Blue first go ahead sorry I didn't see that can you back up to the last figure let's do blue and then me and then anyone else so um I just wanted to ask and this is kind of related to the next figure but um I just wanted to ask that uh when you did the hopper task and the um pendulum so I know that that was only based on the extrinsic value like part of the free energy equation so like you left off the exploration because like exploration probably had no value in in that problem I mean I'm assuming there but I I was wondering here like on the exploration problem sorry did I say exploration before exploit anyway so here with the exploration problem um did you ever think about leaving off uh the extrinsic part of the equation and just using like the information gain as a reward like can you structure it that way would it be different like if the object of the agent was just to gain more information about the environment do you think that it would have succeeded in um you know climbing the hill because it would get to a new part of the of the environment or how do you think that would work yeah yeah uh not in this p but it does um so that other paper I mentioned the reinforcement through active influence because it solves it in the very first task it literally can't be because of reward we've tested it without reward as well to confirm this but you know it hasn't ever experienced a reward and it's still solves the task so it's exactly what you're specifying it's just purely trying to reach all the parts of the uh the state space uh if you just left it with expiration what you tend to find is that it will solve it for say the first 10 trials while it's still the peaks of the Hill are the most kind of interesting because they have the most extreme Dynamics with the most variant um but then after a while uh the top of the hill is no more interesting than the bottom of the hill because it's kind of explored a bit whereas if you've got the reward and the expiration kind of slowly transfers from solving it because of expiration to solving it because it knows how to get reward awesome awesome thank you I didn't see that followup paper I like I'm going to go read it right after this I know we need to do it a a Afterparty active Afterparty with next paper um but uh Mel and then anyone else yeah just um what you were describing with uh like stringing a bow something like this um I think that was what really got me excited about active inference in the free energy principle in the first place is is the idea that um with something like a like a a towers of Hanoi style problem are people familiar with that uhhuh like you've got you've got three sticks and and you've got discs of various sizes on the sticks and you've got to get them in in a in a like a ascending order right um and the idea is that uh the puzzle requires that you you go backwards before you can go forwards and if you're just minimizing or maximizing some function on a single level um if you've got a like a a ond optimization you're you're going to not be able to solve a problem that requires that you backtrack in order to make progress um and something like active inference the free energy principle where we've got temporal depth we've got hierarchical depth of the model um is really equipped to solve these kind problems in a way that a lot of traditional um problem solving approaches are not I I totally agree and I think there's a quantitative and a qualitative so at the quantitative level there's control policies that we can't have computers look Beyond some locally non-favorable states to get around so we want to do these quantitative policies that's what this paper is about but at the qualitative and really the philosophical level how do we come to grasp with processes where yeah we're not always strictly walking a staircase directly to the top of the mountain or it might not seem like we can do it at all initially if we just look and there's so much there with how we think about challenges and about exploring one actual uh link there cuz plue asked about what would happen if you just let it go wild on exploring and Alec what you said is that the learning gets you to the top pretty quickly because in that sense it's similar to this model that also prioritizes reward but then after getting to the top you spend a lot of your time learning on the most extremely variant areas of parameter space so it's so much like curiosity driven learning where a lot of times curiosity driven learning with no reward or scaffolding it ends up learning learning learning a ton shocking amounts but then also spending a lot of time in the most extreme ranges of space and that doesn't always just play out like in the kind of online gutter but even in the literature we see a lot of the attention being spent on the extreme hyperboles and then the middle ground where it's like yeah it's kind of balanced and we can work on it together that is not as extreme of a Viewpoint and so people spend less learning and attentional regimes on these kinds of projects so there's a lot of like parallels that we can quantitatively model but also help us think about how we can say no you're you're just trying to make me explore the top of the hill I get it but the flag's on the other side so I've heard about the other side of the Hill how are we going to enact a policy to get us to the flag together that's a little bit better than your Hill is worse than my Hill for example how could these kinds of things be ported onto like human decision-making is a cool area any other questions on one otherwise let's talk about two a little bit well just just one quick piece just picking up on what blur said there I thought was quite interesting is if there's this idea of exploring the environment in different ways but say there's exploring or Information Gain in terms of personal preference so for instance maybe it was a bit more of a con I like to always feel what it's like to go slightly up a slope and turn to the right that feels cool right so then you have this kind of like it's not necessarily in it's like an Information Gain about having fun so would it like say the car is like hey I really like to do this type of things because cars like me like doing that I know you know if it was a living animal so it kind of ties into what sort of exploration of the world and then What exploration of just being an entity that likes to do certain things and that could end up taking you there in another in another way or together they help great Alec yeah I just wanted to comment on that because this is uh I totally agree and I think it's one of the most uh promising kind of directions or perspectives that active inference gives and I wrote about this in a in another paper that I can link afterwards uh of this notion of I don't really know what to call it but but goal directed expiration that you derive from something like expected free energy and that means you don't necessarily or it's just too inefficient just to have everlasting expiration with no constraints you know um uh and it's also too inefficient just to do have exploitation what you need is some objective function like expected for energy or model evidence that contains these two things so that you're not selecting actions to just explore or just to exploit that each action is kind of shaded with both of these things and that way you're getting a a expiration that's um geared towards your goals and it greatly constrains the type of exploration you'll be do and I think from nine you know that also fits with our kind of experience maybe of of daily life or maybe not but I think from an engineering perspective that that's crucial because in real world task there's just too much to explore and you need to prioritize that exploration I think that's something yep that's so beautiful and even another layer is The Ensemble of mountain cars so now imagine we all see different things and we don't know the way to get to the top of the mountain we don't know the policy we don't even know what the end point is we don't know if the one that we can see close by is the best one or if there's a way better one way further away and then everybody is who they are and they all have their own landscapes and then through the ensemble's modeling collectively with or without information sharing of whatever kind The Ensemble as we're seeing gets better performance there isn't just one best policy of mountain car there are so many different ways to ascend the mountain and then you open it up with what the objective functions and the policies the goals could be and it really is a a great space Shannon and then anyone else hi thanks I was just um thinking about your Ensemble of of cars here and comparing this to like flocking behaviors in birds or even um people who are foraging together and maybe I was wondering how so maybe this entire flock is minimizing its free energy by getting closer to award which is food but any individual bird in the flock is just following local rules about like how close to fly to other agents or when to follow them or when to break off and find a new like go away from the rest of the flock or drag the flock along with them and I wonder if every single bird is having their own little mountain car model or if there's just one gross model for the entire flock as a mountain car finding finding its food yep well one angle on the foraging and affordances is Imagine That ensemble of human foragers but they're not exactly the same size or they don't have exactly the same preference or they have slightly different Visions some people see closer or further they see color or they don't see color and so all these differences are what allow The Ensemble to explore and exploit especially with information sharing oh hey I was over under this tree and I found this but somebody else wouldn't have found it so we can all use the Ensemble to take advantage of our um you know differences of the nestmates or the flock or cognitive diversity and then in the question that you raised about is there a single uh layer being enacted by the flock or is there a little mountain car as you said by each bird so from a modeler's perspective if it turns out that we can explain variance about the bird's trajectory by putting a linear aggression on it it doesn't say it's linear aggression that the bird is doing just that it helped us explain variance in the world so similarly the low bar here is that we can use this kind of model just like we could use another type of control theory model or action policy selection model model to explain variants about the real world and especially because we see organisms succeeding in fact it's really the only ones that we do see this helps us explain those successful systems as opposed to like funny little gifts of you know robot flailing and then whether there's like one layer that's being enacted and then the group is purely epiphenomenal and there's no downward causation there's no influence of the group states on the lower level states there might be some systems like that there might be other systems where there's an analytical solution like a well-defined solution at the bird and the flock level there might be another one where it's well defined at one level but then because of interactions and emergence it isn't as defined or isn't defined by the same class of model at a higher or more coar grained level any thoughts yeah that's all great thanks y cool let's look at figure two so similarly as you did for that um Mountain car which was like such a helpful direction to take what can you tell us about the hopper V2 um or the inverted pendulum which I don't have here but those were the two TXS maybe more on the hopper um or the pendulum whichever one just what are they about what Will machine learning people recognize or know these models as signify um so Hopper isn't um it's not expiration based it's quite a dense reward that you get so you kind of get rewarded or dis rewarded for each of your action but uh it's slightly more High dimensional can't it says two dimensional there but that's just the that's not when I speak say Dimensions I mean how many observations it receives um it's more than two it might be like 16 I'm not sure uh and it's just generally the Dynamics so you've got to learn you know you've got maybe four actions which are real value numbers between minus 4 and four you've got to learn how that changes 16 variables over the course of your however long you're planning so it's it's a much harder task to learn um so I guess that's why it was um included in in this just that it's generally valid is a bit of a hard task in the machine learning commun cool and so what is happening in figure two what can we say how is it different than ddpg and what actually is ddpg for reference ddpg is uh the so acronyms and deep deterministic policy gradients um so what is that one doing and then what is active inference doing that different that maybe enables it to have such better performance um so what is uh ddpg doing so it's just a it's a model 3 reinforcement learning algorithm so you have a policy that maps from states to actions and you essentially do a load of maths from something like Bowman equation to get to um an update for your um policy parameters um the reason that it's doing so much better the reason that modelbased reinforcement learning in general does so much better um is because it's essentially it's a planning algorithm so um what's the best way to describe this yeah um due to the fact it's model based it can learn from every single um bit of data that it receives so that is in terms of each state transition and as well as the reward signals um whereas this model 3 reinforcement learning is is just simply learning from that single bit of information they gets at each time step which is uh um the corresponding reward or 32 bits or what however your rewards work ex thinked um so I mean eventually ddpg after enough EPO would probably asent it around or higher than um model based that's a kind of General uh pattern you get with model base and model 3 reinforcement learning model base is far more sample efficient uh but model 3 takes a hell of a lot more samples but is um ASM tootes gets you know levels out at a higher reward let me also add a layer there um with a few other aspects from machine learning and the idea of the ruggedness of the landscape so if you're doing this task which was described as a dense task another way of thinking about these is you're trying to keep something upright so it's like very obvious if you're succeeding or failing and in the big landscape there's really easy to tell differences between succeeding and failing that being said policy planning especially when you have four variables of control that are projecting out to like 16 potentially nonlinear connected outcomes when you're trying to do that there may be many strategic mappings there might be many policies that help you keep the pendulum up like for example going a little bit Back in Forth with a certain speed or you can go a little further with a different Rhythm so there might be a many many different ways even for a single joint of control many policy sequences Through Time many learned relationships that help you stay in that yes no area now if you have a model-free reinforcement learner it means it's learning basically the raw connection it's model-free socalled between the reward and the policy and so it may spend a lot of its time exploring locally a policy because it's like in the spotlight it's working and then it goes to a slightly different area it's not working where do we go from outside of the spotlight it's all dark no idea model free with the Deep generative model it's not like this Spotlight in or out reinforcement it's like we're learning these intrinsic relationships which actually helps us get a grasp of the maps that are going to guide us to search on the territory a little bit more exhaustively but then that part at the end which was so interesting that the model fre sometimes gets to higher final performance in some context that can be because a truly model-free search can result in some wacky combination that wouldn't have NE necessarily been approached that it just uniquely potentially not in a resilient way but it uniquely allows some performance on a task and so that reminds me of like the evolutionary computation where it its goal will be to travel distance and then some will actually go down the walking Road and those start slow but they could walk forever and then other ones just like fall and so it's kind of like hacking to get over um a barrier without necessarily uh a deeper understanding of the ecology because it's so blindly pursuing just performance so I know there's a lot of stuff there but this is like really an illustrative example and it does highlight a lot of the differences between the model free performance and the state-of-the-art in that as well as what potentially active inference could bring any thoughts on to now we'll just have some general areas basically the first area was implications I know we've probably talked about some of them but three areas that I thought about were like robotics resources and allocation and then rugged Landscapes this one probably in the southwest um so any thoughts on these I think we've almost touched on few related ideas but if anyone wants to like speak to one of these areas of possible implication whether they do they don't see what an implication could be or what would another domain of implication be um yeah Alec go ahead so um I also am becoming interested and so are some other people in um whether some so I'm very invested in this idea of kind of a Bas and brain and um as active inference kind of as FES but whether some of these ideas from Bas and machine learning that are used in this paper so stuff like amortization um might also be employed in nervous systems um so you know we've had a few very very uh big proposals for how the brain might Implement Bing infl stuff like population coding predictor coding uh the processor that's most commonly associated with fact of inference in this terms of message passing but it might also be uh you know this amortization is also um another real possibility of how the brain implements some type of um inflence so I think it could have implications for understanding neur science wow did not expect that message passing and predictive processing predictive coding type models can be understood alongside amortization model models as an alternate implementation or mechanism of basian brain as a specific testable hypothesis so this is where the rubber hits the road with the modeling and with showing the Deep mathematical isomorphisms between these different kinds of relationships like there might be some paper I'm sure there is where like message passing is equivalent to basian networks and then that allows us to bridge two big areas of literature and so if we do a lot of Investigation in in brain and then we find out wait there's actually multiple ways that that could be implemented in different systems whether that's through message passing so maybe that's more applicable to a computer network maybe it's also going to be implemented by policy planning ensembles so then whoa what else is doing this kind of basian like processing so there's so many cool directions there great topic because also robotics or at least the question of implementation of action and selection of policy and from sensors with actuators resources in terms of informational attentional whatever they may be and then rugged Landscapes is just that's everything that is policy selection under uncertainty for any kind of system that wants to stay alive let's go oh sorry Sasha or blue oh see you later blue oh wait Sasha out blue please thank you wait muted but yeah go ahead uh so I just I had the same reaction that you did like basing brain amortization like wow I just was wondering uh Alec if you could maybe just unpack that like in a very simple way or elaborate more on on that yeah uh sure so um the best way so I mean amortization um as it's realized in stuff like variational cers but also other schemes um we can like maybe discuss a bit more to get what defines amortization are just an instance a way of learning a g model and well more precisely doing inference uh the key defining features for me although I don't think this is ever been properly defined in the literature is this notion of having an encoder um how that would play out in the brain um how it could play out in the brain is just kind of a um a feed forward mapping um feed forward part of say the visual cortex that maps from some data or a lower part in the hierarchy to some posterior parameters higher up uh in the hierarchy and when you have stuff like um population coding or predictive coding there's not this notion of quickly in a fe forward manner mapping from data to parameters you've got this kind of iterative procedure that slowly um and um sequentially updates the beliefs those posterior beliefs based on the data based on some learning so it's another um route towards B INF it's far far more fast and doesn't require recurrent processing and that kind of speaks well with some things we know about the brain it's not all the brain the brain definitely does have Rec of processing but a lot can be done in in a kind of Fe forward manner very quickly uh and and the second thing is that um the parameters that you're optimizing of the encoder are optimized um over the entire data set whereas in some like predictive coding it's optimized individually the posterior parameters um of your beliefs optimized uh based on the current data point uh and I think that has some that might speak to some of the generalization capabilities of Bas inflence um and that's something that we're looking at at moment so more obious I hope that kind of answers your question R bit no definitely thank you so much totally epic to connect that feed forward encoder model to the machine learning um side so so instead of doing like a back and forth expectation maximization where you're um just updating back and forth or especially a uh like a back propagation type thing where there's very very complex interactions about how parameters are trained this allows us to kind of train on the Fly and just learn as we go now to the second point which is the usage of the entire data set versus Point by point this is like we're learning on the fly but um we're learning on the fly in the state space that we want to be learning about with respect to the entire data set that we have access to which is a little bit different than like you're kind of trailing your finger along a Time series and then you're updating based upon that data point and what you believe and what you've recently seen that's like a kind of Point bypo way to use a whole data set to update parameters versus this variational approach which actually enables like almost a simultaneous utilization of the entire State space I'm not sure if that's totally correct but those are just two parts that hit me about the two sides that you mentioned Alec Stephen and then anyone else so I've not heard of this amortization so it's quite interesting it's it's almost like an reverse then is it it's like you you'd have a a big complex space of knowing and you quickly discount stuff and the process is about disc scouting away rather than building up the model is that is that kind of the the idea uh I'm not sure if I've understand understood exactly what you're at but for my from I'm not sure if it if it is but maybe we could get into that a bit more how would you define Alec how would you define amortized inference like we see it here in the paper but how would you define it you mentioned some benefits we've talked about them a little bit but like in this kind of new way of thinking about it what what does it mean or what would it mean for the brain to do it or what happened programmatically I mean so this is what I was getting earlier it doesn't really have a a clear definition um I I try and Define in two ways one is how it's generally used if you see amortized inference in um machine learning uh you're generally you should think of a encoder Network so you've got a neural network it takes in your data and it will output literal values for your parameters that would normally and those parameters are the ones that would normally be optimized in traditional variational methods here it's just um they're just spat out by the network um and then you have some other um learning method which which updates that en code for you and also your generative model but um in terms of what amortization actually means going back to the word I think derives from economics um I think it's this idea that you come to the second point that I made where uh you're sharing the parameters of that encoder across all of your data so you're amortizing the cost of inference um is how I think about it I could be I could be very wrong um so that just means you you kind of don't reset with um each new observation or you're not trying to optimize with respect to um each new observation and then kind of discarding the information you're just sharing that encoder um the the encoder which is the relationship between the data and the parameters across uh your entire lifetime so you're amortizing the cost of infant is how I think about it so a better quote so is it like is it like you so as opposed to just discounting data it's discounting ways of chunking up the data or ways of analyzing it if that makes sense like you could have lots of different models and they say okay I'm going to apply them in a more efficient way over time as you start infer it's almost infering what models should be deployed is that the kind of idea and you start to take away the models which are less useful in a way I guess you could another kind of people sometimes refer to it as learning to infer so in which might be what you're getting at um so in normal inference you're just doing inference but here you're learning how to do inference so the learning um yeah so maybe that was what you're getting up with your kind of mod selection tape cool thanks yeah that that that makes sense I don't know but I'm trying to find my way through there's a few Dimensions so it's really helpful to unpack it there's the learning how to learn there's that meta learning element but also this is really important that in the paper is the number of parameters remains constant with respect to the size of the data so if I'm trying to estimate the mean and the variance of a normal distribution I know that I want to squeeze down whatever data set I have into two numbers and so if I have three numbers coming in the pipeline I still want a mean and a variance if I want a billion numbers to come in the pipeline I still I want mean variance and so it turns out that by knowing and by specifying ahead of time that you know exactly the size of the outcome it allows you to uh scale a lot better because you can know that no matter what you're going to put in on the inside it's going to be able to be distilled down very rapidly whereas another method that results okay well we put a data set and then we find out how many principal components explain it best that might computationally scale in a way that is very um disadvantageous with respect to the size of the data whereas this method can um do that and then as a sort of correr there it's happening through a single forward pass of a network which can be AB initio sort of like denovo setting it up from the beginning and or updated in a learning fashion and so there's there's a few dimensions and Alec really look forward to you know seeing what you continue to do with the amortized inference seeing as that concept becomes a bit more formalized because this seems like really powerful and then if it also turns out that it's an implementation of basian Statistics or basian brain then it just Taps into the entire message passing compute graph um basian NET Framework which already has been the um most helpful for machine learning in a lot of other areas too very very cool so I guess in the next steps which is really I guess the last thing that we'll talk about for the last few minutes just thought of a few areas to go into for Next Step so Alec definitely you take the first pass but the areas could be like computationally what does that mean Hardware software from a math perspective what could be analytically shown or what relationships would be good to know um from an applications perspective what robots need this kind of software update and then just educationally how do we come to you know control our attention so that we understand these Concepts as well or what did you start working on after this paper I guess it was yeah no worries no worries I was I wasn't yeah but I guess you did this one or where are you at now steps are um I guess on this line of thinking that the um this line of work the stuff that I'm still thinking about is what was mentioned right at the beginning which is the balance of uh exploitation and exploration and whether the active influence perspective provides you um anything in practice um because you know we've got this nice objective functional expected for energy uh and we can optimize actions with respect to it and it does contain exporation exploitation and it motivates expiration and exploitation from first phasing principles but in practice you're going to be in any model you're going to be uh fine-tuning those um the the balance between those um you've got a bit more of leeway to do it because now you can um change the shape of your PRI beliefs for instance uh and change a few other parameters that are going to lead to that playoff rather than just having some scal of weight that defines them so some of the stuff that I've been looking at is yeah learning prior beliefs in order to facilitate exploration exploitation um because I think the the to summarize that yeah the balance between exploration exploitation is is probably one of the big unsolved questions in um control reinforcement learning Neuroscience Etc um and for making these machines work um as well as we'd like them to uh and active influence offers a new route to try to solve that I don't think it has been solved awesome that's really exciting there's just so many aspects to the control theory question when there's a lot of solutions that could work and maybe it's even unclear what it would look like to work especially as we think about some of these systems that go beyond um the merely just keep a pendulum standing but what if it's give someone a massage well that's a little bit of a different question it's relational or how are you going to exercise in a way that's comfortable for you these are control theory questions that are literally about action sequences and so when we start thinking about how we're going to apply it to different systems having a framework that is at least moving in this direction of what you said is like with the reconsidering the explore exploit by adding things like reconsidering the shape of your prior so how can you step into a framework where the explore exploit is a dimension that could be calculated or described or kind of summarized but isn't simply the underlying framework of the model we're going to get exploratory Behavior we're going to get so-called exploitative or narrowly searching behavior that is not being ruled out by what we're discussing we're talking about another way that optimization could proceed that doesn't use simply the explore versus exploit um or the coefficient waiting of these two through time or statically to um outline its whole learning approach so Stephen first and then anyone else if they want to give any like last thoughts but this has been a great conversation so even than anyone else I mean also if you go into the real world environment you've got this explore exploit situation but in high stake situations you've also got um like they often talk about risk mitigation versus gain optimization if you're in a kind of um a conflict or you know a legal situation because it may be that it's a question of you know which is in a way gain optimization is a bit like an explo exp it but it depends which way you're going it could be an information game but whatever you're you're trying to do those two two aspects sort of come into play depending on the state of the of the situation you know so you know explore exploit is less relevant when you're standing on the edge of a cliff playing a game you know because it's like there's a risk mitigation issue so anyway I thought that might be interesting I just thought about you know doing learning action policy but if you're learning an action policy where you could die if you're rock climbing or something it's going to change how you learn and so when you're trying to minimize risk versus failure attempt or minimize the number of opportunities you have to see it successfully performed or you can only infer it by observing it these things may contain a ton of information in terms of how real systems learn where failure is not an option Mel and then anyone else who wants to close it out yeah sorry if this is I guess a a sort of an ignorant question but I I thought some of like the beauty of these act inference and FP approaches is is that they're sort of doing a a kind of like multi-level Dynamic aoms razor type thing so they're they're maximizing predictability well well also kind of minimizing the complexity of of what they're doing and so I guess what I don't quite understand is how the amortization of inference process works but if you're fixing the params doesn't that kind of uh constrain the ability of of these approaches to to do that kind of aroms Razor type type uh um not really because the parameters of your encoder aren't the parameters that you're they're not the free energy parameters they're mapping to the free energy parameters so you could map you know to uh you know fre just says that you've got to maximize yeah the accuracy or the likelihood while minimizing um the complexity so as long as your encoder maps to the part of Lea stat that is maximally uh accurate and also minimally complex then you're still minimizing um complexity while maximizing accuracy uh and then obviously there's what we haven't discussed is the learning schemes for so the way you learn your encoded parameters is so that they out but something that does conform to minimal variation yes okay so you're not actually constraining you're not actually placing like additional constraints on on the parameter of the actual not on the parameters of the encoder on the output of the encoder yeah I guess that's a good point uh so the the encod the I guess a problem of armatization is that um the encoder is kind of not part you shouldn't think of it as part of your generative model it's just almost like a a tool that can um map to the belief space of your generative model um yeah just so just on a closing note there that really returns us this dual instrumentalism with rappers and rappers and is it what the system is doing or is it how we're looking at it these questions are very rich and so it's a great conversation really thanks everyone for participating this was such a helpful discussion and I think definitely while R listening we'll all pick out some questions for ourselves to follow up on and some curious things to learn about because there are so many good ideas brought up so if people who are on live if they check their calendar event they will see a feedback form which would be helpful and anyone else who's listening or watching please provide us with feedback suggestions or questions but other than that just um stay in touch and everyone awesome work for this helpful discussion and we'll see you soon