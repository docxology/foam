hello and welcome everyone it is November 13th 2024 we're live and kicking off the fourth applied active inference Symposium it will be a great program and we will begin with Carl friston and Lance dcosta discussing scale free active inference we discussed in live stream 58.1 with Lance last week and in 58.0 with Arun two weeks ago a bit about this work and today Carl thank you so much for joining and presenting on it so looking forward to it and throughout in the live chat please feel free to write any comments thanks again everyone for joining and looking forward to this well thank you Daniel it's a great pleasure to kick off the Symposium this year um I'm going to rehearse um a presentation I made at the um active inference Workshop um a few months ago but this time I hope in a slightly more relaxed uh and unpack way so I'm uh fondly hoping for lots of questions and uh embellishments from the uh from the audience and uh and from Lance um this is a bit of a colloquial presentation it's just going to focus on something specifically that we've been working on for the past few months which is effectively addressing the question how can one scale active inference for high-dimensional real world problems and the solution that we've been pursuing is to adopt a scale-free approach um this work has been written up um and put on archives um however it's also been submitted for uh publication peer reviewed publication to a special collection um in uh memory of Herman hen um and we've equipped the peer-reviewed version with a little forwards I just wanted to frame or preempt the scientific part of this presentation just by motivating why why the work of Herman hen is foundational and underwrites many of the um ideas and procedures that underly this scale-free approach to active inference so Herman hen uh was famous for synergetics um a physics Le approach to self-organization um and in particular the importance of understanding the coupling between different scales of Dynamics in self-organization and if you go to the Wikipedia entry you will read about macroscopic and microscopic variables and how they are coupled to each other in a in the fashion of circular causality um so there's both bottom up and top down causation where the top down causation is sometimes read in terms of um the slaving principle I'm going to present this approach um to harnessing this um notion of a scale-free universe uh in terms of installing that scale-free aspect in the generative models that we bring to the table to solve decision- making to uh simulate or um emulate active inference under discrete State space models so let me start at the beginning um the nature of things and the free energy principle um and this is just to rehearse the fundaments of the free energy principle it all rests upon the notion of a Markoff blanket where we can separate internal and external states by boundary States or constitute the markof blanket namely the sensory States and the active States and for those people not familiar with um this particular partition of states of any random dynamical system it's just specifying Sy chemically um a system say a brain in terms of its inputs and outputs and um just formalizing the conditional dependencies that Define what is an input to a brain or an agent and what is this output in terms of the sensory and active states that thereby render the system of interest in this instance a brain open to the environment the econ Nish um the heat bath if you're a physicist or a chemist um in virtue of the fact that the inside can influence the outside via the um active States the outputs and the outside can influence inside via the sensory States or the um or the inputs and with this setup we can elaborate um a kind of physics namely uh um basing mechanics that is almost well forly um related um and inherit exactly the same mathematical and functional form as all the other mechanics that we find in physics including quantum mechanics sta statistical mechanics and classical mechanics the the only thing that separates the um the basy mechanics is the fact that we have in mind explicitly this partition between internal States and external States and the uh blanket states that intervene and couple the internal to the external and that basic mechanics is is effectively summarized here in terms of um variational and expected free energy um where we can read the expected free energy as effectively prior beliefs about the kind of outputs or actions that um in this instance a brain could exert on the external States um and the variational free energy keeping it implicitly the um the internal States a good model so it's tracking in a probalistic sense the um the Dynamics on the outside and we can combine this to uh create a a generalized free energy um that now theologically or perhaps even anth anthropomorphically we can now read the gradient flows that subtend the phasing mechanics in terms of action and perception just by identify Ying the internal and active states of any agent as autonomous States and noting that they effectively um um flow on the same free energy functionals so that's the basic setup um but what I want to do before um applying the free eny principle in the service of things like active inference is just back up and reflect upon the fundamental role of the Markoff blanket and how that leads to a scal invariant picture or view of Dynamics and self-organization so what I'm going to assume is that ultimately we will assume that basing mechanics um means that the Dynamics of anything that is defined in terms of bit boundary or Mark of blank it can be described in terms of Act of inference but before that we have to think about the nature of things um and in particular um if one goes back to the particular physics paper um the nature of the states that constitute things or particles um and there's a bit of a subtlety here because before you can start talking about States you really have to Define what is a state is it the state of something is it the state of a particle um and that presents an issue because you're defining a particle in terms of this particular partition into internal external uh and Boundary or blanket States so there's a um um there's a slight toor not topology but there's a slight um problem in resolving how you define states of things or particles and particles that possess um States and the resolution of that is to adopt um a recursive definition so that the particles constitute a set of microscopic blanket and internal States while the states are a set of macroscopic igon functions igon modes or just simply mixtures of the blanket States so I'm here deliberately um using the notion of microscopic and microscopic States just in uh as a um a nod to synergetics and in particular kind of synergetics that um Herman hen developed and subsequently uh championed so this uh is cartooned or Illustrated um on the right hand side here so we sort of take some states um and um uh and then um apply a particular partition on the basis of which states are coupled to which other states and the condition independencies that ensue um and then having done that we can then take mixtures zon mixtures of the boundary states that then generate new States um and then we can um effectively by taking those mixtures we effectively reduce the dimensionality so that the higher scale um coarse grains the lower scale and then we reduce it and then then we just rinse wash and repeat and we do that and every time we sort of pass through this cycle of RG operators um reduction and cost graining operators we move to one scale above successively and this is the scale invariant aspect of um that I want to leverage in in the remainder um of this presentation um I use the word RG uh deliberately because um one way of looking at this or articulating this is to use the apparatus of the renormalization group so this is where renormalization gets into the game that you're recursively um applying these operators course graining things um in a way that conserves their Dynamics and that's an easy um um uh that's an easy criteria to meet for us because we know that anything that has a particular petition can be described in terms of a gradient flow on a variation or generalized free energy so we have everything that we require in order to apply um the apparatus of the renormalization group so that what that basically does for us once we recognize that particles are composed of states where the states are ion mixtures of particles in this scal invariant fashion gives us a view of any universe that comprises things of things of things um as um inherently uh resolving a chicken and egg problem through scal invariance by application of the renormalization group so here's just another illustration of how that actually works in practice when you write down the equations I'm not going to bother going through the equations this is just used iconically um just to make the observation that as you move upscale um so for example in an evolutionary context this would be the sort of the Dynamics of phenotypes and then we move up to the uh an evolutionary scale um by um effectively Co graining in a way thaten um would exactly prescribe in terms of taking those slow macroscopic variables that decay very very slowly are technically the igen mixtures equipped with very small um igen values whose real part approaches zero From Below effectively saying that these patterns Decay very very very slowly and they have a kind of persistence that lends higher and higher scales a temporal dilation so things change more and more slowly as you increasingly uh C grain things and that this falls out of the maths um when you simply apply a particular partition and put that partition in um as the um the course grading operator in this uh in these in the scale free uh formalism so in short basy mechanics entail a generative model of external Dynamics but we've just seen that just by defining things in terms of particles that have States and defining States in terms of the I functions of particles that external Dynamics are inevitably scal invariant it can be no other way and therefore the generative model must be scale invariant or when expressed as a graphical model scale free and I just slip that in as a um a nod to the fact that people often talk about scale freeness um generally speaking when people apply the notion of scale in variance to anything if that thing is a graph you can call it scale free so a scale free thing is a um is a graph that has has this scale inv variance crucially in both time and in space because we are dealing with dynamical systems and we're doing this um Dimension reduction or course graining um using these um migen functions that necessarily pick out these slow macroscopic sometimes called stable modes stable because they they don't Decay or dissipate um almost infinitely quickly so they're stable in the sense that they Decay very very slowly so how can we harness that observation that truism at least trism under the world as described by hen um in um the construction of plausable generative models and in particular um generalized off decision or partially observed Markoff decision processes um well what we can do is we can just look at the different kinds of depth um and ask at what point would we um be able to leverage um this renormalizing aspect of the cause effect architecture of Any Given world that we're trying to navigate and predict so here's uh the normal um depiction of a um a generalized um Mark of decision process and by generalize what I mean is that there's um an explicit representation of not just the states of things that are generating outputs or things that can be measured or observed on the um on the sensory sector of a marov blanket but also their Dynamics in terms of the paths that um specify the transitions between states as we know normally encoded in terms of a transition tensor be here um so just by putting in the paths as explicit random variables that accompany for each given Factor um the hidden States we have a generalized kind of uh Markoff decision process uh encoded in the usual way in this instance um in terms of tensors um that themselves can be parameterized with derish distributions so um that parameterization equips these kinds of models with a parametric depth in the sense that if something is deep parametrically that means that the parameters of various probability distributions are themselves random variables that have their own probability distributions so for example the likelihood mapping that maps from sensory states to outcomes here uh usually encoded by an a tensor here mapping from latent states to observations is itself encoded in terms of um dislay parameters um lending this uh kind of model A parametric depth and um one can ask what the implications of that are well the implications are another kind of depth in terms of um you now have to optimize or the B apply basic mechanics to the latent States the hidden States and that would be inference and then you also have to um consider at a slightly slow time scale the optimization of the derish parameters and that would correspond to learning and so on um if I wanted to go to the structur learning of the model in of it in and of itself we also have um implicitly a temporal depth in the sense that we're going to be raring out into the future and that there are dynamics here um and this temporal depth is generalized in in virtue of having um these explicit uh path variables that we're going to uh exploit later on we can also have hierarchical depth in the sense that we can put um compose many Markoff decision processes um um in the in the sense that the outputs of one process now constitute the inputs or the empirical prior the top- down constraints the inductive biases on the Dynamics and states of the level below and the Dynamics are um um the kind of inputs or sorry outputs of the uh upper level that provide empirical PES on the paths and the initial conditions are supplied or mediated by a mapping between higher levels slower States um and the uh initial conditions or initial states of the lower uh of the lower level by this D tensor here that you can Lum together in terms of sort of high you can lump together the D and the E tensors they play the role effectively of the a tenses in sort of um mapping from one level to the uh the next level there's also factorial depth which many people will be familiar with in terms of um for any one of the um lump States here there may be many many different factors that have an a certain Independence structure such that you have interactions between factors in determining outcomes um you can look at this in terms of uh a ger model when equipped with factorial depth effectively entangles latent states to generate um outputs or outcomes which themselves can be prior straints on the states and dynamics of of the level below such that the inversion of these models effectively disentangles the data the content that is being observed to explain it or um in terms of a disentangled representation which is the um the latent State and dynamics of the generative model so what we have here then is um a generalized discrete State space model with Paths of as random Vari um just to um return to this um fundamental aspect that as one as send scales in real life in um you in terms of uh the external States things slow down in virtue of the um the the the the synergetic um mapping from Fast microscopic States uh to um slow macroscopic States um the same thing can now be implemented in these um Markoff decision processes that have a hierarchical depth um and the way that is done is effectively to discretize or quantize time such that there there are more updates at any given level than any um than the level Above So for every um update at this level there are uh say three updates or two updates at the level below and that um affords a temporal scaling um in accord with the scaling variance um implied by the synergetic view of self-organization I've tried to cartoon that here in terms of if you imagine lots of these um little Markoff decision processes populating this uh circumference here and that as we go deeper and deeper and deeper into a model with hierarchical depth as we consider updates in Universal or clock time there are more updates at the peripheral level exposed say to the you know the um external states of entire brain um relative to the relative to the updates that as you get deeper and deeper and deeper in into the model so that's where the temporal depth um becomes if you like intimately tied to the hierarchical depth the higher the hierarchy the slower the updates which is why um we generally parameterize the initial conditions um or generate the initial conditions and the initial path from the states above and that path can pursue um um a trajectory over multiple time steps until a certain um Horizon uh and which case it then sends messages back to the level above likelihood messages um that then cause belief updating um and the subsequent Step at the level above and then generating priors about the subsequent path the next PATH at the level below and then so on recursively uh to the depth of the model um so that's the time bit what about the spatial bit what about the spatial core scening the lumping together of various States um and of course this depends upon the particular partition so in the General Physics formulation you'd be looking at uh Markoff blankets and markof blankets in Practical applications one can take a shortcut um and that shortcut effectively rests upon um the simplifying assumption that for most systems that are um composed of um coupled um coupled uh dynamical systems um the interactions are usually local so if we um just think about um some um um system that um can be described in terms of local interactions so for example um in a um a ukian world um that comprises some ma massive objects that bounce around and if we ignore gravity for the moment then they're only going to influence each other or depend upon each other when they're touching when they're um when they're proximate um effectively what that means is the the the mark the particular partition that defines all the Markoff blankets in um in the this particular State space um is composed of lots of local little tiles um so what we can do is we can use a um a device um very prevalent or indeed ubiquitous and uh applications of the renormalization group which is called a uh a block or a spin operator that just groups together local tiles or quadrants of a say two-dimensional arrangement of tile of U of States so I've Illustrated that here just by um in terms of um How We Do the spatial cor graining in these um what will refer to as a renormalizing generative uh model with spin block uh Transformations um in terms of um this little um um this hidden state or hidden factor with its states and paths here um is responsible for generating predictions about the Dynamics of these two um factors um at the um at the lower level um and in so doing what we are effectively doing is successively grouping together sets of sets of sets of sets so that any set or group at one level is accountable for or trying to generate the Dynamics of a small local group at the at the lower level um this is quite fundamental this kind of Architecture is quite fundamental because what that means is anything at the lower level only has one parent and if it only has one parent and if you remember the markof blanket is constituted by the the the um the parents and the children the parents of the children because there are no co-parents there are no parents of the children and what this means practically is that we can now ignore dependences between or effectively resolve or dissolve any dependences between the factors at any given level because each factor is only responsible for its children and it doesn't share any children in virtue of this um particular spin block transformation so this leads to um a very efficient kind of generative model um well certainly an efficient kind of inversion of the generative model um where we've got all these converging streams but that the streams don't have to um uh talk to each other until at the point that they come together through these blocking or spin block um um uh transformations so at some point all the tiles or the groups at the lowest level will actually come together in um uh at the top or very deep in the model at which point we're now encoding quite long trajectories technically two to the power of the depth minus one um in terms of updates um but at the lower level um there are no interactions between the different blocks as we Ascend them and this is just to um remind you we can treat these d and e the um the initial States and the initial paths effectively as likelihoods the couple um one scale or level to the scale below so how uh with if that's the kind of model that we aspire to because we think that's appropriate for any world that shows this scale uh inv variant aspect in space and in time or in state space and in time um how are we going to build one of these things um and this is the sort of second um if you like um thing that we have been working on um um because it's going to be very difficult to specify by hand these um scale-free or renormalizing generative models we really want the model to learn itself so basically how does one build a renormalizing generative model and what we're going to do we're going to use very fast structure learning and this particular kind of structure learning is necessarily recursive and uh in the sense that we're going to build these models through the recursive application of these um RG operators or renormalizing operators so what's fast structure learning so fast structure learning basically equips the model with unique St and transitions as they are encountered so to put that simply what we're going to do is grow a model from scratch from nothing and how are we going to do that well we're effectively going to take one observation and say okay that was generated by some latent State and then we're going to take the next observation um and say okay I haven't seen that particular observation before um so I'm going to induce a second latent State and for FM I've seen these two observations in sequence therefore I'm going to um encode that with a transition from the first to the second state and then I just repeat I take the the third one and if I haven't seen that before I induce a third latent State and a third a second transition um accumulating all the Unseen or unique instances or um um of observations in a likelihood M mapping that this grows and grows and grows until we have seen something before so we don't induce a new state or implicitly column of the likelihood Matrix if we've seen uh the previous state that's just a um um an instance of something that has been seen before um it may well now come along with a different uh path so the next state may not be the same as a the subsequent State following the previously seen instance of this state and therefore I'm going to induce A New Path and slowly grow my B tenses and my probability transition matrices until I can um basically encode all the dynamical structure of the effectively the training sequence of observations summarized in a a maximally efficient way and I mean maximally efficient in a technical sense um in the sense that the implicit mapping that we are growing from from scratch um has the highest Mutual information between the latent States and the observations that are at hand um the way that you can think of that in terms of um free energies that the expected free energy um is just the mutual information in the absence of any expected cost so the expected free energy is just the mutual information of any mapping um um um plus um or minus an expected Cost U where cost is defin the usual way in terms of constraints or PRI preferences so one can actually look at this uh fast structure learning um as a particular kind of active inference but not about um not active inference Over States or Active Learning over parameters it's active selection of the right model in terms of the size of the likelihood tensors um and the transition uh tenses um and the criteria for whether we accept a new latent state or a new transition is simply does it optimize or improve the expected free energy namely does it preserve or conserve or maximize the the mutual information so we can actually look at this structure learning as just another instance of active inference but here deployed in terms of um um in terms of basic model selection or so some people call this uh structure learning that has exactly the same rules as um planning um and um uh and um accepting um parameter updates provided they they maximize expected free energy as well um the application of this kind of uh uh fast structure learning to um these renormalizing models U looks complicated here's the algorithmic description of it um again please don't worry about the details here it it's relatively simple you know all we're doing basically is ingesting um observations but we're just doing it for local groups um and then we are combining the initial conditions and the paths into new outcomes for the next level um and so because we are doing this for local groups the number of groups just gets smaller and smaller and smaller as you as you get higher and higher and higher but obviously the number of combinations of paths and state initial States increases as you get deeper and deeper into the model and you may be asking well you know how you ban that you know how can you um possibly assimilate all the data without having an extremely large number of latent States very deep in the model that are now encoding effectively evolving patterns or paths over quite a large number of time points into the future well it's trivial to do that because the size of the highest level I'll call them generalized states that basically generate paths and initial conditions for the lower levels um is upper bounded by the number of elements in your training set um so that you can control and upper bound the size of these renormalizing operators or likelihood mappings and transition um um tensors simply by giving it canonical training data the kind of data that you want this um agent to generate or this um uh agent to recognize and thereby predict um so here's a a particular example of that um what we've done here is just ask can we very very quickly learn a gen a model that will generate a um a simple movie and this movie is simple because um it begins and ends in the same state so it just Loops over and over and over again um so we only have to supply one Loop um to this fast structure learning so that the model builds itself and can then be used in generative mode to generate the movie time and time and time again um so here's one depiction of um the results of the learning the the fast structure learning um and subsequent inference when exposed osed to um this kind of image so the coar graining here at the very bottom in in fact um uses um um a slightly finessed version of the spin block Operator by um harvesting um local igen modes or igen IG images um of little patches of of the video to get it into a reduced or coarse grained representation that then is just passed up using these spin block operators and in this instance we just need uh two levels to the model to uh have the highest level see the entire image and then if we present the image um um after discretization using the singular Val decomposition or the sequences of images to a model it learns the model and then we can generate um we can generate um little movies simply by moving through these paths or episodes I use the word episode um because remember these generalized States at the highest level encode paths or trajectories into over um um 2 to the N minus one time steps in the future when n is the um is is the uh the depth of of the model and these episodes themselves have Dynamics so we just go through a little Loop here go back to the beginning and thereby generate um this image at the bottom what else can we do with these kinds of models well one thing we can do is um present not the entire data but just u a little prompt um so in this example what we've done is actually show it just the upper right quadrant of the movie and the and see what this um this generative model um believes is going on in terms of the um the predictive postera in output space in image space in this instance now because during training it's only lared to recognize a dove flapping her wings um that's all it can believe come is the cause of its Sensations even if those Sensations are very partial so basically what we have here is a a kind of pattern completion that inherits from the fact that this generative model only knows about latent causes that constitute an entire Dove flapping her wings so um even though this is what the dove is actually um exposed to it's sensing this is the predicted after a couple of frames it very quickly um is able to to predict um what it would what it should be seeing or what it would have seen had it been given the entire uh data and another way of thinking about this um remarkable ability to sort fill in the gaps um is from the point of view of compression so just a little digression here um a lot of the basy mechanics and um variational inference that uh from variation mechanics can also be likened to um the to um the same fundaments underwrite um um efficient information transfer so we've talked about maximizing Mutual information in the context of uh um algorithmic complexity and um the associated Universal computation um then what the object objective function looks like is basically how much can I compress this content um to represent it in the most efficient way so you can look at this past structure learning under these renormalizing or scale-free generative models as the most efficient way of compressing whatever your data you've been exposed to um and because we've compressed it in such a way that it can only now recognize a dove FL flapping her wings um then it can if you like fill in the gaps in a very efficient way when it's given partial or noisy data and I use the word noisy here because you can regard the missing three quadrants here as extremely imprecise data data that has extremely low signal to noise for example so um here's another example in the previous example we we um took a um effectively um a movie or a dynamic or um a system that had a periodic orbit um um which was each flap of the Wings will this work for um a periodic orbits and and generally for um stochastic um uh chaos um yes it works it works relatively efficiently so this example um we um use training data that was generated um um through creating images of a white ball that moved with a chaotic trajectory governed by a uh a loren system so this is the kind of system that governs or is used to describe uh chaotic events um in meteorological systems or in fluid convection um so by applying exactly the same procedures as in pre in the previous example um we now uh generating this instance a three-level um um hierarchal or renormalizing model um that now accommodates the um stochastic or Randomness induced by not only random fluctuations on the Dynamics of this stochastic uh random dynamical system uh but also if there were any deterministic aspects the deterministic uh um um chaos that ensues from exponential Divergence of trajectories that now has been summarized stochastically in terms of probalistic um uh transition matrices here which means that um we can having compressed the training data we can now withhold not a quadrant but just simply withhold the data to stop the data uh but the model will keep on generating um what it predicts would have happened given the SA ICS of the Dynamics that have been installed in this renormalizing generative model and what we've done here is um um for the first um few hundred time bins at the lowest level we basically rendered the images or the input very very imprecise so that everything is governed now or everything is generated from this top down Prize or posterior U um um posterior um predicted posterior that inherit from from precise prior because there's no precise likelihood um and then we've actually uh removed the um um removed the um imprecise input completely and just allowed the um the agent to generate its own beliefs about what's about what's going on um so here this is fairly precise in the first instance and then we remove completely the um stimulus but it still uh imagines or generates its or predicts um what's going on in terms of this chaotic um Dynamics um and then um with a degree of uncertainty then um continues to then generate um the um the Dynamics associated with this kind of stochastic chaos here's a pretty example um using exactly the same uh technology and approach um but here explicitly um talking about um video compression um so we literally just presented a video of naturalistic Dynamics in this instance a little bird a robin um feeding um itself um and um compressed that using these uh this renormalizing um generative model um such that we can again simply remove stimuli at certain points um in the video um and yet the uh the in in the agent's mind nothing has really happened uh because all it knows is that if it was like this um previously then this must have happened um and so on through this in this instance um orbit that doesn't have a a closure it's just a trajectory through from the beginning to the end of the sequence and here's the posterior prediction after just being exposed to a couple frames right at the beginning um in the first um six uh time frames of um of about 12 sorry of um 18 um this example moves applies exactly the same um approach um as previously but now we're not talking about um ingesting or learning or compressing images we're talking now about compressing or gesting sound files um and that is if you like a simple or limiting case of this two-dimensional uh compression that we talked about before so in this instance now what we're doing is compressing one-dimensional frequency summies of a particular um wave file or a um file um that um describes fluctuations in the intensity of uh of a sound that then is um summarized in terms of a continuous wavelet transform um and then the succession of different trajectories in this um time frequency space is then ingested or assimilated or learned by the generative model um now unfortunately you can't hear this um so I'm not going to bother playing it but um it um this is what the agent was trained on it's basically J piano um and these are the um ensuing predictions in the absence of any inputs it's been used to generate music um using um the stochastic transitions um through various bars of Music um that it has learned um returning to one or other bars of the munic of the music when it reaches uh the end of the orbit to generate something that um sounds at least Jazzy not oh started it but you can't hear that so I'm not going to pursue that so to summarize what we've done so far um um we've gone beyond generative AI in the sense of um um having a a true generative model um uh that entails active inference in agency um and in the setting of these um well sorry what we are going to do now is go beyond just simply generating uh content um having been uh trained on the basis of some training data um and we're going to now consider um agency by putting action into the mix um and in the setting of um the simplified setting of optimal State action policies where we can effectively um ignore the imperative to maximize expected Information Gain or or um um explore in the sense of uh responding to epistemic affordances um we can um do one of two things we can either use some Universal function approximator say deep learning to optimize the mapping from sensory to active states to maximize the um expected cost or the expected utility part of the expected free energy um or we can use active inference to realize predictions under a normalizing generative model of rewarded events so what I'm doing here is deliberately setting up a dichotomy that tries to distinguish the active inference approach to purposeful Behavior where the purpose is provided purely by the prior preferences or the um um or the the uh the constraints usually encoded in the c um uh tensor um against a sort of reinforcement learning approach and this is a rather busy slide but um I think it's probably worth just um uh briefly uh rehearsing the distinctions between active inflence and reinforcement learning so the you know the story with respect to action as a gradient flow on free energy which basically reduces to um picking those actions that maximize the predictive accuracy um in the sense that if you look at the expression for the variation free energy the only thing that you can change um is the accuracy in the sense that free energy can be written as accuracy minus complexity minus accuracy and the complexity um is does not depend upon action whereas the accuracy does so that picture basically says um I can describe Behavior in terms of a generative model that um is optimized through um perception uh in the usual way um cast in terms of say variational inference um that basically generates predictions about what should happen next and then I can pick my actions to fulfill those predictions in continuous state for formulations this would be found minimizing the prediction error usually the proceptive domain but you can generalize this to any any consequence of action I'm just going to pick that action that brings about or is most likely to bring about what I predict will happen next and what I predict will happen next um is optimized through perception through variational inference based upon my learned and selected um generative model so this is very similar to sort of um model predictive control um and neurobiology could be C in terms of um um perceptual control theory um which puts action in the game basically of realizing evidencing um predicted States so I've tried to summarize that here just in terms of um you know our Markoff blanket where we're optimizing our beliefs about the world um noting that planning in this instance so um planning as inference uh is all about the paths it does not have to be about action it's just about how does this world unfold um and from the point of view of these scale-free generative models we're really thinking about how do episodes follow each other which episode is going to follow this going to um um this episode and we can say well you know you are in control of that um you know imagine a um you a benevolent World in which everything happens according to um your beliefs about how to be an expert in this particular world and the agent can plan a series of episodes generating predictions all the way down to the bottom which then slavishly action can fulfill just by fulfilling those predictions um so one could in some sense uh think of this in terms of controller's inference at the bottom level in the spirit of model predictive control and planning as inference on the top level so belt and braces in terms of realizing active inference in this particular example where I repeat um we are really just worried about um State action policies um why are we worried about State action policies well these are the kind of policies that reward learning schemes um are aimed at um and have a similar architecture um but um to my mind a slightly more um um over engineered architecture first of all you actually need explicit inputs which are rewards um there are some proxy for um the constraints uh you know um um under which um the basic mechanics would normally operate um but more importantly you're you're you're basically sending um your inference scheme is basically about selecting the right policy I've used Q learning here um um to maximize the discounted reward um so similarities and differences but we're going to focus on uh active inference um uh in this in the particular setting of these renormalizing models where rep planning and thinking or imagining a future can proceed at the very highest level that has no notion of action they're just patterns that unfold they may or not be caused by me I don't know I don't care um but the action at the lowest level the reflexes as it were um do um are in a position to make my um post predictive posters come true simply by picking those actions that realize the predicted outcomes um so how are we going to leverage that kind of merely reflexive active inference um in the context of building or learning from scratch um renormalizing or scale-free generative models that would be apt for doing something implementing a state action policy um and I'm going to um appeal here again to this notion of um active selection as one way of um looking at um action which can always be at least phys mathematically expressed in terms of um of um doing something if it minimizes expected free energy um and in this instance um what we can do is actually apply it not to the selection of the model but the selection of the training data so if we just um apply the the dictat that you only select something if it um um minimizes it expected free energy that includes um the expected cost what that basically means is I'm only going to select those data features or those training sequences that don't come along with a high cost namely those that are rewarded um and that's what we're going to do here we're basically going to um learn to play um a very simplified um game of pong where this ball bounces around inside a circle um and we can move the bat um in a way to uh return the ball so that we can avoid penalties when the ball hits the upper boundary um and secure a reward um whenever um there's contact of the ball and the bat and how can we learn this kind of dynamic these episodes in exactly the same way we learned the video of the Robin or the the um the dove flapping her wings well what we can do is we can just um select um instances of random play that conform with um our um dictat that we cannot um include those costly events and so what we've done here is just generate training data training sequences using random but omitted anything that does not that contains a costly outcome and furthermore just to make things more efficient we've restarted the random play after a reward is secured and then just use the subsequent play if it actually attained a subsequent reward so basically chain together bootstrapping together or splicing together sequences of rewarded uh play um and then use that to um uh to um submitting that for fast structure learning and now we have a generative model of expert play effectively or a generative model that has that entails a state action policy that can be realized through um um reflexive active inflence so active data selection um and I'm um sort of cartooning that here in terms of applying a certain kind of maxel demon that only lets through non-costly episodes um basically yes admit this sequence for fast structure learning if there is a reward if not ignore this ignore this sequence um and this effectively learns um or furnishes a generative model of events that lead to a reward and nothing else so now this agent can only recognize expert play and therefore all of its predictions have to be uh the predictions of an expert player and because action is fulfilling the predictions it will look as if it's an it is an expert player and here are the um the Learned trajectories um at the highest level of different episodes that we'll see uh um shown in in an alternative format uh in the next slide so here's the initial starting episodes events here uh the red dotts correspond to reward so you know first hit second hit and then it falls into a nice little orbit and this comp completes um this orbit indefinitely by in it's the way that it has learned so remember it's only been trained on random data so this particular style of play is unique to this agent but it is sufficient now to play with 100% um um uh performance um simply because it's now going round and round its orbit very much like the wing flapping her du flapping her wings um these are just different ways of portraying the same um the same Dynamics and the same the things that have been learned um the first eight frames and then subsequent frames um of expert play um this is the usual format showing the posteriors at different levels the posteriors over the states and the posteriors or predicted posteriors over the paths and then rewarded play as a function of the uh free energy at different levels here illustrated by the by the red dots every time there's a hit up until 512 time frames here and here's this rather long orbit through 50 episodes um each four uh time steps in in length that subtends this kind of play this is an interesting one and speaks to the fact that we've actually switched on inductive inference at the highest level um so what does that mean well what we've done is just um um basically take each of these highest level States worked out the sequence that it encodes as by um um Computing or projecting down through the model and if that sequence includes a hit um we said that's an intended State that's the kind of state that I want to work towards through the success Ive um for um time step epochs um and if I want to be at some point in the future in one of these intended States in this instance rewarded States or episodes um what states must I avoid in order to preserve or conserve the possibility of getting in or getting to one of these intended States um basically that's the technology behind inductive inference and it's sort of summarized here in terms of the latent States of uh you know 50 or so of these latent States in terms of how many time steps would it would it what's the shortest number of time steps until the intended State um and basically um we um as we move from state to state to state we want to keep in the white areas here avoiding these dead ends if you like from which there is no possibility of reaching an intended State and it's really trivial to evaluate these um areas that you have to avoid because you know the probability transition matrices so you just multiply it by itself a given number of times and you can work out can I get from this state to this state and if you can't um then you get this um black area here and you know you have to move from here to here to here to here to get to um your um your intended St so that's inductive inference in brief and we've applied at the top level so we just the you just told the agent this world um is under your control imagine the you know imagine what you want um noting that these particular episodes or events contain you are things that you should pass through or should experience um and then deploying inductive inference to basically predict what will happen all the way down at the bottom and then using reflexes to make that happen um we can apply exactly the same um technology or the same procedure to different kinds of games is a slightly more um complicated game um meant to emulate breakout um where we've introduced uh certain stochasticity in terms of the initial conditions and uh sticky action but the the overall behavior is the same it's just basically choosing paths through episodes into the future given what it knows it can be achieved in order to get to uh rewarded uh States so um effectively what we've done is create um an attracting set of episodes at the highest level of these scale-free um um generative models where all the allowable paths and paths now in this generalized State space or event space lead to reward um we can go a little bit further and just um add in uh various episodes by merging uh or assimilating new data new training data if and only if um the path actually leads to an existing um part of this attracting set and I've just Illustrated that here um in this final example so this is the path or the orbit or the attracting set that was learned at the highest level by the um the pingpong playing uh agent but now we can um fill in um fill in this um effectively um allowable Dynamics to include all states that lead to a path on the orbit um and or thereby all paths will eventually lead to reward or to roam so we can augment the model of expert play with paths that lead to an orbit namely this attracting set um and paths that lead to event that lead to the orbit um recursively and get quite a dense set of allowable transitions that we can be guaranteed always keep within the attracting set that will contain these preferred States or um states that we have um constraints uh where um we have constrain those states that are not visited simply because they cannot be recognized and that's it thank you very much indeed amazing Carl thank you awesome okay Arun if you'd like to lead in with a first question and Lance too thank you but go for first uh yeah thanks very much Carl um so my first question is um based on a sentence in the paper which talks about any model being sparse must be renormalizable did you say a little bit more about why that must be oh I can't remember give me a clue uh I I'm presuming it has to do with Markoff blankets and yes yes yeah that's as far as I got I see yes no I I remember now um I it may have been in reference to some work done by Dalton um showing that with probability one provided the system is large enough there will be Markoff blankets and as soon as there are Markoff blankets there is now an opportunity to take Markoff blankets or Markoff blankets in the spirit of um that applying the renormalization group that that I started off with um so as soon as you've got um as as soon as any um sparely coupled random dynamical system uh um can now support a um a particular partition into markof blankets you've now got the opportunity to take Markoff blankets and markof blankets and then apply that renormalizing procedure that I said before so all you need to posit is any system um that has a Markoff blanket at any given level and as soon as you've got that you've got markof blankets all the way up um not necessarily all the way down but um you've certainly got them all way up um the the physics approach would would actually um also actually um require for for there to be Mark blankets all the way down as well but that's that's that's another issue um so um what that basically means is that um all you require is a sufficient degree of sparsity that would um lead to the condition independencies that are definitive of Markoff blanket and in one sense much of the sort of um the um the maths behind the free energy Principle as it inherits from random dynamical systems or uh in physics say Lan um equations is simply showing that a sparse coupling a sparse causal coupling in a physics or control theoretic sense not a philosophical or Granger um or um cause effect structure sense but in the physics sense of a coupling so the the change um this state um influences the rate of change of that state so that kind of um which could be written as an influence graph so if there's a sparse coupling on the influence graph the equivalent basian graph or probabalistic graphical model induced by that sparse coupling um now uh can be interpreted in the in the sense of um um pearls markof blankets um so all you need to get to the application of the renormalization group to markof blankets is sparse causal coupling hence sparse Rand random dynamical system that is sparely coupled just out of interest um the spin block um the motivation for the spin block applying the spin block um or blocking Transformations that basically comes from the assump that um many systems that people deal with both in computer vision and in and in idealized um models in physics um has an exquisitely sparse distribution and that's because the causal inferences are all local so as soon as you in um you look at a a coupled map l ltis or a globally coupled map you'll notice let's take the ltis for example let's take a sort of idealized um uh um um Crystal uh so what you're saying when you write down a latis is I'm only connected to my neighbors to my more neighborhood or to my Markoff blanket um that means that in a large latice uh where I could be connected to n s neighbors I'm only connected to 1 2 3 4 5 6 7 8 so a fantastically small almost empty set of possible connections and I use that phrase because there I can't remember who said it now but I really liked it the notion that if one looks at the connecto in the brain in terms of the connections it's almost empty you know the number of actual connections you know white matter connections external connections and indeed intrinsic um um exteral connections are almost zero in comparison to the total number of possible connections between the 10 to the 11 or whatever uh neurons in the brain so that's not surprising and I you it's one of those nice examples that the the anatomy the functional architecture of the brain recapitulates the anatomy of the lived World in it has this incredible sparcity um um and that just means um the universes in which there is um the action at a distance is quite rare um um means that that you you're just interacting you just coupled to your neighbors and that's and that gives you if you then look at the mark of blanket structure or the particular partition for these ltis structures you get to the spin block um blocking Transformations as a way of carving up nature you know in terms of little patches or Markoff blankets I do have a follow-up question on that but then I'll hand it back to Daniel or Lance um so the ltis example is really interesting because I think there's a symmetry there that maybe we lose when we talk about blankets that only have one parent so in a normal lce as you say you're just connected to your eight Neighbors in 3D space when we looked at the spin block Transformations before we were going let's look at this square and in this square has one parent but then the square along is connected to a separate parent so now if you're at the boundary we are breaking some symmetry there yes technically or practically a very good point um it doesn't really you it doesn't really um uh have any material impact upon the the the algorithms or the partitioning um so just practically what happens is you you you you you just take your tiles and then you have a half tile or a partial tile um until you get to the boundary so these these groups in the grouping operation or the or the blocking operation are not necessarily the same um you know they're not the same size um and indeed in uh um current um work we're actually dealing with sort of multiple streams so you can have sort of audio files and video files separately tiled and then converging right right at the top um ideally what you'd want to do is is um not use this um um blocking transformation but to try and identify the Markoff blanket structure at the particular scale in question and actually use the Markoff blankets based upon um well there are a number of ways of doing that you can either use a Markoff blanket Discovery algorithm of the kind that Jeff Beck um Works upon um or you can use um empirical um analysis of the um the dynamical coupling um uh by estimating effective the jacobians you know for those people are interested there's a worked example of that in the particles and Parcels in the brain paper in network Neuroscience um it's really founded in terms of data analysis but you can use the same principles to um in to identify a particular partition from a large number of um um States or boxes or or or or pixels um um you know in three and 2D um respectively um there's another thing though that I think your question speaks to um which is um you know you've got you got you know one block of four and then another block of four and then um because of the sparsity they're separately U modeled at at the level above so I thought you were going to ask how in Earth now do you start to repair the obviously rather facile assumption that you've got just local action because of course in our world there's lots of action to distance you we are um talking to each other over over over thousands of miles or Kil kilometers and you know we have Vision uh so all the and we have uh linguistic communication via sound so you know it is interesting that we have actually violated much of the local action if I was a virus this would be perfectly okay because the only thing that's really going affect me as a virus is the things that AR are in molecular contact with me but for you and me um and the worlds in which we live in um we can't there is no such thing as just the action distance now starts to really matter um and effectively because the two um latent States generalized and States generating stuff um initial conditions and paths for the two blocks below because they themselves now constitute a single uh contribute to a single um latent State at the at the level above as you move deeper and deeper you start to now repair the simplifying Assumption of conditional Independence between the groups so it is at the higher levels that you now become you're able to see these action this action at a distance these coupling your what what happens over here really matters in terms of what's going on over here but you'll only ever see that you'll only have that as part of the generative model right at the top which is why all the inductive inference is implemented right at the top level where where you can see everything um so that it becomes you know if you like you are now you are now in a position to um um model and thereby generate appropriate prediction that rest upon um these long range dependencies um and of course you know using the word long range I mean in the sense of the the coupling distance U youit you in a dynamical sense is that what you had in mind uh that was exactly what I had in mind thank you thank you Lance do you have any remarks or want to add a question or I can yes please go for it yeah I have lots of questions uh thanks a lot for the talk so I'm just wondering about this all roads lead to Rome slide that you had at the end and I'm wondering whether you have some sort of a curse of dimensionality as you increase the dimensionality of the system so it sounds to me like the manifold of expert play because um because you're taking temporal trajectories that's going to be a onedimensional manifold now as a state space of the game um increases in dimensionality you're going to have an ambient space of trajectory that's much much higher dimensional and so it sounds like the amount of trajectories they need to you know construe to bring me back to your um onedimensional expert manifold is just going to grow and grow and grow and it sounds like it might might be hard to fill that space so could you could you comment a little bit on that and whether it's a problem right yeah in general in my world you know these things aren't problems but they're challenges um and I think that challenge is yet to be met to be met um so what I imagine people will do um is that you will um ingest um extra episodes or sequences that bring you onto your attracting set up until some bound um and then you will switch off active selection by which I mean the structure learning um so you stop growing the model and then you switch on parameter learning so that now you're forcing the Transitions and the uh um expression of various generalized states to accommodate variations around the training data that you have been previously exposed to so that fixes the size of the tensors and hence the dimensionality of the state Bas State spaces in question um um but will in principle now accommodate by preserving by using sort of um Active Learning in the technical sense that you only update a parameter if it um does not uh increase expected free energy or decrease Mutual information um then um you you you're now in a position to become more robust um and perhaps I should qualify this answer the whole point of this adding in extra if you like insets onto an attracting set is just to equip the agent with a certain robustness should itself find itself outside or off that attracting set so um I know Lance knows this but I'll just make this explicit for for everybody else um this kind of learning um and state action policy um implementation is very much like learning to ride a bike um so you you basically um just accumulate and remember those experiences that were successful so for example I you know managed to um rotate the pedals by one cycle and I don't fall off and I remember that and then I have another go and I wait until I've actually done two cycles and I don't fall off and I remember that I forget everything uh every time I I I fall off and then I keep on going until at some point I connect back my last cycle was identical to the first cycle and then I've learned to ride the bike but because I've forgotten all the falling off I can't recognize or know where I am when I when I've fallen off so I've got a very brittle expertise that will not allow me to recover if I fall off the edge so another example which makes it slightly clear I think is like mountain climbing you know if I want to be an expert mountain climber I cannot learn by my mistakes provided I'm climbing a sufficiently high mountain should I fall off it then I will die another example here is learning how to cross the road as a child you you cannot learn by um um experiencing being run over but the cost you um you pay or the price that you pay is you can't recognize when you are being run over um and that I repeat lends a certain brittleness and a lack of robustness to agents who may be exposed to Danger that doesn't actually terminate them so putting this robustness back in by growing the um by growing the attracting set or basically putting these insets on um in this sort of generalized um phase space um you know it's just to make it more robust um in numerical studies it it does seem to um converge it doesn't seem to grow indefinitely it you so those are likely violations are moving off the attracting set are themselves quite small relatively small number um you know practically speaking uh if if you've control the sources of stochasticity or unpredictability so it may not be always the case that you need to have an explicit bound on the upper the size of the um the very high level tensors if if you do then one has to now think of a principled way of making that upper bound and Emer property of um um expected for energy minimization it may be that you you violate now the mutual information that certain rare events are just so rare you're actually compromising the mutual information by including them as a particular option um or a particular column of of a likely likelihood mapping or yeah but I don't know but these are all really interesting questions to pursue thank you Carl all right and go for it um yeah so following on from that I think there an interesting question on the uniqueness part of ingesting data and the question is are there many more ways to fail than there are to succeed so if you're passing your data into a sort of Maxwell demon Maxwell's demon uh with a gate saying accept this data reject this data if this uh improves my Mutual information if there are many more to fail but then get back onto the uh successful path but there are only a few paths that actually keep you there would it not make sense to just continually accumulate these sort of recovery States because the mutual information will keep increasing and then you stop there then you parameter you learn parameter wise because we're not including the sort of Prior distribution of How likely you are to be succeeding at any given moment because with the uniqueness thing you don't know oh 10 times out of 10 I'm cycling or like nine times out of 10 I'm cycling and one times out of 10 are fallen off the bike yeah I think it depends um again I don't have any explicit numerical analyses or analytic analys mathematical analyses of this but intuitively um I think during learning there may well be um um good mileage in ingesting failures um or roots to success if you like or to the attracting set that that actually Traverse regimes that would normally be excluded by my constraints or my PRI preferences um you know just taking the view of the role of of the C tenses or prior preferences from the point of view of physics what that basically is saying is it's actually carving out vast regimes of State space Laten State space that you should not be in because the it would be uncharacteristic of you to be in these things so it it's really not so much about the rewards and the preferences uh it's really what you where you shouldn't be and that's what defines the attracting set um however before you've learned that attracting set it may be useful to include constrained regions of State space that offer a route into or onto that attracting set so I can certainly see that that would be a useful device but notice that once you're on the attracting set you will never visit the constrained regions because you now keep yourself on the attracting set um um and therefore if you've encoded those but you never visit them your Mutual information will actually fall so what that means operationally is what you'll probably do is have this sort of slightly over inclusive structure learning learn how to cope with all the necessary sources of Randomness so that your attracting set is sufficiently robust and you do actually visit from time to time but not um not never um s of excursions from the attracting set I drawn back onto the interacting set um and then use basing model reduction to eliminate all the states that you don't want to recognize simply because you never go there um you know once in a you know when I was younger I would certainly be able to recognize what it's like to fall off my bike but now as an expert bike rider I really don't want or need that so you and that's just basically getting old and wise and you know implementing pay in model reduction to remove the Redundant parameters which would in this instance just be the columns um and um Associated slices or or rows of the of the transition matricies so I think you could perhaps another way then advanc string l question is yeah you just keep um accumulating um but um start to engage Bas in model reduction at some point you should reach an equilibrium which will be the attracting set that allows you to survive in the character states that Define you in this particular environment with this particular volatility thank you awesome okay I'm going to bring in a question from the live chat okay Javier wrote so stochastic plus dynamical probabilities learned by the agent itself are scale-free will this hierarchical learning also be applied to the goals of the agent different abstractions granularities of goals [Music] self-learned my answer would be yes but I'm going to I'm going to ask Lance to to to to answer that one oh that that's cheeky uh yeah I would say for sure um because typically when we have when we oper operation operationalize sorry the goals in our gen model they need to have the same structure at our gen of model itself so I would imagine then we would have a hierarchical renormalizable it's kind of make sense cool okay I'll bring in oh yeah go for it Carl I was just GNA say um you know just to um unpack what Lance just said in terms of practical things you know if you're trying to um you're trying to say um what my goal is is to go uh is to have dinner um that naturally unpacks hierarchically into I've got to move from the living room to the kitchen then I have to find the you open the fridge and sorry I have to find the food which then unpacks into I have now open them and all the way down to to the finest finest muscle movement so almost by definition um you know intended narratives can always be um decomposed in this scale-free way you um there there's a this is very similar to sort of um motor chunking and hierarchical decomposition in in um um theoretical motor control for example and you probably find it a lot in in robotics that there's a global long-term plan that that provides empirical PRI of constraints on shorter little chunks that themselves provide constraints on shorter and shorter chunks and that's exactly the um if you like the the architecture implicit in these scale free models awesome and totally ties back to the the synergetics okay I'll ask two questions from ml Dawn he wrote how can we inject the same separation of time scales by imposing smoothness constraints on say the parameters as opposed to playing with proportional update rates at different levels and this relates to something Arun and I talked a lot about in the zero and leading up to it is identifying which Windows to use you could imagine picking the right separation of time scales if the classes on campus really do shift on the hour then the hour course graining is going to be great and effective but the 59 or the 61 minute might Alias you with the real variability of the system and so there's not necessarily even a smooth landscape for determining the windowing or the binning of coar graining and coar graining may have dependences on each other so how do you go about playing and finding viable approaches and robust ones amidst all these different levels of of meta optimization that are opening up right I'll take this one and L can take take the next one um then that's a really good question because there is an implicit separation time scales between um so vanilla implementations of inference and learning however in these schemes there isn't um so the the the um the Matlab version or implementation um of um active inference under these and in fact all uh markof decision process models um unlike um the um procedures that you might find in data analysis where you acquire you basically do some filtering on a on a an Epoch of data and then at the end you update your parameters and then you rinse wash and repeat um in um in active inference um simulations and also in some um applications of what's called generalized filtering you have continual learning so the update the parameters actually occurs at every time step um at each level of the model so the likelihood parameters at the lowest level are updated every time that routine is called um so there is continual learning the learning um is yes you're absolutely right is smooth in the sense that you're slowly accumulating drish parameters but the tempal scale of that learning is the same as the inference um and you could also argue well could one also put Basi and model reduction um into play at the same time scale so basically can you at every time point at each level in the model um identify redundant parameters and remove them uh using Bas and model reduction in effect that's what's done uh using Active Learning what I said before Active Learning will only be proceed um if the expected free energy of mutual information improves that's actually also implemented so this the kind the the the rate the rate at which it looks as though things change um between inference learning and model selection is certainly distinct I don't think that's quite the same kind of separation of time scales that is implied in the renormalization group that's as you were intimating uh Daniel exactly specified by the number of ticks or updates at level n per single update at the uh level n+ one um and that is I think you know the you the key question you're asking how do you get that right um and of course the answer to that is the the model that has the greatest model evidence or smallest free energy or greatest marginal likelihood for the kind of data generated by this environment in this world so um are there any um sort of useful priers you can put over this um well what one one useful prior is um that um things are changing as uh to to well one PRI which is actually used in operation is you take the lower limit on on What's called the uh the temporal RG flow which is basically the the length of the window at each and every level um and the moment that's two what does that mean how can you motivate that well it's the most expressive in the following sense that if I um just imagine um what imagine what this dis discrete discretization of time means from the point of view of classical Notions in continous state space like position and momentum or position and velocity if um if I use u a um a scaling of two which is basically means there are two time steps or every time step of the um the level above what I am effectively doing is um um modeling two time steps for every state at the level above and that basically means I can model the initial condition and the subsequent condition basically terms of the change which of course is the velocity um and what does that mean for the level above that well it now means that I'm modeling the change in the velocity so I'm now modeling the acceleration and then so on up until um you know the highest order of motion so what that means is by taking this limiting case of T the temp the temporal horizon or the the you the ratio of time time steps from one level to the other level um as two means that I got a very expressive model that can handle um can recognize um things that change very very quickly and their acceleration their jerk and all and the implicit um context sensitivity that that that gives you if I didn't do that I'd have to assume that I C grain over long periods of time and therefore the path is consistent during that time now that sometimes is a useful approximation and indeed there's a whole industry called U you know called linear switching linear dynamical systems that makes that approximation it means you can um approximate all the nonlinear complicated high order motions in some evolving pattern or system with a series of um um linear trajectories that basically go you know that intercede between a series of points at which you which you which you switch the problem with that is that the length of the linear trajectories for as for example I'm committed to One path youth so one slice of my B Matrix for the next eight time steps the problem with this is that you don't know when you when you deviate from that transition Dynamics so then you get into the game well perhaps I could make my scheme reactive so sometimes I do three steps sometimes I do eight steps sometimes I do one step and I have some criteria for saying your predictions about my trajectory have now been met please um I'm going to tell you give you the evidence that been pursuing this path you update your privately level above and tell me what you expect me to do next so this now speaks to um a particular kind of reactive message passing um that starts to have implications for the computer science and the sort of CR in the crud um um or um as um as I think we've rehearsed before um The Institute you the original conception uh behind um the actor model in computer science from the 1990s so it's all about reactive message passing you I will send you a message when you send me I will return a message to you if you send me a message um and what this um what these um renormalizing time models have and I should say that this also applies in the absence of the the spatial renormalization so any deep temporal model will have this aspect what that means is that from the point of view of Any Given level I will um be responding to messages more quickly from my subordinates than from my superiors so once in a while I'll get a request from a superior my response to that is to ask my juniors to do a series of things they report back to me and when they report I give them the next bit bit of request or guidance uh until they complete their job and when all my juniors have completed their job of course the re normalizing specially re normalizing context I'll have a whole group of of Juniors to worry about um then um I have gathered enough evidence for me to then return the the message to my to my Superior so you get the in terms of the reactive message passing in this sort of actor model of um of computer message passing um you get the the separation of temporal scales inherits from the number of messages or Rea um requests and uh responses um U exchange with the people lower down in the hierarchy relative to the number immediately subordinate um and in principle they don't have to be fixed so there could be a crit IIA um before I pass the message back back up to the to the higher level I think that's Daniel probably what you were getting at um I'm not there may be somebody who's done that um um but I'm I haven't done that and I I I don't know uh I can't give you an example of somebody who has done that you may find you may find somebody um in Bert de's group um well sort of you know some of the world experts um in react message passing who have considered these kinds of things um you know you in in principle um you know there should be a way of um each level testing whether I have fulfilled the predictions of my superiors um and now it's time to get more guidance you know uh based upon the free energy awesome the organizational technology I didn't know that my PhD needed okay I'll ask a question as we get close to the end and and this will be for for anyone ml Dawn wrote how does the training time of active inference compar to that of reinforcement learning and how does their scalability compare and it would be interesting just to hear anyone's very 202 24 takes where are we at with that entire Arc from problem conceptualization on through the training and the inference how do we even grapple and describe with similarities and differences in the training time and resources and all of this that's definitely Lance for for at least five minutes I think I'm I'm a bit incapacitated in terms of talking um I think this is actually a good question for Arun um but yeah I mean I can say a few things um so uh scalability in active inflence I would say it has been a challenge and there has been um much less man work hours that have been put into that than in reinforcement learning now for the past few years we we've had a lot of work um a lot of it led by C actually um trying to scale the structure learning in active inference and and I think we've done a lot of mileage on that so there's been the supervised structure learning paper there's this paper uh there was also some interesting work um presented at the international workshop on active inference um I personally think there are still some uh challenges ahead and and I think uh one of them is is the one that we just talked um a few questions ago about you know if you if you learn an expert orbit how do you get back on the orbit as the dimensionality of the system increases so this is definitely um One Challenge um now I know much less about reinforcement learning so um yeah maybe Arun has more things to say there um but but what I will say is a lot of reinforcement learning is powered by Deep neural networks when we use deep networks we sacrifice uh the interpretability of our generative model this is one of the strength of active inference is having more interpretable um Dynamic Cal models where we can actually here into them and see okay well this representation enclos this this other one enclosed that and so forth um the fact though that reinforcement learning typically uses deep neural networks means that it has been much more scalable uh just because there has been so much work done in like training deep networks this is a bit of my uh my perspective I don't think it fully answers the question though I can come in with a little bit on that but I must admit that I've not trained any reinforcement learning agents myself I've worked with people who have uh I'd agree Lance that the infrastructure generally is much more advanced for training RL agents for doing stuff uh and completely agree with as you say the explainability and the interpretability of why did an RL agent pick a particular action is very opaque uh and I think the deeper those networks become the hard is to understand what's going on in the hidden layers and uh generally why they do what they do I think with Act of inference yes we do have more explainability but I do wonder if we go the far structure learning approach I think KL had a particular uh comment earlier about looking at the highest level um identifying the desirable States learned by the agent from what I understand that identification has to be done by a they have to look at the model that's been learned by this uh renormalization approach look at those top levels and then say ah what do these levels mean so an example in the paper is the mnist uh classification it's the case of a human being looking at those top levels and going ah these top levels each correspond to a digit in the handwriting task so we are uh sacrificing some explainability I think by doing this fast structure learning compared to what might be the oldfashioned way of creating uh Genera models By Hand by thinking sitting in my armchair and thinking this is how I might model the world these structures these likelihoods these states and when you do that I think just by hand you have 100% explainability that model just might not perform very well so I think there's always going to be that tension there um but in terms of how quickly these things learn um having not trained either I'd be having a reckon and at the moment I would say probably an RG um active inference agent would train faster simply because it's able to use the structure and the data much more effectively that would that would be my take and I think as well really interesting thinking about the active data selection part of this too I know that there are some stuff in the paper about the efficiency being both a curse and a burden but I'll hand over to Lance and Carl to give more details on that yeah Lance yeah just to um echo echo what Arun said um also so so the structure learning active inference and in reinforcement learning is is also very different um in reinforcement learning we typically have a a model with deep n networks that's massively over overparameterized but they has some fixed represent presentational capacity from the outside because the structure of that of that model will be specified and then typically the model will learn and then probably you would prune it using some techniques like Dropout or or other things if we looked at that from the perspective of active inference it would be like specifying a massively overparameterized model and then pruning it with Bas model reduction the um but but now in active inference we have this Basin model expansion for example in this paper and what this Basin model expansion does is it learns from the GetGo the minimal model for explaining the data and so in that sense it's extremely efficient and also provably because it extremes the um uh the marginal likelihood or the mutual information yeah I just wanted to Echo that last Point that's a really important Point um so Lance is drawing the distinction and there are many distinctions you can draw with um deep RL um and um deep active inference read in a more in in its um generic sense um and that that distinction at the level of structural learning or model selection I I think is absolutely crucial so you know deep RL starts with an overly expressive model with a known functional form um that is deliberately overparameterized and then shrinks it or reduces it to um maximize the mutual information the expected free energy sometimes they um referred to as a cross entropy in machine learning so the reward function now become uh there's no explicit reward in unsupervised learning it's just replaced by the mutual information or the cross entropy um which should be distinguished between supervisory enforcement learning such as you know digit classification for example um whereas the you know active inference um under these re under fast structure learning is exactly as Lance says it starts from nothing and stops when it's minimally complex which means it's maximally efficient and that efficiency translates in many guises through to sample efficiency through to um um statistical efficiency through to thermodynamic efficiency and this matters because from the point of view of climate change this is the right way to do it as opposed to building power stations to power large language models and Transformers that need big data so um the efficiency um on all levels whether it's the the sample efficiency or using the right data uh or whether it's right through to how much electricity do you need to actually train your model and how many millions of dollars you need to trade a foundation model um all of these are reflections of the failure of um machine learning to write in the efficiency into the ultimate objective function which as Lance said is just the marginal likelihood of the evidence for this model of of this kind of content or or World awesome okay final question and a very apt one as we head into the rest of the Symposium so just give any short thoughts Tintin wrote hi all thanks for the incredible event does the sequence does the sequence in which the training data is fed to the AI structure learning model impact what the resulting learn structure would be so the learning curriculum AKA Symposium program challenge well the answer is sorry I'm mindful that Lance is recovering from his um Jew surgery um yeah absolutely um and this is um again just in relation to to to to the last question something I think that eludes uh conventional RL certainly that it rests upon um um classification procedures and um because um the active inference as an application of the free energy principle is all about the Dynamics of self organization and Dynamics means time and time means sequence matters so inevitably at some level the order in which you see things is absolutely crucial for getting the right kind of generative model um that is even in the case um actually of static image recognition because you're assuming that there's no ordinal structure or correlation structure to the presentation say of M digits you're assuming that they are selected at random as opposed to seeing all the tens and then all the all the ones and then all the all the sevens um so even in these edge cases um um which are you know the focus of um 20th century RL um you know static image classification for example um your order does matter but they they it matters absolutely acutely because of course this the order in which you um see um content in uh um this infast structure learning determines how you populate the um the the transition tensors and it's the transition tensors that enable the separation of temporal scales that enables the ability to um encode narratives and plans that far transcend the actual rate of which you samp sample the environment you could also argue that you know this issue has emerged um in a in a strange kind of guise in Transformer models so the attention heads operate on the past so the the where you select the from the past in terms of which token is going to predict the next token most efficiently um again is exquisitly dependent upon and sensitive to the order in which you train train train these models So Daniel have you thought carefully about your ordinal approach today I I let the structure I let the structure of the niche and others' constraints provide the first pass and make requests from there so thank you Carl thank you Lance for joining again thank you run for all the contributions with azero Carl great to see you so next up we'll take a 30 second break and we'll be back with Michael Lennon so thanks fellows see you soon e okay welcome back from that brief brief break we're here with Michael Lenin and the session is co- inferencing organizational symbiosis so Michael thank you for joining take it away hello yes um I am uh so grateful to be here I would like to um approach today's conversation in um in a conversational style where um I share what led me to active inference and how the discovery of the principles of free energy principle and and active interest changed how I understood as a practitioner and and how that has led both to both personal professional and and organizational um shifts in conversations um one of which will conclude in an invitation to this community to um experiment with an uh an organizational structure uh designed for um building both the global n knowledge comms um as well as um uh building individual Ventures so similar to the way in which uh for example Linux is open source uh and then you have Ventures building off of Linux to do things in the world that that experimenting with such a an organizational model uh for uh the active inference comments uh is some an experiment worth taking on so uh I I I don't want to let the the final invite get away but but I'm going to backtrack and do this very much in a storytelling fashion so um so um I have some slides but I'm not going to uh spend too much time uh well in fact let me just um skip that just just to recap what this um uh what this conversation is about is different ways in which um not just active inference but um science has collided with internal intuition and led to transformational journeys of different kinds and so um and so um I would describe the um you know I I started out as an individual uh as a uh Economist um and um and as a technologist working at um institutions such as the World Bank and IBM and U doing large scale um it implementation projects and um and that work often although there was a lot of um engineering um it was often the human factors that got in the way of success or failure and so um that has been a kind of orienting um how to both be informed by what the the the science and the math provides and uh then build uh how does the biologically based if you will way of sense making um affect or um alter the the the the evolution of of successful project implementation so um just um a little bit about um about where this uh project um prior to coming to active inference um well I say project my journey um uh led me through was was um I had a uh I had as I said been trained as a as an economist and um one of the things that economists have is a point of view about what it means to grow the culture to grow the society and um primarily denominated in dollars and using um GDP as a way of um of uh tracking whether or not one was impact ing the experience of flourishing and um I came across a uh a chart by um by an economist that showed that GDP had tripled in the last 60 years both in the UK and the us and that well-being had not changed at all and this economist who had been in office in the UK at the time when when he left office was kicked out um he went back to Academia and started looking into what what does Science Show moves the needle of well-being and how do we make this socially more contagious and um and I both U loved the idea but I also was um uh shocked by that information because as someone who was trained in a particular lens a particular perspective what I had been trained to do and how to um cause effect and impact was in fact not moving the needle of well-being and so there was this kind of personal and professional crisis um that arose from this uh from this knowledge and um as I say that that led to this this journey of unlearning in some ways and um and and in both personal interpersonal and organizational ways of recalibrating the experience of what does it mean to pay attention to thriving and so um that Journey has been very SA is fine um not only do I understand how I need to be to experience uh more goosebumps in my life uh I understand uh how I need to be to experience connection and belonging and to co-create with others uh a sense of um of purpose and of meaning that um that I had not had previously and it's uh one of the ways I sometimes uh describe this relationship between the subjective uh internal experience and the scientifically objective let's call it was that I might know how to go to the gym and flex uh my arms to get a workout but by understanding Anatomy I could learn um the bicep had a particular placement in the body a particular Arrangement and I still had to do the workout but I could more effectively and um skillfully uh do the work that needed to be done of working out and so similarly the study of the science of well-being um and social uh social uh Sciences of of connections of groups and of um and of um how how for example um one of the surprising findings for me uh in in the realm of Happiness was um a finding by an epidemiologist and um [Music] basically he um was studying how do emotions spread and um and he in studying this realized that your happiness um uh was not only more heavily influ uh influenced by the people that you're surrounded by but also by the next degree of separation the people to whom they are connected and even the third degree of separation and so that these three um Degrees of Separation predicted your happiness more than uh the way that I explained I had been explained what happiness was which was that uh it is an inalienable right like life and Liberty the pursuit of happiness and that it's subject to individual traits of my own that if I just do me the best I can that I'm going to be happier and so all of a sudden there's this disorienting um insight in that it's not about being my best but it's just as important to inter be my best and so it's like well what the heck does that mean what does it what does it look like to to inhabit one's own experience and one's own skin in in a completely different way and and I know a conversation that uh you and I Daniel once had about um about the impact of active inference you were describing how you might have been driving on the right hand side of the road your whole life and that all of a sudden you um you uh go to a place where everybody drives on the left and so some intuitions still work exactly as as they used to but others are completely counterintuitive and so this developing the capacity to to go against intuition against the The Familiar is um is part of what sometimes these these these new forms of knowledge there's there's knowledge that that adds to what we know and then there's knowledge that changes how we understand and and happiness first did that for me intuitively at the personal experiential level right at first was like okay I I am playing the game of my own life inappropriately um how do I how do I begin to interb my best instead of be my best and this is where um uh some subsequent work in working personally and then with groups and it and organizationally led to um paying attention to what was happening in in a different way and uh so I uh as I say I this this um crisis led to a very fruitful um Learning Journey and and um and uh excuse me I um I got distracted um some of this led to writing different ways of describing how to do work and so for example one of the spaces that I've been working in was in U government Consulting and in government Consulting top down um uh uh is often the way to direct the effectiveness of the organization um you come up with the right strategy you figure out all the components and you then execute on the plan and uh and so in a sense uh if there are two forms of of uh probabilistic prediction basian being one at the heart of of active inference and another one being fisher fisher is one that that says it knows where you need to go and the way that you relate to to surprises and to mistakes and to errors is you correct back to what you felt your original plan was whereas basian probabilism probability obviously um can have similar results but also can update instead of reacting to surprise as something to be fixed it can sometimes be something to be adapted to and uh changes the way that uh manage you manage um the the uh emergence of differing information from what you originally predicted and so in the same way that I described that happiness uh had a disorienting effect on how to practice it active inference has a similar um disorienting effect about how to co-regulate um how to supervise the um the evolution of of differences between what you expect and what you what you get and um one way in which I I describe this uh to to folks is it changes your understanding or change my understanding of how cause and effect works that um you know as as as I arrived to um to um active inference there were there were a couple of intuitions about uh how the world operated that active inference helped um evolve the first was that instead of instead of the world being composed of objects um that were fixed and stable that um that the mathematics of how to explain things was much more of a dynamic verb and so to look at the world as being not nouns but verbs and and and a way that I describe this for myself when talking to people about it is that instead of seeing myself as a self a noun that I'm a a be a being and so um that paying attention to the continuous verbing and the relating uh to to to others is is a is a different um uh a different game and so um the interesting thing about active inference was that there was a math ma matics for describing this that um that related the internal States and the external States um through uh mathematics of information that the way in which um the the way in which um uh science Works um is is is familiar and intuitive to to to many of us um Let me let me let me rephrase this in a different way that you know when I say the inside and the outside are informationally connected um that that um that the outside is all the information all the stimulus of the world and that the inside creates a map of that world and that the mathematics of creating that map not only um uh reduce the world and is a mathematically um efficient way of describing how to make the how to get the most signal for for all the information that's being received but that it also um it it also um enables like I was describing earlier with the understanding of the bicep a more accurate way of sensing the world of um orienting to the world and um and of responding to the world um I um I should have uh shared my my slides instead of trying to talk through them but I'm going to take a a pause and ask any any questions or shall I proceed yeah uh I'll ask one question and then looking forward to slides DS wrote there has been a lot of talk around GDP not being a good enough metric of national development has your research informed you on a better metric if so does it have a working name so yes and it's a different um answer than a single metric so the metric a single metric was one of the problems and the fact that it was denominating currency was one of the problems so one of the things that uh has shifted um some of the work of uh the UN of of really a group led by Bill Bowie and Ralph thurm on um on what are called contextual indicators are that um are that instead of an indicator being um GDP is both currency and it's um it's set to a single the denominator and the nominator of the indicator are both set in in the same thing so um let me describe this in a in a different way um there has been a diversification of what are the things to pay attention to so not only um dollars but also subjective experience of well-being for example better predicts the outcome of Elections than does Financial income and so um the idea that dollars are are predictive accurately predictive of um of outcomes as been often relied upon and turns out to be U maladapted right and so that's um whether whether you're talking about biological phenomena whether you're talking about social phenomena there's a whole range of extra indicators that are now seen as um not a single one but but better visibility into these different nested levels of of Life the second what I would call breakthrough type of indicator Beyond GDP that I was describing denominator and uh not numerator is that um most organizational reporting for example is is um for example um Starbucks might say we are using 10,000 gallons less of water than we did last year so we are improving um our consumption and the way that we are related to um The Watershed that we might be related to and and all that um reporting is if you will individual Centric and when you Center your just like as as with happiness you know where if you're just looking at the at the individual you're missing an important part of the picture when when measuring your relationship to resources to Commons to um natural Commons and social Commons um if you if you put your um uh individual Behavior within the cont context of the collective so let's say Starbucks says uh we used 10,000 gallons last year out of the total Watershed then you can begin to draw comparisons about how everybody is using the Watershed as a whole and how the percentage the percentage behavior of of usage of the water has shifted that can be compared from one to the next to the next so if you will you're putting these indicators in a in a similar language in a similar uh they're speaking in a in a similar base than um than when they are um reported individually and so you know when when looking at at well-being it's it's important not only to to add additional things Beyond money but it's also important that these indicators be um set to the same um denominator or or you wind up not having the visibility that you need and there's a huge cat fight about this because uh there have been a whole series of um of Privileges and a whole series of um social practices that are grounded in the way in which um uh individual corporations are um are structured and so this this kind of points to a different kind of natural meme that I wanted to bring up that might be of of Interest so so um we talked about how with happiness um you know we we're doing it individually and that we're missing the relational um when you look at uh cells that um are part of an organism if they are traumatized uh they become more individualistic they become uh they consume more they reproduce more and they grow into what we call a cancer because their if you will relationality is traumatized and they become more and more selfish and so there's there's both a huge importance of paying attention to relationality at the level of the cell at the level of the individual human and some would say at the level of clusters of humans such as organizations and that with cancer part of the um new form of treatment has been to call the cells informationally back into relation so instead of punitive burning those cells and killing them it's to remind them of their relatedness to restore them from a traumatized State back into relationship with other cells and they begin to behave again in better relation to the body around them and so that metaphor speaks to the importance of how do we call each other back into relation if the language of individualism is meeting both our individual Behavior and the language of Corporations is equally um traumatized because originally when when uh corporations were first formed in the 1600s and uh in the Netherlands there were really people coming together to venture and do together what they could not do alone so trading with um the Far East was was something that no individual uh Capital uh no individual business owner could could afford to venture but by pulling their funds they were able to do and they were able to become a great trading power globally this model of groups behaving as groups so a different cluster of um of uh of existence groups um and then behaving um pro-socially with each other is what grew the next level of um economic performance and econom IC um um Effectiveness um however over time some would say that organizations have become traumatized and have become disconnected both from the places where they originated as well as from all the stakeholders that they were serving and so for example here in the US when when the Supreme Court last century two centuries ago now almost decided that financially Financial stakeholders were the foremost important um stakeholders in corporations they distorted the relationality of of Corporations and so it has become a kind of cancer that that has become extractivist both upon the its its employees upon the the the ecosystems in which it inhabits by architecture so there are plenty of people that will tell you in in an organization we want to be more attentive to these um other considerations Beyond Finance but but we don't even have permission because the agreements of the corporation have evolved from mutualism to um to selfishness if you will to more consumerism more productivity more Surplus and those all have a place they're they're they're important but they've become like like the cells in the body that become cancerous they've become overly um overly uh they become individually uh oriented alone and and not sufficiently skillful at at mutualism and so this all comes back to a new basis for equipping organizations to pay attention to how they're interrelating within between and to things beyond their current purview and it is as much of a disorienting and challenging uh um uh bit of information as I was just as I was sharing the idea of learning how to be more skillful at happiness was to me personally you it's like oh I have to pay more attention to you to for for us to be happier or to be the third degree of separation as I was describing earlier means that not only the people to whom I'm directly connected but that really that it's my dumbar number the dumbar number being presumably the 150 to the 250 people that you have meaningful relationships in your life the dumbar cubed so it's beyond your direct sensory observation and it's like how do you co- Haron eyesee across groups how do you sing to use a different kind of language songs in common to which you're Co cooriented and um and these are some of the things that um active inference provides a mathematics for how this works you know that is fascinating of how the synchronization of the individual to the group and the nesting and and how the signaling happens across the nested groups and so it's it's um it's fascinating the mathematics can be daunting but the promise of it and the ability to do these things more skillfully and reliably is just it gives me goosebumps it gives me goosebumps and this is part of um why I'm trying to tell this somewhat ineffectively I would say today in a story Manner and through experience so that people can pull on the intuitions that they have the memories that they have and say I dare to tackle this uncertainty building upon these intuitions and feelings that I have and corporation that um that has as part of its founding documents when you when you plant a seed and you use the template that says we are we are organizing according to these rules even if you want to change it's it's like it's a lot harder to change the Constitution then it is some minor policy in a back office in a you know in your in your um you know in some app that you might be uh writing the code for you know and and similarly it's important to to incubate um uh organisms that have the relationality the mutualism and the holistic perspective codified into and and the intering codified into how they behave individually inter inter clustering and then in relation to the commons that they might serve and so before I take a turn into the case for um the experiment that I was hoping to invite people to uh do together any any any more questions popping up sure I can ask some short questions then slides and invitation okay lightning round okay Arun asked isn't there a cyclic dependency of economic factors and happiness for example High inflation made everyone sad over 2024 so there there is um uh there are um time lag factors uh that impact um uh when we how the economy operates and how individuals operate and these are not necessarily in sync one of the things that um that uh I found from the um from the science of Happiness was um more effective ways to carry the suffering and so um some have that this change that just happened politically for example um is a Bad Thing often when things break open there are whole new Realms of possibility to to be had and so um happiness is not just about having all positive emotions it's about taking well-being is the word actually I should be using is about absorbing all of the signal that the universe is is giving all that's coming in and then optimizing within that and so um the breaking down can also be a breaking open for what there is to to uh become and so that is not to say that there is a lack of struggle or pain or friction one of the great intuitions for me um that that arose from from studying people explaining active inference was um how things emerge and the person explaining it was using water and uh they they were describing still water as being the way in which current conventions of how things got done and how people calculated so if you're if you're in Still Water if you're in a lake and you're trying to get across um the way you Rod your boat or the way you swim is all set by the conditions of the water in um in flowing water um even though the those that same water has the same physical properties uh as still water weighs the same no matter whether it's moving or not um when when the friction of the water over rocks um creates new dynamics that emerge it creates whirlpools right and that those structures um that persist um you know the creation of order out of disorder um allow for the emergence of also other meta patterns that are again described by informational mathematics rather than the physics of the water itself and and this is this is again trying to get it what is so hard to Intuit it it's like oh these the whether whether it's a single Whirlpool coming into existing or whirlpools inter World pooling into into greater movements that um that these frictions are also creative and so to really um be willing to settle into the into the dis into the what you were calling the cyclical disconnect in a symmetry um as as something that is a signal also that can be driving learning driving better calibration um can be driving um new learning uh because at the heart of all of active inference is a mathematics of continuous learning um yes I I love that just flows like water with our previous sessions okay next question and then back into the flow we go social Design Studio wrote so curious about the transition away from individualistic and mechanical into Collective and organic where is the most levered locality to start well that's a a great question and um here's um a way in which I would answer this in the in the world of project evaluation so there's a point of view um in the world of systems thinking there's a certain order of what to uh what are the you know Donella Meadows is well known for for uh the way in which she created this categorization of ways to intervene in a system and um uh in the world of evaluating um projects and programs complex projects and program over time uh there's been some some meta studies done about what are the things that best predict uh that if if the supervision is done uh attending to certain factors what are the things that seem to produce effective and positive results because we're talking about complexity you could pick a thousand things but in in this research they identified three things and I'll add two the three things that they said you know when people look at the following things they they tend to not only manage what they thought they were looking for but also what emerges in other words managing the surprise and the unexpected more effectively and these three things were looking at the inter relationality surprise surprise right we're talking paying attention to the interbeing and the interbeing across not just the individual or whatever whatever our our individual unit of focus was originally but but who are seeing the system more holistically second um multiperspectivity um and the way that I in my the way I uh describe this for myself is um I say looking within looking between and looking beyond having one or more um disciplines and perspectives and ways of knowing about uh those those lenses of of attention on what is happening um uh if we look to if we look within we often become a better compass for our own lives um uh if we um if we look between we often develop and harness the intelligence uh that collectively and the Insight that we have and when we look Beyond we often detect uh patterns that are beyond our Collective purview that might uh um break open and um enable us to to transform in in powerful ways so again to return to that second Point multiperspectivity um and I suggest with those three uh lenses which is what I call the me to we to all you know um and it's it's kind of the label that I'm putting on on this invitation I'm going to bring up later um is um is a is a good starting point is it enough no but it's a great starting point and then thirdly and here's where it gets messy and is the non-answer to your original question is um scope definition um because the moment you pick what are the boundaries what is my mark of blanket you know what what what is the what are the boundaries of the map that I'm going to use as my lens you begin to abstract everything else into what fits in that lens so so if you say we're going to have this conversation in English the way that we everything we describe will be Den dominated mostly in nouns instead of in verbs and with Concepts that exist in English but don't exist in other languages and so um the attending to what are the boundaries that we will use to focus is is a gigantic mechanism for seeing um how you're holding your attention and so again it's not a fixed answer but it's a way of attending to perception again repeating relationships diversity of perspectives and scope definition uh the other two that I like to add in that um we're not part of that research so that research by the way was um a 100 Years of developmental evaluation um you know hundreds of studies and and so it's very robust and you can you can go to the bank that that's a good place to start building your dashboard or not the dashboard your conversations about the dashboard um the other two that I would add and and these in part come from active inference and from other systemic view places and Views is looking at patterns how are we you know what are we clustering and looking at flows you know um what what are the core flows are we looking at energy if we're looking at biological systems are we looking at the water flows you know life organizes around water um so do do we understand what are the core flows of information of energy whatever uh in the system that we are observing that that that gives us a a key insight into the way in which causality unfolds um and causality uh organizes itself around and that it's not the mechanistic you know geometry that we we may have intuited in uh for for the longest time but that by looking at Source by looking at flow we we develop a better compass for for the world thank you it's it's awesome and the live chat is super lit you'll see it after um in the last 20 minutes do you want to share your slides and open the invitation letter yes so uh let me um switch here to share um share are you seeing my screen okay uh me so um the invitation uh was um was to test a new way of organizing and a fair share Commons is a kind of um a mutualistic co-op that um first originated in the UK and has been endorsed by the financial I I I should have read it down here because I can't remember like the finance trust accounting Board of of the UK it's spread through many countries of the Commonwealth of the last uh 15 20 years and it's primarily an adapted form of organizational structure that is intended to both cultivate the practice of mutualism within the group so it's kind of like a co-op um in that everybody has power uh to to participate it's self-governing um and that it attends to the relationality of the founders the workers and uh and external whether they're investors or they they self-organize it's really uh there is a a pattern language about forming co-ops and one can use that pattern language to to um to to form these Commons and that these whether they are to create create Ventures um or to create um full businesses that the pattern language is useful for that effect um and so as I described earlier a kinda Linux um you know it's it's intended to grow the commons but also to support the application of those Commons and build the organizational traits uh for successful Ventures and what I am uh hoping to find allies uh around um oops let me um so it um the these are again suggested and there are many ways to to go about it but um you know there there's certain core agreements that differ from if you will the extractivism and the foundation is uh relationality um which drives mutualism and symbiosis um and uh the the invitation um is to uh um is to constitute one in service of the active inference uh Community for growing both the commons and the capacity to um to prototype and develop both the the experience of mutualism but also projects and that uh there' be two starting experiments um the first around prototyping uh agents for accelerating access to money so often um folks incubating an project are looking to find funding for that project um find uh funding for what that project might serve and often they're people willing to fund those projects and the idea of writing up what are the sources of different funding what are the sources of what what are the needs that they have that they're looking to fund how do I um discover and engage that um funding is an important agent that can be used in common after common after common and so possibly developing a um an agent or uh there are plenty of methods but but agen toying if you will uh that component as one of the key flows you know that in business the flow of money is is an important element to to inactivate and and that the second is about um finding um resources of a different kind competence and complimentarity and um what are the what are the components often for example a uh uh the active inference Community has a wide diversity of competencies um the but but it is highly highly Technical and so to engage with people who who um have complimentary sko skills um uh and and um and to support the accelerated discovery of allies not only based on competencies but also on um uh other holistic considerations is um is the attending to the flow of mutualism um and so those those three um actions is kind of a starting point is um is something that I would like to um I'm I'm already in action on the uh with with other groups um and a variety of groups on the constituting the legal structure of a of a fair sharing Commons um and I would like to enroll um the active entrance Community to become an ally with with kind of other competencies and other types of mutualistic eco-minded socially minded groups to grow um the capacity for mutualism and its early stage I literally um this this is germinating you could say and really would like to um would like to find allies who who knowing that we don't have all the answers but that we're willing to grow not just the methods but the relational soil um is um is what this is really about so um yeah I'll stop there awesome exciting I can ask some questions from live chat and we can explore just a little bit of a shall I leave this on the screen or keep going sure you can leave it up I'll ask there's a lot of great comments I'll just start with the direct question so people can add more in this last 15 minutes okay social Design Studio wrote does the commons use emergent hierarchy so as to accommodate contextual expertise and thus relational power so um great question and um there are several definitions of Commons um but I'm going to go with elrom she won the Nobel Prize for it and um and she she basically defined it as it's whatever Mark of blanket people choose and then self-organize around so so um there are many variations and um and in order she she identified eight principles for common to not just be a nice idea but to be able to persist sustainably right um just to do a little sidebar she she was trying to make a business the reason she won the Nobel Prize is because in the profession um everybody was explaining most of the economy's activities as either being driven through hierarchies or through marketplaces the Dynamics of those two Collective groups and she said there's a third kind of group a common and it has the following properties and um the socially mutualistic uh side of the world was just in love with her and this is why she went the Nobel Prize because she developed not only a body of Science and a series of eight principles that predictably describe those that succeed these eight include you know a sense of boundaries that they have in common and whether they decide to self-organize around a fishery whether they decide to organize around um a data Commons around uh a knowledge comments that is up for the group to to decide and then um the the way in which they co-regulate um she did there there were many variations but but basically there was a um there had to be agreements on where to place shared attention and as people diverged from the Norms that there were ways to call them back into relationship and to have escalating um consequences if people deviated um now all of that sounds lovely but um but that was not enough either there had to be the ability to enforce one's boundaries not only internally but externally and so I'll give you an example um I I uh have some First Nation friends with whom I've been meeting every Friday for years to work on decolonizing our minds and they have a point of view about a piece of land that um that is their Garden of Eden and sacred and how it should be used or not be used and they use all these principles but they cannot enforce their agreements on other people and so they really what they have is a wish list I mean I know that's kind of rude and they'll be very pissed to hear me say this but without the capacity to enforce your your boundaries you um you don't have a Commons and so um this idea of uh how enforcement happens and whether that is done in a hierarchical manner or whether that's done by self-organizing you know like when when uh in Valencia recently there was a a a a gigantic flash flood you know the government um did a very poor job for the first week and the response was done by self-organizing ant behavior of neighbors helping neighbors coming in doing whatever they and that happens in crisis after crisis after crisis and um and so there is a a capacity to self-organize if you will and get things done um often what a government structure can do is help prioritize bring order to the what are the most pressing needs how to discern what will make the biggest difference collectively and so there is a value to having a a coordinating role but um but uh you know how that emerges or evolves or unfolds is um can either be by by cultural precedent of what the rules are or it can emerge through and from the group so um so that's my long comment about Commons and and commoning and how to be in relationship to the common awesome I I'll read a comment from Scott even before you mentioned Ostrom Scott had written comons Elanor Ostrom is a co-management regime framing nicely consistent with active inference in human organizational contexts C Scott Shackleford at all at you of Indiana for Awesomes Commons programs analyses have historically been focused on quote asset based Commons this nicely anticipates Commons co-management Beyond Asset Management I.E risk management skills coordination Etc y i I should have made the point that um one of the things that Commons do that uh that the marketplaces and the institutions are not been able to do is for example it is financially rational to to fish to Extinction to deplete the forest because the price just keeps on going up so so so um the logic of markets is insufficient for sustainability and this is why organizations that don't have a logic of of of that friction of of um of considering other factors other than profit maximization will inevitably um deplete the resource um and so what she created with this um case for Commons was that there were sustainable organizations and that these existed both in indigenous you know in in original people's cultures they exist uh throughout developed economies they exist in a variety of places and are unrecognized and what what you're hearing this in this invitation is like let's let's let's use commoning even though it's unfamiliar even though it's different it's actually much more widespread than we ever have experienced you know it's it's like believing that there might be something other than McDonald's you see McDonald's everywhere so you think I have to eat burgers but there are other ways to eat than McDonald's and not trying to pick a fight with h fast food but how are we going to carry this forward into 2025 and Beyond yes so um so the invitation to join um a conversation is is the beginning one one of the Allies is U that is both co-incubating other Commons and that I wanted to use for if there's energy here is um a a group called uh evolute 6 based in Germany and and uh South Africa and UK uh it's a small firm dedicated to incubating uh and helping Commons type organizations grow um it's kind of the we're uh um and and so uh trying to both invite the interested to begin join the conversation sharpen uh a vision and an understanding of what it is that we think we can be in action on uh what is it that how that active inference can be applied to particular problems the search for funding there might be a better way I had to I had to scramble to get something plausible but if as we have conversations we might very well identify much more um important points of intervention with the question that somebody else was asking earlier we might decide that there are certain flows and certain dynamics that are in fact a great a better starting point than what I imagine right now but uh the invitation is come let's convene and do this in a systematic way and both be transformed individually interpersonally and ripple out into the world awesome okay few more questions while we're still here Frasier wrote I'm not sure what this language of trauma means I'd like to hear this elaborated yes excellent sorry um I forgot to explain that so um uh lately in Psychology and in organizational psychology uh there has become a modified use of the word trauma and um and what it refers to is the equivalent of memory in in computation so you can have a traumatic event and be traumatized by it and then the trauma is as as I'm using it is the memory the scar that's left either in you or the um if it's it's a if it's a collective trauma that has a different um prop I'll describe it in a moment but but when I say it's the memory the scar is what remains in you and how you carry that whether you heal fully or whether it becomes a a debilitating Dynamic is is when the word trauma can be used to describe it Collective trauma is um and this is an important distinction because sometimes the same language can create confusion um uh when I'm acting as an individual cognitive system I am all the trauma AA happens within my body so I feel something I fix something or I have a story about something happening to me all of that is personal trauma if I'm part of a hive the the um hiving is happening in the signals that we are sending back and forth to each other and so um the uh the this the the language of how we signal and the meaning of what that means is what can become traumatized and let me give you a simple little example uh and and those are called trauma agreements ways in which you agree to carry the the significance the pain the feeling about the situation that you're looking at so so my father took me fishing when I was 8 years old for the first time and we caught a fish with fish and Hook and the fish was flapping and resisting you know and I was kind of shocked I'm like oh my God that fish is suffering my father's like no no no no no you just grab you know whack them on the head knock them out and they don't feel pain and in that moment I entered into a trauma agreement a different understanding of how to be with what I was observing and as a group we would talk about the our reality assuming that shared trauma agreement that shared way of of understanding that same kind of dynamic whether it is how what it means to be a certain gender what it means to be an organization that is um beneficial to the world is it because it makes a profit is it because it makes a contribution there's a whole series of assumed agreements that that it's like oh we need to surface them that they're not the truth they are an agreement into which we stepped and we're now willing to re-evaluate and this is both the the the gateway to disorientation but also transformative adaptation um because it changes what has been uh a habitual way when when when I started considering that maybe animals are still an omnivore but when I started considering um that maybe animals had feelings I had to start thinking differently about what had been my ordinary eating patterns and so this is a language for surfacing um anti- symbiotic ways of being that we are now developing a pattern and a way of recognizing them and renegotiating them awesome great answer last question musicm minded wrote how does Michael see the future of businesses or more simply do you have Exemplar businesses that are commons type in your view yes there's a whole range of Commons businesses and and and so even though we're talking here about incubating a new one in a sense um I I believe that the the important work is about what I was describing about treating cancer calling back into relation and so um some of us and this you know if there's interest in this I've been working with with global standard setting groups about how to measure and monitor well-being and then how to so there's the sensing it and then there's the what do I how do we digest it you know and this was another benef another you know benefit from from active inference was the way of framing the cognitive process not just think but sensing integrating predicting responding adapting and and really helping organizations think about in that cognitive flow in that Collective cognitive flow how to recalibrate them their their attention so that they see more of the world with with a denominate a planetary denominator rather than an organizational denominator and how do they then more skillfully inter relate with the different factors in which they're they're they're engaging perfect thank you Michael great presentation a lot of people appreciated it in the chat so see you next time thank you thank you e e okay welcome back everyone we are here in part one of the workshop that John clippinger and our partners at first principal first are coordinating so with that for the next session please John to you and take it away thank you very much um I would like to just to share a a deck with the the group here if I could um and uh and provide a sort of an overview of of um what we'll be talking about in this particular session can you see that now is this is this good for you okay great um and we'll be anticipating with Andrew is uh sure will be coming on and providing the tutorial but what I would like to do is in the first sense of this is is being able to provide an overview of why active inference uh is important why active inference agents this whole perspective that we're taking here is worth people's attention and pay uh and being concerned about um and what it has to offer to address some of the the the the limitations of the current U AI systems example large language models and some of the uh agenda architectures have come out of them um and and in that in that context what I looking at is said what is the current state of large language models and and agen uh and agents and I think you there's a lot of activity that's been talked about but there there's still no Mission critical applications yet there's a lot of an exploration you have people like um Salesforce developing little Ein Steins that can do uh various kinds of uh workflow automation you have co-pilot and Microsoft but it's not it's not it's not really core critical not to say that it won't be but it it is still uh it is still on the experimental phase and it's still a back box we do not know real lot how it works hence you have issues of data Pro Providence who have a question of how to govern it um and there's more recently there's a concern about you've invested a trillion dollars but there's an unknown business model coming out so there's a lot of money that's going into it but it's not clear that the current framing of uh of uh AI um as is now known is really is going to be viable and part of the issue here is is that we and I love this example from The New Yorker is like who's modeling who we're actually putting data into the models we're tring representations and we have that interaction but there's not necessarily an abstraction that's coming out so it's just it's kind of this uh relationship of of seeing ourselves mirroring ourselves and going into this sort of cycle um and what we have now is a certain hype cycle so it's around the peak of the this is 2023 from the uh Gartner group and it's the generative AI is at the top of the hype hype cycle and then if it's hard to read this particular uh graph but you'll see that principle AI is is uh basically um on the early cycle it's on the early phase of it but it's not seen as taking very long where something like uh you're going to have AGI is seen as being like 20 years out it's the top of the hyp cycle I think if we update this there now I think it's starting to be appreciated that the generative models and the expectations and the kind of problems and the cost associated with them and the lack of improvement the the the lack of degree of improvement the successive models are starting to show through so the the Next Generation open AI shift strategies is a r of GPI improv and slows other words the new Orion model is not did not have the level of improvements they were expecting I think that is something to take in account um and hence the cost of of of training and inferencing is extremely high and and with that it the the M the fact that you have this enormous consumption of energy uh you the Microsoft is going to have to recommission a three mile island and drive one of their own data centers this shows that the impracticality of having to um have so much computation um in order to have work and make these models work and I I think that this is starting to be recognized within the the the limitations of large language models and sort of the the the parametric approach to to uh modeling uh AI modeling and I think to to start to step into what what we have now what we're looking at I I love this is another a cartoon from the the New Yorker and since says to think it all began with letting autocomplete finishing our sentences and well autocomplete is a sort of form of Markoff chain so we are we are having the sort of chains on top of chains that we've been able to to develop some pretty rich representations of of of human uh activity text and actually forms of cogn but there's not an underlying model there and so what we're looking at there may be some foundational issues that are coming up in in for large language models and that the fact that it is a black box is not explicable uh it locks internal causal models um there's not a way of reducing the complexity it's sort of a batch processing it's there's no real-time updating and that batch processing we see is extremely expensive and and so we we we're caught on on on that that issue um it's not atic it has no no no means of a spontaneous adaptation and correction on its own um and it's doesn't know what it it doesn't know it has no way of representing its completeness of its knowledge and and adapting coherently to that and and and also I would say it's not it's not derived from sort of fundamental scientific principles it's really sort of an engineering phenomenal on a caseby casee base uh basis now this is changing um and I think the same issue if you start to see foundational issues for multi-agent uh llms in other words the transition to to making agentic architectures where you can um create a particular agent that's optimized to do a particular task or role a lot of the success of that agent is depending on the quality of the prompts of that and sometimes those prompts are good sometimes the prompts are bad but there's not a really principled way of doing that currently and so they have a very fixed way of of of and specialized kind of goals it's sort of a a a mechanical it's it's so the instantiation of an industrial model um that we see um now what we we rather than having a a a sort of biological self-correcting model it is these fixed static models now what I would say the active inference difference here um is is that it's really developed from first principles and I think and and I think you're going to see the Andrew is going to go into this much more detail but it really Coss it cuts across different disciplines that themselves have been maturing very quickly in the last two or three decades and so it is going in basic basic physics Quantum information Theory the whole idea of computational Science Biology and cognitive science all these are intersecting into creating a new way of looking at what intelligence and life are and how they're interconnected um and it also builds in wellestablished statistical methods including mark off blankets blazing belief models so stff the work has been done with JD a pearl um and basically what you look at active infine does and and just to highlight is it codifies a predictive brain thesis that's been nurturing in in neuroscience and so the pr and then that in turn sort of mimics the processes of the scientific method and when I get so we think about what the method is that you're you're trying to codify and and turn to Richard Fan's comment well quoted comment that I can't what I can't create I can't understand so there's a new kind of Criterion for what it means to have a um to understand something is be able to model to create a digital twin if I can recreate something then I understand what it is and if I can replicate that behavior and that that's sort of the the the principles that you find in active inference from the very beginning is ah how do I grant a generative model how does a living agent have a generative model of it s in its world and this this really owes his work his Origins back to to uh the work that's done with J Pearl on his understanding causal reasoning and also Markoff blankets you how does some what is an autonomous thing and this is where Carl Fritz anal work how how does some maintain its separation from the world in which it is in and so it is not governed by that things but maintains its autonomy and there's a certain kind of topology there that in some has certain kinds of inter internal States it has external States Observer States and action States and there so there's a lot of deep Concepts that's in it I'm sure Andrew will go into that much more detail um and then what we have is is the active invers agents themselves as sentient live things so you're creating something and there very different than I say a large language model you're taking something it has agency and it has a representation of itself in the context in which is trying to stay alive um and so it's self-organizing it's self-healing is self-replicating and it's symbiotic in the sense that it evolves complexity Mutual synchrony with other living things in its World in its Niche it's a joint Niche construction uh hence sentience is is sort of a is a kind of situational intelligence an awareness that's granted in the survival of that particular agent and the person that's sort of the the context of the person who's really behind this is Carl friston and and um my own experience of being someone who is very interested in sort of an artificial intelligence from from many many years ago and self-organizing systems of cybernetics it really hasn't been until the last five or six 10 years that this has come together in a coherent uh not only coherent Theory but a whole coherent formalism um and through what what car would call physics of living things and he's one of the most sided sinuses in the world it's a neuros sinus that's done um basically developed a reputation neuroimaging and inferring the structure from the brain from Neuro Imaging uh and he applied the free energy principle which we'll go into more discussion to later um and so one of the I love this idea of a tree foil knot it's sort of like okay is it is sort of this autocatalytic biotic system it's really important it's a living system and it has certain kinds of characteristics that are all intertwined with itself one cycle synchronizes is exception or it's a representation of the external world to minimize uncertainty the other does it internally it has to align that with the external world and then you have another cycle that has expected free energy minimization is makes predictions about how to change itself in its World um and so what we have here is basically a a a what I think active inverts all offers is a really a soundness of grounding in bi in biology and physics and so you have a sort of a ground truth of evidence base that do I make predictions and do those predictions allow me to act and survive in the world um so it can be explicit it can be explainable and and we'll see we'll talk about this later but you can have since it has cause of models and language in knowledge graphs it can explain why it doing what is doing and also it can talk about explain why how much it doesn't there's degrees of uncertainty associated with it and there's also the potential of self- policing self-correcting mechanism and I think this this is really important in other words you can design a active inference agent to to live within certain constraints and Bounds and you can build in certain me me mechanisms that have for self-governance that cannot be captured uh so it's actions are within the limits of its designed Mission and not to have generate negative externalities so this I will sort of leave with this one and then just to quote William Blake I mean who talks about this a poetic form is a fractal nature reality see the world in a grain of sand and heaven and a wild flower and hold Infinity in the palm of your hand in turny an hour this just sort of pulls it together in a more poetic sense so on this I would hope to uh hand off to uh to uh Andrew if uh thank you I need still great thanks John um and so hi everyone of me and Drew um Andrew pase I'm uh currently involved with the active inference Institute it's been great partnering with with uh Dr clippinger with the director of The Institute you know we've we've been doing some really exciting work lately um and I want to address a lot of the points that that John has made although most of that will come tomorrow when we give some major updates on our biofirm so today in the spirit of like an initial workshop and kind of introduction to active inference I'm actually going to refer back to uh this tutorial that I gave a couple months ago uh at a International Conference for computational social science meaning folks who are involved in sociology economics uh you know a broad range of fields and and uh that the context was applying active inference a lot of these principles that John is referring to to social science research uh and and I I basically make the the claim and the argument this is already being done so I'm not necessarily providing a bunch of new information but it's just to kind of extend the dialogue to computational social scientists like here's how we can apply active inference to you know your particular setting in different ways so uh we'll just kind of cover some of of what I went over and you know I olizee if there's any Whiplash from how quickly I go through these slides um but we're just going to move a little quickly um so the context again of this this uh this tutorial that I gave was it was split into three parts it was a 3.5 hour Workshop where we actually we started with talking about agent based modeling you know how it's traditionally been done and then followed by that something that I would call a cognitive turn in agent based modeling that's happened over the past couple decades as you know there are these kind of call SC you know we have the computational resources now we have an immense amount more of you know Empirical research and and validation of different kinds of principles related to human cognition coming from Neuroscience neurobiology uh you know cognitive modeling uh studies of neuronal Dynamics uh organisms in their environment all those kind of things so so there's a shift to that and then uh the most recent you know means of trying to establish that the state-ofthe-art area is uh using reinforcement learning agents so we'll touch on what those are and kind of how that works and you know anyone who's kind of been looking at the the agentic uh framework for for what's being done today they'll probably already be familiar with reinforcement learning then after that we'll go into AC conference and how that kind of kind of bridges together a lot of these different things um and apologies I want to just mention real quick uh you can actually go to this this link uh the quickest way of following that is just by going to Google type you know my name Andrew pase and then you could type GitHub or ic2 S2 in any case you'll come up to uh the landing page for this you'll see links to all previous slides I've made uh slides that I'm making for tomorrow uh I'm also making a lot of this Open Access uh so so there's code that people can you know play around with and things like that um so agent based modeling uh this has been done you know this goes back to roughly 70s 80s with a lot of you know uh initial developments of computer technology and being able to realize that oh we can we can simulate uh people right in some loose sense in in some kind of simplistic sense uh how can we simulate people and why would we want to do that we would want to do that so that we could maybe imagine uh hypothetical scenarios imagine you're designing some kind of policy or want to test a policy uh and see you know oh if I recreate that in a simulation and have people in the environment where I'm implementing that policy uh what happens what's the collective outcome you know what occurs so all kinds of fascinating work from uh shelling and the the traditional way of looking at uh housing segregation for example th handling those kinds of problems how where do people land where how do they settle you know in in uh like a residential environment uh even more interesting work later after that Santa Fe Institute recreating an artificial stock market uh where you where you have agents who are basically interacting with this kind of financial environment where they're looking at stock prices changing they make choices to you know buy or sell Etc and see how that plays out Recreation of housing booms and bust excuse me not housing Financial so this can be applied in many different ways point and so what what is agent-based modeling you know in its fundamental principles and how does that relate back to active inference which we'll get to um key components of agent based modeling is that we have agents kind of the catch word of the day um traditionally agents are you know that they just represent some entity uh a living organism you know this this can be used for for biological and Zoological studies of trying to understand you know how ants or or insects you know interact in environment or at a broader scale how humans and entire you know societies and cultures might interact with their environment and and so what's really important about this with agents environments simulations is that know we just kind of run a simulation over a series of time steps what and so we let Dynamics play out and then what is it that that composes those Dynamics it's the assumptions we make about agents what is their Behavior what do we think that they do you know how do we program them in this way or in that way uh and then also what are the aspects or characteristics of the environment you know and so again given that a lot of this work was done starting in the 80s going forward with with with very you know we nearly archaic at this point uh forms of like computational resources it's like things were kept as simple as as possible and there are a variety of guidelines that need to be used to develop agent-based models for running these kinds of simulations uh you know and so so some of them even running up to to 2007 here this is the work of of doctors laser and fredman who are respectively uh have been involved with Northeastern University in Stanford um you know that they give some guidelines for how to do this um guideline one and two uh in some is you need realistic uh assumptions that you're making about your agents but not too realistic in the sense that they're too complex one because we don't have computers that are Advanced enough to run highly sophisticated complex agents and then also you know if we make if we make too many assumptions then the situation won't generalize well you might be making an agent-based model for one very very particular scenario where it it's not adaptable across different real world circumstances right so we need some kind of balance in terms of complexity of the assumptions we're making and the Simplicity of the assumptions we're making the rest are a bit more plain which are one uh reproducibility you should publish your code you should document it very well other researchers should be able to kind of take up the torch and be able to experiment with your code um you know we at the Active inference Institute we're constantly putting out new GitHub repositories and documenting or processes so that other people can come get involved so I just you know want to mention that there's is a big problem in reproducibility in science today and it's very well known you know so uh just wanted to touch on that the non-triviality it's like why do agent-based modeling uh if it doesn't help us uncover anything you know why why not just sit down and have a conversation about you know what what do we expect to happen what are our thoughts about what's going to happen why you know do all of this um with with setting up these programs and and agent based models and designing them if we can just come up to the same conclusions through speaking so you you want your results to be interesting you want to make realistic assumptions that you know we can kind of have all these things synthesize um together so I made some points about balancing Simplicity and complexity this is just a representation of uh you know this these These are common problems in machine learning and deep learning you know you you go and get uh you know University education these are the core components of all of your courses you know this isn't specific to active Ence is like wide ranging you know concern how do you uh create a model that is realistic enough to capture uh and and explain the en the phenomena you're looking at um but uh simple enough to where it's adaptable to different circumstances you know you know these are kind of in part uh you know just as uhh Dr clippinger referred to Richard Fineman you know just some reminders from you know famous statistician George EP box you know all mod all models are wrong but some models are useful right we're always trying to approximate a reality and so the question is how do we come closer to approximating uh our reality and so how do we actually you know using modern computational resources using all these new advances and all these fields how do we actually make you know new forms of what we call cognitive agents who exhibit the kind of mental Dynamics and social dynamics between them that actually better approximate real human behavior so I give some definitions the differences between traditional agent based models and what agent uh assumptions are are being made and uh more often than not for the sake Simplicity an agent is pretty one-dimensional if they observe X then they do y otherwise they do see these very simple pre-program rules they're not autonomous they're they're kind of like you can think of them is like a thermostats or something like that right the temperature is too high you you turn it down if it's too low you turn it up um that kind of reactive framework uh there's no beliefs about what's going on there's no internal goals per se aside from a single homeostat that is you know maintain the temperature at what you set um cognitive ages however the expectation for those this throughout the the recent uh agent-based modeling uh literature is like how do we achieve cognitive ages we exhibit internal cognitive mechanisms they exhibit perception action they actually learn and update their models I'll make a point that you know as John referenced with in the case of LMS which I'll cover much more deeply tomorrow llms do not I mean it's so expensive to train them and you you typically just train them once right and so um they don't learn in real time they're kind of stuck with the knowledge that they have they have a lot of it to be fair they've been trained you know thousands thousands thousands of documents and so on um but but that's it like that's what you have and then and then we have to figure out how to work with LMS from there with cognitive active inference agents as well as other kinds of cognitive agents the idea is that they can learn in real time meaning in the moment and adjust their expectations their beliefs to what's actually going on now um and then also planning so the capacity for being able to infer or guess using evidence that they take in like what's going to happen in the future right and that's immensely important in today's climate right where we're that I mean that's in part the reason we're doing HB modeling is to attempt to plan to test things to to come up with our own inference about what's going to happen in the future and then act in actionable ways to to realize our own goals in the future right so that's that's you know ideally that's what an agent would do and be capable of um and then then with you know this cognitive turn we we're also looking at a broad synthesis of all these fields you know as John mentioned we're looking at things like cybernetics and machine learning we're looking at physics information Theory um people are starting to it's great it's it's fascinating that so many uh interdisciplinary conversations can come up and collaborations between researchers of various backgrounds because it's like oh you know once we once we realize what it is we're trying to do here we can start to pull from maybe principles or relate princi Les from other fields to this field what works what makes sense what's empirically you know what can be validated so on so it's it's it's um you know it's a very rich field right now and then furthermore a lot of traditional agent based modeling has been done using uh uh software called net logo which is very specific uh it's it's good if you want to do agent based modeling like you just learned this one programming language there's a GUI like an interface for using it it's highly simplistic it is not integr with other tools not integratable with you know llms with natural language all those kinds of things um and so it's it's very narrow you know it's good at what it does uh with cognitive agents we would want to be able to pull from all kinds of other resources that are you know available including open source ones uh being able to use apis being able to pull in other kinds of real world data not just purly simulations you know so um there's just a lot of richness behind this idea of moving to cognitive agents then there's some basic you know uh ideas of what makes a cognitive agent again perception they take in observations and they infer what's going on in the world and kind of like we what we do my stomach growls when I infer that I'm hungry for it's a very simplistic example um and then agents are are generative the sense that they actually come up with uh their own plans or you know choices of what actions to take based upon what they believe such as I observe my stomach is growling ier I'm hungry I will now choose to go eat food as like an affordance that's available to me in my environment so this is a new way in a certain sense of doing agent based modeling where before we were looking at modeling an environment with a bunch of one-dimensional Agents you know predictable agents on the inside if they CX and they do y otherwise they do Z but now we have models in a model uh in the sense that we're actually able to internally inspect our agents right by having this beliefs based framework where agents observe real data or information and then infer beliefs about it we're able to then retroactively look inside the agent and see what its beliefs are as opposed to these kind of you know very difficult to to interpret uh blackbox models that we find very commonly in deep learning and and and you know very very highly performative yet very uh uh difficult to interpret like neural networks and architectures like that with with uh agents we would want to be able to see in a in almost like a human sense like what are their beliefs that they're holding and why are they doing what they're doing so we need to be able to accomplish that in order to actually realize all the additional benefit of doing this which is being able to look at you know internal cognition what are agents learning whenever I whenever I Implement a new policy or something right I want to be able to know what's going on with them internally I want to know the full impact of of what it is you know this plan I might I'm considering like actualizing so let's bring it back um you know how how do we address all these traditional criteria how do we make realistic assumptions how do we um you know how do we make our code reproducible that's an easy one because like well we're still coding the whole time uh and then you know how do we how do we derive useful insights from um and what are other problems that are going on like now that we're getting into cognitive architectures um and and we we are starting to brush up against the potential of recreating more blackbox models right we we need to keep things simple enough like how how do we navigate all this a common problem in in social science research in general but also in data science machine learning and those kinds of fields uh reinforcement learning common issue is what is h how do we handle What's called the explore exploit tradeoff this is this is famously an issue uh if you have an agent who comes up with a suboptimal uh solution to a problem you know let's say an agent who's you know they're trying to figure something out they find something that they deem to be good they get a positive you know reward signal or positive feedback from that but it's suboptimal there there's actually a better solution exists they just don't know about it yet how do you get an agent to actually be willing to explore same thing for humans right we we don't always stick with the you know the first decent looking you know option available to us sometimes we we explore in the sense that we seek better solutions to problems and this is especially important and something to be addressed in agentic Frameworks where if you're trying to use agents for say software application or a use case like you want them to be able to find better Solutions ideally you know at any given moment especially in a changing context that might call going forward for better Solutions you know say agents who are maintaining an environment that's subject to uh natural disaster or or or or climate change right like that that might call for a whole new set of of of policies or actions that need to be taken uh you know that that couldn't be relied upon in the past so we need agents to seek new information um unfortunately in reinforcement learning you know that this is still an issue um people are trying to figure out how to kind of manually Define what what defines exploratory behavior and it tends to boil down to kind of simple mathematic of of of defining like okay well we'll s we'll we'll modify these agents so that 90% of the time they go with the best solution they have they know about and then 10% of the time they'll just pick something random that way they're at least exploring 10% of the time you know you could put it that way active inference again I'm sorry I'm kind of zooming through a lot of this call this is over three hour tutorial um but uh rein with active inference ages this kind of gets natural naturally resolved in terms of What's called the free energy principle which we'll be getting into um but basically the idea is if you're able to to have agents who naturally seek information perhaps in line with how we do right that when they come up against something they see oh there's value in seeking new information to find even better Solutions the ones I already have and being able to to do that in a way that isn't some kind of ad hoc rule of you know the 90% versus 10% as I mentioned that's that's um kind of a key component you know a key consideration in designing agents um so this brings us to the active active inference which I I describe here as a framework for doing doing agent-based modeling it uh begins with the free energy principle uh I I'll make a lot of mentions to uh this nice textbook that uh Thomas par Carl friston and uh you know their other co-authors published a couple of years ago um but you know there perhaps even some more updates to be made because it's such a lively field but uh that textbook really lays out a lot of the core principles and and ways of getting started with active inference um the the free energy principle everything that changes in the brain must minimize free free energy now despite all of this interdisciplinary communication dialogue between you know again cybernetics with psychology with behavioral economics um that actually sounds quite simple right to be able to boil everything down to oh we have a brain it's trying to minimize free energy that that that almost sounds simple enough to where we might be able to employ that as a kind of you know simpl that we made in agent based modeling it's quantifiable uh it's it's a presume presumed is the wrong word uh but but something like a principle that can be applied in any given circumstance whenever we're looking at you know the these organisms who are self-organizing such as humans such as you know living organisms such as you know plants in an environment that's being taken care of uh you know that they exhibit this this kind of uh characteristic of minimizing free energy as a as a form of quantifiable uncertainty so we'll touch back on that again in a moment but the the the what active inference provides is a kind of normative holistic framework for what I argue should be applied to agent based modeling it it itself addresses agent based modeling that phrase is used in the textbook this is not a foreign you know out of left field kind of invasion of active inference and agent based model and and proposing there's so much overlap and I think it's just an underutilized connection that's already been made right um active inference is based in you know it provides neural process theories meaning that we're not just you know assuming in the same way with agent-based modeling we might assume oh all people are utility maximizers um in this case we're not just assuming it we're we're looking at neuronal Dynamics we're looking at you know Decades of research into using neuroimaging studies and looking at how how does the brain really function how does it handle uncertainty different kinds of Behavioral Studies and tasks where you can you know record fmri or EEG along the way and look at the Dynamics of those um uh of that data that's being collected along the way and um and then from there we can start to kind of model how that works so um as I already mentioned uh we're still looking at a kind of belief based framework which is to say uh you know we're looking at uh for for agents who have beliefs and they update their beliefs and they do that by way of the free energy principle which is minimizing uncertainty U we we start we start to ex we start to see these organisms that were modeling humans or otherwise they start to have naturally emergent adaptive Behavior right by by trying to figure out what's going on in your environment and then adapting your actions in order to better understand it to better realize your own goals you're effectively minimizing your uncertainty and uh and the fre energy principle basically uh is is a kind of free energy is like a proxy for that uncertainty that that renders a lot of these things computationally tractable meaning that we're able to quantify it we're able to report it whenever we run agent-based modeling we're able to like actually look on a chart and a graph how much of free energy is being reduced uh over time you know and look at the agents's internal beliefs about you know what what is it that they're figuring out uh within their environment over the course of simulation um the view from from this perspective is that uh another aspect I I already addressed it for exploration exploitation balance and tradeoff uh self- evidencing this means that agents in order to minimize their uncertainty um one way that they can do that is is um to try and act to realize their goals there's something they want something that they expect right and so it's as if you have this prior set of beliefs of what you want to see in reality this is the you know it's this is the kind of framework that active inference deals with but also referring back to you know Juda Pearl as clippinger mentioned uh you know this is a kind of basian idea you know this is rooted in something like like basy and uh statistics or even mechanics where the idea is that you you have your prior knowledge you bring it to the situation and then that will impact how you update your beliefs based on new information right so we're not always starting from some blank State some some starting from scratch we're always coming to a situation with what we thought before and then that gets updated how does that get updated from active inference that standpoint is through free energy principle um so you know it's I made some points in this tutorial because I realized we're we're we're really uh uh you know I want to make sure we have time to to kind of bring this back um but uh with active inference that we already have so many aspects of uh reinforcement learning and active inference which overlap we're trying to create cognitive models you know active inference I don't want to say it's necessarily competing I don't know if that's the right word but it's you know it's it's it's making advances that reinforcement learning is trying to make itself so um and uh and with active inference it's like well we with active inference we're actually basing this on you know a variety of of of principles you know that again sorry zooming through you know we're able to like cognitively model you know what's what's going on in the mind in various ways that reinforcement learning is really lacking um today it's it's really lacking that it's it's uh you know these models are being built to be highly performative but they're missing the mark whenever it comes to something like realism and basing uh basing these reinforcement learnings and something like a you know science that extends Beyond just the mathematics of machine learning right we we want to be able to find biological physiological substrates for how all these things happen right uh we want to be able from there to be able to apply everyday words and interpretability so what is it that defines what a habit is what what process is involved whenever a goal is formed and made and then from there uh you know attempted to be achieved through action uh you know what defines planning what is prediction um so I just want to see if there's anything else I might be able to touch on this is a nice um you know for for for anyone and maybe I should have started with this but this is a little bit more of an intuitive glance and this is actually these are not words from uh Carl friston surprisingly uh they're from uh Sarah Feldman Barrett who's at Northeastern she's uh you know nearly as rep reputable as Carl friston in the Neuroscience field and I just personally as someone who's also interested in in in neuroscience and cognition you know I find her work fascinating she's done a lot of work on um emotions and uh you know kind of affective Neuroscience so it has to do a lot with looking at what's going on inside the mind of you know especially humans and so this is kind of trying to provide an intuitive glance that you know how can we think about Basi and Frameworks about um um inference about active inference as well so it's like uh this is a long quote from a podcast that she did your brain receives sense data from the world sights and sounds smells and so on receives sense data from the body so when there's a loud bang or a tug in your chest your brain receives that information as the outcomes of some set of causes but what caused that loud bang what caused that tug in your chest your brain actually doesn't know it only receives the outcomes it has to guess at what the causes are this is called an inverse problem you know what the outcome is you don't know what the cause is so your brain has one uh you know another source of information available to it that is past experience memory um apologies for just like doing a slideshow karaoke here but you can read it for yourself I suppose but the point is and with some of this highlighting it's like we're actually taking account of a variety of things you know we're taking account of interoception meaning observations that arise from within the body or for an active inference agent uh you know it it it it has to kind of model itself you know I mentioned earlier that you know if I if I'm hungry this goes beyond the five senses in that kind of perspective right touch taste feel all of that um with this it's like well I need to understand what's going on within my body I need to understand what's going on outside of my body as clippinger you know referred to earlier the idea of a Markov blanket it's kind of like I need to figure out what defines the boundaries between myself and what's around me you know on the one hand I need to have a model of myself in order to maintain it uh otherwise I succumb to the environment around me to use some of the language that's used in uh you know the textbook it sounds a little Bleak but you know we're referring to real world circumstances if I you know if I don't take care of myself I will naturally Decay like that is kind of the sort of the principle the world is entropy so to speak right it's like if I don't eat food you know I I I fall apart so I I need to have a model of myself what's going on within I need to have a sense of what's going on without uh and then and then I need to be able to kind of synchronize those things in such a way that I can continue to exist as an entity within this environments around me that's a that's a that's a very intuitive you know perhap perhaps not very well technically described but for now an intuitive sense of of kind of what defines a marov blanket so you know for this is a sort of foreshadowing for tomorrow we'll talk about this this really interesting project we're working on called the biofirm uh with with with uh you know Daniel and and and under guidance of of John um you know in that in that sense we're we're trying to develop homeostatic agents uh homeostatic meaning agents who you know their own sense of their survival is predicated on uh the the the homeostasis that their environments around them so we're trying to create agents who actually take care of and attend to the uh the environment around them as if their own Survival depended upon it you know this is a very interesting kind of uh kind of flip you know where where where you actually want to match and model the environment around you but also take care of it in a way that you can maintain it within those bounds just as you know I have to eat in order to maintain my energy levels you know that the agents need to take care of their environment you know within some kind of range to where it has you know enough caloric energy so to speak um so um I think for now John is there anything that you think I have not addressed that might be useful to to our audience or if anyone else would like me to touch on anything maybe something I skipped or or otherwise I'd be happy to to speak more no I I think you you you you've uh covered some of the really key points that um that are that what I I think is you've done is what differentiates active inference from the more traditional agent-based modeling I think is is important because when people think about agent-based modeling they they sort of default to what the traditional models are and I think that you've done an excellent job of sort of uh highlighting what the difference and with the the the having having that broader perspective that more grounded interdisciplinary perspective you you're able to more not only represent the complexity of it but also provide a a way of modeling agents in within that complexity um and I think that the the the the notion of a the the homeostatic nature of of of an agent trying to live in in conjunction or really harmony with the world around itself is depending upon its survival and its ability to stay alive and differentiate and have complexity but the synony the environment is really another key point because I uh I think well a lot of people they think oh well you may say like navigating a fitness landscape you're just trying to optimize different levels but in fact the landscape itself is shaped you're doing joint Niche construction so it's a much more complex notion than traditionally understood General modeling and I think in in consideration you looking at tomorrow this whole idea of the btic character of model having it based upon principles and biology and nature and having being able to create an economic agent that acts that way I think is is is really quite important um and so maybe some people have some questions about that that that we want to to focus on or not but that I that's something that that I think we it's important to highlight yeah awesome presentation Andrew Ernesto if you have a question also anyone in the live chat is welcome to ask a question no questions thank you very much this is so inspiring well and just to to put it in context I know this is something that you're very interested in in this whole idea of a btic economic system and been looking at this yourself so this is really trying to lay down the principles really the the scientific principles and computational principles with doing this and and and uh and we'll get in more to that tomorrow but that that's the the intent here yeah lot more to say again in our remaining 40 minutes I'm happy to read questions from the live chat I'm also happy to screen share and look at some of the registered participants backgrounds and interests sure okay okay you can unshare Andrew and I'll pull up some word clouds all right okay okay so let it load okay these are word clouds that are generated from the participants registration information and we'll will zoom in on them and and uh I I'll just kind of mention them we'll we'll zoom in and uh if anyone has a a remark or something that they want to uh like highlight or call out about it just go for it okay so this is a word cloud of the uh 150 or 200 registered presenters overall asking about what is your personal background so the biggest and and and this is scaled just by number of uses of each term so one person mentioning a term a ton of times might make it large so just to be taken as sort of a representative we see those tiny two letters AI which could mean almost anything at this point and a lot of other terms like computational research systems Neuroscience cognitive philosophy social academic robotics technology practice this kind of speaks to the broad range of topics that that that leads people into joining and at least entering that information on the uh form any thoughts on this or we'll keep on exploring these word clouds okay okay all right again just raise your hand or or or go for it interesting okay well there's I'm interested the notion of community and systems I mean uh applications the community seems very strong yeah yeah and that that's the the centor of a of a later one so here we asked what what would be useful and by asking what would be useful and what would be interesting we alluded a little bit to the Dual composition of the expected free energy what would be useful what would be utility what would be pragmatic and then what would be interesting what would be epistemic so hearing what would be useful you know here we are in in applied active inference Symposium wondering about how does active inference go from being a unifying framework for ecosystems of shared intelligence and all these different kinds of things that we've heard already and we're going to hear in the Symposium to come so interesting that systems a way to operationalize and really move forward from framings of complex adaptive systems to having the implementations and also the community and the people understanding research models collaborators and then a lot of different topics that are being explored in more mainstream computational science Fields like multi-agent algorithms Andrew alluded to that and kind of brought that up in the context of the the Goldilocks parameterization question the agent based model can be so simple it gives you a great intuition it shows at a a demonstration layer how simple Behavior can lead to complex outputs on the other hand the agent based modeling can go the other extreme where it's so sophisticated then it loses its transferability so that's what people would find useful and then here's what they put for what would they would find interesting applications make sense as that's what this symposium is all about and again a lot of other ideas thinking about different specific systems research ideas architectures synergies toolkits insights implementations support Collective directions here's asking what would help you in your learning and applying active inference and Community looms large other people as well as some of the more tool resource textbook education examples Frameworks interactive tutorials different languages python what does anyone thinking about this I'm uh a little surprised that we not seen the phrase llm anywhere um I just uh kind of noticed that so one of the one of the points that that I you know I've been trying to work out and I hope to speak more on tomorrow is um you know integrating active inference with llms and uh you know there have been attempts to to do this and even even with priston and Par and other folks you know that there there's been a attempts at trying to pull out an agent's beliefs and then use an llm to process those beliefs like kind of decode them and re-encode them in natural language so to speak it's as if the agent is speaking and saying what it thinks and believes and why it did what it did is you know another layer of interpretability and uh I don't know I'm slightly surprised to not see llms anymore um you know it maybe it's a little too maybe that's a little too on the frontier right now um I know it is in many other areas you know and it's um sorry in the couple minutes we have it just uh you know something that I've been working on personally and this is feeding into the the biof firm is um how how do we how do we use llms in a way that we can actually integrate them with you know all these major priorities that we have uh where where we want to have agents who actually have defined goals right and and and ways of trying to stay alive you know in a way that they interact with their environment in the way that it's as if their own Survival depends upon it we don't want LMS which you know again I'm just saying this quickly you know I don't need to sound disparaging but llms in one sense can act as like functionally as Talking Heads where you just send in some text and they try to predict what would be the best text in response they still subject to hallucination to confabulate making things up so to speak even though the answer looks structurally sound you know and and and so the my point is that rather than viewing LMS as uh as agents I don't I don't I don't think they are I think that uh you could have an active inference agent who is augmented with the capacity for natural language by having it interact with an llm right so yeah sorry TOS my own yeah scan scanning across all the registration fields for llm mentions so H how does active inference apply to the design of llm systems AI outside of llms better ways of Designing internet skill protocols and governance systems and should converge with tiny ml to challenge lm's deep learning so it's it is interesting that that it's uh referenced with only a glancing blow and and uh We've even seen earlier like in the responses to similarities and differences with reinforcement learning with friston earlier about the increased amount of attention and Tool development and the later and more professional maturity of training environments and this is like one of the key fosi for the open source active inference ecosystem and for the Institute is to develop those kinds of methods and trainings and all on boardings all these different aspects of of increasing the professionalization so let's look at the challenges so this was what what what are the challenges facing your learning and applying active influence people interesting the problem problem and the solution models the math software maths again mathematical understanding difficult physics scaling simple examples you know it's it's it's interesting um because in the larger conversation about what AI is I mean it's it's that that conversation is so dominated by uh large language models and quote generative models within large language models um and and well Le I've been observing I tried to refer to my my initial slides is that they're starting to appreciate the limitations of of the large language models moved towards more agentic model and more towards causal models and hence creating aent architectures that do work with large language models but they're still there's not they're still not based upon biological principles that we're discussing active inference there s more of the mechanical models being applied over and I I think to me to from the from a point of view of sort of the um the sociology science or soci there there seem to be these different worlds that that I mean the number of people i' I've known sort of in the open AI a well they're they're very unaware of active Ence and some of the critiques that are if you if you listen to the Gary Marcus he's got a very pointed critique of of um of of large language models and not being able to achieve abstractions but that doesn't extend to appreciation appreci iting what the the opportunities are in active inference and so there are there s of these islands of understanding that they still exist um and I think that's one of the challenges and when you talk about developing an infrastructure me you need there's huge financial resources are going into the the the large language models infrastructure and where I think that the totally biased on this I think all the the opportunity is totally within the active invers free energy principle and I think that's where the science is going and yet it's it's still it's sort of in in this isolate World um and I think the only way to get out of that is start to to really start to sh the applications and showing that with results and and uh because uh but it it's it's it's interesting me to see they're they're different they're different worlds um um but I I think that what's there's a peaking and so the hype curve around uh uh the large language models and and it's going to they're going to descend my bet is they're going to descend into a valley of doubt and you suddenly you're going to see that showing up in the in the Nvidia stock going down and then people are saying you can't you know destroy all the natural resources or run these mils you don't have to you know the other and Carl alluded to this in his earlier comments today you know I mean he was talking about climate change and models compression I and I think that's going to be a driver of this um so but that that's some my my observations on on these yeah these clouds is yeah I'll bring in a question from the live chat also welcome Ma just thought I'd bring you on if you wanted to participate in this panel or just chill so Steven cette wrote is the lack of lm's mentioned because large language models are different to Dynamic agent belief modeling Andre you want to take that you could you could you repeat that uh yeah question is the lack of llms in the word clouds here but just more generally thinking about similarities and differences between active inference generative models and llms is is that because large language models are different than Dynamic agent or belief modeling I I don't want to approach this this question in the in the wrong way but I I will say that that that as far as you know whenever it comes to llms and and dealing with you know their internal parameters are obviously highly quantitative and then they're dealing with with natural languages tokens and then switch back to looking at active inference agents which you know are highly quantitative in nature and typically you know if you're making continuous time models uh you're looking at again quantitative information and then even with discrete models you're looking at you know these kind of discrete values that you could potenti moveing the direction of trying to you know make it I think this has already been done uh but uh you know there there are agents who um you know have been used to try and understand like oh we can read individual tokens then move up to another layer and then we have entire sentences and then move up to another layer we have entire you know paragraphs or ideas that kind of thing is done but I think this kind of like llm with quantitative uh uh grounded agents like that connection uh is still in my view that's kind of the frontier of where things are at and so it's difficult for people to conceive like how can we how can we translate encode decode re-encode natural language in a way that's readable by these quantitatively oriented agents and then vice versa how do we transition quantitative beliefs into natural language that's the very thing that I've been uh you know looking at thinking about uh recently I made like a active inference like PDP agent into uh you know they what they do is that they use their beliefs about which action to take and then that action that they carry out is actually corresponding to a prompt to an llm and then you can actually take those beliefs the quantitive beliefs find some way of meaningfully interpreting them and then putting that into these blank spaces in the prompt as if you can have the agent say in real time I currently believe this with probabil this give me you know a solution to this problem you know based on so so being able to find this kind of numeric versus the quantitative and the qualitative you know it's it's a historical problem when working with with data that goes back centuries at this point so um it's it's really exciting though it's really exciting to think how to bridge that kind of Gap and I I still haven't seen anyone you know nail that down this this is the frontier this is where we're at and I think that's kind of the next thing for everyone to really be considering thinking about working on um and I'll give my view on how that could be done tomorrow so well um yeah I mean effectively llms don't really encode explicit beliefs or really maintain a persistent model of uh the world um even when we sort of give them a sort of loop by which we um communicate with them that Loop in itself doesn't actually factor into the predictions that it's going to do um so they they just lack a framework for recursively updating beliefs which is probably why it's not being thought of here um but also mainly that you know llm agents are only agents by virtue of sometimes being connected to stuff and having um explicitly stated goal but I don't think that actually qualifies them to the agents you know yeah I I totally concur with that I from my perspective I I think that the reason that the LMS are are are not looking at that are are not looking at or that whole mindset is not looking at at belief structures and in subjective experiences I think it's a product of a whole sort of objectivist reductionist methodology that um there is has been tuned not to look at agentic systems I mean in my generation if if you if you were in biology you in and you talked about agentic systems you you were you were outed I mean that was that that that was verboten and I think the the there's a whole school of thought there's a whole set of of in aonian sense a normal science built around uh a sort of objectivist reductionist non agentic nation and and and uh and so there it's a new generation that's coming in in both in biology and in and modeling and cognition there's recognizing there this notion of antic perspective of beliefs and updates and and and that's a huge shift I think in in sort of the epistemological perspective of of a lot of what has been traditional science that's really interesting there's also just operationally the difference between taking a very large fixed number of parameters in a given model 70 or 7 or 405 training on existing data and then finding patterns or igen modes or activation um summaries that have secondary interpretability but there's not really an effort of trying to find interpretability at the level of activation of a given neuron it's about looking at patterns and trying to find Collective level interpretability like in in the AI interpretability field contrast that with designing from the ground up in example agents where the beliefs are explicitly encoded numerically and so that's a different mode of building and also one that's hinted at in in this 2023 paper with Ma and others that highlights that element of explicit semantic interpretability and also explicit semantic interpretability of uncertainty estimates whereas a string of tokens emitted by an llm that says I'm not sure it's sure about saying that but that doesn't mean that it has a given certainty about the reference so understanding what the communication semantics of llms are is incredible and what they do and I think when we look at this Cloud for people's responses how can active inference make a difference in 2025 where there there is llms here many many kinds of systems and that's sort of like a microcosm of what the Symposium is different people with different systems of Interest asking H how do these different attributes of the model that we're talking about the first principles grounding semantic interpretability surprise and uncertainty and its bounding as a central component as opposed to reward how do all those things really play out what is that demonstration look like in in different systems here's go for it Andrew I was going to say on that that that previous one you were just looking at I I I really appreciate that kind of broad-mindedness that that people seem to be exercising I like seeing terms like uh symbology scientist governance um industry I mean those are some very I'm very excited to see people who who want to you know so much work has been put into kind of the theoretical and then sort of like in in in vitro and and in lab and and in simulation kind of work that's been put into active inference so I'm just really excited to see you know with the the um International the the conference the the back of inference conference that was given recently and being able to see like a drone that's flying based upon the free energy principle and things like that I'm just really excited to see more like kind of real world applications right and and kind of uh creating uh you know systems and and applications for like real world use cases it's it's really cool to see some of those terms there yeah I think that's going to be really critical see see yeah here's the cloud for how are you applying active inference again systems I think just reflecting people talking about variety of different systems types but and also reminding me of my own journey to active inference being excited about complex adaptive systems however not really having a software toolkit or a formal way to proceed past okay we think about nested multi-agent systems and then where's the software toolkit and then we're right back to where Andrew showed us on the slide it's like well we can do a really simple one in net logo or we can engage on this Enterprise scale engineering research program and yeah here's a more incremental interoperable way to have portfolios of motifs and generative model components and be assembling and scanning across these different kinds of compositions like three ant nestmates in the desert three ant mate nestmates in the forest and these different kinds of environments being able to compose them and over the last several years seeing a lot of the advances in the category theory in the first principles physics surrounding all these sort of research things and it's like that's kind of the water drawing out and then that's the question about how it looks when it is applied and that's what I guess we convene around in this Symposium and and are seeing differently in 2024 than in previous years that's curious so what what's the change that You' seen in pre previous years versus this year and then so looking forward more of an application and or ma what what what do you think I was yeah but absolutely that that that is I oh yeah please yeah I mean I feel like um the work of Maxwell ramstead C Casper hesp I mean obviously Carl but um a lot of people who perhaps came from more Dynamics uh decided to take a real interest in what we could do with something that was so scale-free and I find that it's been really really rich in terms of the different kinds of theories we've tried to sort of reforme I know that's what most of my work is is a Reformation of existing social sciences um and to your point Daniel the the problem we still have is that we're missing like these key little plugs um that yes you could couch as you know just different schemes of message passing but you still need some interesting tools to to have a a very good description of the vast vast variety of types of structures that we have Which social sciences have done a really great job at if not intuiting at least describing properly but we still don't have the best tools to make accurate predictions and that's part of the reason why most of our polls when we make them are are n they're useless and that should be the main thing we apply them to is just literally just understanding where we're at not even making predictions just understanding where we're at and we still fail at that so I'm very um I'm very encouraged by by seeing this and I hope that we will start valuing more and more interdisciplinary perspectives that are sometimes more technical but also sometimes uh you know more philosophical or um or social awesome also while we're in this in in the coming workshops tomorrow tomorrow will be more of a Biore Regional biofirm focus and then day three will be entirely with clippinger at all on this topic and and I want to show one really operational open science Crossover with llms and active inference which is and I'll put it in the live chat as well it here I used perplexity which does an internet search summarizes and translates information using LLS so here we'll pick on Alexi there's a a profile a learning plan and a set of small medium and large project proposals about active inference so this is just summarizing expertise and experience here we have a learning plan it's just a starting position but this brings a new low bar to the kinds of resources that everyone can take on in their learning and use as like a syllabus or a curriculum and even as an interactive tutor you know it will not get bored with you asking to explain Concepts again and again or using different metaphors or making it applicable making it like this poetry format putting it in this narrative context and so even asking how do I start with this or can you help me understand what I don't know that puts such a centrality and and highlights metacognition in learning and then the project proposals each of them structured according to essentially what a grant officer would be looking for and what a what a project manager would be thinking about in terms of small medium and large projects so this unprecedented High throughput way to fuse even scanty inp put from a participant in a inclusive open science setting into real projects that if enough people think hey that looks kind of interesting then we're well down the road of actually making it happen using different kinds of of funding which is something Michael lennin mentioned a little bit earlier so it's sort of like we can get really detailed and Technical and Architectural thinking and and and finessing the differences between reinforcement learning deep learning active inference and we saw slides to that like when Carl showed the action perception Loop similarities and differences okay active inference doesn't have a reward function it's not optimizing based upon this trained auxilary reward concept it's just doing something direct with likelihoods like that is critical that is what makes the active inference generative models what they are but then looking at something like this which wasn't so accessible one to several years ago that just feels like opening up and unleashing what people can do and about using these as Tools in our Niche and I think it's a total open and unknown what happens when you can get to this level of project and learning plan for for an arbitrarily complex field with the patience of an AI That's just going to accept questions all day long like that's almost the bigger applications question again just zooming out from how the architectures and the parameters are similar and different to how are we going to do what is authentic and right and best given the tools we have there's a bigger picture Beyond just holding the magnifying glass to different computational artifacts that's well said Daniel I really agree it's transformative if anyone has questions in the chat or here or any are things they want to mention yeah what's going to happen in the next two sessions John or what would be your goals or preferences for for this Arc of the Symposium for you yeah so I I I think is as Andrew alluded um this the session tomorrow is going to be really on the application of of U of active agents uh to a a design of a e e omic agent new kind of firm biofirm this homeostatic um and the objective of that is to develop a new kind of incentive structures AO to Collective action um that are based upon memetic principles that I think we'll see address a lot of the issues that people are having in the whole idea of bi Regional Finance and how to deal with climate change and how to create those incentives so there one of the things I'll kick off with I've been talking to a number of people in the whole fields of of um um regenerative Finance bi where they go bofi um this it ties into the work that was done in Elan ostrom's work on uh how common pole resource how do you manage a common pole resource how do you incentivize people different scales I really think that and what you identify so the requirements that people are looking for it in order to address these problems at scale and I think what we're be talking about is to the implementation of active infen uh principles into an economic agent that it provides I think a a a reasonable way of of addressing these issues um and that uh um and and so that's part of the conversation that we want to have and and and bring together these two these two worlds um then the third session is so really look at this um from a a broader point of view is saying where is this going and they so the people that we have on the panels are very of very much involved with Enterprises um applying it to nonprofits applying it to S problems looking at what they can do with llms currently and what the certain current agentic architectures are and what happens when when you s what happens when you go to full autonomy what does it mean to have autonomy um in a in a in a biological sense something keeping alive and what does it mean to build systems that have autonomy this issue of autonomy is sort of fundamental uh when when when people think of of autonomous organiz autonomous cars or decentralized autonomous organizations or the whole idea of U the Central Bank the the the the the the principles behind something like a Bitcoin is is like oh and that that that is autonomous but is it really autonomous what is this new notion of autonomy if we base it on biological principles um and I I think that's that opens the door to a very different set of discussions than than we're currently having and it also opens the door to new Notions of governance and how we can build governance in to maintain the homeotic states um and Andrew is alluding to that so I mean that that that that that's where I become a an optimist Andi I'm not an optimist around the extension of llms and open Ai and and the tech the tech Giants but I am an optimist in being able to apply these principles to to some of the fundamental issues we having around climate and social equity and economic equity awesome okay in the closing minutes if anyone has lost comments and David's question too if you want to address it Andrew or anyone else any thoughts about how we start to develop a multiscale approach to ethics um I want allow time for other people so I'll briefly say uh so so tomorrow um you know we'll we'll we'll present uh various aspects of the biofirm there will be a little bit of a Redux of some of the things I went through today just to kind of thread The Narrative um and I'll kind of talk about you know homoeconomicus and all these kinds of uh ions about human behavior and things like that but uh um I mean multiscale governance what we're trying to do with our biofirm is to uh you know in clippinger refer to um uh Elanor ostrom's work on the commons and so I I'm gonna address that a little bit too but the idea is to be able to have a kind of like adaptable system of multiple agents uh you know and so we have a kind of small Society so to speak and uh you know they're kind of governing the commons around them and by being able to have something like the capacity for one shared goals uh and two adaptive learning such that they can actually adapt what they do and what they believe and actions they carry out have those be localized to the current context which is an essential part of asam's like work is like it's not so simple to just give a singular top down instruction that applies in all places I mean to be able to have that kind of adaptivity and awareness of the local um you know while at the same time having agents who are kind of synced up in these various ways I think that's a one way of trying to approach governance and then from there you might be able to have kind of layered you know you could have individual hubs you know and then you could have some kind of like interconnecting system between them that that extends that to an even broader scale of multiple hubs and interaction between them so just some just some breadcrumbs there um thanks yeah Ma and then John with the last word on this session yeah so I mean the question of Ethics is a bit it's obviously complicated but the the the key part about whether we coordinate around multiscale goal sharing Etc share potention that's intrinsic to paral active inference right it just is the question that isn't going to be solved is what is good uh because you can always say well you know what is good is good by virtue of being contextually sort of stable but there may be other ways to be stable or that might prioritize other kinds of individuals or entities Etc so I don't think we're going to solve that question however what we can solve is the is the global versus local coherence of certain kinds of beliefs relative to what might constitute goodness at given scales and find where the friction points might arise because this version of goodness is compatible with this version of goodness and therefore Uh something's got to give somewhere an error is going to have to propagate so I think that's going to be answered but the the larger broader question you you'd have to look into different schools of thought around what constitutes uh good because we don't necessarily want to go towards um moral realism um because there's a lot of things that are baked into that uh but you might want to go into the uh ethical implications of neom materialism and how it it is about interactions and locality and at that point it becomes about how do certain pockets of ethical framework sort of form and allow to just coexist away from other pockets and what happens when they start interacting uh which again we may be seeing right now in the US yes good point John any last comments on the session oh this is great um and I think U we we no surfacing some of the issues that are really important to to talk about and and um I think Le least what we're going to be presenting is is a is a framework um that at least we can have these discussions in a more rigorous way and testable way um and uh I think that's part of the way moving it forward so yeah awesome thank you okay we'll take a one minute break see you soon okay thank thank you e welcome back welcome back hello Sonia and Ma again we are in the session physics and metaphysics of social Powers so thank you both for joining and take do I thank you so much for having us I was just asking if we should introduce ourselves which uh which we will um so I will introduce myself and then ma can introduce herself so that I don't make any mistakes uh because my brain is on hyper hyper abstractions at the moment anyway I am Sonia I'm a PhD student uh based in Rotterdam at the araso school of philosophy and uh we've been working on this paper with with ma recently and Ma please uh yes so I'm a PhD candidate at the University dubec Amal in cognitive Computing and I'm also a director of research strategy at versus thank you so yeah as Daniel said we're going to be talking about the the Spectrum the big spectrum of between the physics and the metaphysics of uh social Powers uh very inspired by the work of uh of ramstead here here HP Hines and uh by Mouse work on on on power recently with SAR Grace mansky um and then we're trying to combine that with some work that I'm doing with two other colleagues uh Wilkins and clav Vasquez on the concept of active ignorance which we will be um continue continuing to work on in the coming time um so yeah and then generally inspired by by ideas of semantic attraction from Hinton and by Major names in in philosophy and sociology who have been treating these Concepts through different Avenues uh less kind of formal Avenues but still relevant nonetheless now that we're actually capable of joining um yeah the physical with the metaphysical that's what we're going to we're going to try here with this presentation so the concept of power we we argue can be explored at that several scales from physical action and process effectuation all the way to complex social dynamics so this kind of spectrum wide analysis of power requires attention to the fundamental principles that constrain these processes and in the social realm the acquisition and maintenance of power is intertwined with both social interactions and cognitive processing capacities so socially facilitated empowerment grants empowered agents more information processing capacities and opportunities either because they're relying on others to bring about desired policies and or because they're able to enjoy more information processing possibilities as a result of relying on others for the reproduction of material tasks material is in Brackets because these tasks need not be material so the effects of social empow empowerment imply an increased ability to compute information thereby augmenting the evolution of a specific State space specific is a keyword here uh really commenting back on what Ma just uh just mentioned in at the end of the previous talk so empowered individuals attract the attention of others attention is one the central Focus uh of of our paper so they attract the attention of others who contribute to increasing the scale of their access to various policies effectuating these these State spaces that we mentioned so the presented argument deposits that social power in the context of active inference is a function of several valuable [Music] variables among them an individuals or groups ability to attract the attention of others with access to or the ability to facilitate desired policies and an individuals or group's capacity to effectively process information as a result of its power amplifying effects this extended computational ability also buffers against potential loss or or possible vulner vulnerabilities semantic social AR effects so narratives um ideologies histories representation at large are attentional scripts which ensure the coordination of social patterns so you can think of anything from psychological schemata habits group think insulation policies Etc and these are the structures under and by which agents mediate and leverage power and they're because of that the main focus of attention here so uh thinking about this the linking between the physical and metaphysical Concepts and the world gravitational metaphors abound in language this is like really prominent in the work of of Shephard Le of and Johnson of course with special metaphors um I've been I've done some writing on it at the beginning of my PhD and also um laand Kent uh did some really nice work that's also up on the on the active inference archive and then in his papers of course um so the idea of power being pulled towards complex social attractors uh was also famously presented by Pier borue who linked various concepts of capital cultural capital political Capital Social Capital to power as this kind of social attraction uh and also traction and he borrowed the metaphor from from physical field theory in fact so there is already the connection is really present and obvious there uh only in in a metaphoric way um which we're kind of trying to expand on here uh so expanding also on his legacy to to you know take the epistemological and the empirical as totally Inseparable things that we're analyzing and his desire to actually ground sociological analysis in in in a scientifically tractable manner we're trying to frame uh these kind of attentional attractors that move systems toward certain configurations through active imprints and this framing allows for a new take on the dialectics between the individual and the collective again linking back to what was just mentioned at the end of the previous talk so in terms of um questions of information processing capacity not only our resource distribution and you know the division of labor a systemic fact but also the modulation and projection of future possibilities cannot be computed at the level of the individual clearly so this is something rather obvious in our context and our framing power this is how why the minimization of uncertainty for for social systems is in fact social right so agents seek alliances and policies which ensure that cognition so information processing and future projection can be distributed and this allows for efficient practical traction because and here I quote from Steinberg at all 2023 epistemic confidence in knowing interpreting and acting together is that traction that happens when we when we're socially distributed uh and this is in contast with this kind of individualized agency where one's estimation of and here I quote again Divergence in mental States between self and others is what allows for the sort of prediction and interpretation of others Behavior which we might kind of want to frame as something that has a shorter range traction on on the future so now we're going to look at the theoretical framework and I pass it over to Ma so um obviously you have heard this 15 times but I'm still gonna go through it because there are some Concepts in here that will become relatively sent after but to go over it briefly active inference explains how biological agents really minimize varal free energy to align their internal beliefs with external reality so effectively they've reduced discrepancies between expected and actual sensory inputs and we sort of Leverage The Descent on free energy to to to couch model uh perception cognition and action through basian belief updating and so free energy is the upper bound on surprise or the unexpectedness of sensory data relative to an agent's predictions and so direct computation of surprise is complex so agents use variational free energy uh to to approximates based on their internal generative model and so um you can see that for example the K Divergence term measures how closely the internal uh beliefs match the true posterior of and so agents align their beliefs with actual data as they minimize it so the log likelihood term ensures predictions align with observations so to minimize free energy agents update their beliefs they allow for adaptive action selection that supports coherent gold directed behavior and so there are two strategies perceptual inference to adjust your beliefs to match the sensory inputs and active inference which is um acting to change the environments and align new sensory data with internal models um active interestingly and as we saw earlier with the the whole word cloud spans or can span multiple scales from cellular processes maintaining homeostasis through cognitive systems using predictive coding to update beliefs based on hierle prediction errors and so this really gives us a broad biological principle where minimizing free energy promotes systemic efficiency and optimal homeostasis across scales and so um reducing free energy really means that you minimize entropy as well and you align your beliefs with reality to reduce this uncertainty important to note is that we also Factor um action and so expected free energy can be DEC composed to extrinsic goal oriented and epistemic uncertainty reducing components that balance exploration and exploitations and so this dual strategy ins ensures that agents achieve immediate goals while gathering information to adapt effectively in uncertain environments and so Central to active inference we really have this concept of a generative model which allows agents to predict sensory inputs and their hidden causes the generative process represents the real world states that produce observations and generative models reflect the agent hypotheses about these processes uh the agent's actions correct mismatches between its model on real sensory inputs uh to align model with reality going quickly over uh the the different terms in a hierarchical structure each level predicts the states of the ones below and it corrects predictions based on sensory feedback and there are conditional dependencies that Cascade between layers that support uh belief updating and adaptation we have observable states that are the sensory inputs the agent receives and tries to predict using its model the hidden States latent variables that represent underlying causes inferred by the agent to explain sensory inputs there's a series of parameters that Define the statistical dependencies between hidden and observable States likelihood function which indicates how well the hidden States and parameters predict observe data we have a prior distribution which is the initial beliefs about the hidden States and the posterior distribution which is the updated beliefs after observing data and combining likelihood and prior then obviously we use the marginal likelihood which is the probability of observed data under the model to integrate over hidden States and so we use this for model evaluation and comparison to determine how well a model explains sensory inputs so the generative model really enables the agents to align its beliefs with the sensory data sometimes through hierarchical predictions realistically most often through hierarchical predictions and feedback loops and they refine their understanding of the environments to improve action selection for adaptive behavior and so this picture is really uh very individual so um we can see though that within this picture it's really really rif to uh to couch power dynamics so we start from Individual agents as predictive systems that minimize uncertainty through prediction uh through perception and action and so brunenberg really made a Salient point about this with this paper on the Primacy of I can where agents develop you know empowerment through embodied interaction which enable them to affect the environment so if we scale this up to Social Power agents use generative models for self- prediction but also to predict others intentions they form the basis of social understanding and Collective action Carl has lots of work about this um Connor Hines as well Axel constant maxel ramstead Casper hes Natalie Castell which I believe will give a talk as well and so attention uh is driven by Deep preferences and it shapes what becomes Salient which influences both individual and group Behavior at the group level shared predictive models facilitate coordinated actions so stigmergic processes as explored by our very own Daniel uh environmental modifications that guide Behavior allow distributed control while deontic cues as explored by axle constant enforce social norms and so cultural practices further shape Collective attention it creates shared perceptual Landscapes which is something that we explored with uh Toby Sinclair Smith I think I said his name wrong I call him Toby so I don't really know how to say his name and finally we have to know um how to couch joint attention and this is work that and Pi at the University of Quebec in Montreal have also positive that posited that this mechanism basically enables efficient coordination through mutual recognition of action possibilities and so power here our structures emerge through self organizing subgroups based on belief similarity so again this is work that has been explored by Natalie Castell which we can call decentralized influenced spreads within Network structures and they amplify certain narratives While marginalizing others and so these these processes create cultural capital where shared norms and knowledge facilitate efficient social interaction um attention here is thus shaved by those preferences but it shows that the importance of critically constructing shared narratives without homogenizing scripts that stifle diversities uh is an important thing to consider so it's a point made by Anna AR simplistic one-dimensional fuse like there is no alternative constrain attention and reduce complex systems to narrow goals which is why fascism tends to be very attractive especially in times of high uncertainty and so distributed generative models over time including historical narratives and generational Dynamics give us the tools to Envision power as possibilistic which is a term that Sonia introduced here this this shifts the focus from isolated agency to Collective Dynamics and hierarchical organization we get a richer analysis of how agents and groups interact Within constraints to shape future possibilities so active inference really gives us a frame for social power beyond very simplistic atomized ideas of agency and focus instead on this emergent distributed process that structure Collective behavior and potential thank you so yeah to return to the concept of narrative because it's sort of the thing that that um frames something that might go from from from the small scale to the large scale in different ways and something that we can all colloquially uh uh sort of latch onto narratives are are are social mechanisms which have the power to make or break power you can say uh the cognitive exploration of of of the not yet engaged right of of scripts that are not yet there but one may kind of uh uh latch onto or sort of effectuate anything you can think of here things like speculations fictions future possibilities of All Sorts um can be these can be understood as semantic attractors that enable kind of relatively risk-free learning right so you're you're projecting to imagine what might be the case if and the simpler The Narrative usually the easier it is to sort of minimize like vast uncertainty so these possibilistic Explorations in the form of narratives allow agents to explore counterfactual scenarios and minimize uncertainty without the sort of exposure to the to the real world consequences of this um this this safety mechanism frames both The evolutionary advantage and persistent appeal of narrative engagement in human culture and in the context of power know leveraging strong narratives as as Ma just pointed out such as US versus them that's like the the the one that you know the the biggest semantic attractor that we seem to keep returning to all the time these types of reductionisms that greatly minimizes uncertainty about an agent or group's current predicaments and they become because of this very very appealing and sometimes self-fulfilling unfortunately policies that bring about their effects resulting in Collective uh Collective behaviors which can benefit H sorry I'm so like which can be beneficial or detrimental to the greater whole the uh the effects of of variational free energy minimization can be observed of course at the level of the basic structures of language in this sense so semantic attractors like redundancy rhyme alliteration Etc can be understood as patterns which Aid memory communication informational retrieval policies because they permanently reappear right so pulling attention to the same phenomena and here I use scare quotes very very strongly the same phenomen uh thus greatly reducing the need to to interpret and organize linguistic interaction and cognition a new every time which is very costly so we can understand for example the possible emergence of rythmic patterning in communication as enabling the storage and access of information collectively by connecting a sound pattern to a phenomenon we reduce uncertainty about reality once that sound pattern is witnessed again in in sort of true pavlovian fashion by way of the same uncertainty minimizing mechanisms simply simple or complex narrative constructs create cognitive situations where agents reduce complex complex aspects of the world down to Preferred scenarios and now we can look at how active inference is a useful applied framework for this pass it over to ma again thank you so I'm I'm here drawing on work that is currently ongoing from other people as well with who I'm collaborating and I want to make sure that these ideas are appropriately Ed to them we're going to talk about individual agency and empowerment and so autonomy uh can be defined as a system's capacity to self-govern using internal Norms uh so again this is a topic explored by Jaclyn Hines Alex kefir inesi poo in part ADV veres um and so empowerment here is measured by the maximum Mutual information between an agent's actions and resulting States so this reflects how much influence an agent has over future States uh higher empowerment means greater potential for desired outcomes and so empowerment maximizes the entropy of actions and States while ensuring strong action State correlations autonomous agents are thus more driven by intrinsic motivation and this is a premise that again Alex Kefir is one of the Pioneers uh here it's thus connected to drives and control to systems that optimize future action State paths and so under active inference agents minimize expected free energy to balance exploration information seeking and exploitation goal Pursuit so this involves perceptual inference to update beliefs and generate model refinements through action outcomes and so here power oper as an attractor in Social systems it it draws agents towards influential entities through access to resources and strategic advantages Happ it leads through it leads therefore to amplification through feedback loops um powerful agents help others minimize their expected free energy by providing some degree of stable predictable Frameworks and reshaping the informational landscape so high power entities have privileged access to information and can model their environment more effectively and they also influence what can be modeled and how impacting the collective predictive landscape powerful agents can offload cognitive demands as well processing complex information more efficiently and directing narratives to manage error absorption and uncertainty so power here enables agents to shape others Focus directing attention to favorable States while diminishing access to Alternatives and this leads to feedback loop where control over the environment enhances influence and perpetuates power the whole story around 1% so power grants a broader range of operational capabilities and extends influence over other State space policies within its domain and so here active inference reveals power as this emerging property um of the connection between perception action and social structures more powerful agents maintain and expand their control by shaping both informational and physical environments so let's go over three main components Precision modulation belief Lea alignment and ultimately minimizing Divergence first Precision modulation powerful agents shape expected outcomes by altering the social or informational environment as we saw they create narratives or scripts that effectively reduce the dimensionality of the state space forming stable attractors such as social norms or shared stories and these attractors direct Collective attention and ensure that other agents align their actions with these preferred States um but here basically the posterior belief about States uh has uh an alpha that modulates the impact of precision on these beliefs and so in belief alignments the power to direct attention and utilize Precision modulation really enables the leaders to make specific aspects of reality more Salient for others and so by establishing these attractors within the belief space the leaders guide others interpretations and behaviors um these attractors act as a focal points reducing ambiguity and encouraging agents to align their beliefs and actions around them thus fostering a shared understanding so the the alignment process naturally emerges from minimizing the expected free energy uh and so for example um uh where P of o given s and Pi is the likelihood of outcomes uh given States under policy pi and Q of s given o and Pi is the posterior belief about States and so this decomposition shows us the intrinsic and extrinsic components which balance uncertainty reduction with coal oriented behavior that is um shaped by this Precision that we mentioned earlier so finally uh minimizing Divergence uh can be represented just using a coolback Liber Divergence uh which is a measure of the difference between two agents belief distributions and so a lower kale Divergence indicates closer belief alignments and so leaders influence this alignment by modulating this Precision which amplifies confidence in specific interpretations and so the process minimizes belief Divergence across agents which promotes synchronized understanding and cohesive group Behavior without the necessity for direct coercion although that is yet another way uh to minimize the state space and therefore is an available practice so Precision modulation sets the framework for control belief alignments ensures Collective cohesion and minimizing Divergence uh formalizes this alignments so basically we capture the Dynamics uh and influence uh the Dynamics dynamics of power and influence within social systems and so finally we come to to the conclusion we have a lot more in the paper that we're still working out so look out for that um yeah so what we're we're thinking about here in terms of this kind of shift um attentional shift uh from from ideas of territorial and material power to this new realm of of informational and computational power um well this this for us requires attention to attention uh for sure something that we can we can look at in uh in ways that we can formalize and of course we propose that active inference provides crucial theoretical tools for for making power structures visible and therefore analyzable and so allowing possibly for the potential to rethink current social asymmetries so again given this sort of apparent transition between the organization of life around the centrality of physical work and matter and things moving in space towards the centrality of attention and information uh especially considering the whole attention economy idea we kind of try to think think about a possible sustainable social future which requires the addressing of both resource and information distributions and the latter is best analyzed through the power of attention um as we've tried to to show as a major factor or one major Vector influencing social dynamics and of course this is very important to mention hierarchies are inevitable in complex thermodynamic systems but we propose if we can begin thinking about this uh fostering transparent plastic hierarchies that remain contestable and thus open to novelty open to learning right this approach um has to acknowledge uh historic and thermodynamic constraints while also acknowledging space for systemic Evolution and while hieres are inevitable social scripts should be read writable by those with an interest in survival within them right and this helps reorient our understanding of an unfolding globalized ing interconnected and interdependent life and participation in a system is only participatory if those involved are able to track the consequences of the variables and th future State spaces at stake a true distribution of information processing cap capabilities is essential for systemic sustainability and proper Equitable development and when thinking through evolutionary biology lenses for example let's try to polish them clearly and not fall into social Darwinism narratives and to quote from the Xeno feminist Manifesto um lovely colleagues there Patricia Reed for example if nature is unjust then we change nature which is what we are all in the business of doing here a little bit all right that was um that was our presentation thank you awesome okay we'll be back in a few minutes e e e this Carl hello welcome back everyone I'm here with Alex aoria we may have Carl join we may have someone else join if you want to join maybe write it in the YouTube live chat and maybe I'll email you a link otherwise write some questions however to kick things off Alex feel free to say hello and start in and and I know that there's things that we can explore together sure you just picked it back to me and I was saying to you all right well I I'll just introduce myself because I'm not entirely sure how most of the audience if they know of who I am so I'm Alex Oria I am a computational neuroscientist in cognitive scientist who has worked for active inactive inference and specifically predictive coding versions of it for quite some time I'm Carl's friend and collaborator uh some of you might be aware of more our recent arguments of mortal computation which is a coraly or a new coraly or another coraly of the free energy principle and uh yeah and you know I'm all about what's wrong with deep learning and machine intelligence and I'm usually the guy you go to if you want to attack back propagation of Errors large language models Transformers kind of relates to a lot of the discussion I was able to catch Snippets of earlier today so if you want me to help dismantle what's what the problem with deep learning you know that that can be fun too so that at least if nothing else everyone now knows who I am Carl needs no introduction maybe start off with that while I try to figure out why he cannot join oh okay uh what part of that do you want me to I mean I can go into Manifesto but I'm not sure that's what this panel should be about maybe beginning with a five minute Manifesto and then there will be some questions and uh we'll see where we get well maybe to try to make it useful to the audience uh for the moment although I wasn't intending the panel to be focused about that um maybe one thing that we can talk about is there was this discussion during the active inference Workshop about metabolism and sort of instilling a homeostatic metabolic Thrive a survival almost orientation to machine intelligence and so uh Carl and I have now been arguing for probably now about more than a year since uh our paper came out like on yeah around November actually start of November talking about uh another way to redesign reformulate to use Ma's earlier uh uh wording uh intelligence itself uh particularly machine intelligence and so mortal computation without again for those that might not be familiar actually my friend Jeff also like two years ago introduced this phrase the idea that computation or intelligent calculations and substrate uh in biological natural systems are entangled and it's h very hard to divorce those two concepts uh whereas computer science and a lot of like what chat GPT is or large language models or your favorite GPT structure is immortal which is the idea that we can copy that program including its weight tensors and its encoded knowledge and place it on GPU server you want and it'll behave roughly the same there's nothing inherent about its physical instantiation uh in this universe uh that dictates or shapes or uh necessarily embodies it right and so there's this like move to uh embodiment by the way active inference and the free energy principle has all of these characteristics mortal computation which is again as the concept I explained earlier Carl and I sort of uh recast it we strongly redid it and said that um uh because while Jeff didn't want to necessarily directly explore it we put it as it's just basically thinking of the free energy principle and taking it to its natural extreme conclusion which is that machine intelligence systems uh because they are inherently bound to their substrate uh must focus on this survival objective right and the free energy principle and one of its ways to talk about it uh basically is saying you know we have a markup blanket or a nested system of markup blankets and our goal in this universe is to protect those internal States uh yet interact vicariously with our external States and so that eventually gives us our ultimate goal which is to prevent ourselves from disintegration into a heat bath it's a very physics uh form of thinking about identity and identity de construction um but this is kind of what's centrally missing in basically all of machine intelligence um is this the completely right path to go there's plenty of criticisms and I'm not one to say that there's only one direction we need to go if we ever want something like artificial general intelligence um but this might be a path worth exploring it also addresses the Energy Efficiency arguments I was hearing earlier discussed uh Transformers particularly in large language models are uh grossly negligent of their energy costs they often times have the carbon footprint of a large city and that's actually now out of date as you know Carl and I wrote that about a year ago um there's even discussions I find kind of humorous by individuals I'm not going to particularly name saying we should be powering now uh these Transformers and large language models with nuclear power plants and so we're kind of going down the rabbit hole of let's just burn as much of our resources as we can to just get this particular type of intelligence I argue very strongly narrow AI uh and we're cheating ourselves of looking in embracing actually Energy Efficiency uh which would lead us towards the real grand goal of green AI uh because CL GL global climate change is clearly and empirically a thing and we should be a little worried about that so uh mortal computation sort of helps steer everything into sort of what I would argue hopefully maybe in multiple years time the emergence of a whole new field because I consider at least my personal take I'd love to have you know heard Carl wherever he might be um but I Envision actually mortal computation is not just like the framework and definitions that Carl and I talked about which kind of take free energy uh and active inference to the strongest conclusion to a naturalistic machine intelligence but maybe perhaps a new domain of thought of biom medc intelligence and where everyone else can come together and play in this really interesting space and there's a lot to unpack there but I don't think we want to make this about that uh there's plenty of talks I've given including active inference Institute we've had a podcast actually I think it was right when the paper came out so for those that are interested I just recommend you to go to the YouTube channel and watch wonderful videos that Dan led oh is there is there a new link okay sure was was I just blabbing there to no one no that was perfect we we we just hot swapped okay I was trying to buy Carl welcome Carl we talking about you okay thank you overview on Mortal computation and and and it was a highlight of the year um Carl with the evening Sunset now how um goes it wait I don't hear you oh oh continue sorry it's all good Carl continue you hear me now yep excellent yeah I'm sorry about that I got locked out I was uh paying the price for enjoying myself for the first two hours I um right I wait just I really enjoyed the um the intervening sessions um and look forward to commenting about some of the Cross cutting scenes yes okay um Alex mute it's a little noisier on jitsy and then uh Carl um also a little quiet but we hear you so um what were the what were the themes that that uh connected the presentations that you were in or otherwise well one thing that um struck me can you hear me all right now is this y excellent I mean there are lots of lots of little uh golden nuggets um throughout all the presentations um but one um one crosscutting theme was this notion of community that emerged in various guises such as the notion of Commons that was the subject of uh a lot of the exchanges in the first um in the first um uh presentation um and then the notion of community that was a Dumont of the the workshop um and then the notion of coherence and synchronization and narrative social um science accounts of that and how that might be formalized in terms of um joint free energy minimization ensembles of Agents you know finding a coherent communal um and empowered free energy Minima so you know I thought I I thought that that was a common theme sort of focusing much less upon um individual agents but what happens when you have have a an ecosystem of Agents um and what principles um can be brought to the table to understand interactions or ensembles of Agents so I I had a number of you know lots of things came to mind I was trying as a physicist to sort of um rewrite all of these wonderful Concepts um in a mathematical way or or a physics way um so for example I was think about the importance of Commons and it struck me that one way that we could motivate the choice of capital c for prior preferences uh would be that these are the common goals and the and the C characteristic States um that are shared between um multiple agents uh and of course um the last presentation took us through some of the mechanisms and the capital c circular capital c causality um that leads to this Capital C coherence so you know which originally was meant to be constraints in the sense of a constrainted maxim mentary principle um or cost in the you in a reinforcement learning uh reinforcement learning setting but coming back of course all this started when I was hearing about Commons and how it underwrote communities it it is very interesting how development and the present ations have proceeded a lot more about the communication and somewhat of the multi-agent coordination as opposed to a longer memory train for individuals or deeper prospection for one individual um and in a way the renormalization method brought a a Clarity or a rigor to what had already been qualitatively explored from 2013 and Life as We Know It And all these other works that we're talking about multiscale systems and about blankets of blankets and within this year to make it timely we start to see some of the ways in which that compositionality can actually be brought into the generative models that we've seen I mean it looks like the textbook figure 4.3 and the Ed models and now that's actually the motif that gives a way of looking at a multiplicity at the more inactive level as having a Unity at a higher level Alex and jur designing or work how how does the single agent or multi-agent design come into play now yeah yeah can hear okay good yeah I was having mic troubles earlier um well so it's kind of interesting I think I've spent most of my career at the seal agent level in the sense that I want to build I don't think we have right the single agent and I and I think it's kind of important to sort of scope that out from an architectural learning perspective I'm very much a fan of like Carl's arguments and you know I Echo them myself a lot which is the idea of the inference learning and structural time scales uh I still think we have a lot to do but if I were to say how in my research the multi-agent approach really comes into play well I do think a lot in terms of assemblies or ensembles right putting together multiple circuits I'm a I'm a huge proponent of another type of uh this comes from cognitive science called the common model of cognition uh I'm not sure how many in the audience are familiar with it but it's sort of like a generic blueprint that has emerged over the last several decades of cognitive science to say that we can think about and take our current Knowledge from neurophysiology and cognitive studies that brain has these rough levels of organization different types of memory we have the motor cortex we have the phas of gangway we have all these very important structures and we can start appending to them some functionality and so in that sense that's where I think of multiple agents right because you can start breaking apart using again Carl and I've argued this the mean field approximation uh there's natural mean field approximations that emerg through the result of evolution where you have these little substructures that communicate with each other um obviously the sparse connectivity that Carl mentioned in his talk this morning is uh kind of indicative of how these little substructures emerge and then communicate and perform sort of like what uh Marvin Minsky would have said you know sort of a society of mind right uh so it's kind of a call back to that and I think in that regard I often uh one branch of my work often tries to highlight emphasize bring forth that notion of cogni Blueprinting or cognitive structure um because then we can start filling in the pieces with what we know about neurobiology like for example I use predictive coding quite a bit uh you know to fill in those gaps and so I think in that sense I think a lot of interacting subsystems you could think of those as agents and then my final comment uh without letting myself blab too much uh I have worked in neuroevolution so there is another scale Above This which is you know you do need to account for how multiple mortal computers or multiple agents interact with each other um and so I think in that regard I have done some work with very simplified agents where you can kind of abstract away the neuroscience and just say I want to look at these little tiny systems that interact and then we can you know modify them through genetic algorithms and neuroevolution so there is a little bit of that like higher level uh societal kind of learning I have looked at that and I think a big challenge just to bring it to the audience for the panel is uh merging those time scales in a in a coherent and efficient way I still think well there has been good work I know Daniel you work on and Colony optimization as well and other types of kind of aggregate algorithms uh I think bringing those time scales together and really interacting with what we currently know about single agents even like the common model of cognition and going back to the highest level which is this multi-agent evolutionary time scale is a Grand Challenge and how do we of course effectively simulate those in a way that anyone can play with this I think is a big Challenge and yeah and I think I'll kind of stop there before I go down more radicals neuroevolution and neural Darwinism something Carl and and others have also explored it's like they're all and then the multiple nested time scales multiple nested spatial scales it's like there's this attractor to want to come back and find the unifying patterns and and it's and how those can be applied is kind of an exciting question I'll read one question in the chat so feel free to give a thought or take it any direction David Highland wrote do you view mortal and IM do you view mortal and Immortal computation as binary categories or are they more like two points along a spectrum I actually want Carl to answer that I mean I could give something but I really want to hear your thoughts on it it's your favorite subject though design I know but I'm actually really curious if you think of it as a spectrum because remember the conversation we've had with Mike Levan and Chris fields and we've been sort of debating you know how important it is to lean into mortality versus immortality do you think that there could possibly be anything in between because I honestly you know I think of it kind of more rigidly I'll just give my answers that and I firmly think we need to like focus on Mortal computation but do you think that there can be a hybrid in between so sorry to turn it on you but I'm very curious to know your thoughts um I mean my my instinctual answer is no um having said that I am sure that you can present some um description of mortal computation in terms of immortal algorithms I mean I'm just thinking back to sort of um um the trilogy due to David Mah uh but does that mean that these kinds of descriptions are apt um for simulating reproducing understanding um or describing the actual process I I I suspect not they may at one level of description they may be but when it comes to um applying the principles usually in computer simulations I suspect that that just would not work I mean because Daniel and I were wrestling with zoom and other um um communication things I I I may we may have missed whether you've taken people through the the the key distinction between Mortal and Immortal uh computation so Al you just want to briefly rehearse the sort of the two Cardinal views of the of the distinction um between Mortal and Immortal computation I did I just wasn't sure if anyone heard me so I wasn't sure what was going on Zoom okay yeah I mean certainly from the point of view of the free energy principle the free energy principle is a description of a random dynamical system um and as such um it is um a system that constitutes the world the you know the lived World perhaps not the Liv world but certainly a mortal world um that has um characteristic time scales and therefore um the free energy principle and everything that ensues from that including active inference um rests upon that primary assumption that you are dis describing a process a mortal process a a a a process that could have U um a physics um although um the actual free energy principle itself is almost pref physics so you have to derive basic mechanics and classical mechanics and quantum mechanics from um the existence that your the the the mortality of this process cast in terms of you know time evolving systems so in that sense um you know there is no such thing as Immortal computation um but I also I don't know if you said it out loud but I also like your um more uh playful take on Mortal versus Immortal that if if software was Immortal uh and it had some autopoetic aspect to it then it wouldn't care about dying so there would be no wait there' be no way there' be no incentive to have characteristic States coming back to another big c um uh so you know in that sense there cannot be immortal computation simply because that is removed moving the definitional uh stipulative definitional aspect of the characteristic states that we are trying to describe through the fen principle and active inference and specific um VAR variants of that such as predictive coding but I just picking up on this uh crosscutting CC theme uh you also um um celebrated that with common cognitive models so again we' we've got this this sort of notion commonality and um and you asked the question you know how How would how would one um apply these Notions and if you like sort of naturalize the common cognitive model under um a less anthropomorphic or functionalist active inference account and you gave the answer implicit in your question which is how would you simulate these things and I think Daniel ask the same you same question indeed the the whole point of this Symposium is about applications so I I just wanted to put questions to the two of you I was um impressed with the very um clear distinction between agent-based modeling and the common cognitive equivalent the co where the agents the members of The Ensemble were actually for example active inference agents that seems to me a very big move and a very difficult thing to actually do um most of the literature I know in this area um has a most taken baby steps into having ensembles of two agents with authentic agency in the sense that they can plan so I'm now limiting um the notion of an agent Beyond a thermostat or a virus to those kinds of systems that um have in mind or part of their generative model the consequences of their their own actions so they can be future pointing and they can plan and select from alternative policies uh and of course the numerics and you know the the Practical difficulties of actually simulating doing agent-based modeling for example um with authentic agents in the mix in the ecosystem um I think is you know um is a really important Challenge and a really important distinction between classical agent-based models where you put lots of thermostats or Rens of tractors together and see what happens and of course they have a synchronization manifold and you can certainly back lyrical about um generalized synchrony and and and Joint F energy minimization of a generic sort but I think that misses the point I think Ma was making um um that you know part of the the multiple Loops of circular causality in sort of you know cultural Niche construction for example um you know really does rest upon um the capacity of any individual within an ensemble um to um act given they have selected a particular course of action a particular path into the future which they have beliefs about the consequences of and you know to me that that that that is an authentic kind of agent so you know perhaps well both of you Dan you mean your your your ethology background um are you aware of anybody's actually sort of Bitten the bullet that was introduced to US during the workshop and actually started creating active inference agents that actually plan and learn from each other and carve out their own Wallington Landscapes you finding those S joint joint Minima I don't know specific uh citation but certainly you've given a research Direction and it reminded me of in the shared pretentions work where thinking about what what does a shared Ensemble what would it be for a planning Ensemble or for a collective it could be a shared protention as in there's the same plan disseminated across the ant nestmates it could be that they have different maybe nonoverlapping but complimentary parts that would be shared it could be having a shared reference but they could have different plans or it could even be a shared consequence without even a shared reference so there's so many ways in which especially when we consider ensembles that that are heterogenous and they have different kinds of umelt different kinds of ways of engaging with the niche and and possibly some common niche as well as some uniquely accessible ible components there's so many flavors of Ensemble that I I can can only hopelessly or helpfully try to think about what tools and infrastructure and coordination would even give us a grasp to be able to sweep or design or visualize across that otherwise we might be locked in way too early to what it means for the on emble to have a collective pretention and then that's not going to apply to every ant species and if it can't even apply to all the ants out there then it's not going to work for aliens Alex can I can I just respond to that and then ask ask your your view on that one so I think it's a really important point and it um one of the big takeaways for me on our paper on the variational synthesis you you're applying the renormalization group to the the coupling evolutionary to um sort of phenotypic uh time scales was that it was not uh feasible or appropriate to talk about consp specifics in isolation that it was you had to have the all the varieties of predators and praise and things that we um that not even animate not even agents in the mix to actually um derive the um the you the non-equilibrium steady States at particular at particular scales which you know speaks directly it is not um it is not U useful to um you know if you like take the idealized gas mentality of 20th century physics and now apply it to agent-based modeling to assume that all the agents are exchangeable and are identical in some sense and then are converging on some common narrative um or some common um is not the right way to you know to simulate these things you can't do Predator prey relationships you know for example you can't have um even simple economic games uh but furthermore you you you when people talk about depletion of resources you actually have to model the resource you know whether it's oxygen food or water as the things in in that ecosystem so I think that's a really important observation which just speak to the challenge of using numerical studies to test out some of the hypotheses that uh that we've you know been been exposed to um you know you know if one just pursues agent-based modeling without introducing that heterogeneity I think one is making exactly the same mistake um that people make when they apply uh equilibrium physics to non-equilibrium systems um you know you know a true gas is not an ideal gas and it only has the kinds of properties and breaks detailed balance and expresses Nona in virtue of the fact that there is no exchangeability you can't assume that every atom or every agent in our context um is exchangeable with it with every other agent and to connect again to Commons and and that model of well within the firm we assume cooperativity and hierarchy in the market we have a flatness and a competitiveness and then there's this Commons that has sort of elements of each so then I think about doing ecosystem modeling it'd be like well we have within the ant colony we assume cooperativity possibly hierarchy among ant colonies you have red and tooth and Claw and competition and then it's like but can there be a unifying framework that would have the two genotypes and two phenotypes and two species and two adjacent niches and keep on adding in these levels of Distinction and seeing all those different variables in their joint Evolution across multiple time scales rather than the concatenation of a Cooperative hierarchical game NE within a lateral competitive game it's like capturing what those models get the the width and the height of but bringing it into a framework where it's being seen as just one joint evolving process so C is the letter of the day Alex can you speak to caes of cells what what that kind of heterogeneity and structured diversity would look like from the point of view of tissue for example or an organ or okay wait what was the c word you said I just didn't hear it it's any any word starting with a letter C you can use but you have to oh I have to use okay this is the game um Community I'm gonna pick that one just because it starts with c um so okay Carl threw in organs and organel but I'm going to push that aside for just a minute to address I think one of the questions I heard emerge from yours and Carl's discussion which was uh is there anything like what you are arguing for which is avoiding the mistakes and by the way Predator prey model came immediately to your speaking Carl um about these classical models that assumed interchangeable agents and is there anything that looks at these heterogeneous systems or heterogenous agent interactions and a couple of thoughts I don't think there's anything like directly what we would want and I think that ties or centers around our even the arguments you and I and many others in the community are making about uh why it's difficult to Showcase why active inference is so important even for places like machine intelligence um but I did write some thoughts quickly uh so I do think uh the first thing that came to mind weirdly enough was an application in the world of machine intelligence where they had some environment where they had a Comm community of even though I say it with slight distaste large language models or Chad gpts or gpts that had some extra modality equipped to them and they had this like little Community now it was a virtual very simplified Niche if you will um but the idea is that they could observe this community um and so these different gpts would and I can try to dig up the paper and send it to Daniel later it's one of my students that actually pointed it out to me they would assign different roles they had different personalities if you will um and I know that they investigated that to some degree now of course that doesn't have any active inference in there and I'm not sure what degree of maybe perhaps light reinforcement learning they introduced but there is something like that and there is slight baby steps if you will uh in investigating some notion of embod multi- embodied agent systems I think the AI Community is still sort of barely starting to embrace embodiment let alone activism and let alone embeddedness which is the idea of a community to go back to the seword of multiple agents interacting with each other um in investigating how do social norms emerge or cultural norms and cultural constraints and relationships at this like more Global level so I think there is a little bit of those Sparks of that thought and obviously I might not be aware of further follow-ups another place that uh I at least see the potential of no longer having these interchangeable particle like agents is in multi-agent robotics um I know that they have done things like soccer competitions and they have these little and again each of those robots they are the embodiment of what you and I want which is a version of a mortal computer so it's a little like assembly of mortal computers and there's a Cooperative nature so and then a competitive nature so you get a little bit of a very constrained real physical real world Niche um obviously it's in the constraints of it's a soccer game or some very simple task so I know in multi-agent robotics there is some development along this path and it'd be wonderful to have like a a a hardcore because I'm more of a simulation roboticist a hardcore roboticist to talk to us about the state of multi-agent Robotics I think that's the place where you could begin to test uh hypotheses and kind of or theories that are constructed even from an active inference point of view uh into these real world World robotic multi- assembl so you know they have like we could analyze the competition Cooperative Dynamics I think that's our best bet in terms of what maybe exists at the closest to literal agents that aren't just very simplified even though they make simplifications in robotics too uh in order to have a little brain in there that you know does something meaningful usually learning is turned off which is my own criticism of Robotics um and I think the last comment I wanted to make and again getting back to to the Notions of what we need so again you and I have a fondness I think for some of cybernetics and cybernetics has this notion of complex emergent systems and we you know you have these little uh building blocks at one level that follow their own physics or their own physical laws and then they maybe assemble or emerge into these higher level building blocks that are subject to their own laws then you have bottom up and top down causation which is a notion that you know comes from cybernetics and we always talk about that but then I think about like Santa you you might be familiar with the Santa Fe Institute and all their wonderful work like uh what was it not just Herbert Simon but uh uh John Holland and all the wonderful work on complex adaptive systems but they always did at the end of the day when they simulated it they did the particle thing this interchangeable particle simulation and then Tred to analyze like those convergence Properties or those emergent rhythms or patterns that kind of emerg in those systems with no to my knowledge and you know and I did study complex adaptive systems a while ago uh not with that heterogenity and let alone any of the complex agents that we could begin to design today even with deep learning regardless of its limitations and so I think that that maybe even turns into an answer to your question highlights an open opportunity maybe revisiting and recasting Notions of cybernetic and complex adaptive systems in this idea of well can we actually build more complex agents uh for example that actually have neural substrate structures or something that we can simulate and then learn to acquire their own characteristics in their Niche I think the reason that maybe a lot of people avoid it or maybe why I don't perceive uh development in that direction is the problem also of Niche and you and I also had an and not to try to highlight this paper but in the appendix we talked about the body Niche problem uh and that is this issue that uh you need a niche if you don't have a niche and you're designing these agents independently Le like deep learning typically does to apply to any Niche uh you're missing really half the problem which is understanding the physical laws and how do we simulate them such that people can then play with those models maybe you don't have access to a xenobot like Michael Levan does or I don't have access to organizes though I wish I did um but you you know maybe we can simulate these and so I think that we need the community maybe this is a call to the community to develop uh viable niches virtual morphologies virtual uh areas that we can then connect these agents and then simulate many of them and try to analyze and understand do the things we predict even with these interchangeable particle models apply to these heterogenous communities that might emerge and again I guess that just to wrap up and I promise I'm done uh you brought up organel and organs and I think that highlights something that I wish I had more easy access to rather than uh being put into this position where I well may I need to build it which is you know these virtual organ systems or virtual organ organel and trying to understand how these dynamical systems interact and then I can kind of take that as that's my body or that's where my Mark and I can kind of start to put some instantiation of markall blankets and then say Okay I want to still maybe a neural circuit maybe I put in a predictive coding model into this kind of virtual morphology so I can analyze and understand does do these interactions of the time scills actually come through when we actually build you know physical instantiations of these models and so I think that's going to be one of the biggest roadblocks to even my dreams of mortal computation which is this notion of myology something that we can actually work with that it gets as close as we can to reality without reality and then Niche um because once you have those problems solved then you can start building multi if you have an efficient way to simulate them and I'm sure lots of people that are watching might be worried uh because it's very hard to simulate you know real physical systems let alone allow machine intelligence researchers to play with them so I blabbed a little bit there but does that sort of give you some something to bite on based on your questions something to chew on something to consider um in the last little bit I'll I'll ask some questions so I'm going to read one from the live chat and then anyone else will just do some quick fun random questions sort of like sampling from the uncertainties expressed uncertainties of our live chat colleagues okay Pablo FM wrote is playing a simulation acquiring knowledge SL experiences through simulation of scenarios and how can we use active inference to do game how do you fellows make your active inference work playful and fun if or when uh let you go Carl you go I'm not sure if there are two there there are two questions there um a practical answer to um why you might want to instantiate um a simulation environment with active inference agents or indeed just a single agent in the in the spirit of computational phenotyping but I think probably more interesting in the um in the context of um communities of Agents um is in the spirit of this sort of cognitive um agent-based modeling um is the ability to um parameterize the prior of the agents to best explain or replicate Legacy data from a particular ecosystem be it Financial epidemiological meteorological um purely ethological um and having optimized the parameters you then roll out into the future and in rolling out into the future you now have the opportunity to have a grounded in principle base optimal Plus in activism and embedded um um estimate of what is likely to happen in the future again in the spirit of weather forecasting but of course this becomes much more um if like useful um when you have control of the system so Financial Market forecasting or epidemiological forecasting or uh climate change forecasting you're assuming that there's a substantive um an um anthropomorphic um um um anthropological um uh contribution to um climate and all the other factors that you know um pertain to um resilience and uh resistance to things he you know health and the like um as soon as you can intervene then once you've got a digital twin of this ecosystem that has been optimized in relation to um the past data in the sense that this now has the highest model evidence or marginal likelihood in relation to that Legacy data you've then got an opportunity to do scenario modeling with interventions so you can now ask questions what would happen if we um cut this Supply CH train chain what would happen if we impose this lockdown in this viral pandemic what would happen and so on and so forth furthermore because you've got um um uncertainty in the mix because this is effectively going to be um you're using variation procedures that we use in active inference to actually simulate um the behavior that best matches the um the Legacy data you've also got for free uncertainty quantifications so not only can you say if I intervened on this system and did this um then we would uh I can predict that in six months time or in six years time that and I can give you Bas in credibil intervals o over that moreover you can now apply the principles of active inference to roll out under different interventions which now become policies and evaluate the expected free energy of each policy and select the one under the constraints the common the characteristic outcomes um that you uh commit to at this point in time and you can now use this scheme to make recommendations on the basis of the um the relative probability of this sequence of interventions versus that sequence of interventions so I think there are great practical importances of being able to put these principles in silico and basically do Evolution very very quickly so that tomorrow you can read the you can read the consequences probabilistically uh of evil in this setting and that setting where the setting is actually something that we can intervene on or even if we can't just to know the consequences of our Collective Behavior Uh today so I think that would be um that's why you might want to make you know to to invest in in these applications um with the fun and uh having uh and play I think that's a different issue I think that's basically um um you know one way of looking at the um the way that authentic agents behave under under the free energy principle which is basically to explore and gather the right kind of evidence that uh provides literally evidence for their generative models of of their world um which basically entails novelty seeking but now complement this with this um basing mod reduction we've been talking about also trying to find the minimal explanation for the data and the evidence that has been secured by exploratory playful Behavior arriving at particular insights which uh those are har moments that say oh I see it's just one of those or this one thing explains all of this playfully acqu acquired data and perhaps that kind of mechanism you know may be a really important aspect of the communal um um you know um cognitive agent based off yeah cognitive agent based I can't quite remember the acronym it was very nicely laid out in one of the I have to go back and look at the live streams a follow-up question from the live chat Arno wrote I agree with the approach to calibrate to pass data and forecast but it's already quite challenging for current models that only have a few parameters to be calibrated how then does one calibrate prior beliefs of millions of Agents I'm talking about epidemiology and economic models sorry I I got to give a technical answer before Alex gives a gives a more entertaining answer uh you do it it's only difficult because 99% of agent-based modeling uh uses the sample distribution um you do not do that you use variational you actually um simulate and roll out the um the um probability dist utions themselves using variational calculus and that means that what was yesterday um really difficult agent-based modeling and certainly epidemiological modeling now becomes really trivial uh orders of magnitude more uh more efficient what you have to take care of is exactly the thing that Daniel was was highlighting before which is the heterogeneity you need to put that into the mix and this manifests in a number of ways um so when you apply um variational procedures to effectively um roll out population density uh probability density distributions over various states of being am I infected am I staying at home um have I died um you have to make sure that the heterogenity is part of the generative model so this is sometimes called stratification so for example in epidemiological models you'd have eight bands for different age groups and if you notice the way that the epidemiological data was generated is usually age segreg ated so that people could stratify their models um another um way in which this heterogeneity becomes absolutely crucial um when you come to um agent-based modeling in the context of epidemological or infectious disease modeling is called over dispersion now over dispersion is the fact that uh you get heavy tailed distributions that inherit from the heterogenity so you get this for free once you put heterogenity into the model but it does mean um for every if you like subgroup of um the population you do induce more parameters but remember the number of parameters doesn't really matter it's the amount of time that you take to effectively invert your model and if you try and do that using sampling based approaches which is currently the the norm you could take literally weeks to uh do something um whereas if you use variational procedure then you could do that within within minutes so it's a really important question a very practical one yeah to restate direct variational inference on beliefs about ensembles themselves being variational belief inference on agent level semantic beliefs rather than sample from the sample what the agents are sampling yeah you're you're you're invoking the The Twist yes how on Earth can you model a variational density over things that have variational densities you can do it under the F princi room because the only observable things about an agent are on its Markoff blanket and of course they are not belief structures so there is a lawful relationship between you can um if you like simulate cognitive agents using traditional agent-based uh approaches but what you have to do is to amortize or to or to um um basically work out the um the mapping between the history of what something sees um and what it actually does but in principle you can do it by marginalizing out the um the beliefs of the agent in principle I'm not and that's that's why I asked that's why I'm so intrigued by this distinction because that will be a challenge Alex and then Carl just in our last minute here what would you say was a favorite memory or moment in theory and practice for active inference this year and then what are you excited about for next year um I'll go I'll go first so a favorite memory of active inference let me think quickly well I I mean I'm going to be a little biased just to give a highlight and a shout out and everyone we'll get to share in that memory if you stick around after the round table is actually some of the results from one of my PhD students um via and he's gonna give a talk soon and uh the idea is that uh one of the things I was proud of is he's been working in the area of robotic which I think is a really good playground uh as difficult and uh hard as it to democratize for everyone to work with robotics I think it really shows you how far we need to go and I think that's a good place to make strong advances an active inference um and one of my memories is that I remember him kind of showing me uh because he's working with partially observable Markoff decision processes and how difficult and unaddressed or lightly addressed it is in the field of working with raw sensory data to try to construct these continuous models and I remember him showing me some of his models rolled out trajectories in the future and I I'm always kind of in a fondest in my heart for ative models and I remember him playing out sort of these robotic memories of this robot trying to you know fantasize where it would want to go to kind of like move this block or to knock a ball towards a go so that was one of those moments where we were really proud because the idea is we actually we actually made a breakthrough uh in actually showing that we can construct a very robust framework that applied across all these robotic control problems and so I was immensely proud of that and so I think my students drive a lot of some of my fond memories um and uh and then we got uh Chris Buckley shout out to him uh to you know uh kind of give us his insights into our work too and it was a really really nice work and so and I think the best way to say it is stick around if you want to know how that all works I was really proud of that so one of my favorite memories of the Year awesome Carl with the last words of a fun memory and uh a direction or an excitement for next year right I'm I'm not going to tell you about my most fun moments that's that's very personal but my my my my excitement for next year is to see whether any of your hairs go gray after you do the mon so many of these of not only these uh symposia but all the meetings because for those people who don't know Daniel has to run this almost single-handedly nonstop for a marathon sometimes 10 hours at a time I don't know how he does it at least unless you're using hair dye you're you're you're standing up physically remarkably well sir so I just see how long that lasts thank you Carl I look forward to the marathon changing continuing and flourishing in the coming year we appreciate you Daniel thank you guys it's been awesome okay thanks a lot see you soon nice talking to you guys bye everyone e e hello welcome back now we have the much anticipated student presentation that your adviser Alex recently told us will be awesome by Viet dwin on Dynamic prior preference learning for scalable robust deep active inference so thank you Viet take it way hello uh hello hello everyone um my name is zet I'm from uh the neuro adaptive Computing Laboratory um Department in computer science sest to technology and I am Alex Adis uh and today I'll be talking about Dynamic PR preference learning to scale Aus active INF so yeah first of all want to talk about the motivation for this so nowadays you know from fure work in robotics um excels in implementing robots behavior and planning for example U creating um we can you know creating autonomous robots that can activ explore in the environment um additionally um acquiring we can also do a lot of Robotics applications for example acquiring knowledge and learn skills continuously um and um this also envisioned um cognitive and developmental robotics um so there exist a goal um for um robots robotics research um that's we can try to build a robots that can continuously develop through interactions with the environment and their learning processes um and it is also also better um to have interactions with their uh physical and social world in the manner of human learning and cognitive development so there's a lot of robotic applications um around um and for example one application can be to build AA robots and um building um social collaborative robots and stuff like that um so underly all these robot work um uh we is like active infs research where we um often build a generative models um um it's so the development of Robotics actually depends a lot on um activ framework because um there's a great need for research um that integrates the generative World model into robotics control system um as also deep learning is increasingly used in constructing complex action planning algorithms for solving control tasks um so um we can see that there's a need for scaling um active in systems especially deactive INF system um to integrate into the real world applications so um resar in active influence has led to many improvements um in terms of robustness of um model based agents that are applied to both M processes um and partially absorbable MVPs or P MVPs um so an example of an mdp environment should be Mountain car um uh for those who um knows or studies reinforcement learning already and um this m carment environments um in mdp we only know position velocity right um and that is the actual state of the environment um but when we get to PVP for example like robotic tasks um we as agents see only the image frame and we do not know the underlying states such as position and orientation of the block or um join states of the robots for example right um so basically active inference is basically a form of inference and learning that sticks to balance first of all the um goal orienting objectives or prior preference um so this signal exploit um the instrumental signal um so that the agents can seek the goal um and try to pursue its goal um and the other thing it needs to balance the epistemic terms or epistemic signal where the foraging it's cares about for aging and exploration driving signals um so researcher often actually encounter great difficulty in providing the agent with effective prior preference as well as Computing useful instrumental signal from the agents the priors um especially in the real war applications where um the environment is very complex the robotic control task and um the trajectory or partway um to be desirable um and task specific goals um often requires uh a very very long um trajectory of preference um so it is not that actually practical to provide uh a prior preference distributions especially the um stream or roll out of goal images so that is the reason why we need to find a way to ALS o learn to generate the prior preference signal in active inference so on the right here is just our um preliminary results um but we'll get to that in a moment so um yes so we actually uh given without uh all of this uh right we need to um uh we need to predict a uh uh drawout trajectory of a goal um States dynamically right um it can be a goal state or goal observations but basically the goal is um if you look on the figure on top here um the top row is the real trajectory where you see the mountain car right there's a car the it is to go backward and to build some kind of a momentum and then go full to the right to get to the um to the flag if it is only going to the right then it can it does not have the enough moment M to to go to the top of the hill so um so in here is the um actual observations you can see that the trajectory is um a successful trajectory and the goal here is to um for each uh observation at each time step we predict a um immediate predict predicted PR preference um image or goal at that single time step so that we have um the instrumental signal or idea of uh what we are trying to learn and then um from that we learn the actions planning uh on uh from the PRI preference signal so in this talk we are we specifically analyze this issue um the issue of um uh prior preference learning um in the current literature um and discuss potential resolutions for scaling the instrumental signal inherence to active inflence by introducing the um contrasted recurrent State prior preference or crsb um this is the model um learning framework that um we also have um done in the past so uh this methodologies frames active inference agents in term of progressively constructing and adapt adapting um a prior preference um at each time step so this also facilitate the Dynamics emission of a useful or dense very dense um instrumental signal uh at each time step so we also um study how our prior preference adaptation scheme uh when operating jointly with the lat and space driven epistemic for aging terms uh offers active infation um a very nice flexibility in challenging problem cont context um such as um spars reward um environments um and also composed of uh continuous observation and action space for example um for example robotic task um they only we often like do not know the spars um do not know the Danse reward um functions in the actual real environments and we we only know if the robot has risk the goal or not right um and also all these environments has you know varying goal States so um we can also see that providing the uh uh or manually craft the uh distributions of go is very not very um practical so uh yeah so here here's the outline of this talk on the introduction I've had just covered um so we so first of all I'll just assume that's uh everyone is new to active inference and I'm just going to briefly cover active inference backgrounds and deep active inference um and also cover a little bit about um prior PRI preference learning and problems um uh of all those prior preference learning framework and then I'm just go on now go through the actual methodology which is the dynamic Prime preference learning um and then uh first of all is the world model um next is the self-revision mechanism and then next is the um learning pre preference models and then finally um putting all them together um to make an agent um and then um I'll show the results and then this cion okay so first of all um I want to um cover a little bit about active inference um so just a brief overview about how active inference work and what is the underlying theories behind it um so um it is a ma mathematical framework that describes how intelligent systems are human interact with the environment um so it describe it also describe the environment as the generative process um and the intelligent systems um are the agent or um us as generative model um so active INF built on top of the free energy principles um this means that we as agents or intelligent system continuously observe the environment and forming our and correcting our belief or our own representation of the world um so the generative process um defines a real world environment here right um this is treated as the data Generation Um process where the information patters are sensed by the agent so you can see that's the um hidden State underlying hidden States denoted s here um it's generate a sense of um some kind of um a observations um denoted o and this is also um sensed by um our in intelligent system so on the right here is the generative model uh is uh the actual agent that we are trying to build on the intelligent system right um the intelligence system also has an internal State um which is trying to um estimates the generative process um States um so this is called um the prior or the internal relief prior to the observations of the environment at a particular time stamp um so this is denoted as the random variable s or state and P of s here um is the distribution over the random variable s um so this make the agent's generative model um the generative model represents the agents's approximation of this generative process in this world um by continuously seeing the observations or making the OB observation o um and modified or updating uh its belief so we actually call the belief um s after uh being updated after uh making the observation for the posterior because it is the posterior after um making the observation right so in order to learn the model and estimates the posterior we first need to in introduce these terms first of all is the prior over State um which is denoted as PS which is also the distribution over State rate P ofo here is the model evidence um it is the distribution over the current observations um denoting how well is our model um at predicting the real data um and in order to compute this easily this is often written out the some probability of observation given every single state and then next one is p of O given s this is the likelihoods um and the probability uh this is the probability of an observation given a state so this is a function it gives a probability given an input State and then the the last one is the posterior functions uh p as given o which is um the posterior function that Maps the observation to the states um so uh normally in traditional activ inference um the per function would be the the transpose of the likelihood Matrix for example um and deep deep learning literature um it is often called a encoder and the likelihood here is a decoder so how do we estimate the postor now um we have to find a belief that maximize the modal evidence right because um at the current time points uh there there is only one single observation o here and it is real right so P of o p of that observation um it's going to be uh one right so it should be the probability of one um and then um we use the states to estimate um the observations and then it has to be the the probability of one two right so uh that that arrives us at the conclusion that we have to um maximize the model evidence P of O So maximizing P of O is also minimizing equal to minimizing minus l p of O right because um it is uh minimizing the surprise um so in the beginning we say that's we continuously updates our belief after making an observations of the environment um we call the belief after making an observations a posterior um this is Q of s right um so to compute posterior we want to put in a um we just want to um call this you know any kind of um distribution um over the states um then for example we can sample it we can randomize it right um and then we try to gradually trick this distribution so that it's used the highest modor evidence or it's used the lowest surprise so how do we choose uh the actual uh posterior um so that it bus the lowest surprise right so in minus not P of O here is the actual um uh minimizing surprise that objective that we are wanting to to minimize right so we can uh analyze it like uh we can uh do it with a base theorem this not base theorem but base formula or theories yeah um and then uh to evaluate the surprise minus log P of O we need to sum out um the hidden variable s from The Joint distribution POS um but well this summations there's a problem right because that we do not know like all the hidden States all the hidden variables in the environments um it is hidden from us right so it is normally very very hard to uh to sum up all these um State values so uh if we cannot sum it up then we can to approximate it so first we introduce a posterior um in into this U sum right we multiply both the denominator and the numerator by the posterior um so this is the belief that we are trying to TW right so um in here we're trying to um took this posterior so that um we can um minimize this term so this gives us a r sum where s uh the ratio um P oos and um over the the distributions um Q of s is wed by the Q of s this is like weed sum right so according to the inequality of convexity or gen inequality says that's the convex function of a weighted means is always smaller um than the weighted means of the same convex functions um so you can see the convex function here is minus lock of something right so minus lock of something is just a hyper uh is just a parabol something like here um and uh this is smaller than a what means of the complex functions also we put this some outside right um so this is also applied to the context of probability Theory um and then we can arrive at this uh yeah this equations so forly it's called an upper bound uh this one right a quantity that approached from the above right so this is also means that this is a free energy um when we find a posterior that minimize this free energy term we will have a chance of approaching the lower surprise points um which potentially increasing the U modor evidence so after we have this terms right here we can just uh put the minus inside and then it's become this form this is the conventional form that we call variation of re energy um and after decomposing this we will have these two form uh which is the K Divergence of between the posterior and the prior uh minus the weighted sum uh of the lock of um the um P of or given s which is the poster functions or now this is likelihood so basically um the K Divergence term is called the complexity and the minus l p of our given s is called accuracy term where the complexity terms computes how um surprise are states um two state distributions posterior and the prior are far away from each other or not right and the accuracy terms uh defines how accurate our model is predicting um the actual observations right so this is also relatable to the variational auto encoder where um we comput the loss of the variational auto encoder um to be the K Divergence of the posterior and the Gin distribution with a mean of zero and the standard deviation of one and then we also have the uh mean Square arrows of the actual predictions of the images right um and then VP and MF is a little bit um different than from variational out quod VP is varation of free energy and MF is marginal free energy as in the um AC difference tutorial um the paper um so marginal fre energy says that um we compute the K Divergence between the the poster and the prior and the prior is computed from the transition model um not the um not the normal distributions of mean of zero and the standard equation of one so uh here is the uh General model of trying to build a debal inference model right so here's first of all we're trying to encode right using the posterior functions So In traditional active inference this is the p s given o is the transpose of the likelihood Matrix P of given s this is normally learns with the cical distributions using dally conjugate prior um and then uh in the Deep learning literature um this is often known as the encoder uh that's uh Often parameterized by the neuron Network um and then o here is often be an image um and state here is just um some hidden vectors that we are trying to compute and then next one is the um La function right so normally we text in the states um of um the agent and then we try to estimate the uh predictions the observation predictions right um so in traditional active inference this is known as the likelihood Matrix or likelihoods um per se um this is also learns with the Catal categorical distribution and using the conjugate prior and deep learning literature um this is known often known as the decoder um that is often parameterized by a NE Network so uh as we know the variation of free energy formula the complexity terms K Divergence here right and the um uh accuracy term is the minus Lo po right given s um so here we compute the K Divergence between the encode States and the actual prior States here and then um next one we comput the um actual L probability of the predicted observations and the actual observations right here um and that makes the um whole objectives where we comput the two uh objectives right here that's uh also be the um variation of free energy formula so we just talk about how inference um in active instrument work but there's more we want to build an INF system that can interact with the world right so when it comes to interaction between the agent and and the environment we also needs to have actions and um also roll out in town step right so basically we have another elements here which is at and remember I also mentioned the transition model right so after we take an action in the world um we um the the world um or the environments evaluates one step right and it uh it change its own State and then emits a new observations um and we as agents or intelligence system also try to predict um given the action in the current states what is the next States and what and this States is the prior over um the states of the next time step where we have not made the observation OT plus1 yet so after we have the st+ one as prior um we do the same process as I have mentioned before so uh we make the observations um try to change um our belief uh after making that observations and then learns from the uh new observations to take our prior into the posterior so here we learn it um using the varation of fre energy or marginal fre Energy Formula so as we can see from the previous slides uh we do need somehow U to somehow plan the actions to interact with the world so if we're trying to minimize or surprise um with the current observations um so in the action planning we aim to take actions that minimize the similar free free energy term in the future um this is often called as the expected free energy by the way um so in here uh we introduce some new terms right theeta um is the Modo parameters the reason why this is here is because we want to also compute the um uh modal the surprise over the modal parameters or the uncertainty of the modor parameters um so you can see that the expected free energy in the future is kind of similar to the version of free energy formula it's only needs um another model parameters parameters and the given the pi or the actual pl's policy or plant actions so after a lot of deriv right I'm I not getting into like the real devation here but um here's the end results that's um something something minus um minus the weighted means of the uh L P of OT right so I'll just focus on this um term is because this is the um goal of the talk today so basically um this term blck of p is the expected free energy derivations um of the instrum uh instrumental term so basically uh this also means that we are trying to maximize the now we're trying to take the action that maximize um the future observations that matches our um prior parap distributions or what we want right so normally um to learn to take actions based um uh to learn to take actions um to align with what we want to see in the future we of needs to craft and provide the prior preference manually in advance um and then we Tex the option based um we take the options um that's maximize the likelihoods um given that distributions um but this is often not really practical in the realistic World um and because we cannot like craft the manual distributions like every single time we encounter a new environments are encounter a new task right so uh what we can also do is we can also provide a goal state or goal observations directly to the agents which is more practical um but in the end the prior preference distribution is very sparse um and this provides little to no learning signal to the agent as you can see on the figure on the right and the left right so left is a prior preference distribution and right is a single goal distri go State distributions or go State image so uh what can you do to dynamically uh produce um the uh uh prior preference right we can try to learn a prior preference model right that's um produce a goal image at each time step so here is uh on top is the prior preference model that predictions right um and uh in the bottom here is the model prediction of the future um given the sequence of planned actions so we are trying to say that we um we want to take the actions um that have uh the roll out in the Futures predictions matching the prior preference uh roll out in the future so how do we do that we first have to learn the model first right so in learning uh we can try and say that each time we achieve a goal then then we Mark the whole trajectory as the prior preference trajectory or the um positive um trajectory or expert trajectory and then for each observations marks as go we use a deep learning like another deep learning model to estimate start observations um for example right and then um in inference time um we actually train when we train the policy planner to propose the actions right we make the policy planner proposing the actions first and then predicts the future observation based off of that um actions U that is planned by the policy planner and then um we actually learn the policy planner based on the signal that is emitted from how aligns the predicted future observation would be to the predicted goal um the predict the predicted goal here is the actual prary props model outputs right um then we train the policy planner with the um mean Square Arrow or um some kind of objectives um that's uh predicts uh uh some kind of objectives between the predicted go image and the predicted future observation so there's this problem uh where uh uh it exists between um all these uh learning schemes because it is very computational expensive um so it is generally expensive because we have to work directly with the images so for example in learning we have to use a decoder of the um of the pr prose model um to actually predicts the goal image at each time step right um in inference time is also computationally expensive is because um it has to learn from the images itself and computed down from uh the um the back propagations will have to flow through the decoder um back to the action planner so here it comes to the actual methodologies uh we actually use Dynamic preference preference learning um in the step space um instead of the observation space right so first of all I want to talk a little bit about the world model so uh this uh model architecture is um basically what I have just um drawn uh in the background sections but this looks with robots images so in here we have the observations um Ot Ot and here's also the observation the same observations OT minus one um and we Tex in the encoder encoder is the encoder posterior text in the encoder and we have the translation prior um and then we have the decoder right encoder encodes the imion have the states and then decode the um the image and the state back to the image and then computes the accuracy loss right accuracy loss uh is minus L prop of the P given S as we have discussed in the variation of free energy sections and then the complexity is the k k Divergence term between the posterior and the prior right so how do we compute the priors we comput from um the previous steps uh States and then we use the transation model um to estimat the next stage prior right so that's um that's how we can get the um prior so uh current work um and popular Works uh regarding the world model or generative models um often integrates um the model called um RSS Lang which stands for um recurrent State space model um in the translation uh model so it's to zoom out a little bit um we are trying to uh integrate some kind of temporal informations Through Time right um so that's the state of the environment or the states that we're trying to predict be more accurate and it carries more informations with time um so for example um we cannot um encod the uh enough information from just a single image because for example um there can be like uh position and velocity right the velocity is acquired um through multiple images um and cannot be acquired from a single image if does make sense so what we often do is we try to integrate a recurrent NE networks that text in the previous States um or the previous recurrent States H my T minus one um and then we also text in the um encoded observations um and then we in transtion um we produce the recurrent new network we produce the current recurrent States um and also predicts the outputs to to put into the uh transition model um and also it's the recurrent stat will also uh be served as inputs to the posterior Network um so that's basically uh the idea and then uh the other way uh around we could just uh do uh the model as normal so uh in active inflence Asian texting the actions um that's it belief uh that would lead to its preferred outcome uh using instrumental signal um to construct this prior preference um Pas has um for example provide a go State observations directly to the agents for example mazaka in 2021 with contrasted active INF um we can also manually craft a PR prence distributions um we can also dynamically produces the preferred observation that each time set um but as I mentioned already um all these method is one way or or another um very expensive um or it's very not practical or in the reward applications or um uh it is very computationally expensive for dynamically producing the actual observations um for for the goals so we can also use Behavior clothing but it's its trajectory diverges from on the training directory due to um small mistakes made by agent as well as environmental um stochasticity um and also it is because of ident IND independent and identically distributed assumptions of positive exper data distribution so we can um we we try we will kind of avoid it because it's it's not going to um be uh the same right the two distributions of the actual um actual trajectories in the environment will not be the same as um every single positive trajectory of the expert data so to tackle this we try to leverage a small quantity of seat imitation data to learn an uh a neur network that's dynamically produce the preference Over States not observation but State um at each time step so this effectively um produces an easily generated dense instrumental go signal um and we also design U the agent such as that it moves according to the trajectory that is is shaped toward its own estimation of the future States or future preferred States so it's trying to notch the its own trajectory on to watch the imitation or positive data distribution so as you can see on the figure on the left um so you can see that the in Behavior cloning it's try uh the actual trajectory is diverges away from um the actual imitation data where the prior preference over the states trying to um get uh trying to be shipped um toward the imitation data itself using the model signal and r is our agent um is called robust active inance um and it learns from both epistemic signal and instrumental signals but in here we'll just focus on the instrumental signals and how to construct it so uh yeah so basically we use a decoder um or the likelihood functions of the generative model to only visualize the preferred observations um not that in training or test time we do not use the mod learning uh we do not use the actual observation uh goal observations by itself we only use the hidden States so the goal here is to um get the states the goal States at each time step be predicted by the prior preference model right um You can see that's uh on the top here is the real trajectory and the bottom here is the actual um dream that's the dream of the robots that it wants to be right right um and then we are predicting that dream right so here is the actual model uh of the uh um Contra recurrent State prior preference learning um uh so basically it has um it has also similar architecture to the rssm to the world model um it also has have an encoder um here it also have the translation um but it does not have the decode um so how do we actually uh learn this uh um PRI preference model so first of all uh we uh learn the world generative World model as normal right um but when we say whenever um it's uh whenever the trajectory is uh positive or a successful trajectory um we try to contract um the state uh of the uh crispy I just call it crispy um we try to contuct the state of crispy to the um world model States and then um whenever the um trajectory is negative we try to push away the states um from each other so that the crispy model um can only predicts the um positive States so how do we determin um if it is a positive trajectory um or the negative trajectory without having a very sparse signal so we will have a thing called braw or the prior preference signal so how do we compute this R signal so imagine um this is the real trajectory um and it is a successful trajectory for example right so we're trying to compute um The Pri preference rate by uh having the rate of the successful States is at one and it starts from one and it it decays backward so decays um very fast and um if so in this like really far away stat from here um they do not have um enough signal because there's no goal um like there's no direct goal from that states and then there's real trajectory that is failed right um You can see that the mountain car cannot reach um the flags on the right here so whenever it's built um we can just plap plap the very very last observation as minus one and then we start to De Decay backward negatively um and then uh we have the prior preference rates uh signal as um very negative so the reason why it is it has to be last is because um we want to um we want to blame the last observations um hardest so um in order to uh in order for the agents to just push away from the failing States very very very strong um in the very last stage so in the real trajectory that have like multiple successes um we try to put all the one in all the successful States and then just DEC backward um as similar to how we Decay backward in a single success case so with that say we now uh know how to compute um the RO and then this will serve as the dynamic signal um to the contrastive loss and it creates a dynamic behavior of the contrastive loss um that's trying to either push away or um contract um the two states of the preseed state and the world model States so for example in here um there's if there's positive row um the contrasted block would be um Contra Contracting all the state together and maximizing the similarity between the precipitate States and the world model States and then we also learn to minimize the K diversions between the um complex uh uh the posterior and the prior uh of over the states of the crisby model um this is because we want to estimate the transations right so we transition to the um next goal State and then this goal state has to match the actual goal um that is the current stage of the envir of the rssm model or the world model um so in contrast right if row is negative then we're trying to push away um all these states away right so we're trying to maximize this um distance or minimizing the similarity between the um world model States and the crispy State and in here notes that there's no complexity objective is because we don't want to um predict the next goal that is um the failing State that's basically the the idea so that arrives at the uh general formula for the objectives of the contrast PR State pre preference model um basically uh it is just masking out um the of the row and then multiply with the the K Divergence or the complexity uh between the posterior and the prior of the crispy model um and then we uh minus the row and seam of U the actual predicted State and the world model state so SE here can be very much um uh any kind of similarity functions for example um cosiness similarity um and then in the in our work we actually use the actual cosine similarity so in training time we have we have just talked about training time right so we have successfully assuming that we have successfully um learned the prisy model that predicts the future goal um and in inference time we've used this U crispy transation model to estimate future roll out trajectory of the goal right so we can see that um on the top here is the observation um actual observations and in the second row is the actual prior preference roll outs or predictions of the future goal uh given the um given the current observation so given the uh plant actions we're also training the plant uh the plant actions right so the actor trying to predict and actions given um the observations the predicted observation in the future um so we're trying to say uh this is uh we're trying to compare uh the actual PRI prior preference roll outs with the actual predictions given the actions um and then we are trying to uh learn the action planner based on the signals or how diverge um the future goal is to our future predictions of the world given the options so uh there can be so many um there can be so many reinforcement learning agents and active in agents that can utilize this technique um of Prior preference learning so active in agents um I put my works here um it's uh ra and then there's also Chris Buckley's work and baren M work about deep active inflence in variational um policy gradients methods um and also Auto uh work is is the scaling of the uh act active inflence in PDP and not in car environments and so on right so reinforcement learning agents there's all these sorts of um actor critics algorithms like dreamer series um soft actor critic dreaming um ddpg tqc um nd3 and so on so there's a lot of agents that can utilize the framework because there's no change um in the original architecture there's just an add-on um to how um to how an agent would apply the priate preference learning signal um into the system so is the actual flowchart of U of the agents basically um we start the agents and for each episodes we evaluates um the environments um using the world model and the policy planer um actually basically we uh uh evaluates in the environment we let the robots play we let the robots do the task and then we collect the uh data and add them to the replay buffer for example right and then we get get the main training agents um logic um and then first of all we sample the training data from the buffer so in here we trying to have um we also have to use um The self-imitation Learning buffer um so uh yeah so self-limitation learning is very good technique um normally to balance the number of um successful and failing trajectories and we also learned um so first of all we learned the world model encoder decoder transation on rssm and then we uh train the epistemics model but in this case we don't need to care about it um and then next is the um Contra recurrence State PR preference model from the data and then um next we imagine the future trajectory using the world model and the policy planner right we try to estimate the future actions um to be um what is the optimized feuture actions for example which we estimates the future action based on how uh current states is and then the world model will be continuing to bouts in the Futures and we dream of the Futures and next is the part that we actually um infer on the crispy model we use a crispy model to imagine the future preference trajectory um given the goal um given the current observations and we predict the future goals roll outs goals right so after that we learn the policy planner right using the instrumental signal the instrumental signals here is computed from the um difference um between the future goal and the um predicted future dream of the world right and then with that the policy planner can learn from uh from the uh instrumental signal so here's our um preliminary results on um active inflence agent so basically um we can see um the two top rows is the mountain and car environments and the to bottom row is um robotics meta World um push button U environment so let's look at the first row for example the observation here is a mountain paru have bried the goal and for each um observation here we uh we predict a preference roll out or preference um or a goal State at that single time step so in here we can see that um if the car has not have the momentum yet um then we have to build the momentum by predicting the car going to the left for example right and then after it has built some kind of momentum we trying uh the actual prior preference model predicting the car would go to the right go forward right when it goes here and uh the pr prence model will predict the car to be um going closer to the goal so something like this so the same applies to the robot environments where the actual observation is the third row and the prior preference roll out is on the fourth row so we can see that uh it is also trying to estimate the dream or the dream of the goal of the robots is to actually push the button so um there's different kind of experiment setup that we are trying to learn um basically we have uh four trials on each environment and then we train the experts um using as soft critic or po proximal policy optimization um to collect um a small amount of seat data normally is about uh 20 episodes something um 20 to to 30 episodes um so we test mostly on three main environments or suits um first of all is Mountain car there's only one task um metal wall is 13 task so here's um all red robots here's metal war and then the robot suit we have two task and then um the cam regarding camera setups uh we use um three views for Middle world and four views for um the robo suit so the green check mark here denotes uh that's the robots has successfully solved the task um here's the robo suit door open door and then um here's Robos suit lift where it can successfully lift the block and here's more U solving agent tasks so basically agent with Crispy exhibits and ability to perform successfully early on um and uh so agent with without crispy uh struggle to actually succeed on solving the environment um and we also test on other environment as well for example gazi and Robotics or and risk task um you can you can see in the middle image on over here so actually you go to the discussion sections so actually um I want to make a note that um with respect to the crispy sub model um our model utilize contrastive objectives to adapt its parameters so basically it solely learns on um estimating the world model in coted States while pushing itself away from undesired World model States or trajectory so this does not require or the learning of the of the decoder so we we only make sure of the Lear decoder in the generative World model um to to visualize the uh to visualize the preferred observations um from the estimated crisy States so experimentally we remark that this is an um that's we only use um this decoder um to as an auxiliary tools um to prove um that the uh states of the contusive recurrence prior preference model uh is useful so theoretically Behavior cloting will be unable to achieve a great uh success in multiple multi goal environments due to its Reliance on a fix and finite size uh imitation sample pool um the expert trajectory in this fix pool might further differ from the actual trajectories um that the agents needs to take to solve the problem at hand um for example uh there's a distributional gap in observations and therefore and um it requires different sets of actions to be taken for example right um so on the other hand right crispy learns to produce dynamically um the gold States and not the go observation right um so we can empirically confirm in simulation that um our agent um with Dynamic prior preference learning does not ER from the goal mismatch problem in the training phase so this is very applicable applicable to the multi-goal environments so um we can see that um this PR preference learning uh techniques has a lot of practical applications in different robotic tasks um with for example those with potential dangers in the operations um and functioning for example when human safety must be considered right um for example a um trying to drive a autonomous driving car system where it has to have or it has to predict um the future um goal right the future goal is not to hit the human for example right um and we can also make a lot of advancement in imitation learning in research community and it can be um applied uh by the wide range of reinforcement learning and active inference agents um and then the last one is to scale the the active INF agents on the real world robotics agents so future work words actually considered um importing or integrating the dynamic PR preference learning in latent State space model um for example varation of Dynamics discret variable AO encoders or tension waiting models um and also uh there's another model's physical neurorobotic system which entails an embodied and active and survival oriented formulations of active perception and World model learning so in this uh talk um uh we have discussed a lot of ideas about being able to estimate the particular goal or preferred State um at any particular time step is very useful um for Co cognitive control agents so as our results demonstrates the agent can then learn to adapt the tech actions that leads to um a estimated goal um and then um in contrast to the approach is taken in the impementation learning and behavior cloning literature um which train the agent's policy based solely based on the fixed collected expert data sets in our framework the expert signal is estimated dynamically um through an adapted prior preference model right closing this Gap the domain gap between the actual trajectories and the collected trading imitation trajectory or preferred trajectory um and then we also discussed the gener generative model um and encode decod transations and how the varation of free energy work and then how to learn the model and then we finally we discuss the contrasted recur and state prior preference model um with a self-revision mechanism and then discuss the model learning and that inference um objective um and how to integrate that into the agent planner learning system and then we show the results for um the robotic task and then mentions how it can be applied to different fields so that concludes my presentation um and thank you very much for um listening thank you that was amazing really great job with the background around math done only like a PhD student can but it was great to see how you brought it all together and connected your research to it all at the end so thank you that's all for now see you later thank you see you e e okay hi welcome back welcome back hi Denise hi thank you for for joining and looking forward to your presentation so take it away oh thank you so much and thank you for having me today um today I am going to talk about the potential impact of decentralized AI through active inference and spatial web Technologies and what we're really going to be discussing is um the potential aspects of this next era of computing uh it'll be incorporating a lot of the research that you're hearing about here over the next three days um and decentral in my talk decentralized AI is kind of taking a main focal point here and there's a couple of aspects um in technologies that are evolving right now that uh I think it's really important to be aware of um and they factor into uh what I'm going to be talking about today so one of the main uh things that is occurring right now is our internet is evolving um we have a new protocol layer that's layering right on top of the other protocols that are part of the same internet that we're talking across today um the difference is is what these protocols hstp hyperspace transaction protocol and hyperspace modeling language is the programming language um what these are going to enable is for us to have programmable spaces so instead of just programming pages in domains with content we are going to be able to program digital twin spaces of anything and this is going to enable context to be baked into everything in every space so all of the attributes about various things in various spaces and nested ecosystems and all of their inter relationships to each other so one of the things this is going to enable is uh active inference agents to be distributed across our internet and I see this as the next wave of innovation and through the through active inference these agents will be aware of each other and the network uh they'll be able to sense the world in real time and learn from each other and this is going to provide unprecedented levels of interoperability um so what we're talking about here is first principles uh automated systems and distributed collective intelligence so first let's look at the challenges in traditional centralized AI structures um or architectures uh centralized processing constraints you have high energy demands and scalability issues limited adaptability to realtime changes and you know we're all familiar with the limitations of uh these deep learning models they require massive amounts of labeled data and processing power they have limited ability to generalize across different tasks and environments AI models uh lack adaptability to real world complexity they're only good for one thing at a time uh like one uh optimized for one type of an output and they lack interpretability and transparency in decision making they have high computational resources leading to high costs and inefficiency and they struggle with generalization across different scales and tasks now we need decentralized Ai and decentralization really matters because it increases resilience and scalability for autonomous systems it reduces es Reliance on Central servers and data hubs and it enables autonomy across distributed networks so this Bridges the Gap in AI we move Beyond centralized AI for scalable autonomous systems that can operate independently and locally now we're all familiar with this guy here Dr Carl friston and uh he he gave a great talk uh first thing this morning on uh renormalizing General generative models um so what I'm going to be talking about are various components in in some emerging Technologies along with active inference that potentially realize this research paper designing ecosystems of intelligence from first principles now you know the this uh paper was published in December of 22 and um it really laid out a new framework for autonomous intelligence that mimics self-organized systems of nested intelligence found in nature and in in this paper it described a cyberphysical ecosystem of natural and synthetic sens making in which humans are integral participants what we called shared intelligence and this in the paper it says this V this vision is premised on active inference a formulation of adaptive behavior that can be read as a physics of intelligence and which inherits the physics of self-organization and in this context we understand intelligence as the capacity to accumulate evidence for a generative model of one's sensed World also known as self- evidencing and at the end of the abstract for this paper uh they also note we also consider the kinds of communication protocols that must be developed to enable such an ecosystem of intelligences and motivate the development of a shared hyperspatial modeling language and transaction protocol as a first and key steps towards such an ecology so what I'm proposing in this and what is proposed through all of this this uh technology together uh with all of the various parties that are involved in building these components is that the future of AI is shared distributed and multiscale an entirely new kind of autonomous intelligence able to overcome the limitations of machine learning Ai and as we know uh from understanding active inference this AI is knowable explainable and capable of human governance and it operates in a naturally efficient way there's no big data requirement and it's based on the same mechanics as biological intelligence it learned learns in the same way as humans so what I want to do is start with the spatial web protocol uh first because this is kind of the um the framework for these agents to be able to have this distributed intelligence structure so what is the spatial web protocol it's just the evolution of our internet into spatial domains um so people refer to it as web 3.0 it's just the nest next evolution of the internet but this protocol has become a global standard a new global standard for the last four years uh this this standard has been in development with the i e uh which is the largest core standards body uh in the world and back in 2020 they deemed the spatial web protocol a public imperative which is their highest designation and this summer it was voted and approved um as the new global standard and it's in the final administrative uh cycle with the i e before it becomes uh public so there's a couple of things that are happening with this through hsml the programming language for this new protocol all emerging technology whether it's iot or ar VR distributed Ledger technology um you know all of these Technologies they're all going to be able to uh converge across the internet because they will have a common language for interoperability and in that you also have a language that AI can understand and uh communicate together and communicate with these various aspects of Technology as well so this is going to produce the next era of computing and deoe called it back in 2020 so it's the next evolution of the internet and it's going to be empowered by active inference AI now just to kind of understand what this means for the internet um let's take a look at what a website domain is today with the worldwide web a domain basically contains a website and these websites they serve up content so whether it's you know images or video or audio or you know information data um that is what websites are capable of of doing now a spatial web domain is quite different because a spatial web domain could be anything in any space right and it can be programmable a spatial web domain is an entity with a persistent identity through time that has rights and credentials so you're talking about going from a library of pages in our Internet to the internet of everything and hsml becomes a common language for everything so it provides unprecedented levels of interoperability between all current emerging and Legacy Technologies uh it's a programming language that Bridges communication between people places and things laying the foundation for a realtime World model for autonomous systems and what you have are programmable spaces hsml is a cipher for cont context anything inside of any space is uniquely identifiable and programmable within a digital twin of Earth producing a model for data normalization and from this we get adaptive intelligence automation security through Geo encoded governance multi- Network interoperability and it enables all Smart Technologies to function together within a unified system and then this produces a Knowledge Graph and digital twins of anything everything potentially our planet so as as this knowledge graph grows within the programming of all of these spaces across the network you get this Universal domain graph that enables a digital twin of our planet and all nested systems and entities within it we get nested ecosystems uh there there will be intelligent agents both human and synthetic across this network uh they will be sensing and perceiving continuously evolving environments making sense of the changes updating their internal model and what they know to be true at any given moment and acting on the new information that they receive so why do we new need a spatial Internet Protocol well the increasingly graph like nature of global data the opportunity for Automation and uh autonomic activities using context whereare cognitive AI the need for composable systems and applications including the governance of such systems the intri the intrinsic need for secure transactions the rise of machine learning and neural network computation and Edge Computing the need for explainable AI and robotic governance and the rise of iot and sensor mesh and some of the guiding principles of the spatial web standards are spatiality representing information as locations and relations in hyperspace ownership users own their data in digital property security secure data collection transmission and storage privacy individual control over digital identity and personal Data Trust reliable permission driven validation of users assets and spaces interoperability seamless navigation and asset transfer across spaces and responsibility creating technology ethically for the benefit of humanity and what what has become of these Global standards is a socio technical standard so the spatial web is a soci technical system of systems uh takes into the account the um so the development of the spatial web must address both technological as well as sociological considerations socio technical standards integrate and balance the technical social physical and legal use of technology and the spatial web standard is is a socio technical standard and what this result m in is multiple scale cognitive Computing so the spatial web plays the role of a collective nervous system for smart infrastructure it hosts cognitive Computing uh inference perception learning and action selection by combining and abstracting distributed information the spatial web enables the formation of complex ideas and facilitates decision-making cognitive Computing occurs at multiple scales in the the spatial web individual nodes host cognitive Computing functions and networks of nodes collectively perform emergent cognitive Computing so hsml hyperspace modeling language it's a human and machine read readable modeling language and semantic data ontology schema there are some key elements to it there are entities activities agents contracts channels credentials domains and it includes hyperspace and time so hsml enables the expression and automated execution of Legal Financial and physical activities in the spatial web and all of these entities are identifiable and knowable by these intelligent agents through this uh language throughout the network now let's look for a second at the agent aspect agents and activities so agents and that can be human agents or synthetic agents um but these agents are entities that sense respond maintain a model of their environment and take actions to achieve goals and the activities that occur they're single actions or partially ordered sets of actions that are performed by the agents and they unfold over time so agents interact with domains and other agents to perform activities in the spatial web so what the spatial web facilitates is a distributed ecosystem of intelligence if you look at current state-of-the-art AIS they're siloed applications they're built for optimizing specific outcomes and they're they're really unable to communicate their knowledge frictionlessly and collaborate with other AIS but this facilitates a a an ecosystem of intelligence that is a self-evolving system it's learning moment to moment and upgrading its World model and in that way it mimics biology while also enabling uh general intelligence so this is shared intelligence at the edge of everything so how active inference works in the spatial web it enables real-time decision making you have agents that are predicting and adapting to their environment from their frame of reference in the network uh they're continuously learning from their environment enabling realtime adaptation and unlike traditional AI which relies on pre-programmed rules and static models active inference continuously updates its models based on new data allowing it to adapt in real time so these active inference agents would seamlessly communicate in real time with each other sharing their knowledge and perspectives and learning from each other these active inference agents excel in handling complex uncertain environments by continuously minimizing uncertainty in fact they can quantify their level of uncertainty and this makes them suitable for applications where conditions are constantly changing so decent calized AI takes the processing to the edge to the edge devices you have a distribution of processing and power uh superpowered gpus are not required the processing can take place on local devices throughout the network with infrastructure that already exists like your laptop or your mobile device or devices that haven't even been um invented yet um but we're already seeing a lot of those kinds of devices is coming into play uh with you know um Apple Vision Pro and and you know the metal glasses and and different aspects of these Technologies so the processing takes place on these local devices throughout the network you have faster response times and reduced Network congestion and it uses reall live data real-time data from iot sensors machines and everchanging context that's embedded to the uh in the network so this is the sensory information that's coming in for these active inference agents so that they can uh you know do the perform this action perception feedback loop and continually update their model of the world of their environment from their frame of reference and it eliminates the need to tether to a giant database active inference agents throughout the network learn and adapt from their own frame of reference within the network enhance data privacy by minimizing centralized storage and it minimizes complexity it uses the right data in the moment for the task at hand and with hstp with hyperspace transaction protocol you there's natural guard rails uh included in this framework so it creates a secure ethical and user Centric environment for AI applications Foster ing trust and reliability in digital interactions uh you have data privacy and sovereignty transparency and explainability security and authentication bias mtig mitigation ethical AI guidelines interoperability and standardization user empowerment and control accountability and governance safety and reliability Dynamic policy enforcement and what what this results in is in intelligence at scale uh through this network of distributed intelligence it enables a cognitive architecture of the collective intelligence of multiples of agents that continuously communicate coordinate and collaborate with each other you have individual and specialized intelligences together on a common Network speaking a Common Language hsml efficient and Powerful cross communication to perform tasks regulate systems and address problems in real time and it scales up and grows in tandem with humans through hsml we can take human laws and guidelines make them programmable for these agents to understand and abide by in real time so we're talking about unprecedented levels of cooperation between human and machine intelligence and what you get that is an Ever evolving collective intelligence uh decentralization is an imper imperative for intelligence at scale these agents represent scores of people from all over the world sharing their individual and specialized knowledge all coming together on a common Network and it's based on each agent's own world model you have unique perspectives from whatever frame of reference uh within the network nested ecosystems and levels of of self-organization and Collective shared intelligence so the idea is that anybody can impart their knowledge and their wisdom into these Agents from wherever they are in the world and this actually acts to preserve all of the uh cultural differences all of the um the belief differences the governing Styles um you know people whether they're in uh Kenya or Singapore or anywhere else in the world they should be able to create these agents to solve their local problems and then share that with the world because then these agents can share their intelligence with each other so the spatial web protocol enables a decentralized secure and interoperable web with the potential to profoundly transform Global Industries and everyday human life now let's dive into um kind of the the potential that exists for this idea of rgms these renormalizing generative models and the spatial web and we heard a great talk this morning uh by Dr friston he uh you know he talked a lot about the you know the science behind these uh these rgms so I'm going to kind of uh maybe maybe break down some of the aspects of how these work as leading up to then what they can do within this uh potential potentially within this architecture so you know with rgms one of the interesting things is that it's a universal architecture um they're a UniFi unified scale-free framework for AI That's capable of reasoning learning and decisionmaking and they can generalize across multiple tasks handling small and large scale data efficiently they combine perception learning and planning into a single system and they can be used to learn scalable architecture over space and time similar to human thinking they require significantly less training training data than traditional models while achieving Superior results so what are these rgms they're a new class of AI models based on active inference they use renormalization to simplify complex data into hierarchical structures and they operate on the free energy principle to minimize uncertainty and adapt in real time so the significance is that they represent a Leap Forward from traditional deep learning methods they reduce data requirements and uh improve efficiency and it's one single framework for all applications so the the science behind rgms renormalization and multiscale learning renormalization is a technique from physics that's used to simplify complex systems to maintain consistency at different scales and rgms break down data into multiscale representations hierarchical structures rgms are actively learning from small details to large patterns simultaneously and they analyze like with an example would be like an rgm can analyze an image by recognizing edges then objects then scenes and understanding the the concepts at every at every scale so key Innovations and advantages of rgms why they're Superior to traditional models they learn efficient efficiently with less data it's a unified system that handles perception and planning simultaneously and they understand conceptual relationships not just patterns and they can adapt to new environments with minimal training uh aspects of them are scale-free modeling renormalization discrete representation hierarchical structures fast structure learning unified perception and planning and explicit handling of uncertainty so if you look at how the brain process data processes data um you know we we perform core screening all the time when we're learning right we break down sensory data into hierarchical patterns um that help us focus on the most relevant information um kind of this chunking of Concepts to to grasp different aspects of um whatever it is that we're we're learning or focusing on in the moment and rgms can mimic this approach by processing data hierarchically focusing on important patterns so it's a first principles approach uh using a way to simplify models without losing important information um they can model space and time Dimensions like we do and can learn the causal structure of information and it helps handle large scale problems efficiently using less data and less energy now when you consider uh scale-free modeling you know uh scale-free modeling these rgms they operate across different scales so from very fine details to highlevel con uh complex systems so unlike deep learning which struggles when moving between small and large scale problems this scale-free architecture allows rgms to seamlessly work on a variety of problems including small data problems and large scale strategic planning so when you when you're thinking of the concept of scale free you know this is a great example here of a green apple you know you can you can see the green apple as a concept of just you know pixels in an image or a volumetric object or on a branch on a tree or in an orchard and you know the the way that these uh rgms can model these conceptual understandings of things is it can understand all different scales of what a green apple is so when you think of like um the supply chain or or factory operations or or anything like that these agents can understand the what's happening across the manufacturing line on a very uh local level but also what's happening across the entire distribution line uh throughout the entire system so they can predict both short-term movements and long-term outcomes so you know the F details as well as uh large larger uh outcomes with the highlevel complex systems all at the same time and they don't lose accuracy when switching between local and global data scales so hierarchical modeling in rgms hierarchical data processing rgms process at multiple levels of abstraction from fine grain details to highlevel Concepts uh they use recursive learning they continually Loop over data learning at each level continually refining their understanding of Concepts as they process new information and you get hierarchical representations the they build models of the World by zooming in and out of different layers of detail and this is similar to how human cognition processes both micro and macro scales so you know think about when you're when you're thinking about the concept of driving a car if you're talking to somebody about driving a car or you know you're you're focused on the aspect of driving you're not thinking about the car as all of its various parts that make up that complex system like the brakes and the steering wheel and the seats and the every aspect of the engine you just call it a car right because that's all you need to know in the moment for whatever whatever you're uh doing with that concept and so this is kind of a similar idea um and this layered approach allows rgms to understand complex multiscale problems without requiring vast data sets so conceptual modeling for decision making um R GMS then focus on conceptual modeling versus just mere pattern recog recognition um deep learning can recognize patterns but struggles with conceptual understanding it can recog recognize correlations without understanding relationships and it can recognize patterns but it doesn't understand the underlying meaning and rgms on the other hand focus on conceptual modeling they capture multiscale hierarchical structured Concepts that represent both fine grained and Abstract features it builds a hierarchical understanding of the world capturing cause and effect relationships so rgms understand the why behind pattern s not just the what and the key difference rgms are not just a better generative AI but a transformative AI framework that builds a hierarchical understanding of the world rgms can reason about cause and effect relationships making decisions based on conceptual understanding that are more aligned with real world outcomes so how do rgms leverage active inference by incorporating active inference Active Learning and active selection while incorporating realtime data to adapt its predictions and actions so what you get is realtime learning uh they adjust their beliefs based on new sensory input making decisions on the Fly optimizing both learning and decision-making processes and they also are able to uh have unified perception and planning so deep learning typically separates perception like recognizing an object from planning like deciding an action rgms can combine both in a unified model they handle both within the single model and learn Concepts that guide both recognition and future decisions so for example you can have an autonomous robot that can recognize a door handle and also plan how to open it using the same model so how what does all this have to do with the spatial web architecture so the potential that I see with rgms is that um the nature of how they work fits very well with the nature of this architecture within the spatial web and just to kind of bring you back to what I was talking about earlier you have hyperspace transaction protocol this manages data flow between interconnected systems and it fac facilitates interoperability between different AI systems and the hyperspace modeling language provides a common language for these systems enabling uh computable context and decisionmaking and it provides a shared framework for context aware decisionmaking so this protocol aims to create a standardized way for different Technologies and AI systems to interact and share information across program able spaces within our global internet and this unified model this becomes a unified model for diverse applications handling complex multiscale systems through unified perception and planning it's a single framework that can handle multimodal tasks so you have versatility they can perform across multiple applications whether it's object recognition natural language processing strategic planning uni using a single Universal model and it reduces the complexity uh it integrates the data analysis the perception and the strategy execution which is the planning within one model so you have cost savings it reduces the need for multiple specialized AI models it streamlines system management and saves resources so this how the spatial web enhances the rgm capabilities the combination of this scale-free active inference with the spatial web protocol could enable seamless integration of physic of the physical and digital these protocols allow rgms to operate across distributed networks exchanging insights and information between physical and digital environments enabling more sophisticated iot uh applications uh context aware Computing through the spatial web rgms would operate in context aware environments these agents uh adjust their behavior based on the physical and digital surroundings and interoperability multiple rgms these active inference agents and autonomous systems can operate in different environments whether it's your Home Smart City businesses and they could communicate collaborate and coordinate actions through the hyperspace transaction protocol their uh energy efficient distributed intelligence instead of relying on centralized energy intensive data centers intelligence could be distributed across networks of smaller more efficient devices and this this creates a nested ecosystem uh similar to a hallon architecture and if you're not familiar with a h what a hall on is it's something that can be both a hole in itself and also part of a larger system so the spatial web and rgms would form a nested ecosystem where each node in the network whether it's a domain device agent or subsystem functions as a hul on it's autonomous yet interconnected contributing to a larger system's goals as part of the larger interconnected Network like a smart City infrastructure um your smart home could contribute the contribute data to your neighborhood grid which informs the city infrastructure decisions all while functioning independently um if you if you look at this picture here of you know uh part of a supply chain image you know each item packed inside of each of those boxes is a hole in and of itself but it's subject to and it's part of the larger system the Box the box is subject to the the trucks that are moving it and distributing it or you know whatever's happening within the factory with those boxes and then that factory itself is its own hole that is subject to a larger ecosystem of the entire supply chain as a whole so when you're looking at what this next era of computing really enables it's just nested ecosystems that can be empowered by this natural intelligence and active inference Ai and the spatial web protocol can work together to create sophisticated adaptive systems at various scales so rgms for autonomy and self-organization with recursivity the scale-free models naturally form a nested hierarchy each level of abstraction in the model is complete in and of itself and yet part of a larger whole uh autonomy and cooperation active inference agents exhibit both autonomy in their decision-making and the ability to cooperate with other agents and self-organization the renormalization technique in introduced in the paper allows the system to dynamically organize information at different levels of conceptualization similar to the self-organizing nature of uh honic systems or these in these nested ecosystems so this last part um I want to explore the transformative potential of decentralized AI for the future of intelligence systems so the impact of distributed collective intelligence the impact on this next era of computing would be enhanced problem solving and Innovation increased efficiency and scalability democratization of knowledge and Technology resilience and robustness enhanced decisionmaking personalized and context aware Services ethical and responsible AI collaborative platforms and ecosystems enhanced learning and education empowered communities accelerated scientific research and environmental environmental and sustainability Solutions and the implications for future systems and Society the convergence of rgms in the spatial web could transform Industries enabling decentralized context aware systems that operate with minimal human intervention industries that could be impacted smart cities you would have autonomous systems that improve efficiency at all levels Global Supply chains rgms could optimize processes from local shipping to Global networks personal to critical systems homeostasis so whether it's personal devices or critical infrastructure a Continuum of intelligent adaptive systems can maintain homeo stasis at each level in our Health Care Systems personalized treatment by integrating real-time data from wearables and medical records education we can have Aid driven adaptive learning systems that evolve with student needs and Environmental Management we can have ecological systems that could be modeled and managed more effectively from Individual habitats to Global Systems balancing local interventions with global goals and the future of AI and human civilization the development of renormalizing generative models within the scope of active inference and the spatial web promises to have farreach far-reaching implications more adaptive AI these systems are highly adaptable capable of operating in new environments with minimal training efficient Edge Computing rgms enhance the power of AI on devices like smartphones and iot sensors reducing the need for large data centers improved human AI interaction with explicit uncertainty models these systems interact naturally with humans seeking clarifications as needed safer AI systems they recognize and handle novel or ambiguous ambiguous situations reliably preventing catastrophic failures in unfamiliar scenarios so they can fail gracefully and accelerated scientific discovery rgms expedite research in fields like genomics and climate science by rapidly identifying patterns and formulating plausible hypotheses thereby guiding further experimentations so the key takeaway here is that uh all of the all of this research and all of these Technologies together can provide us with a future where AI systems can interact seamlessly with the physical world autonomously optimizing operations and improving human lives so um yeah there's my website I have a podcast and I'll stop sharing now thank you Denise awesome I don't know what time how how's how are we on we have uh 15 minutes and I'll read some of the questions in the live chat okay sure I'll start with Pablo who wrote how will all of this affect the gamification of processes in organizations well I mean the the interesting thing is is through hsml you know all of these emerging Technologies are are going to converge together and become interoperable in ways that we've not experienced before you know we've had this um kind of vision of of this augmented reality existence but the framework hasn't been there and this is going to provide that framework so I think when you're talking about gameification in every aspect it'll probably become kind of a natural part of of our existence and how we move through our day how we interact with with our work with uh brands with you know anything um you know I I think that there's going to be a lot of of really interesting ways to kind of perpetuate that cool there's a lot of great questions in the chat so okay I'll read one from Tintin so they wrote it's impressive the obvious question I have is of alignment of this system in terms of that alien message from elaser gudowski for example and then they clarified what that exactly is so the key idea there is that simulated agents evolve and came to think faster than their creators so at a moment they took control to ensure their own survivability it's the question of how to ensure that way more intelligent and faster systems keep aligned with our values right and so one of the things that uh is really interesting to me about this is through hsml we can actually collaborate with these systems in the sense that we can take human laws and guidelines and make them programmable so these agents can understand and abide in real time and you know um versus uh who you know Dr Carl friston is is their cheap scientist um they were involved in a a program called Flying forward 2020 and and they were doing a lot of uh testing around this and because it was it involved a drone project that I I don't know there were multiple countries throughout Europe it was with the European Union that were involved and you know testing whether the they could program in laws around airspace laws and you know things like that they did all kinds of scenarios of like you know Medical Supply delivery uh perimeter security all kinds of things and what they found was through hsml you could actually uh program these guidelines and these these agents abide by them so I think we are going to experience this unprecedented level of of cooperation and collaboration so we can grow in tandem with the the you know with these uh agents and their intelligence and I think that we can also you know take some um solace in the fact of how you know active inference works and you know how our human knowledge grows you know I think we're going to see a similar pattern with this knowledge among these agents and the only difference is and and this is just me speaking from you know my own you know my own uh perception and and perspective of this but you know they're going they're going to be dealing with real-time data and with real uh a real grounding of this programmed these programed spaces and those are the things they're going to be operating off of to make their decisions right whereas if you look at a human you know we have so many other variables going on and every when we learn through activ inference it's a very subjective uh growth that's why you can have you know multiple children growing up in the same household and they all experience things differently right but when you have these agents and they're getting this they're getting their learning through actual tangible programmed information I think there's a a stability there as well if that makes sense and and and and again that's just my own uh you know interpretation and feeling around this and I could be I could be wrong but you know just kind of throwing that out there that's how we move forward yeah okay next question from David Highland who wrote thank you for the informative presentation Denise can you share any thoughts or information about the ownership model for agents is it a decentralized market how do we decide which parts are public goods yeah so with the protocol with a with hyperspace transaction protocol um because so it's interesting it really shifts to this uh kind of self- sovereign identity uh aspect of the internet because right now think about how web domains work you know if you want to interact with a web domain all the transactions are taking place under the umbrella of whatever centralized organization owns that domain and you have two choices you can either say yes you can have all my data and you can decide what to do with it all or I don't want you to but then I'm opting out of being able to engage with your domain well in the spatial web because now transactions can be at every touch point of every entity within the network then permissions can be programmed in a very nuanced way right so it really puts that back in the hands of the of the user of of the entity within it you can you can can program you can program guard rails around certain aspects of data while allowing permissions and free like Freer accessibility for other aspects of data so whether you're talking about a business you know they can guardrail off their data they can guardrail off certain aspects of it for certain Departments of employees some for their customer base some for you know um Partnerships whatever right um and going to be able to do that kind of thing too and the thing with this is this takes into account time the passing of time so you can set expirations on permissions as well so I think we're going to have a lot more self- Sovereign control in this in this area yeah that's a part that really excites and Peaks me as well like with the web domain or the email domain it'd be like saying well on one hand we're not going to look in your email let's just go with that for now but we do or don't accept letters from this area so it's kind of all or nothing and it's based upon like you pointed out the the custodianship of the all or none permission based around what boxes you can send where whereas having a more nuanced way to talk directly about belief sharing means that instead of just like plugging the USB into the port there could be even a active inference negotiation around what elements to share how to share when to expire all these other features which can can help like incrementally build rapport and and add to the relationship instead of doing all or none which can be just very prone to false positive or false negative in cyber physical interactions yeah you're so right Daniel yeah what do you think about learning trajectories and paths I know it's a interest of all of ours I guess like what what can we do to be learning and thinking about all these things well um so you know I I have I actually have put together a series of courses um to kind of give people a a real um kind of appr prerequisite knowledge understanding of what this transition looks like and the different components and what it means from all aspects of like you know basic understanding of of active inference and uh you know the free energy principle and the spatial web Technologies to you know decentralized AI what does that mean convergence of all this Tech across the network what does that look like safe AI how does active inference benefit that and then rgms let's let's look a little closer at at that and what I try to do is I try to take the complex ideas but make them more understandable so that people can really wrap their heads around the these Concepts and then they can go dive into the more technical aspects of it and I know you with the Institute you have a lot of technical knowledge there so I think that you know between resources like you know some of my resources your resources and then I'm sure there are resources out there that maybe I'm not even aware of and you know I think there's going to be more and more resources available around this modeling these agents you know uh building out you know these these spaces uh within the network um I know that versus is going to be launching a platform that'll be the first interface for the public around this technology and I think it's coming to the public at the end of first quarter next year and with that there will be tools where people will be able to build these agents as applications within the network and and you know I think at that point it's going to get really exciting um so yeah I I just think over the next six months we're going to see a lot coming about with this and if anybody is interested in the courses I've put together you know you can you can visit my uh my website and and sign up for them uh they're not available yet but they'll be available very shortly uh within the next week or two so yeah cool okay I'll all for last question ask the same thing that I asked uh Alex and Carl earlier what is a fun or a memorable moment from learning and applying active inference this year and then if you haven't already mentioned it what's something that you're excited for next year um this year honestly I I think that I have been on this Learning Journey in creating this course material around it you know and at the same time I've been building community and kind of having them be my test guinea pigs around it and stuff so I you know there's been this whole learning aspect for me around building this educational material so it it has been you know a constant uh feedback loop and next year I'm just very excited about the potential of um building these agents and what that means and and how we can tackle some of the the problems and the the issues and the challenges that we have um through this new exciting technology and this this uh framework so yeah well thank you for all of your great learnings and digested and and sharings and and all the ways that that what you talked about today I think connected a lot of the dots from hearing Carl's more technical view on the rgm on through what what does it mean to have multi-agent decentralized as opposed to just building up one agent so thank you again Denise thank you Daniel thanks for having me thanks everybody till next time bye e okay the next talk is going to be by Bradley Alysia and colleagues purposefully non-standard cybernetics an alternative to purpose driven control this is going to be a 1hour pre-recorded talk so thank you enjoy hello my name is Dr Bradley Al and on behalf of Jesse parent Morgan Hoff and Amanda Nelson I'm we're from the orthogon research and education laboratory and the cybernetics interest group and I'm going to talk to you today about per purposefully non-standard cybernetics an alternative to purpose-driven control so we start with a question what is the definition of behavior and this is varied throughout history according to the behaviorist Behavior was identified as a reflex elicited by stimulus associations the consequence of reinforcement and the current motivational state if we go forward to the cognitivists which is more or less the modern view they would say that behavior is a set of internal mental states that are responsible for observable behaviors with gold directed end States now it's of note that both behaviorist and nitive views involve feedback this leads to learning a notion of what constitutes a state and predictive capacity so we start with this paper Rosen BL that all in 1943 Behavior purpose and teeology published in the philosophy of science and it's a formative paper in the field of cybernetics so in that paper they have this typology of Behavioral classifications so we're going to return to this typology throughout the talk and we're going to think about how we can reorganize this typology but it basically defines Behavior as a set of nested things the red text Non classes or they're basically all other things other than the uh black thing that we'll be talking about at each rung of this typology so behavior is active it's purposeful it involves feedback which implies some sort of chology or end goal it's also predictive which involves extrapolation and involves an order of prediction so you might have first order second order or third order prediction so to understand what teeology is and where it comes from we start at the Middle Ages where teeology was the end State or the end goal of some behavioral process was thought of as Divine causation we then move forward to teeology As Natural causation then it branches off into two directions and the first was the vitalism of bergson and evolution by natural selection made famous by Darwin leading from Darwin's work we had AR smar who talked about tonomy which is purpose versus history and we'll talk a little bit about that later in the talk kind of discontinuous to this are two different streams so the first at the top is where behavior is viewed as habit that's by William James and Thorndike that leads us to the an molecular anti- teolog of Francis CIT and feedback is complexity which is summarized by control theory and dynamical systems at the bottom is our target paper that we just talked about these are the views of Rosen blue thin liner and this is feedback relative to a goal this led us to cognitive science and reinforcement learning at least the modern versions of this where people talk about goal directed behavior and variations on that in molecular biology such as molecular vitalism and the tame and U methodologies of leban janca and Ginsburg so we'll talk about a couple papers here and frame some of this in context so if we think about Behavior it's it's essentially a form of functionalism in the first paper here rod and Germain they argue that functionalism is a pre-scientific relic so they actually argue that the whole idea of functionalism is something that comes from this era of divine causality biology is not designed so why should we expect a function and this is something that of course maybe is assumed about function that it comes from some sort of design or comes from some sort of divine intervention on the other hand toonomic systems are defined by spatial and temporal information and constant iterations of States so we do have these t systems but include modern components only select behaviors can be defined as selection of a goal or derived from signals from the goal so that means that we can't necessarily paint Behavior with a broad brush of teeology only some behaviors may be theological and others maybe not not all behaviors and this includes biological and machine behaviors are go selecting and while gold directed behaviors seem to be purposeful and it may be true that some of these behaviors that are known as gold Direct are purposeful are there Developmental and evolutionary antecedents so are those Place those uh ancestral behaviors or those basil behaviors or behaviors that occur earlier in development are those also purposeful and the answer may be no so Rosen Luth at all Define behavior as any change in an agent with respect to its surroundings this is much broader than the Neuroscience psychological definition and it sets a contrast between all behaviors and a subset of purposeful behaviors so if a behavior is put under some sort of selection or if a behavior is selection of something in the environment this might lead us to a subset of voluntary behaviors which then some of those may be purposeful and so purpose is actually purposeful behaviors is actually a subset of all behaviors the demonstrate this we see that clocks both mechanical and biological are not purposeful as there is no final aspirational state mechanical clocks can keep time and you can set an alarm on them but that alarm is imposed by the outside and it's not a property of that behavior and of itself furthermore biological clock runs via physiological processes like a circadian rhythm and it can be entrained by external light but that again is not a final aspirational State more generally iology requires a signal from a goal so it's some sort of signal from a goal that's specific to that behavior and purposes progress to that goal so again with our uh example of circadian rhythms it's entrained by light but that light itself is not a goal it's just something that's out in the environment and the system entrains itself to it we can also see this with brains in that brains produce behavioral outputs but the ex mechanism is hard to pinpoint and this is the Rost and Lal article that we saw earlier and this asks the question signal from what to goal so there's a signal coming from something to a goal but it's not clear what that is this is something one of our co-authors Jesse parent works on this is a frontier map for teeology and related complexity Theory Concepts so the data from this is drawn from the complex world book by proc hour this is published in 2024 and it kind of goes over three different concepts that are interrelated so we have tileology organized complexity and emergence and so you can see that our paper Rosen blue 1943 is at the top and this is sort of the first paper that talks about teeology in sort of a scientific and in a a manner of complexity and so this is the first paper that talks about t ology is a property of complexity and of complex systems so a lot of papers were influenced by Rosen Bluth this uh theology was inherited by Weiner's book cybernetics and control in 1948 two other papers adaptive systems in uh by John Holland in 1962 and organizations and complexity also in 1962 the good regulator of course in 197 inherited this concept but it also inherited another concept from organizations and complexity which is organized complexity a good regulator then passes down this concept of tileology to the book order and chaos in 1993 and there's a direct link between Rosen Bluth 1943 and the tame concept of Mike 11 in 2022 now we going back to the concept of organized complexity this comes from organizations and complexity the good regulator in 1970 and the autop paper in 1974 both share this intellectual lineage the autop poli's paper lends that organized complexity concept to this paper on decepity in 1982 again to this book order and chaos in 1993 and then to the tame Concept in 20122 finally we have emergence which starts with the paper by Anderson war is different in 1972 inherited by the autop poli's paper 1974 in turn inherited by the dissipativity paper in 1982 in turn inherited by Order and Chaos 1993 so you can see that we have not just theology as a concept in which we've kind of talked about already but also a number of other related Concepts this is 11 uh this is the tame paper and then janca Ginsburg this is the U paper and so Lan proposes the tame or technological approach to mind everyone Levan's principle here is cognition all the way down we can see cognition in terms of Minds we can see cognition in terms of molecular phenomena we can see cognition everywhere in between and so behaviors all those behaviors are cognitive to some extent and as a result we get these diverse Minds that have these different goal directed competencies that allow those systems to achieve embodied agency so in Levan's approach you know chology is a very important concept not just for cognitive type behaviors but behaviors that that go all the way down to the molecular scale and maybe even down to the Quant scale second theology is a means to explain why agents develop towards goals and so you know we can look at uh agents such as artificial agents simple organisms like bacteria then we can see that like teeology as a way to explain learn how to move towards goals the next paper is this U paper by janca at all and in that paper they propose a concept called a limited associative learning and this concept is a minimal marker for Consciousness so in this paper they also Champion teeology is an intrinsic reinforcement system you have this inter internally regulated model in service of attaining external go andology plays a role in attaining goals so again in both cases there's a strong tendency to talk about goals the strong tendency towards to but we might ask the question is behavior at these different scales and any sort of behavior that sort of motivated towards some sort of function to logical and purposeful or is it simply a product of the history of the system so you know we could talk about heliology and purpose and we've kind of set this up that you know this is something that has kind of come out of the natural study of behavior but sometimes you know we don't really fully you know sometimes we ascribe heliology and purpose to things that maybe are just simply a product of the history of a system first example I want to give is the basov jabotinsky reaction or the Bez reaction and the lower left you can see a image of the bz reaction and this is a deterministic internal model that we can model with differential equations and we can solve those differential equations and get interesting solutions that produces a cave chaotic behavioral output or the Dynamics of the system which are sort of the unfolding history of the system so if we take an example of some sort of chemical soup we can achieve theet s ttin reactions or bz reactions by setting up this sort of set of differential equations these ordinary differential equations setting up an initial condition and we get these spiral waves and score wings that result and so if we have slight variations in the initial condition we end up with very different systems very different types of patterns in these um in these systems and we can describe this with ODS and one might say okay there's purpose to this but the purpose isn't necessarily clear when you run this simulation again and again with different types of initial conditions there's a chaotic Behavior at a variational dynamic dynamical output so this is an example of the tip trajectories of BZ waves this is from a paper I brought an angle in 1993 and it shows how there's a lot of variation in the pattern but there's a sort of um you know constant aspect of the pattern in terms of the tip trajectories so one might say okay the purpose is to produce these tip D trajectories instead of the actual pattern and the interactions which may be true but then of course you know that's something that I want you to think about as we go through the rest of this talk we can also plug the bosov Jaa tinsky reaction into a cybernetic model with higher level feedbacks and so I talk about this in this dimensions of morphogenesis lecture on the dorm YouTube channel and what I'm getting at in that talk is that there are different dimensions of morphogenesis there's onedimensional morphogenesis two-dimensional morphogenesis higher dimensional morphogenesis and so you see this kind of output and so I'm asking the question here is this purposeful or is this a product of sort of a a chemical dynamic IAL chemical system with a history and so we can also look at this as a cybernetic model where we have feedbacks and we have other sorts of complexity and you know we can fit it also into that behavioral uh typology of Rosen Bluth okay another uh another example is this thing called an Al estasis machine so an aasis machine is is basically the output of an internal model and so an internal model of course as we talked about is something that has some sort of capacity for memory and learning and maybe some sort of output that looks Behavioral or cognitive in some way so what we can do is we can take this internal model we can have an oscillator which is a stochastic process or a deterministic process depending on our oscillator and we can have a memory with capacity so we have this oscillator that produces an output and we have sensory information that can drive the oscillator forward this output creates a trajectory which is an interaction with between noisy perturbations and a historical trajectory that's shown in this red line and the historical trajectory is stored in the internal moduel as a memory with a capacity so as this red line unfolds there's a memory of it in the internal model and it responds to external perturbations which drive this uh Red Line in different directions and the oscillator tries to recover to the original state and so you get these dynamics that may look purposeful but really have this stochastic and adaptive component we get more into this aspect of dynamical control systems I look at intrinsic motivation in dynamical control system system so this is this tiamin paper in PRX life and so in this paper they talk about how you might model maybe something like an alist Asis machine as a set of actions generated without a specific reward signal so in this paper they talk about that as intrinsic motivation they model this with ex oft States and as oft actions so an action maps to a number of states and this results in a trajectory forced bya observation stochasticity this is entropy so you produce the trajectory there's an observation that you have and that's the entropy of the system and so you maximize actions that increase susceptibility rather than entropy these are noisy observations that wead do in perfect observational States and you can compare arbitrary length sequences of such sequences in the future this gives your internal model the ability to sort of integrate information and to find sort of solutions to problems the idea is that you have the system that can work towards a goal but it isn't driven by any sort of thology it's just driven by this memory component and the stochastic component and that's all you need a number of other papers here that I'm going to talk about I'm going to put this in the framework of anticipatory systems some our paper on tonomy and Ben and friston so to get to this we need to re kind of reformulate our behavioral typology so remember we talked about Behavior having these different components we have these non- classes so in this case we're going from Behavior to active Behavior to purposeful Behavior but that's contrasted with historical or marvian behavior then we have feedback prediction and Order of prediction and so er Mar refined the Rosen blue that all theological category to two parts purposeful agentic and historical Marian so if we go back to the Benny and friston paper they talk about the free energy principle is managing different forms of gold directedness and how you need negative feedback to attain homeostasis there's purpose in in cases where the goal is known which is a marvian system and surprisal was minimized which resultes uncertainty then we can expect purpose purpose isn't really just some sort of magical thing or it's not some sort of thing that requires a strict goal it's just certain conditions under which our behavioral trajectory exists we can also look at anticipatory systems it's the Louis paper and this anticipatory systems come from a generative model with depth so this is where we have temporal aspects of the system that lead us to teeology so it's more that the teeology is the history of the system and this depth comes from an internal model so basically if we go back to the internal model we have some process that generates Behavior we have this memory the memory is basically something that stores everything that the system has experienced and that's how we get toyology to kind of understand this a little bit more we want to distinguish between a marvian system and a non-markovian system so a markovian system is where you have in arrival times so there's a series of things coming in in time this is exponentially distributed on this diagram we have sensory information coming in we have some sort of encoding this encoding is a binary Channel with noise where we have these lines that represent some sort of sensory input or perturbation of the system and the inner arrival times the area between these lines so the inner arrival time is obviously not equidistant in this example but it's in between the red and blue lines or the red and red lines and so we can take those in our arrival times and if they're exponentially distributed we have a Marian system if they're not then we have a non-markovian system and so we've kind of developed this uh distinction between marvian and non-markovian sensory information and how that relates to an information measure in our gibsonian information work so gibsonian information is a marov but can also be non-markovian information processing system and the citation is here so going back to that behavioral typology we have active purposeful feedback prediction and Order of prediction feedback is everything that is circular non-transitive this is the present past and future where behavior is controlled by a margin of error Corrections of agent location the time te relative to a goal position or state and so if we think about this in terms of a feed forward process feed forward process is where the past is your input present is your process and the future is your output in feedback however it's a little bit different your input comes in the past and then you have this present State here and then you have feedback which is the past and that Loops back to your input which means that your input is now the future and so the future is actually present with past feedback so you see the loop here you have input to the present you have this feedback which is the past and it goes to the Future so now you have this future State before the present State and if that keeps going out you get this all output with alternative Futures so your feedback loop can be positive or negative and in this case it doesn't matter we're interested in is sort of the order of past present and future so we end up with this input we go to the present we go to the past we go to the future we go back to the present so we're sort of interdigitating different types causality here instead of just having this past present and future aspect to things we have this sort of convoluted aspect of past present and future if we go back to our historical map we recall that non theological things can be characterized as feedback as complexity and so there are two papers here that talk about pit control that kind of get us at that idea so pit control is proportional integral der ative control and if we think back to our example of chemotaxis and phototaxis we find that PID control can be applied to chemot texis and other types of biological control so this is The Balter and Buckley paper they talk about this in terms of variational free energy minimization given a linear generative model they talk about pit control resembling a reflex arc take mechanism so this is the reflex arc we find in sensory motor control and the free energy principle provides a more direct account of control this is where we don't have any hidden states in the process of proception so they're focusing on proprioception a but they also talk about chemotaxis and so they kind of give us this sort of uh variational free energy interpretation of this um in gool which is a more engineering take on this the pi and PD components are the proportional in the proportional derivative components and directed Network synchronization are all sort of uh components of this control mechanism in particular the pi and PD components of PID Define present past and future components in an internal model control system we can then also start to think of feedback is complexity in terms of nonlinearity and complexity as well so this paper by Wester or at all stimulus history not expectation drivve sensory prediction errors in a melean cortex this is from the bio archive so the author has framed this in terms of predictive coding so predictive coding explains cortical responses in two ways a feed forward model where an internal model of expected events occurs or in terms of an internal model of expected events or a feed forward approach and a calculation of prediction error for unexpected stimuli or feedback approach the latter are actually Global oddballs which defy expectations in a local context so in Mouse and macak which are their two model organisms they see two types of signal they see a normal response where the internal model is matched by m stimuli and a global Oddball response which is AER which emerges as a higher order response to this normal response we also can look at this nonlinearity in terms of Behavioral output in general and then burstiness or nonnormality of out this paper burstiness of human physical activities and their characterization and quantifying postural swaye Dynamics using burstiness and interent time distributions kind of gets at both this aspect of markovian systems and a behavioral system that exhibits sort of nonlinear complexity both papers focus on the burstiness of postural sway in postural sway we have this Paradigm where someone stands on a plate and that plate is a force plate that measures me or the center of mass of the individual and then we can measure this and we get this nice dynamical trajectory that has different features that we can analyze so it's a behavioral output that exhibits a lot of nonlinear characteristics and dynamical characteristics kuchi andano focuses on burstiness and postural sway in terms of adult housework and children's play behaviors they studied physical activities of children between ages 2 and five and they found that burstiness is not an acquired feature it's something that's inate burstiness is present in healthy healthy individuals not in people with movement disorders or impairments the also looked at in event time distributions and Associated fluctuations so we talked about inner event times as being key to Marian and non-markovian systems they found that they're bursty so they're these stochastic signals that have a noise component and sometimes this is a function of someone trying to maintain their balance and sometimes it's just noise that's generated by nervous system that's added to the stochastic signal in both cases This Is Not Gold directed it's generated by physiological system it's a very complex behavioral signal and it's not explainable by either a pan or gausian model which means that it has this very complex set of features so if we do some experimental cybernetics on this we can find that if we have the right internal model we can produce very interesting output signals or behavioral signals some of these T kind of look like they could be chological but they're not this is just simply running some stochastic sign one example is just a pure stochastic signal the other one is having a stochastic signal with feedback that then takes that signal and integrates it at a later date so what we have here are two Fe forward processes one is a spway stochastic signal in Blue on the left and then the other is a su signal that a sto a stochastic delay of that signal or the signal on the right so on the right we have the blue signal you can kind of see it under the red signal so the the red signal is the one that's bursty and it's just basically where at random intervals we sum previous output and we add it to the present output so we end up with these bursts and so the delay which is this uh sum signal that we uh that represent our bursts is the buildup of the input signal it produces these bursts or what we might call avalanches and so we can see that we can produce these nonlinear signals from a simple internal model so this is the classic coel paper from 1982 it actually has a couple of interesting things to say of cybernetic oriented things so we have this feed forward which is where you have a process that goes from A to B to C to D and we also have these systems that have strong back coupling so we have a process that goes from A to B to C to D then we have these back couplings from D to C C to b b to a and so what's more likely to be the case of like you know actual sort of complex behavior is not the feed forward signal but the strong back coupling and this is where we have ubiquitous feedback strong back coupling can be intractable to analysis but produces the most interesting aspect of network interactions and strong back coupling produces things like Collective Behavior emergent properties stochastic outputs and nonlinear outputs are Avalanches which we saw in our experimental cybernetics a few slides back we can think about feedback and we talked about this relationship between past present and future a few slides back and we also talked about maybe some of these types of behaviors that are non-random but maybe structured in a certain way and we can think about feedback just sort of revisit the idea of feedback that Rosen blue F all gave us which is that feedback was purposeful so we can ask the question is feedback purposeful or simply non-random and so we can think of past present and future relationships as a trip around the feedback loop namely that we can take our input we can take the present and have a feedback loop that gives us past information into a new present when we get a new time point and that ends up giving us a future that can then be recycled so this has implications for prediction we can go past present future to prediction or we can have inputs that produce these sort of differently ordered past present Futures and use that as the output so feedback mechanisms reorder processes systems with a lot of feedback aren't teeology per se maybe they produce outputs that look te but they're not te olical and so going back to the different orders of prediction which were kind of like the final output of our behavioral um type typology what are the different orders of prediction well we have our first second third and fourth or higher order feedback so we can have these feedbacks that have PR past present future but we can Nest them together so that we can have a past of a past of a past contributing to the present or to the Future and so that's really interesting because what it's suggesting is these higher level feedbacks maybe are also not te logical but there's some sort of memory or maybe there's some sort of um you know intervent interval and the way it's structured so there's some structure aspect of feedback that might be really interesting that isn't to logical per se but it given some of these other examples might have some structural significance so if we build a a cybernetic model that is really kind of complex and has a lot of interlocked feedbacks and things like that we can get some really interesting results so our different types of higher order feedback you know we can kind of think of these in terms of the derivatives of position so the derivatives of position revisiting that from undergraduate physics you have velocity acceleration jerk snap crackle pop and these are just where you start with velocity and you get a a derivative of velocity which is acceleration you get a derivative of acceleration which is jerk and so on and these can be thought of in this way of having these higher order feedbacks or these reorderings of past present future to give you these really complex Dynamics in cases of derivatives of position and feedback feedback if it goes further into the past the effects persist further into the future so if we go back a couple of feedback loops nested on top of one another and we introduce that to the new present or the new future those effects persist further into the future so we get these really longterm effects in a dynamical system instead of just being local how does this translate into the neuroc correlat of behavior well we go back to our idea about Behavior sort of being naturally te logical and we have to think about like kind of how we kind of assume that behavior does magic things so if we get a behavior like this with higher order feedbacks it tends to resemble the sort of magical aspect it could be something like emergence it could be something like burstiness or something like like a set of avalanches that occur and so these are things that you know you know give us a really interesting Dynamics but are no more sort of magical orological than any other kind of behavior and so more fully connect systems or perspective or might be sentient or conscious that's sort of the idea of does magic things so you know we don't want to us misattribute sentience or Consciousness to something that isn't and of course this is something that we've have a problem with with modern AI systems that people think they're sentier conscious when in fact they just have this higher order feedback in this sort of um you know behavior that isn't necessarily what people think it is magic things or you know what we talk about like complexity results from embodied function a good example of this is kinematic chains which we'll talk about in a little bit but now we introduce this concept of convolutional Cy cybernetic architectures or ccas and so we go back to our behavioral typology and we ask the question can we better understand the role of purpose or meaning and feedback or temporal order so here we have active Behavior purposeful Behavior theological Behavior and the noncl classes of those are nonpurposeful or random behavior and non-feedback or non phological Behavior respectively but we're going to reorder this behavioral classification where now we have non- prediction leading to the order of complexity rather than prediction leading to the order of prediction so we start off with behavior that's active instead of passive then we have nonpurposeful or random Behavior instead of purposeful and non nonactive or passive and purposeful or now non- classes then we have feedback which is non-teleological versus feedback which is theological theological feedback is now the non-class non- theological feedback is now the path to the order of complexity and then in instead of prediction being extrapolation we'll have non- prediction which is also extrapolation and then we have our order of complexity first second and third order so the more feedback we have the more complexity we have so this is an example of a convolutional cybernetic architecture we basically have this toy example where we have an input and an output the input is 00 0 the output is 11 one one and we have these boxes which process the present to the Future and then our feedbacks are give us like some past State and it feeds it back to the future or the present so we have a ubiquitous feedbacks here we have some feedforward elements we have elements of feedback that do certain things to the sequence and the question we ask here is what is the maximal number of steps needed to transform the sequence from 0000 to 111 so we start with 0000 we put that in we kind of move to our feedback loop our feedback loop does certain things to the sequence and then we end up with 111 or we actually want to know what the maximal number of steps is because we want to know you know how far we can go through these different uh feedback loops and get our answer this can also be rest ated in terms of temporal phases past present and future so we also have more explicitly this input of the present we have the future as an output and then we have these different uh Loops that give us the P give us past information and again we're interested in getting the most diverse information possible and so this is again this maximal Loop that we're looking for so we're trying to get the most variety out of these uh convolutional cybernetic architectures instead of trying to find the most efficient path or the tightest path or the most gold directed path we can further understand this or restate this in terms of semantic components so we can talk about this in terms of symbols reference and understanding symbol being the input reference being the feedback or part of the feedback loop and then understanding being the output and so we want to cumulate as many reference as possible for symbol so that we get an output of understanding and again it's the variety that counts not optimizing the process or trying to find a go directed aspect to this so if we think about how these ccas are assembled uh we can look at the different components of feedback and then we can start to assemble these more complex structures so let's look at first order feedback so CCA has input and in this case we're looking at past present future future and maybe alternative Futures and things like that and they all produce a predictive output so first order feedback would be an input going to a future going to the present and naturally the future is just like the present plus the past of the present comes in as input it's passed to this feedback which is the past the present and past are added to the Future and then the output is some sort of predictive input so again we have this reordering of past present future in the first order feedback if we have a feed forward only case we have input which leads the present which leads to some complex output but of course as a feed forward only system we just only get the benefit of having the present we don't get to talk about the past and the future it's just like every input results in a an output and that output then is complex output within any context then if we put together a larger set of feedback loops we can see this sort of convolved mixed feedback where we have first order feedback second order feedback and a feedforward component so these are all different components producing two different complex outputs so we have an input that gives us the present we get a first order feedback that gives us some past and we add those together to a future state that produces complex output we can also have a feed forward part of that where the input goes to the present and then the present isn't part of that state and then we might have a a a second word feedback where we have an input which is the present it passes A first order feed Loop uh feedback loop to the past and another first order or second order feedback loop to the past and those can both be combined into the present which is then informing a future state which involves a longer memory which has more information and that gives us another complex output but we also have this path from the second order feedback up to an alternative future which then has passed out as a complex output so you see what we have here is we have an input it goes to the present we Sub sub reference the past we sub reference the past again and that gives an alternative future so this happens for every cycle so you end up with these three different types of complex output and given this topology we can end up with these different types of characterizations of the system remember again with this convolution what we're introducing is this variety which is actually a concept from uh the ever good regulator theorem and a lot of the work of w asby so we're dealing with producing the maximum amount of variety so we you know we need variety to control systems so this is actually overall the appearance of purposes replaced with an ensemble of positive and negative feedback elements with diffuse effects so remember that we need to have not only a single closed loop feedback but we need to have multiple feedbacks it maybe some openl feedbacks in there so we can have uh maximize a variety so the imperatives of such a network are to gain processes or to maximize the number of processes and this leads us to maximizing variety and of course variety was a concept that was introduced by W Ashby in the ever good regulator theorem and he talked about how variety is necessary for control but it also creates the conditions for adaptability modularity and modifiability so we have systems that are both not purposeful but they also have this sort of imperative for control but also for other types of conditions in a in a system that allow it to adapt be modular and be modifiable and so we can also add in polarity we can have different types of effects resulting from feedback as we talked about if we reorder past present and future that's all well and good but this also has relevance to different types of processes that result in systems so in in complex systems we have these different effects like Avalanches runaway feedbacks and dampen feedback these are nonlinearities and we saw an example of avalanches earlier in our experimental cybernetics and that can result from a number of feedbacks of different polarity and this can produce unpredictable and stability we can have things like runaway feedbacks which are basically just the product of positive feedback and that can lead to mass instability and we can also have something like dampen feedback which is a negative overly negative feedback which sometimes makes things more controllable and so we can have these effects that result from these processes that have a polarity that lead to these different complex systems phenomena that then lead to things that are maybe look they look PE logical or purposeful but they're actually just the product of complexity and so if we kind of go back and look at like how we can model things like connectionist learning and reinforcement learning with cybernetics we can actually take the equations of connectionism and reinforcement learning and put them into this sort of cybernetic system so this is where you know we want to take modern learning systems or artificial learning systems and understand them using cybernetic approaches and maybe apply them into these kind of ccas and so this this is congruent with hopefield and Sutton's alternative Viewpoint which is that the weights of a network are reinforced by the activity and selection of previous inputs so this is where we have we're looking at Q learning here and we're looking at connectionist learning and we're looking at sort of the wids equations are structured and we're thinking about how you know we can get learning from these systems of equations and you know this is a dynamical system you know as as much as a learning system so we want to understand kind of how this is how this works and so we start at a random set of weights and lack structure due to selection we start at a random set of weights and lack of structure due to selection we proceed to a structured set of weights and optimal policy unwittingly we kind of move from a marvian system to a non-markovian deterministic system and so there's a tar question here which is how do we go from something that does not resemble purpose to something that is purposeful the answer is is that purposes of post talk determination so we can model this as habits with weights so we have this input we have these outputs and we have this uh CCA that's that has polarity and we can have habits that are the serve as the inputs and the weights are the polarity and we have non-teleological behavior in a directed graph so this directed graph kind of moves towards the end points the outputs and so the the lesson here is that purpose is basically whatever we determine these outputs to be so when we observe a behavior a behavior is generated by the CCA graphs we interpret the output the way we'd like to interpret it as I said before we can have these different outputs that are that look purposeful or they look peely logical but that's just really a post talk determination Now we move to wier's Alternative Viewpoint so weiner is more of a cybernetics guy he also has an alternative Viewpoint and his Viewpoint focuses on communication and information which have both a control layer and a semantic layer so again we have to go back to the past present and future formulation so instead of thinking about this in terms of polarity we want to think about this in terms of not just the control but the semantics as well and so we have this past present and future we have an input which is the past we have a future which is the present with past feedback we have the present so the present goes to the Past goes to the future but the future also moves to a present where there's a present with past and and future components and then an output so if we run this in time if we run this as a dynamical system we end up with this the present being sort of you know it's not really additive but it's basically you're concatenating things onto the present with information from the past and you're informing a future trajectory and you're you can actually generate alternate futures of a system from this and so this there's a control layer a semantic layer and this gives us information about the system both in terms of contingencies and synergies so connectivity and ordering provides both contingencies which is that nodes depend on other nodes and synergies for nodes working together so in this model we can't knock out any one node and have the system work we can't knock out the the feed back and have this sort of Rich self- reference we can only have you know a feed forward model and we've seen that that's much less interesting we also can't knock out the future and we can't knock out the present we have to kind of continue with all of our nodes but also these nodes working together produce the synergestic um trajectory that gives us rich information about time it gives us this present it gives us a a future but it also gives us these intermediate states where we get past feedback and we get integration of those this stands in contrast to the imperative of purpose and teeology is cybernetic control and behavior purposeful or stochastic activity tempered by recursively recombined temporal ordering and so this is again one of the ccas and a CCA is also a series of steps that add recursive functionality so sometimes if we think about this in terms of our internal model memory this is memory with sub referential power so it allows us to have a memory again where we can reference the past of a past of a past and it informs the future of the present and adding present into future and things like that this creates a hierarchical system and high degrees of intentional order so again we can add these things together or we can run these ccas and can produce this memory with sub referential power and the output can look very organized and hierarchical but the thing that produced that output is not necessarily ordered and it's not necessarily there's no overarching goal or any other sort of information going into it so once again thinking about habit and purposeful Behavior we have this paper creatures of habit the Neuroscience of habit and purposeful Behavior this was published in by biological Psychiatry so this is where they talk about animal behaviors being composed of Habitual angle directed categories habitual is non- theological well gold directed is theological they provide this sort of breakdown where you have theological versus non-t logical behaviors and deterministic versus stochastic Behavior if we think about te logical and deterministic behaviors we get gold directed behaviors those are things like reaching for a Target or moving towards a goal but of course we know that some of these types of goal-seeking behaviors are chemo like chemotaxis or phototaxis are stochastic and so if we have theological and stochasticity we might have something like chemotaxis or phototaxis we have something it's non- chological and deterministic we have habitual work so we have something that's habitual it's not moving towards a goal but it's deterministic something like uh scratching or like a reflex you have nonic on stochastic behavior however we know that that looks something like a random walk so we can actually characterize these different behaviors in this way the problem with chemotaxis and I put a question mark in the table so chemotaxis for example which is a movement along chemical gradient we already established that that was not toic the gold directedness in that case is due to the environment not the internal model and so we can look to gibsonian information or we could look towards sort of thinking about why it's not teleological and but it doesn't also doesn't resemble a random walk so that provides us with a problem which is where does this fit into this typology so a clue to kind of where chemotaxis and phototaxis fits into that typology comes from this paper brain like neurodynamics for Behavioral control develop the reinforcement learning and this is from the bioarchive from this year cot all test two scenarios supervised or extent and reinforcement or developmental learning so we have extent and developmental learning only reinforcement learning produces gold directed behavior and neuroactivity similar to biology it provides robust short-term behavioral adaptations to perturbations and it produces an emergent emergence of policies that are not purposeful in and of themselves but lead to things that look the part so this is reminiscent of our ccas it's also reminiscent of our to our discussion of chemotaxis and phototaxis building a CCA from a small Motif is a form of purposelessness or purposelessness reinforcement learning build up you build up to function and sometimes useless structures and that's okay another example of nonpurposeful behavior that kind of fits into our CCA model comes from feudal substrate Cycles in Biochemistry and so this is the paper stochastic amplif amplification and signaling enzymatic feetal Cycles their noise induced by stability with oscillations this is from pnas so this is where we describe these feudal Cycles as these feedback loops where you have an input of energy there's a enzymatic reaction and you get nothing that comes out except entropy or heat so you basically don't have an output your output is entropy it's heat and of course this is a problem because we expect there to be an output we expect this reaction to actually do something and in fact it's creating heat but it seems like that's a lot of work for just heat so we have a biochemical pathway which is a Clos feedback it does thermodynamic work for no reason heat is produced from an energetic input but we don't get any biochemical products and this seems like it's a waste of energy that it has no purpose but it's actually important for regulation and other processes it produces complex higher order Dynamics or amplification effects throughout a greater biochemical system so if we go from convolution to order we can see that the evolution of cybernetic control is a causal recursive chain of contingencies and so here we look at the contingencies in a CCA we go from 1 to two to three and then that's a feed forward system and then we can go from 1 to 2 to 3 Prime to four which it feeds back to two and gives us this output four prime or we can go from 1 to two to three to four or 1 to two to three prime to four to five to six to 7 which is an output so we have these different outputs from these different numbered boxes and we have these uh resulting processes so let's take three prime as an example three prime is dependent on two and one and once feedback is operational four and five so four and five actually do feedb to three prime which of course is a dependency so if you knock that dependency out you lose your entire functionality of that part of the network seven on the other hand which is another output at the top is dependent upon 6 5 4 3 Prime 2 and 1 there's no feedback from delay so you can see the different types of order and how there's a lot of recursive chains that lead to contingencies so again if we think about our feudal cycle our feudal cycle is like something at the bottom our her or feedback is like something at the top of this diagram and our alternate outputs alternate outputs are at the right and they are of course maximizing for variety and so if we think about the evolution of cybernetic control as a causal recursive chain of contingencies we can think about the kinematic chain which is where we have these dependencies so D is affected by the activities of ab and C we get this chain it's this is this robotic arm that extends out across two joints and the activity in D with kinematic is constrained by A and C and so we can describe it in that way if we go back to these kinds of contingencies we know that seven is constrained by its intermediates and thus it is with the kinematic chain in in the kinematic chain we can do this in a linear fashion we can add on conditionals as convoluted cyber as the convoluted cybernetic Network evolves this restricts the system to a deterministic path behaviorally this leads to interesting consequences for a developing sensory motor Loop and a kinematic chain is basically sensory motor physics with contingency so here are examples of a kinematic chain in robotics and we can think about uh embodied robotics here where we have these control systems for these kinematic chains maybe the CCA where we have different types of morphologies and we can design them appropriately we can design and we can design them appropriately we can design behaviors that aren't purposeful aren't chological but still give us behaviors that we can that look intelligent we can also think about purposeful behavior in fore cognition so our approach to purposeful Behavior can be applied to Fore cognition so we have these different types of cognition some of them being embodied some of them being embedded some of them being extended which means outside the brain and some of them being inactive if we put this in the framework of pros and blue th all's behavioral classification does for for Force us to rethink that behavioral classification as we did in a number of instances in this talk now there's one Insight of 4E that's interesting to us which is that the morphology and environment of a system is more important than its internal model so we've been talking about internal models and how these ccas model the internal state of behavior and how that shows that in a lot of cases Theology and purpose are sort of elcer but if more ology and environment are more important than the internal model then maybe we need to incorporate our ccas or or recast our ccas in terms of morphology and environment as well as an internal state in fact or CCA do that to some extent but it's uh sort of a a way to put this behavioral classification on its head and so in the case of 4E purpose is not ascribed to a mind feedback is diffuse and complex prediction its highest orders are not teorological they must be open-ended we talk about this in a paper from 2023 in Royal Society interface Focus so what is behavior and what are its properties so yeah we we're kind of revisiting what Rosen Bluth talked about we're looking at a number of examples from neuroscience and from psychology and from other areas animal behavior Rob itics and we kind of break down kind of what we're looking at here so Behavior can be intentionality it can be goal directed and supervised and so these are things like control systems P order cognition and of course some of those are goal directed some of those are regulated complexity then we have Rich higher order complex Dynamics and those are things like Bey reac c s which we talked about and embryogenesis which we didn't really talk about but also qualifies in that category so these are outputs of dynamical systems we also have connectivity and convolution which are things like connectionism avalanches and open-ended learning and then finally we have behaviors that are hardwired but recursive so with our ccas we were modeling some Hardware behaviors in a recursive fashion we also talked about this in terms of non-markovian systems and habitual behaviors we want to go beyond the standard of teologico and purposeful Behavior so a standard way of thinking about behavioral regulation is that inputs are deterministic and outputs or Collective outputs should be Clos ended so if we expand the basic clu feedback model we get an interconnected network of retrocausal ordering so this is something that we're seeing in the CCA where we talk about the order of causality past present future we have this basic CL feedback model but if we expand that out we get much richer retrocausal ordering which leads to of course to variety but it also leads to these interesting paradoxes and interesting effects of high feedback behavior is often open-ended and this can be characterized by embodi Dynamics unpredictable which can be characterized by a chaotic Dynamics non normal which can be characterized by non- markovian behaviors and systems Avalanches of different types power law Dynamics and so forth and variational which means that it's characterized by multiple goals and redundancies and so this requires a reassessment of Rosen blue theal typology and has wider consequences for the practice of active inference thank you for your attention uh one of our co-authors Jesse parent uh produced a a lengthy blog post on the Rosen blue that all paper talks about its enduring insights and it's con uh it's context in cybernetics and then we have two papers we talked about in the talk from our group Al estasis machines and a primer on Gibson information so I thank you for your attention e e all right welcome back here with Alexi tolinski a friend and colleague and collaborator on this paper temporal depth in a self and in depersonalization theoretical model so go for [Music] it well uh I'd like to say hello to everyone and thank you for being with us after such a long and fruitful day at uh 6 pm on on the east coast and uh it is I appreciate your interest and thank you Daniel thank you active inference Institute for this fantastic Symposium um so uh uh we'll be working talking today about this paper on dissociative States and it's a collective effort uh this paper is in peer review right now uh early uh preprint was posted on the site and it was uh a work by Michael Levan Chris Fields lanot D Costa Rachel Murphy Daniel who is here uh David Pinkus and myself and um this is the link to the paper if you'd like to take a look at the early print uh perhaps some of you would like maybe to get a feel for what we're talking about and this is not a patients account this is a poem uh from uh Kurt V and his song Pretty pumpkin I woke up this morning didn't recognize the man in the mirror then I laughed and I said oh silly me that's just me then I proceeded to brush some strangers teeth but they were my teeth and I was weightless just quivering like some Leaf come in the window of a restroom um and this uh poem was cited by Paul moleno he's a specialist on did in England on the Psychiatry and Psychotherapy podcast you can also find patients account on YouTube I couldn't post the video here for copyright reasons um so dissociations are a spectrum and they range from mild to severe transient uh to Chronic uh they can be benign and pathological and certainly they're not a bright line category uh the causes vary for them as well uh we call it iology in the clinical World they can be caused by psychological trauma neurological conditions substances seizures anesthesia and uh different sleep phases there's a subset of conditions here we called dissociative disorders typically in the history of these patients we see prolonged elevation of stress accompanied by repeated traumatic experiences in circumstances where they feel There Is No Escape typically childhood abuse and neglect and we call that complex trauma which has a different presentation than acute trauma and we'll touch upon why this specific pathway of uh iology uh is is important for dissociative disorders um well I'd like to say why uh this matters what's at stake what's the motivation for this paper and we have many authors and they have variable motivations but I'm a clinician I see patients day and um uh for me the main reason for that is these conditions are hard to diagnose many patients go undiagnosed for a long time they're very hard to treat uh and we're sometimes content with partial results and the risk of self harm or suicide is higher for these patients and there are many many reasons for that uh we think that perhaps one of the reasons is that there is no unified theory of dissociations we have kind of um neurobiological ideas the corticol limic inhibition hypothesis we have uh psychodynamic ideas we have uh attempts to moral dissociations from first principles by anuna and other colleagues but uh these different attempts are not tied together and certainly not tied together with the patients uh subjective experiences so we are trying to make steps in that direction and we don't claim that we've built a unified theory of dissociations but what we're doing here is we're focusing on one aspect of dissociations one focal point that is a collapse of the temporal depth in any kind of dissociation regardless of the cause by temporal depth we mean how far into the future the person can plan what we're hoping for is that this uh uh focused approach has the potential to uh allow us to to to um perhaps develop uh more accurate diagnostic uh uh procedures and and to POS possibly um help increase efficacy in in therapy with full respect to how difficult complex and messy this work is um uh additional point of convergence is we explore multiple theoretical perspectives in the paper and they all Converge on the same um point and here is the main thesis um the collapse of the temporal depth is necessary and sufficient for the onset of a dissociative episode so we're going a little bit further than colleagues who already mentioned a relationship between tempal depth and associations Dean and and an unik and others and Carl friston were saying it's causal that there is no dissociation without the temporal depth collapse and the collapse of the temporal depth will reliably lead to dissociative episode further we are um uh saying that temporal depth collapse can be conceptualized as a common pathway for the onset of dissociative symptoms regardless of itology and finally we suggest that the focus on restoring the temporal depth can and be an effective strategy in therapeutic interventions one of the theoretical perspectives we explored in the paper is Michael 11's team framework technological approach to mind everywhere which I think is very useful uh and unfortunately I haven't seen many uh attempts to uh use it in clinical applications this is the title and the link to the paper uh there's so much in Michael Len's work that is useful uh his uh paper on cancer which is based on his experiments is also one of The Inspirations for our paper here but here's another inspiration if you dive into Mike 11's work you'll probably see uh what he claims based on experiments that all intelligences are collective intelligences however at our subjective level at the level of the conscious mind we don't feel that way uh we perceive unity all our percepts Sensations thoughts feelings come together in a cohesive and coherent ho a single conscious experience in the here and now and uh we will talk about this seeming uh Paradox uh if you wish in in our paper um some of the core tenants of the team framework not all of them but some key points uh commitment to graduality there is no bright lines uh Michael suggests that intelligence is built gradually and uh by accumulation of complexity material Independence uh our model is not based on neurons or central nervous system we will talk certainly about a subset you know I'm a clinician I work with human beings so we'll talk about brain based models but generally the model applies to synthetic agents and cells that are not neurons commitment to empirical uh research as opposed to just exchange of opinions on the subject um Dynamics the S are not fixed they're malleable and plastic and selves can be nested and overlapping cooperating and competing both laterally and across levels uh this is an illustration of one of the key Concepts in Lev's framework um uh called a lightone and what you see here depicted is various organisms in the middle of the slide you see a tick and a dog a human being possible AI compound intelligence such as an end colony and you see this sort of nested and embedded structure on the picture on the right and light cone has two uh Dimensions there's a spatial Dimension and uh on the horizontal plane and the vertical Dimension is temporal and TI for example is not particularly concerned uh with things happening a mile away uh it cannot recall far from the past and plan far into the future while human beings are capable of planning things until uh that happen after we die a living will and uh with the use of language and culture and books we can uh use things that Happ happened before we were born so we're capable of huge light cones and I'm highlighting this word capable of it doesn't mean that in our wakeful hours and alert hours we are walking around with the significant light cones they contast contract and expand depending on various circumstances but we're certainly capable of them while ticks or not and the vertical dimension of a light cone is exactly what we mentioned before that is the temporal depth there is of course a perspective on the same thing in FB and I like this very suced phrase by Carl friston who said because consequences follow causes the generative model acquires a temporal depth and more formally uh some agents are only capable of modeling their immediate environment and live in the Here and Now While others are capable of planning actions into the future and the generative models capable of planning are referred to as temporally deep generative models so temporal depth under fep is the length of the temporal har Horizon that is considered during planning uh we're moving on to describing the if you wish healthy self um and we do mention that it is a materially independent um structure where you know in principle we can talk about the selves of synthetic agents and you know cell collectives uh uh sell depend on an organism's ability to infer and here we will uh provide uh a viewpoint on the two clinical iCal terms familiar to clinici such as derealization and depersonalization so the organism's beliefs about the world are the inferences about the external environment and the Continuum of issues in this process we will call derealization while the organisms beliefs about its own internal milu its body and mind we will call the self and the Continuum of issues of this process we will call depersonalization the SES are hierarchical composite nested and with boundaries uh so we consider our s to be a system consisting of hierarchically structured beliefs which effectively makes it a dynamic hierarchical generative model and taken in its entirety the self is informationally separated from the external environment by a mark of blanket or near Mark of blanket by a boundary would like to emphasize I apologize if this is Trivial to the audience of the active inference Institute that this boundary is not Material like skin it is informational uh through which the self and not self interact so to emphasize the self as we have defined it is a model it is not an organism the organism constructs and implements it so the self is quite literally the construction of the mind this is an illustration of that from early work by Michael Lan I think this was his doctoral dissertation materials and what you see here is um duck embryos that started from oite and then you have blastoderm you know after cell division you have a collection of cells and at the top left if nothing is done then this blastoderm will develop into a single ducky you see this green shrimpy looking thing that's a single duck however if you introduce a scratch in this blastoderm and the wound heals the same exact cell collected May develop into twins or triplets you see the red yellow and green duckies now think about that for a moment what happened here um this Collective of cells computed what is me and what is not me and the hardware did not change but in one case it made a decision that this is all me and the rest of it is not me and another one it said there's three M's so you can see that this is not Hardware um s are also as I mentioned hierarchical composite nested and they have boundaries so now we're moving on to components of the self uh that are each component is separated by their own boundary and Mark of blanket and because we're using the word hierarchical uh we will uh Define that there is a core self uh core component if you'd like a visual metaphor that Mark SS uses we can think about an onion and there's different levels of the layers of the onion and there's a core uh in the middle we will talk about that as apply to humans and mamals and rodents so now we are in organic uh in organisms uh this term is from effective Neuroscience from Yak bip and Lucy Bin's book um and also you know I believe SS and teral used it uh so core self is non-reflexive non-reflexive and dominated by raw feelings uh and part of purely effective core form of Consciousness so the effective homeostatic mechanisms allow an animal to problem solve in novel environments and I'll give you an illustration from Mark PMS um to maybe just um bring it home uh imagine that I'm in the room full of carbon monoxide and I've never been in that before uh so I don't really know what to do but I feel badly I feel badly in a very specific way there's air hunger Suffocation distress uh and let's say I'm moving around the room but if I move to the left and I feel a little bit better and that's information and if I move to the right and I feel worse that's also information perhaps there's a door on the left and there's a draft underneath the door and I feel better so this Compass this internal mechanism the homeostatic mechanism allowed me to find out a possible solution and um we have a whole collection of these in a DI mention you know the actual hunger uh for food and thirst and social homeostat such as rage and fear and panic separation distress Etc so the collection of that is what pip and SS conceptualize as the core self uh uh therefore the set of predictions in the core self that all the life sustaining affects will be at or near their settling points which means in emotional terms contentment all my needs are met I am a okay thank you very much right and that is the minimum of the variational free energy if there are deviations from it such as hunger or thirst or uh air hunger then that would be a prediction ER accompanied by a negative feeling like in the example that I used above uh in mammals we're not putting the equal sign but we're describing the anatomical structures that are uh participating in in serving this function we're talking about the upper brain Stam regions including the reticular activating system and the periductal gray they mediate the functioning of the core self and you can see a lot more details in Mark's book Hidden Spring a journey to the source of Consciousness so um or self is thought to be present in animals without the cortex such as decorticated cats that are actually more effective and not less and the cases of human children born without a cortex hydrolic children they have emotions when you know uh you put a brother on on the girls you know uh chest she smiles and she cries so the core self does not require a functional neocortex the higher levels of the brain structures in humans including the cortex mediate the higher levels of both Consciousness and the self and sometimes it's referred to as extended Consciousness which allows us to reflect on what's going on such as to say I feel anxious right now uh there is an asymmetry in this uh um model where the higher levels the peripheral levels of the self and and Consciousness cannot exist without the core um while their reverse is not true the core can't exist without the higher levels one of the other illustrations uh all that which PM sites this is based on Fisher's work in 2016 is that a tiny lesion a 2 cubic millimeter lesion in a parabal nucleus will reliably and IR irreversibly reduce a coma the patient will never become conscious ever again while you will never have anything like that happen in the cortex you know this small lesion in the cortex will not result in catastrophic outcomes like that this is one of the reasons why neurosurgeons don't usually like to operate around the brain stem this is too consequential to risky so everything we said about Consciousness the core and extended is applied to self as well that pereral self components cannot function without the operational core self and we will illustrate that with specific examples in humans later on in addition to this onof where the thing is completely operational or not we can talk about the influence and the core self has the ability to change the regime of functioning in other more peripheral celf components by inducing phase transitions in them and the Brain mechanism for that is generalized arousal um when the higher levels of consciousness are present and functional then we have representations such as fruit and vegetables and you know in the core self we have raw effects such as I don't know separation distress or hunger but with the higher levels of consciousness presence we are capable of uh uh thinking things like I want an apple or the aect that as in Freudian language and in s's language is attached to the object and similarly we can have these constructions about the meta object such as I uh such as I look pale or I am a pessimist and I here is a mental object and therefore I is a metacognitive construct an abstraction and it only exists at the higher levels of the self such as autobiographical self it is not used or even needed in the core self to reiterate core self as a system has the capacity to detect uh the effective prediction errors and uh attempt to minimize the variation of free energy through action without any eye we're moving on to peripheral self components and we'll describe only a few uh and the terminology here is different from what you may read in wonderful book by NE Neil Seth being you so the first one is autobiographical self and that's a system dynamically representing our lives here history if you for example remember your 10th birthday and the cake is on the table and Mom is there and your friends are there and the sun is shining and everyone's smiling and it's Tuesday that's an episode that's an episodic memory and a collection of them organized through time is our sort of Life narrative um there is a bordly self which is a system dynamically building inferences about our body and various representations and interceptive processing there's a social self which is a system representing inferences about how we're seen by others and how we present ourselves and act in the social environment each peripheral self component such as bodly self for example has its own Mark or blanket and each component is embedded into the whole self and also contains subcomponents creating a nested architecture that you saw on the T light cone picture before about the unity that in humans at the higher levels of the self such as at the autobiographical self level uh uh uh these these levels create an impression of a unified experience and this is just the experience of the autobiographical self not the entire system the autobiographical self claims to itself and others to be the entire self well it is not so Anil Seth refers to these things as delusion we have a delusion of being a unified I don't know Alexi right but you know what we're saying is that it is that system that is diluted um uh the autobi graphical self subsystem I'm ambivalent about using the term diluted I think it it's not the optimal one because we use it in the clinical world and I prefer it uh called inference or belief that we're unified stated differently we can say that a stable belief that I'm whole in the um I'm complete in the autobiographical self um its generative model contributes to us feeling as a Mantic self therefore the subjectively perceived Unity the coherence of self is an inference related to coherence another thing we can talk about is continuity and nearly as a taty stemming from the definition of Mark of blanket our self remain the same persistent time while all the processes or Communications across its Mark of blanket remain functional and we can um offer you another view point on the same exact phenomenon from if you wish more neurobiological perspective or psychological perspective if you recall maybe you're at the train station or you know anywhere in the city and you see someone and you think you recognize them but you're not sure you know this sensation of recognition of familiar uh so the familiarity novelty calculations is what we're talking about here and depersonalization is a breakdown in that something that is familiar myself suddenly abruptly becomes very novel and not recognized but actually these calculations neurobiological are not uh bright line dichotomous categories they are probabilistic and they are Spectrum but we co grain them in the mind often into yes or no familiar or novel so that subjective impression of yes I know this person or not is just the course grading of the continuous variable representational capacity is important uh the Deep generative model implies that the agent is capable of planning into the future which in turn requires an ability to generate store and retrieve counterfactual data generate store and retrieve is memory so therefore when the representational capacity is completely or partially impaired for any reason the agent completely or partially loses an ability to infer leading to depersonalization and dualization if we talk about health and pathology briefly the two key terms here are voluntary and involuntary you know again all of us during our day we can either sit down to plan things you know for you know 10 year temporal Horizon which expands our temporal depth or when we uh meditate or uh rock climb you know then we can be more in the moment in the here and now but uh trauma doesn't ask permission I mean perpetrators are you know there's nothing voluntary about it and so that's an involuntary collapse of the temporal depth uh which usually is leading to pathological conditions this is a perspective from the nonlinear dynamical systems you know if again I am in uh you know adult healthy alert functioning and I may have small deviations from ime say I'm daydreaming or something like that and the collection of these points from which I reliably return to ime can be described as the attractor Basin and the whole thing in terms of attractor landscape is the set point attractor regime ime and ime would be the variational free energy minimum and that is a fairly stable system perhaps in the autobiographical sum right now in order to for that system to change some energy uh needs to be applied and with external interference of sufficient power this attractor landscape can change when that power is applied temporarily there's a phase transition the attractor landscape evolves into something else perhaps during that transition you have a chaotic regime and if we are in the pie of trauma then the functioning can be chaotic for a while but then it may settle down so to speak into one of the possible postraumatic presentations there may be multiple postraumatic presentations and one of the possible options is depicted here this is not again Universal but you know it's one of the possible options what you see here is a multiple coexisting uh Point attractors represented by local Minima of the variational free energy and they can correspond to Alters in the uh and in this example you also see that they're surrounded by a global minimum of variation of free energy now also if you look at the uh area the region between the local Minima then in dynamical systems that's called a repeller and uh repellers are uh notoriously chaotic so uh that suggest that you may not have very good predictability of how you will transition from alter X to alter y y or alter Z to say a few more words about this when we switch between the altars then we are in regime of itinerancy this is the term Carl Frist likes to use a lot and you know justifiably so also when we have these transitions from X Ultra X to Y that the time is disrupted you know in the idea you don't have this flowing continuity of time there's a series of disruptions effectively then you're uh coherent large light cone is broken into uh pieces here now imagine that this patient with this condition reached out for help and they want to get better but this uh attractor landscape already acquired some stability and so again we need some interference to destabilize the pathological regime the difference here would be that in therapy in the process of healing the this uh destabilization of the pathologic iCal attract landscape is voluntary controlled and guided ideally by a licensed and competent mental health professional and uh that's that's the uh the difference from the onset of uh acute trauma so psycho therapy can be one such intervention and it's the treatment of choice we have very limited efficacy with medications and now it's very uh uh uh kind of um fashionable to talk about augmentation with psychedelic treatment or perhaps neur stimulation it's important to say that just destabilizing that landscape doesn't guarantee anything at all so when people present this idea that you just go to like I don't know and have some iasa or anything ketamine infusion and you're better is a myth because you know the important part is that the patient settles back into health and just the destabilization of this landscape doesn't lead to that you still need uh therapy you still need you know to process the trauma not just to destabilize the pathological regime um and that therapeutic process when you destabilize the the pathological attractive landscape is another phase transition temporarily temporarily chaotic with the idea that eventually it leads you to a healthier functioning either a attractive landscape corresponding to cohesive uh system or at least a little more coherent system with less Alters partial Improvement let's talk a little bit more about the phase transition so before things happen in in in health we have a nested and embedded uh self and that can be described in in FB as a hierarchy of coupled interconnected attractors Carl friston you know uh wrote that slower attractor can possibly control the phase transition of the faster attractor and in our system the core self is is feeling based it's effective and it's one of the components uh of aspects of feelings is generalized arousal and we know that the CH changes in generalized arousal level can lead to the phase transitions of the entire neocortex from periodic to chaotic State and back so this is one of the possible uh kind of you know uh explanations in our model how this works the phase transitions um you know a few more uh um thoughts about it trauma is not a um guided transition it's an effect of storm it's a flood of cortisol that is abrupt and severe and that uh abrupt increase in generalized arousal leading to the destabilization of a healthy attractor landscape in the autobiographical self but then when there's an onset of a dissociative syndrome after trauma then uh the regime settles down to a lower energy uh perhaps effective numbness that patients talk about and in a a Psychotherapy not immediately but when the patient is ready which can take a year or more you know when there's active trauma work than there is again an increase in in energy level with with the idea eventually to settle down to a healthier regime of functioning a few clinical observations starting from dissociative disorders which are conceptualized traditionally as uh defensive response to prolonged and inescapable trauma clinicians talk about uh in patients report this this quality of emotional flatness or deadness or numbing sometimes they call it freeze so in our model it corresponds to issues with Communications between the core self and the peripheral self components for example that autobiographical self in this regime operates as if it is uninformed by the vital emotional informational flows that originate in the core self one of the possible ways it can be achieved not the only way but one of them is the frontal prefrontal cortex inhibiting the lyic structures that's the cortical lyic inhibition hypothesis and in FB terms that means lowering of precision associated with effective prediction error messages um we're moving on to also severe dissociations but temporary such as general anesthesia propal and uh uh uh it's a very profound change in the body that influences the Gap Junctions between the cells and recovery from anesthesia is a very interesting process Michael Lan is fascinated with it it's actually a little bit of a miracle that we come back to be the same same person we have been before and some people don't uh you may wake up a pirate uh and uh these conditions are called postoperative delirium dissociative Amnesia postoperative cognitive dysfunction this is also an illustration itself self is computed and plastic and malleable it's not a hard um you know Hardware um and perhaps we get oriented with the use of our memory systems including the episodic memories another severe case is a seizure where not only autobiographical self is impaired but also the social and the bodily self and one of the reasons for that is the necessary resources for the functioning of the self components are not operational in seizure uh moderate dissociations uh in neurological cases but permanent ones are for example the perhaps one of the most famous neurological patients hm who had a bilateral hpoc compus damage uh and uh in Britain there's Clyde wearing and Anil Seth describes C wearing in his book being you these patients do not have episodic memory encoding and effectively believe in about 10 minutes of the here and now uh there is no past and no future beyond that and if you read Clyde wearing's diaries that he talks about the series of Awakenings he said oh I just woke up and 10 minutes later oh I woke up and then I woke up and if you show these Diaries to a clinician who is an expert in D they will which I've done you know I talked to Roxy wolf one of the fellows of the international Society for the study of traum associations and she said that you know this patient looks perpetually depersonalized there is no continuity itself Coro patients they don't recall the past they can fabulate the past and in Alzheimer's in early phases there's issue with recent autobiographical memories but some access to the remote autobiographical memories all of these cases are discount continuity in autobiographical self um uh more frequent perhaps cases are substances ketamine ey doses of THC or iasa lead to depersonalization and dualization and importantly if you read the studies then these patients actually report they tell you that their sense of time is impaired not only that the planning is impaired as well um if you have a alcohol blackout and the person tells you on Wednesday that they have no idea whatso ever what happened on Tuesday after 6 then uh this is a discontinuity this is a gap in episodic memory uh there's also something called traumatic Amnesia this very sad cases of high school shootings where you know the person literally doesn't remember you know 15 minutes of what was happening in the cafeteria what happens in acute PTSD is that you know uh hippoc compus is reaching cortisol receptors and there's a flood of cortisol and the effectively the episodic memory of trauma the cohesive context contextual time stamp memory is not laid down in a stable fashion you have shards of glass you have phobic memories semantic memories procedural memories but not the episodic one which is also discontinuity in autobiographical self and that is involuntary uh and we talked about mild benign everyday uh dissociations uh next time you wake up you know maybe tomorrow the day after you know try to monitor how what's going on uh and you may catch that phe of what met Sinker calls you know a nonic self where things are just flowing through your mind but your eye is not fully out borous here uh to summarize the level of Consciousness regime of Consciousness pathological States and all the underlying resources necessary for the successful operation of each self component collectively influence the complex dynamics of the system um a neurobiological perspective briefly the historically most famous one is uh corticol limic inhibition hypothesis by Siera and barus who postulated that uh prefrontal cortex is uh inhibiting the lmic structures including the amigdala and the interior singulate cortex ACC the subsequent studies don't fully support that hypothesis but partially support it I like the work by Ruth lenius and her colleagues and guess what but you know her uh revision her refinement of the cortical limic inhibition adds Dynamics transience context and itinerancy all the things were like uh uh they talk about two regimes even in patients with dissociative syndromes over modulation and under modulation again itinerancy in not one regime but multiple regimes over modulation is indeed hypoactive amydala and hyperactive prefrontal cortex which is described above but Al these patients less frequently but but you can see it are in a reverse regime where amydala is active and and PFC is less active uh uh over modulation dominates in dissociative uh conditions they refine the anatomical Network and talked about different sub regions of amydala the central medial amydala basolateral amydala dorsal lateral periductal gray vental lateral periductal gr Thalamus and other regions and this network of regions collectively Works to create specific postraumatic regimes they also described uh the regime where patient has access to painful memories usually uh is accompanied by under modulation very active state with amydala active and uh when they don't have access to painful memories they're more in a dissociative State we can also briefly talk about what are the anatomical substrates and the neuros psychological substrates of the temporal depth collapse and again acute trauma hmle damage and dementia uh are affect the episodic memory system uh if there's an insulin damage then you can expect the disruption in the bodily memories and perhaps in camine use you may have a dysfunction of prefrontal Cortex and front parietal Network which renders the work in memory uh of the patient dysfunctional you can imagine other circumstances when you have PFC and front of parial network impaired that usually leads to a psychotic state where you know it's very hard to think you know we need work in memory to be able to think um about dissociative disorders again the point made at the very beginning that prolonged in escapable exposure to highly stressful environments can be seen as lasting stress beyond the agent's ability to manage which results in the breakdown of um tame light Conn into the uh components a fragmentation decoherence and discontinuity when the landscape uh attractor and repell Landscape corresponding to a pathological condition takes root and in contrast to that if you look at the single uh episode of camine intoxication then you have transient work in memory disruption leading to transient dissociation again to summarize lasting or temporary severe or mild dissociative experiences are accompanied by the collapse of the S temporal depth and we are ready for discussion which I very much look forward to thank you Lexi great presentation thank you okay where should we begin what would be interesting or or given where the work has gotten over the last few weeks where what do you feel like is most relevant well perhaps any colleagues have comments I don't know if hus is with us and he's a esteemed neuros atrist and he sees a lot of patients and perhaps neurological patients and he can comment on some of the things we've mentioned I I had a clinical meeting with with uh colleagues who do a lot of dissociative works and I mean I'm seeking feedback and critique and so whatever people have whatever thoughts people have yeah if there's anyone in live chat they can write a question a few pieces that stuck out differently in how you presented it uh in comparison from what working on the paper was the narrative self's role in the maintenance of The Narrative of unity and about what is the experiencer of that or is that all it can do why does it do that why does the autobiographical self have that kind of a voice or a cognitive function is audio working okay can can we hear each other I I think why is a tricky question you know it might also be teleological question I would say that it's probably functional you know uh for us to have coherent s uh and um I don't know why uh but um um we are observing it so we have descriptive level we have observations that I feel like the same elexi and it's a little bit of a miracle that as we know that the cells in the organisms they are they die and they're born but I still feel like the the same person even so uh we change developmentally I'm not the same person as I have been at age five or age 15 but I still maintain this story if you wish this Bel that I am the same and um I also protect the internal milu from external threats you know of the same organism and if we look at other species say an oak tree that you know uh um grows from an acorn it is still the same Oak it's the same organism uh so um I I perhaps in biology as well there's a need uh for the system inside the mark of blanket to maintain that saying whatever is inside the mark of blanket is the same me but it is a belief you know and uh um I don't know if I'm answering your question maybe I'm just free associating to it so yeah is that a clinically valid method um well it it reminds me of mortal Computing and there's a lot of focus on how behavioral activities in a niche relate to the material stability and I think you really Drew carefully on Mike Len 's work to talk about the analogous Dynamics on a landscape of a narrative self and that's just kind of that was like a trampoline but then you also complicated that with multiple selves like the social and the narrative and so then there are these multiple disassociative or like it's kind of they each have their own unity and yet they can be discordant and unified yes and so then is there any general framework for different possibilities open-endedly of different regimes these are great questions or even maybe challenges to develop it further but I agree with you if we look in the Bly self right I don't perceive my hand to be a separate entity I believe I I feel for it to be part of the whole organism myself my body right and so in the bodily self there's also a level of unity I'm just saying that this is again a computation within that specific subsystem that my body is one whole self and we can see how that gets uh perhaps distorted in somatoparaphrenia or even rubber hand illusion where uh this hand is perceived is not you know so uh and so we see that the system is plastic and malleable and and prone to change but and when when no uh interference is happening it continues to be cohesive is one body and one mind you know this autobiographical I I I've noticed you Ed the term narrative uh self I think that's an Neil Seth's term and I prefer autobiographical for the reason that uh rats they have episodic memories and they don't have our verbal abilities um uh and so they they also have episodic memories which are what where when the contextual memories right and I think that uh there's the same eye in all autobiographical memories and that's the unity uh yeah yeah thanks yeah a few comments it's it's it it it probably is more accurate to say it's a narrative cognitive capacity that encompasses autobiographical or Alo biographical type narratives um and I'm reminded of a NPR radio discussion from a few days ago that I listened about Memoir and about what Memoir was and could it only be written when the person did or didn't change a lot but what or who doesn't change in every single one of these ways so then what would that mean like what so that that that was kind of a funny connection and then and then um do you have any thoughts on that or have another active comment no I'm just enjoying your I don't know if I have a a thought on that yeah you you've see many clinical Memoirs and presentations of self- understandings and all of this so there's just so many and and it gets brought out so sparsely like and indirectly and it makes a lot of sense to the person who might be saying it but that that could you could be to or not confident enough about that interesting you brought it up thank you for that Daniel I think that you know when we theorize saying Dynamic theories then we uh usually think about different sub agencies of the mind like Freud for example in his you know structural Mo model he had the Eid EG and super egoo where I want pistachio asce is the Eid andasio ice cream is fat and calories that's the super ego response and stacher ice cream is for and you have three and a half is the ego uh but there's a whole bunch of them we can name a sort of a sub agency of an internal judge you know which is sort of super eish or a neutral Observer or uh an accountant who is running Itali or and all of this so we all have sort of a jury there of different kind of regimes of operation if you wish but uh the patient is often surprised because when they for example self criticize any talk about that in therapy and say do you notice the voice of your judge it's kind of you know uh not very fair I mean there's no misdemeanor everything's a felony and judges present all the time like no it's not the it's me I'm doing it you know I screwed up you know so again you know the default regime is is me myself and I that I am you know I am bad and when we we discuss this thing and we open it up and we show the different kind of uh we show a theater with different voes or a jewelry with different representations it's usually kind of an Insight it's a surprise but that allows a bit more control um for the patient in clinical work so okay I'll read a question in the live chat Pablo FM wrote thank you very much I find it super interesting I wonder why humans seek for drug experiences having been so decisive in human history coffee could be a great example tea tobacco alcohol have also been very popular among civilizations it's a great question I don't know if I have a good answer it's a huge question and there's a good paper you can find it on Google's call or a pbet by me zelner Mark SS and other colleagues I think Yak pun was alive it's called why depression feels bad and what do addicts really want and it's based on effective neuroscience and what it postulates is that uh of course there are individuals who have multiple drugs that are you know using all the time so there's an upper and the downer and other things but there are usually some innate kind of predispositions where people who are wound up in tents they sometimes crave alcohol because it's uh binding to The gabber receptors and it's sort of you know sedating us a little bit it makes you a little more less kind of wired up people who are very painfully lonely and uh uh detached and they crave human uh comfort and you know to feel together and loved and accepted they may if they discover opioids that would be a you know lock and key match that would be a dangerous combination um and uh perhaps when somebody's oriented to Performance and they're very bored all the time and they want to something you know to do better and more they may gravitate toward uh stimulants and so there's these things uh and there's some common factors among different agents uh such as you know I believe that multiple substances they have the dopaminergic effect and you know a lot of motivations are subjective in individuals I've had patients say that I'm smoking po to slow down time I've had patients tell me that you know PO is my oldest friend I've been doing it longer than any person that I've been with uh some people seek this state of numbness and they want to kind of not feel as acutely they want to sort of feel desensitized so these states of dissociation can be desired um and that's briefly I guess what I what I wanted to say about it so well thank you I'm reminded of a comment from ma in the live chat earlier this morning and she was kind of pointing to how one mode of how we've seen octave inference applied in these presentations is looking like athology the sequences of behavior but not really looking at the self-maintenance as a behavior like especially talking about digital agents or or the it's just about what do they perceive what do they do and it's taken as a granted or a separate question how they exist yet also there's this self-maintenance self reconstruction character that doesn't even need to have anything to do with the athology its Dynamics can just be like morphologically playing out not necessarily doing some cognitive task out in the world in any particularly effective way yeah it's a very interesting thought and for some reason I myself the Liberty to free associate again it reminded me of IAL Harari who went interviewed he said you guys go to the gym You observe your diet and you take care of your body and he said I take care of my mind I meditate an hour a day and uh it goes to silent Retreat a month and a year and the interviewer challenged him I think he was from The Daily Show he said how do you manage that and harar said I got no kids you know so but in addition to that he's also in therapy so there's a uh that idea of really maintaining the mind and not just the body with behaviors we're all oriented to what do I do do do what do I think think think which are sort of verbal virtual behaviors but how do you feel as my great colleague rajie uh Warrior says we're human beings not human doings we have a very tough time being we're all about doing more that's and so uh yeah and also Harari talks about informational hygiene where a lot of stuff in the social social media is noise and trash and a lot of it is consumed and we have pollution but you know so he doesn't do you know he reads books and he thinks in Millennia but I don't know I mean I went sideways from your comment but it just that's what where my thought went so yeah it it seemed like a very relevance when people are discussing the the the unity or the articulation of Mind and Body which is kind of what this unified promise on the cover of the textbook suggests is what what would it be to have a unified model of brain mind behavior and so on then the the analogy or the connection with the physical and the cognitive is just it has to be taken as a starting point not like a hot take yeah yeah but what you just said reminded when I read Michael lans's work and got more familiar with it I actually had regret I'm like why were we not taught that that was a bit of a revolution in my mind there was a big pivot because if you go through graduate program in psychology it's all about the unity you know I am the same you know cohesive and coherent and the fact that we have intelligence at the level of individual cells and then the cell Collective and then tissue and organ and all of this thing that I think is one montic Alexi is a pyramid with Communications bottom up and top down and laterally is is a revolution I think that you know I I wonder what models of psychology can be built with that and they don't exist yet it's all about one big block and what happens to it I am depressed you know there's no collectives this is one big block right yeah it's a really important Insight I think that's the kind of frictions and and experiences will start to learn more about and see produce in different ways like just seeing how you connected the topics to specific quotes translated the ontology re reworded descriptions using kind of like a translation sometimes really just as simple as a lookup of words just authors using one word and being able to see an act in on ology term equivalent or roughly related it's uh and and what what does that look like or I guess as your last comments what do you think after the paper after this now like what are the directions towards which people would say okay that's applying otive inference in a clinical setting I my my goal eventually I think is to get to Therapeutics and for example you know um actually I probably will not use that example it's it's a good material for the second paper that we'll follow up with uh but I I do think that you know the question is what do we do in therapy differently how do we understand how do we formulate cases differently with this information I I haven't heard it clearly said out loud in the did literature there's a lot of perspectives there's a lot of endless complexity and there's a lot of nuance but if we can highlight a central point and say there is a temporal depth collapse I think it allows us to do something with it in clinical work and in diagnostic work and that's my hope it may not materialize but this is what I'm hoping for so well even from a researcher's ethological perspective it's kind of like that's the transferable and the interesting research question how could you be observing not on the inside and make some kind of inference or assessment that would matter clinically with no map territory pretense not that it's 100% reliability just taking a clear look at the Diagnostics available rethinking it from dynamical systems perspective maybe different uh biometrics maybe pupil it's like it's not it's just kind of that's the open question and then where it provides clinical utility that's where it matters yeah thank you perfectly said yeah I agree okay thank you Alexi see you thank you Daniel see you byebye e e all right that has concluded the live presentations section of the Symposium I'm now going to play the pre-recorded videos so one second to rearrange get it in position now we'll continue with the pre-recorded videos hopefully all of them can c pated into the next few hours okay this talk is by Leonardo M Rodriguez Groove as the musical glue for social bonding ladies and gentlemen hello welcome to today's talk on Groove as this musical glue for social bonding my name is Leonardo Mula Rodriguez I'm a PhD student at Sim and this is a neuroscientific and cross-cultural perspective now I'm a bit self-conscious by the fact that the only audio you'll be hearing today is my voice no music but do books sing No we're doing a deep dive into the scientific literature not musical theater maybe musical theater in the future of uh my active inference well my my name is Leonardo Mller Rodriguez I'm a PhD student at the Cambridge Institute for music therapy Research Center and my focus is on sleep music four dimensions and if you're curious how does this tie in with danceability where it relates to musical entrainment um more specifically slow Beats I'm also a composer music producer producer and musician so how this groove Foster social bonding there's a little outline of today's presentation Groove and its relationship to spotify's stability the Neuroscience of it the social bonding the crosscultural aspect flow States in music what musical features Define Groove an active inference application of Gro and a l update to Spotify uh suggestion so what is in essence it's the Apex of an inverted u-shape relationship between rhythmic complexity and grooviness so and pleasure if you wish uh grooviness is at a moderate level of um rhythmic complexity which is um rhythmic complexity is if you use the musical term syncopation is the amount to which the Rhythm deviates from the meter or the beat or alternatively you can think of um the tempo the beat is a Tempo and it relates to how certain you have your predictions of that Rhythm so Groove is a pleasurable sens sensation of wanting to move the body to music driven by rhythmic patterns that balance predictability and surprise here's another study looking at the same thing and we'll see into the Neuroscience in a bit that proves this now spotify's danceability is a measure they use and they Define it as so we don't have the the actual computation that they do it's the suitability of a track for dancing based on a combination unknown combination unless you work at Spotify of tempo Rhythm stability beat strength or pulse Clarity and overall regularity and this data sets um someone put this on on reddits and they've looked at a million tracks and like okay you beat me in terms of data set um and so they've categorized all the musical genre several musical genres in terms of uh as a function of dance ability and based on the discussion and it makes sense um on Reddit it's more about rhythmic Simplicity seems to analyze because it's more more danceable music is techno deep house and then you have salsa music which is lower and people are in the comments who are Sala Dan like what what is going on and especially metal heavy rock or hard you know metal fans that were thinking like wait is comedy more dancable than my favorite genre so they're a bit offended with that um at least sleep music makes sense that it's kind of low um yeah so it lacks this relationship between the complexity and predictability fun fact if you listen to all the million tracks it will take you seven years and 9 months if you listen to it on 24/7 basis here's my data set I compared sleep soundtrack SE playlists um that are either very popular on YouTube or that I've created my own self or playlists that I have millions of subscribers on Spotify two high energy music such as gym music that has this beast mode playlist has a mill 11 million uh subscribers so this reaton uh music with reaton is a Latin popular dance music very popular here 11 million and this adds about 5 million and you see the danceability is in general higher for this higher en music compared to sleep music and here compar veence and high energy music is also more positive apparently uh sleep music is more negative anyway let's dive into the Neuroscience of it and here the main thing I want you to focus on is the fact that beat perception is present in the brain and uh frequencies increase their strength at the onset of the pulse here there are different tempos that were used to different speeds uh between pulses and as you see there's always an increase just after the uh pulse regardless of the tempo it happens all the time and it happens at the level of beta and Alpha brain waves and gamma brain waves these higher brain uh frequencies higher frequencies and um it activates these uh brain regions here and we'll go a bit more in detail but it seemed it's quite wide range and it turns out that we can call these the musical Rhythm brain Network because it makes use of the dopaminergic and rhythmic sorry the dopaminergic reward system in the brain um for beat perception and positive effect of groovy music and this involves a supplementary motor area premotor area nearby the pmen as well as for pleasure and the urge to move to the music is the nucleus acaban and this connection to the medial orbital frontal cortex because we're talking about an auditory stimulus um auditory regions must be involved and uh the cereum also form fine motor movements turns out that musical Rhythm predictions and surprises are measurable in the bra in the brain on in two forms so the first one is about short-term habituation to a rhythm via repetition of a certain Rhythm and then the Devi from that the surprising change to that rhythm is seen here in this paper where this this measure called the mismatch negativity which measures prediction errors in relation to Rhythm deviations changes or Omission there you don't have that note that used to be in that Rhythm and the other form um of surprises and predictions is by the recruitment of long-term memory or uh we can talk about it in terms of implicit it music theory syntactic knowledge where here regardless of you're being told in advance that there will be a non-traditional chord sequence um that will be played you will have this sort of mismatch negativity which is more specific because it recruits long-term memory here is called the electric response in the auditory nerve all and um yes that's an interesting aspect um by traditionally I mean traditional music western music theory um for harmonies here again we're just looking a bit deeper into this whereby beta waves we saw earlier and gamma waves The High Frequency waves we saw earlier in terms of beat perception are responsible for top down so the beliefs the predictions better and the updating the surprises um through the gamma high frequency um activity in the brain so another aspect of of music is that it's usually done in groups such as musical performance um appreciation music therapy or learning about music and turns out that music in groups synchronizes brains and this is called interpersonal brain synchrony and in addition it recruits the same musical Rhythm Network that we just saw with this you know a bit more broadening of it it has the amygdalin insula which is usually involved in visceral um uh activity and more generally the prefrontal cortex the Moto areas uh you know could involve actually moving if you're performing and the basil gang in general which incorporates all the dopaminergic reward system which nuclear acans uh and dopaminergic system Etc putam in Cate we saw that earlier now music the real function of Music beyond the auditory cheesecake that Stephen Pinker says it's not just pleasure not just reward systems it has to do with social bonding and building trust music is a participatory um activity and dancing actually is a way to participate without any musical virtuosity or even familiarity to the music this is why I call it embodied Democratic music participation open tool and fosters joy and social bonding and music strengthens this interact uh interactive rituals you can talk about it in terms of ritual and it's a social tool for enhancing uh groups so as as you can see here you start to predict you the Beats you see repetitive motions this induces Groove induces dancing to it and this enhances social bonding within this sort of you can frame it within an agent environment system moreover uh seeing others dance and move encourages dancing so this reinforces the social bonding aspect and this was examplified here where eyes open uh with low Tempo High Tempo High roof actually increases the collective movement um in a group of people listening to music and this is what they were measuring here with this cross wavelet and uh dance promote also here interpersonal movement interaction promotes self other integration it enhances social cohesion through synchronized movements and more specifically moderate uh syncopation and low as well for also an implication for paron disease low and moderate syncopation which is groovy music um does so as well encourages self other inclusion Which social bonding and I would talking about music The Groove and is it applicable to a crosscultural uh definition of Music well first there has been this is with Japanese music actually there has been an optimal Groove Tempo range identified um which seems to be a u inverted u-shape relationship non linear relationship with Tempo is this seems to be from 107 to 126 BPM beats per minute uh Tempo and The evolutionary interpretation for this is the fact that it's near 120 BPM which is the optimal tempo for walking and dancing so this is where Groove might come into sensory motor synchronization is most optimal in that around 120 BPM um and there are also universals cross-cultural universals of Music which has been identified using statistics uh um and the main aspects that are crosscultural is Beats dancing making music in group percussion repetitive melodic sequences these are musical features that are Universal just going a bit deeper into this they used a music for all around these spots it's quite a geographically varied data set although only 304 recordings you know there's millions of music but I think it's a nice General generalization we can make based on the Black Box the black box here are musical features that are at least 50% uh shared in globally and the most uh which are the most you know the whiteness is is shared um the most shared of beat subdivisions by binary tary subdivisions or scales that have up to seven notes so this is interesting and this was controlled phenetically meaning that this ensure that the these musical patterns and associations are not solely due to Shared cultural or evolutionary histories another aspect that is crosscultural are musical bodily Sensations and emotions what I want you to focus on here is this correlation they asked um Asian participants 5 500 people and Western participants 500 people to associate emotional labor to musical features and there's a very strong correlation 94 uh between these so there's a sort of crosscultural aspect here obviously it's not all cultures maybe African Latin and uh Middle Eastern cultures do uh feel differently but here's just a Latin um influence that's indeed instrumental sinking in syncopated music which is uh groovy music which here was Samba um encourages the urge to dance so Groove at least is present in Latin American music now musical flow is not restricted to groove what is flow flow is a state of absorption total absorption in an activity in other words the feeling of losing yourself in the moment uh flow state is usually characterized as a state of emotional resilience against trauma and here emotional expressivity is one of those not Groove uh uh musical features that induces musical flow and here the two uh main things I want you to focus on is the blue um histograms which are states of flow here were performers piano performance and this is a musical feature and so there was higher flow during swelling dynamics of crescendos De crescendos which volume um uh fluctuations as well as um repetition of um Melodies and not only just any Melodies but stepwise Melodies that are not too far apart in terms of distance on the keyboard uh or in terms of notes so the closer they are blue is again Flow State the closer they are the The Melodies the more flow inducing they provoke um another musical element that provokes flow that is not Groove is the narrative Arc storytelling emotional Journeys in music and this interesting paper by colleague Annie hesit identified stages emotional stages of the hero's journey which is a a narrative Arc found in myths films popular films yeah and stories narratives in general they've identified emotional stages of the heroes journey in patients and clients specifically in this case was eating disorders but applying to just more generally undergoing music therapy so more they were look they were doing guided imagery with music which is a type of music therapy and depending on what the saying the clients these themes were identifiable within this uh the hero's journey narrative Arc so that's very interesting um and this is very important for music therapist because this uh informs further the scanning aspect that they usually do in GM which is that they try to identify using the music where is the emotional stage that the client and the therapist should start their um work emotional work and so this is actually a emotional Journeys are actually a concept that is applied by DJs they usually talk about energy arcs for their DJ sets um and it's also very prevalent in film soundtracks um you know film soundtracks are using so music is being used to guide the emotional journey of the audience now flow as neural and TR I think it's neural entrainment is evident for beat perception we've seen it several times has more evidence like accurate beat perception increases neural entrainment here is just saying that beat sensitivity uh there's a sort of tempo range where this sensory motor synchronization decreases uh with very slow Tempe um but here again be perception this the beat is being um being identified in a neural activity and even if you imagine certain rhythms these frequencies appear also in the brain activity so neuron train with beat perception is clear but how about for emotional expressivity and um emotional Journeys well this study um reconstructed the music that was listened to by the uh participants from their neural patterns it was a combination of EG and fmri a technique that made use of EG fmri and deep neural networks to reconstruct the music based on the neural activity and you can see that not only very impressively resembles quite a lot in terms of spectrogram information but obviously there's not just Rhythm activity or yeah Rhythm information that's being displayed there's more this frequency activity so this takes us to this study that demonstrated that spectral flux is actually the most um accurate measure for neural entrainment to music and you can see this with this red line is the highest correlation um and with neural entrainment and in addition they've uh demonstrated that it is a metric that encompasses amplitude Envelope as well as beats they they have mutual information here and spectral flux or spectral novelty is called is a measure of nov energy increases in the frequency spectrum um so that's an interesting aspect now nevertheless as you can see other strong neural synchronization musical features um in addition to spectral flux are Tempo range so uh roughly as you can see here with the orange uh line 60 to 1220 BPM uh High familiarity with the music and pulse Clarity weasy to receive the Beats so let's keep it simple what are the groove's musical features two things moderate rhythmic complexity within an optimal range of around the walking tempo of humans um this these two Encompass most of these characteristics syncopation Tempo micro timing event density this has to do with and even beat sence has to do with rhythmic complex complexity so of attack is a variant of beat saliency a low frequency band is the base enhances um dancing uh volume and then this is the core progression and that now keep in mind that sensory motor synchronization is not limited to this uh Groove Tempo range the sensory motor uh synchronization Tempo range is shown to be between 60 to 180 BPM below and Above This the sensory motor synchronization which is usually finger tapping or dancing to the beat decreases um which is probably linked to biomechanical reasons um however probably training increases your capacity to do so uh to synchronize however the beat perception still can occur Beyond those uh temp this Tempo range fun uh I recommend watching this YouTube video by Adam Neely which uh goes into this uh the extremes of be perception let's apply active inference into Groove but first before we do that we need to understand that the brain is constantly generating predictions about how the world Works in order to minimize the uncertainty about how the world Works two the brain is a lay thing and it balances top down and bottom up influences where higher layers send beliefs about the world down to lower sensory layers which the sensory layers as we can see here um actually send back up sensory stimul to adjust the uh beliefs through the mechanism of minimization of prediction errors or minimization of free energy now talk about groove groove balances confident prediction uh confident predictions with surprise about the Rhythm uh just reminding you it is an optimal playing with belief updating about musical rhythms now listening to groove is accessible to groups of people and therefore can become a shared stimulus for motor and emotional synchrony and therefore under the active inference uh framework um generates generalized synchrony uh which means that people have shared World models the listeners have shared World models and this is depicted here um uh with this quote it says neither a model of my behavior or your behavior but a model of our Behavior quote by friston um and generalized synchrony actually requires flexibility in terms of your higher uh beliefs um and because you're trying to model uh flexibly your Collective identity and Collective behavior and because of this and the social bonding aspect I place musical groove on the spectrum of the Rebus hypoth hypothesis which is a relaxed belief under psychedelics meaning that it's not as uh the the flattening of the free energy landscape is not as Extreme as psychedelics are supposed to do but musical Groove can achieve this through the the social bonding so dancing to the same music not only increases trust and social bonding but it also allows for individual creativity compare an expert dancer and a novel dancer um a novice dancer and an expert won't be having motor mimicry but they will be sharing emotional synchrony and this uh provides the mechanism for trust and bonding while considering uh creativity of individuals and you can make use of a Federated belief multi-person system um to model Groove as the shared marginal likelihood that people nonverbally communicate um so because if sycratic dance moves have been shown to even increase more uh the social bonding and Trust I encourage you next time to take the risk and bust on some moves on the on the dance FL all right so what are what I want you to consider right now is the current models of the context that encourages dancing yes they say musical properties they have temporal predictions that you can do induces pleasure induces arousal and this to together with regularity Chris the urged to move to the music and um this will generate motor planning or policy generation under the active inference uh framework and and this desire to move and in training um body movements create social connection but I like to add a bit more to this in terms of contextual factors based on the literature review we're presenting today this is a high HP pathetical contextual Factor um framework for danceability um basically Groove is the mechanism for uh dancing but there are several contextual layers that people individuals might need to cover um before starting to dance for instance I have two example individuals a white individual here this white uh Arrow who requires um seeing other people people sitting here their friends to before starting to move whereas this red individual just requires to feel a change in emotions to start wanting to move and these layers uh have a nonlinear relationship with the groove meaning that uh your entry level contextual entry level to start dancing um might be seeing a friend starting to dance and you might not need familiarity with the music to start dancing to start connecting to the groove uh whereas DJs sometimes make use a familiar music in their DJ sets to encourage people to start dancing they say getting those first three people to dance is always the most difficult part and uh once you have them the crowd joins and you have a crowd dancing so this was modeled with a reduction of entropy to do it relatively simply uh whereby depending on the distance to the group there's a higher um entropy level that has a nonlinear relationship uh with grooving and K has is based on personality and the relationship between each layer there's another way of conceptualizing this as a the gravity of GrooVe the layers uh are done like this so implications for spotify's danceability we saw that is more likely has to do with rhythmic complexity comedy is more dancable than uh Blues apparently or or heavy metal head banging is not as you know dancing as as as laughing veence as well the most joyful music seems to be as Sala and the less joyful music is sleep maybe Spotify is inspired by uh the rapper Nas who has a lyric saying sleep is a cousin of death so let's go beyond this and uh and Beyond Rhythm and tempo and incorporate what we've been seeing today into dance ability so if you want to generate a formula that goes beyond theability unknown equation which is most like rithmic Simplicity we convert uh we Sorry first uh I want to point you to this study which asked musicians to perform groovy music uh with simple and complex M Melodies and they showed that there's about 30% occurrence of past notes specifically 16th notes um when people perform groovy music and this was also confirmed in terms of listeners saying yeah that's groovy um and so here I've just uh used the optimal Groove range and transformed it into seconds so you can have an idea and ah too quick for Spotify they can't no I'm just kidding uh you get to see the the the equation um so what I've done is I made use of uh spectral flux which is this spectral novelty makes use of for transform and then logarithmic compression to enhance uh weaken spectral components and then you compute the difference between time frames in terms of positive energy increases and so measures positive increases in Energy Event density is another measure that's been used in these studies and it's a number of events onsets per a Time window so here's a Groove uh formula suggest which is open to to your feedback um which is 30% of energy onsets uh that is in the 16th of the note um time window um divided by a measure that's dependent on the tempo and a measure is four times a beat because most music is on 4x4 meter music so I want to give you this quote dance is an energetic mode of of participation for a large population regardless of age familiarity with music or instrumental singing virtuosity to summarize we found that Grove is a musical mechanism for social bonding through movement and therefore through movement is danceability right and we can make use of the active inference to model Groove as a musical mechanism for G generalized synchrony even uh as Federated beliefs to um model individual creativity further Works um would encourage the usage of uh um other social bonding metrics in music such as group singing uh chanting is seen around rituals around the world and this has implications for group music therapy life performances and team building music recommend systems Etc so please do comp contact me I'm open to collaborate for this uh Groove U project and discuss any of your ideas uh we could discuss about musical uh uh you know exchange music if you want and I just want to show here my YouTube channel which explores thanks to the YouTube channel I've actually learned how active inference works you know they say teaching is the best way of memorizing and in here for instance I made explored Chris Fields um work in active inference with the holographic boundary um for social interactions which is which could also be a a further uh direction of of trying to to conceptualize musical Groove in that manner but shared marginal likelihood is probably very effective as well uh thank you so much please do ask me for the references um I will send them they have been sent or yeah I should be on the chat by the way asking answering some questions thank you very much for your time see you very soon byebye thanks all right I'm going to play next a short video from Anna Pereira and active inference Institute research [Music] fellow hey Anna pra here I was disappointed uh that there was an irresolvable conflict uh during the time of the symposium but wanted to share uh or contribute um a small update nonetheless uh for those of you um who haven't had a chance to hear from me before um we've been looking at accelerating and disseminating active inference for the Humanities uh we're actually just starting to call it aih uh and we continue to make progress um and quietly incubate in the background we're not really at a point to do um a large more solid update uh but we wanted to quickly say hello and and say that we are incubating uh we continue to be available uh for collaborations or conversations um as others find useful uh and also to provide an update that uh we've been really grateful for the opportunity to incubate within the larger AI structure um this has provided several mutualistic affordances um um has been a really great um construct within the the larger AI uh space so we look forward to being in touch uh at Future conferences and symposiums and available for conversations um in the meantime uh wishing you a great Symposium and uh be well all right the next talk is by aswin Paul bridging biology and AI active inference for biologically plausible decision-making models this is a 15-minute [Music] video hello everyone uh my name is ashin po uh I recently finished my PhD from monach University in Australia and is currently working as a machine learning researcher in versus AI so today I'm here to talk about using active inference to model biologically plausible decision making uh and thank you active inference Institute for organizing this Symposium let us start uh by mentioning a recent Cutting Edge experiment that was developed by cortical labs in Melbourne so what they managed to do is they developed a chip a silicon chip and they cultured real biological neurons like mice cortical culture and human cortical culture and they managed to couple this chip with a computer game like the game of pong that you see on the screen right now so the experimental result is that given meaningful feedback these neural cultures actually demonstrate relative improvement over time in playing this game as you can see from this graph right like the MCC and SCC neurons actually demon at that they can actually improve in game play when compared to Baseline groups like the silico um agent or the control groups so given such advancements in experiments from a theoretical perspective active inference is a great candidate to model such experiments and biologically plausible decision making so we have emerging literature and a lot of evidence um suggesting this so when we actually try and do modeling there is a clear need of scalability in the active inference literature where we are used to um modeling very small environments with very low number of states uh and uh very low number of actions Etc so in order to actually um use active inference in such scenarios we have to scale up the existing active inference algorithms and in the first part of the talk I will be talking about some algorithms and techniques that we can use to scale up uh model in decision- making and active inference and in the second half I will talk about using these algorithms to actually model the particular experiment that we saw earlier so when we talk about um scaling up active inference or scaling up decision- making in active inference the first technique I want to talk about is evaluating um decision making backwards in time and not forward in time as we usually see in the active inference literature so when we Define the expected free energy um and um compute which action should be taken given a particular observation uh rather than evaluating expected free energy or Computing It Forward in time as we see in techniques like sophisticated inference we can actually use dynamic programming and model decision making backwards in time and this result is actually published in paper uh in this particular paper if you are interested in this particular algorithm and a more General version of this algorithm was recently published as the inductive inference algorithm um by um our colleagues in versus Ai and um in that paper we note that um inductive inference algorithm scale up decision making drastically and can be applied um to um bigger generative models um another technique that we can employ in this regard is to actually learn prior preferences and in order to do that we actually take um inspir from the optimal control literature uh and this particular paper and this particular paper introduces an algorithm called said learning that um improves um scalability and um efficiently computes um the optimal action um when compared to um Cutting Edge algorithms like um the Q learning algorithm and a particular Matrix in this algorithm called the said Matrix um is actually very similar to uh uh the prior preferences in active inference and we can actually learn this set Matrix orders of magnitude more efficient than um classic reinforcement learning algorithms and what we do in in um in this paper is that we show that we can actually learn prior preferences using a similar learning rule uh and once we learn this prior preferences then decision- making is really easy because we don't really have to plan a lot forward in time and we also demonstrate that learning is robust in this particular algorithm and um talking about computational complexity both these methods one is DPF method um that computes um expected freeny backwards in time and the other one that um learns by preferences are actually orders of magnitude more efficient than the classical active inference we see in the literature or the recent um sophisticated inference and both these methods actually learn um um or scales linearly with time against the exponential computational complexity of decision- making that we see in the active influence literature um so we tested these two algorithms uh in terms of its scalability and adaptability in um State spaces um as high as 900 uh and what we saw is that when we actually um introduce stochasticity into the grid these algorithms are more adaptable uh then um Cutting Edge reinforcement learning algorithms like din Q uh and we demonstrate that these algorithms are scalable which gives us confidence to go ahead and model um experiments like this and when it comes to modeling neuronal Dynamics we already have existing active inference algorithms uh that model neuronal dynamics that is from um CBS ricken developed by professor isur and colleagues and in one of their Works they actually talk about um a similar experiment where um neural cultures are actually performing a perception task uh which is called B blind Source separation and in a more recent work they also talk about decision making in neural cultures uh and um develop a decision- making model uh that can be equated to neuronal Dynamics so going into details um we have this algorithm called counterfactual learning algorithm uh and uh there develop a version of the variation free energy uh that is term to term equivalent to a neural network cost function uh and this is valid for a particular class of neural network and more details can be found in this particular paper and there are two terms of interest uh in this algorithm one is a Time dependent risk parameter um and a Time dependent State action mapping that this particular agent um uses for decision making and because of um how it is developed it is biologically plausible and can be shown that every term is one-on-one related with the cost function that a particular class of neuronal networks actually minimize so in one of our recent Works uh we um generalized this algorithm to have memory and we tested it in a similar control task like the card P game that we see on the screen and we can actually draw comparison with the card Pole game and the pong game because both of them are reactive task uh where are where where the agent is supposed to actually react to the environment uh and take decisions quickly uh to balance this pole or to deflect the ball back um so we observe that um compared to planning based algorithms um like DQ and DPFE uh the counterfactual learning algorithms perform really well in this particular um task and gives us confidence to go ahead and model uh the biological um neurons or the or the kind of experiments that we saw earlier right and in addition to being able to model this uh we also have um the advantage of explainability in these algorithms unlike um deep learning algorithms that are black boxes so if we actually perur the environment say in a particular episode we can actually see that the risk term is shooting up in that ex episode and um this um particular term captur those um changes in behavior of the counterfactual learning agent and we can actually try to explain how decisions are being taken at every time Point uh unlike blackbox approaches so given these um several algorithms that we have can we use them to actually model um the manifestation of natural intelligence uh in experiments like dish brain right so we for that we actually develop an experimental based generative model that is based on this particular experiment so for example um this is the pong game environment that they use in the experiment um um precisely so for example um we have the x coordinate of the ball that being communicated to the dish brain chip uh through 38 distinct frequencies in the experiment um and uh the y coordinate of the paddle and the ball being communicated through a different voltage levels in the experiment um so for example like we have three observation modalities uh in this experiment and we note that these are the number of states and the ways of communicating that to the agent and we um uh enumerate the state space of our generative model um using this experimental details uh and match our generative model to be exactly like um the experimental generative process and we also have like the active inference parameter s um in our generative model like the transition dynamics of a pom DP and counterfactual learning based State action mapping the risk term and the prior preference distribution Etc that we discussed in the talk earlier and given that we have this experimental based generative model we can model decision making um of in silico active inference agent and hope to compare them with the experimental result to draw insights and so on right so first of all we uh perform simulations using the counterfactual learning agent uh and we can see that counterfactual learning agent um with memory Horizon um as low as two is actually uh performing at par with the HCC group in terms of performance so here CFL um T stands for um CFL memory Horizon T So as expected with more memory Horizon it performs um better uh and uh we can actually look at um which memory Horizon is performing similar to the experimental group and so on and in one of our recent works we note that the active inference agents are incredibly data efficient compared to deep learning methods and it is only the active inference based model that is counterfactual learning uh learning method that demonstrate data efficiency at par with the biological neurons when compared to deep reinforcement learning algorithms like a2c and poo if you are familiar with the Deep reinforcement learning um literature um so we also compare the planning based methods that we mentioned earlier like the dynamic programming method so in this particular task or the pong game um planning is not really useful so with increasing Horizon of planning um we don't really see um Improvement um and um planning Horizon of five performs as good as HCC neurons and planning Beyond a point is not useful in this particular task because of the nature of the task and we also not um the explainability of our models for example if we look at the average risk um in the counterfactual learning method we see that the risk is actually only reducing um significantly uh with a memory Horizon of four and which and this can also be correlated with um the performance Improvement that we saw um in the results graph where with memory Horizon 4 there was a significant jump in performance and these kind of explainable insights um that can be drawn from our models as a clear advantage over black Mo methods like um a deep reinforcement learning uh and we we can also look at um uh the entropy of the CL parameters and we see that the entropy is decreasing over time that is um the evidence in learning and parameters reveal um differences in behavior of different agents and that is a clear Advantage here and to summarize um we actually developed an experimental um informed generative model and simulated decision making in the particular um uh environment uh we tested different active inference based algorithms in this setting and uh we also uh noted the similarity of memory augmented counterfactual learning method um to the biological neurons and we are interested in investigating similarities um of the memory based CFL method more to the biological neurons and also insights like this can Inspire more experiments where um aspects like memory can be studied well uh there are several um limitations to this work where we haven't conducted um a really um involved statistical analysis in comparing these agents we are right now qualita ly comparing them um with um average parameters but we would like to do more statistical analysis uh and we would also like to um Inspire um experiments from our theoretical predictions in our future work so I thank um all my collaborators in this work and if you're interested in this particular um work and the algorithms that I discussed uh you can actually um get access to all the papers here and uh feel free to contact me if you have suggestions or more questions about this work so thank you for your time uh and um I look forward to meeting you all when when I have a chance next thank you so much all right the next talk is by Bert Burgers towards autonomous Urban digital twins this is a 20 minute video for the next 20 minutes I will talk about transport policy planning and how active inference could be applied through digital twins urban planning is a very broad field encompassing everything that occurs in urban regions from the design and operation of infrastructure to social programs to scope this I will focus on the transport system giving a rough sketch of what active inference May entail for this domain modeling the transport system relies on operationalizing decision making many individual choices lead to busy roads therefore the main story line focuses on the difference between active and passive decision- making the presentation is structured in three parts first is the context giv an outline of urban digital twins I will focus on livability as it is an objective of Transport policy next I will present some domain specific building blocks which are then combined in the final part finally I hypothesized that renormalizing generative models can be used as transport models concluding that active infant based modeling might require A New Perspective on trans transport planning first I will give some context in order to frame how active inference transfer planning and digital twins come together the field of Transport planning is moving towards the development of digital twins these twins enable quick calculation of effect given an intervention rather than performing a comprehensive study per project which can take months a digital twin can be reused many times an example application would be the construction of a new neighborhood what are the effects on congestion air quality and inundation risk there are roughly five levels the first introduces a compelling visual interface often involving a 3D representation of the real urban region next is the addition of information on top of the visualization any Geographic data works the third step not only Maps data but incorporates different physics based models H it is called predictive transport models are one example others are airflow or water simulations high performance computing speeds up the scope of parameters which can be tested the fourth level Builds on top of the predictive models by adding an interpretive layer the added value is scenario based planning where a range of outcomes is calculated for a hypothetical intervention instead of configuring parameters exactly you could propose a preferred outcome to the computer and that would only simulate what is necessary as it understands relations between models the last level introduces autonomy not only are physics based models configured intelligently as in step four but the TR actively goes out to construct its Niche transport policy evaluation is going to somewhat of a change and decades before the development of Transport models allowed the Paradigm of predict and provide we could forecast traffic flows and accordingly invest in infrastructure to prevent congestion more recently the fielders acknowledged that models trained on past data are inherently flawed in predicting in certain Futures as such it is moving towards scenario based planning sometimes described as desired and provide along with this change comes a reflection on the philosophical context for Simplicity I rely on two Frameworks the first is utilitarianism it relies on the Assumption of Ral Choice enabling the quantification of benefits and costs in terms of util utility and dis utility between every origin and destination there are several alternative Roots each has attributes such as the distance traveled and the cost combined with preferences we get generalized travel cost the rational Choice assumption enables fungibility this allows us to take the ratio of estimated beta parameters in utility functions these ratios can then be used to calculate for example the monetary value of travel time in practice we always use monetary value as the final measure so that we can compare the cost of implementation against the benefits experienced finally utilitarianism leads to aggregate increases in welfare but the ignorance of the distribution not only does this apply to wealth but accessibility too take for example the discrepancy in infrastructure spending between core cities and the periphery to address these concerns contemporary policy evaluation attempts to move away from utilitarianism instead of attempting to relate all preferences to monetary value we leave them separate not only do we reject theability of indicators but we also consider many more than before shown in this wheel spending from living environment accessibility health and safety finally the distribution of costs and benefits is explicitly considered and much work is now spent on reworking existing modeling Frameworks to make these distributions explicit there are three main objectives in planning infrastructure accessibility safety and liveability capturing liveability is much more difficult than accessibility and safety livability is generally defined as the fit in an ecological relationship between resident and the living environment with regard to the spatial temporal scale of this relationship livability is local and sustainability is global for practical reasons fit is operationalized through valuation the more an environment can satisfy preferences the greater the fit theoretically it has been long known that fit could also be operationalized as a continuous transactional process however little progress has been made and this is where active inference comes in these two approaches to livability are best defined by contrasting them to each other on the one hand we have the static approach which is concerned with measurable outcomes on the other hand there is a dynamic approach in which liability is viewed as the hidden driving force behind that which is measurable by Framing liability is comprised of three components indicators percepts and needs desires I can show that the location of fit determines which is which either it is a weighted function between percepts and a preferences through valuation or liability is a way function between indicators and percepts which are biased towards our preferences finally these two approaches might be complimentary and this might be useful for modeling purposes there are many different transport models when it comes to those used for urban planning these can be distinguished into micro and micro simulations the micro scale simulates individual vehicles and their interactions meanwhile the micro scale applies constrainted optimization over a network preferences constrain this optimization here I will focus on micr models which combine transport and L use patterns fundamentally we rely on seeing the transport system as a market Travelers generate demand while the Network provide Supply the four-step model forms the backbone of microscopic transport modeling and I will go over each step first is STP generation this step determines how many trips start and end in each zone is about determining the row and column sums see also the top right figure for trip production we look at residential zones and use socio demographic factors like household size car ownership and income levels for trip attraction we look at the number of jobs shops or school capacity trip distribution iteratively balances row and column sums to fill the origin destination Matrix balancing is done using a gravity model where the number of trips increases with the mass but decreases with the travel resistance with the full origin destination Matrix we can split these flows per travel mode used the utility of each mode can be weighed against our order in the choice set logic models can be estimated for different segments of the population uh that is the figure in the the equation on the bottom right finally we assign trips to Links in the network according to wardrop's First principle Travelers select routs to minimize the travel time leading to an equilibrium but no user can reduce the travel time by changing the entire workflow is iterative because assignment affects travel resistances due to congestion not shown here is the modeling of land use which affects zonal data next I will Outline Three conceptual building blocks which will be combined in the synthesis as we just saw modeling transport networks requires a representation for example the travel time PL link additionally we would like to approximate human perception for example by using images or sounds the field of urban representation learning attempts to automatically capture the content and structure of cities luckily there is a strong inductive prior from geography similar things are closer together as such major design choices revolve around which measures of distance to include examples are ukian distance travel volumes or accessibility generally an application of urban representation learning relies on three choices first is the need to discretize the urber region into spatial units each acts as a clause the figures on The Right Use hexagons secondly is the type of data data used where traditional transfer models only only use tabular data we can now also use street view images and text last but not least is the way in which parameters are structured and learned cph neur net networks are widely used as a naturally represent the transport Network Additionally the loss function can be used to steer training accessibility is a major field of study in transport planning that connects well with active inference Concepts in urban in urban representation learning we aim to capture spatial similarity in edings accessibility provides a theoretical foundation for which similarities to to capture intuitively accessibility is akin to Landscapes of affordances which are shared by everybody living in an urban region location based accessibility is theise from constraint maximum entropy and can be configured to equal utility based accessibility the calculation is straightforward for each Zone we Sim the product of destination attractiveness and the deterent function based on generalized cost the deterrence function captures how travel cost relates to similarity I shown it in the maps accessibility patterns differ significantly between bike and car networks hierarchy is a well established element of Transport networks think of how local streets feed into high into arterials and highways each operating at different speeds and distances where local streets facilitate access main roads enable through put to capture this hierarchy in urban representations we start by defining the study area creating Columns of course hexagons and then mapping them to the finer resolutions I have used a unet to capture both bottom up and top down information each block in this net is a graph convolutional neuron Network a pre computed accessibility graph sparely connects centroids of hexagons where the edges are the accessibility value reconstruction loss ensures we capture input data while consistency L ensures Columns of hexagons are integrated combining context and building blocks I will now go over the synthesis we started by discussing digital Twins and two approaches to liveability static and dynamic the dynamic approach frames livability as Niche construction when it comes to policy the problem boils down to the objective function used good free energy acts as an objective function for policy evaluation unlike utility free energy cannot be summed to cost decision makers perception is itself is subjective meanwhile with utility we all live in the same fully observed world if Urban regions engage in Niche construction perhaps we can create digital twins that embody the niche construction process autonomy then becomes about influencing an existing process rather than creating something from stratch there are three potential approaches agent based models on a hex crit simulating individual Travelers simulating each hexagon as a cell with the expectation of emergent Behavior or use a renormalizing generative model with a fixed hierarchy the third option is the most promising as it requires a single model rather than the combination of many smaller ones in line with this third approach I want to consider a fixed Hier key and renormalizing generative models are perfectly suited to this purpose the idea is that these might already be transport models as they up Origins to destinations using cost functions sticking with the notion of a market equilibrium there's a demand and a supply side to the operation of renormalizing generative models just like in a four-step transfer model the demand side determines the landscape of a fored embeddings and these embeddings capture the spatial similarity the supply side aims to calculate the distribution of flows through the network it is just that here the network is a hierarchical structure not the physical transport Network on the demand side the landscape of a fores or the landscape of aorded embeddings is perceived through the combination of top down prior and bottom up data from the top down we impose our cost function on traversing the lateral accessibility graph attributes on edges are valued against each other for example travel time and the price of petrol we also impose our preferences in term of the content we wish to see so the content of the embeddings for example the concentration of emissions in the air from the bottom up we receive information for this lateral accessibility graph like the travel time at this moment as well as the content of this creete spatial units and this content is in the form of embeddings Ander from high dimensional data using neural networks so that they can so that this process can be automated on the supply side we have a landscape of affordances at each layer in the hierarchy and we need to pass messages between them supply is about narratives of attention either about errors flowing up or predictions moving down to move laterally you have to go up and over in doing so carve the transport system at its joints for example when you need to do groceries you do not zoom out to the courses layer instead you will find CP much more quickly and walk a few blocks using active inference as a framework to understand decision- making in urban regions is still speculative the standard workflow of xant evaluation to study the benefits of a project might not be applicable anymore as the digital twin goes out to construct its on its own the separation between technical calculations and political motivations in planning becomes blurred how do we capture both travel choice and policy objectives in one model accessibility is already a central topic yet it has now gained additional consideration due to the reframing into Landscapes of affordances these affordances may not only be in terms of tabul data which can be high dimensional like images accessibility heavily draws upon constrainted maximum entropy ban mechanics treats this the same as free energy minimization it therefore seems worthwhile to reframe accessibility in this way too active inference is useful not only for autonomous Urban digital twins but also for level four twins to refresh level four twins use physics based models with the addition of an interpretive layer and this additional layer contains an understanding of the sensitivities between models which allows an urban planner to quickly Converge on a set of designs rather than simulate thousands of Alternatives therefore a practical starting point would be to take level three digital TS and upgrade them with an active inference module to explore the parameter space efficiently the apparent complementarity of static and dynamic approaches of liability suggests that we could use representations learned through valuation so a supervised um way of learning Urban representations to then use them in learning the structure of an active inference model in s it is fair to say that autonomous Urban digital tends still need a lot of work regardless considering free energy as an alternative to utility is a valuable exercise in understanding decision making in urban regions lastly I would like to thank the team of Symposium organizers for the time and effort all right the next video is 23 minutes long in s hippolito dysfunctional markof blankets from stuck states to [Music] adaptation hi everyone I'll be presenting today are dysfunctional Mark of blankets from stock states to adaptation these a paper that has been um in progress for quite some time and it is um a collaboration with Maas Alex keifa and Daniel fredman so I want to start by uh highlighting that the are stuck systems or stuck States all around us and that with find that example in neurocognitive um levels such as in OCD patterns in PTSD trauma Cycles or in addiction circuits then on the social or ecological scale uh we find it for example in poverty traps in eoch chambas and Market uh monopolies and climate tipping points then in AI we also find them for example in a large language models Loops who hasn't been stuck in a I apologize kind of cycle then uh in reinforcement learning local Minima in recommendation filter Bubbles and in neural networks training fails so what um these kinds of systems uh share is that uh they are some sort of critical threshold uh they have um or present um self reinforcing loops and uh they have this persistence to uh change so they tend to remain in the same state hence why they are stuck so then the core problem is going to be a sort of a paradox so um the view is that systems adaptive or complex systems um coupled with the environment they should adapt but some sometimes they don't so then uh from this perspective a few questions um become Salient why do flexible systems become so rigid and do not have that flexibility to adapt to the environment how do these adaptive boundaries uh between the system and the environment break down and what maintains that dysfunctional State what is there that must be so strong that um the system cannot uh change to a different uh or look for or act for uh to attain a different more preferable kind of state so then of course that these presents um these are presented as Challengers because there is an everchanging world and the system that needs to adapt to that everchanging World um and uh it seems like um existing approaches um such as dynamical systems theory and network uh science um still have some limitations to quantify and address this kind of phenomenon that we call a dysfunction so then from our perspective uh we need um a framework that unifies that uh unifies this explanation so we need three things first we need to identify and characterize dysfunctional States then we need to provide a mechanistic explanation for the stuck States and finally we need to offer a principled approach to designing these kinds of interventions and that's what we set up to do uh in this paper and that I'll be presenting to you so um our framework Bridges um theoretical Frameworks uh from complex systems theory and the free energy principle the main or core um Concepts uh that are going to be useful are Notions such as emergence self-organization and adaptation from complex systems theory and coupling self-maintenance and free energy from the free energy principle by uh utilizing these two Frameworks uh we developed uh what a construct that we uh defined as dysfunctional Mark of blankets and with that uh we hope that we will offer a formal theory of uh systems dysfunction and we hope to develop quantifiable measures for this phenomenon of stuckness and finally practical recovery strategy so that we can act upon uh the system that is stuck in a therapeutic way and we uh uh hope to kick the system out of that stuck State into a healthy State okay so as to adaptive systems we all know that um they are comprised by components which interact and from that interaction we observe some sort of emergence um for example in the case of an immune response we have different immune cells each one with a specific role and they interact in the form of cell signaling Rec recognition of events and that interaction then gives rise to emergence that we observe in the form of coordinated defense or system level memory then uh from the free energy principle which we uh are very acquainted with uh in this conference uh we learn that systems that exist act to minimize their free energy and um further to that um the understanding or the Insight is that systems are coupled with the environment and they act upon the environment to precisely uh attain or look for more preferable States for their self-maintenance and we can explain um that behavior through um active inference then another very useful uh construct is Mark of blankets of course the framework is a dysfunctional Mark of blankets and the markco blanket uh as we all know is a statistical boundary between what is inside of the system and what is outside but further to that uh an important uh Insight is the conditional Independence between internal and external states which give um the system sort of a very important boundary um to have a distinction between the self and the no self and that can be seen also through the notion of it's an operationally closed system system system that self organizes and maintains itself for that maintenance it's also quite important that it is thermodynamically open it is an open system it's not a closed system and that's what allows them to remain alive by adjusting to the environment and here you see for example we have a a neurons in the form of internal states that could be part of um Network so a key Innovation concept that we bring in is the notion of selectability and selectability is a system's capacity to explore navigate and select among possible States these can also be seen as the system having multi-stability many points um that are afforded to the system in the environment such that uh the system can be healthy in that navigation so you can see that for example that comes represented here in the form of uh State A B C or D different points that the system can explore within the environment and select the most preferable ones and um the notion of selectability allows us to um think about State space accessibility or affordances the adaptation capacity and the boundary flexibility to adapt rather than being rigid and stuck in a particular state so then uh within the free energy principle we can think through that Paradox it becomes if systems minimize free energy if they Accord to the principle why would they get stuck does the principle not work or is it the system that is uh not in a stuck state so then uh what we realize is that um what is not what is really within the Paradox is that we cannot explain these persistent maladaptive States and it is not really a formal treatment of the dysfunction so then there is a missing link between Theory and intervention and our solution is the dysfunctional Mark of blankets there are two types of boundary dysfunction that we can observe uh in these systems that are not adjusted attuned adapt to the environment so they can come in the form of Stu States or blanket breakdown and stuck states are mentioned this in the first slide and uh for example in the case of depression what we observe is rigid kinds of thought patterns reduced emotional flexibility impaired Social uh information processing and shortcuts bypassing healthy emotional regulation and in a more General way we can Define these stck states as the self-reinforcing kind of Cycles being trapped in deep energy Wells and being resistant to environmental perturbation then on the other hand we have blanket breakdown and an example of this is the immune system when the immune system attacks its own tissue and what happens here is a very interesting phenomenon of excessive permeability um on the boundary of the blanket right loss of system Integrity so the self no self distinction the system doesn't know what's self and what is not self that's the loss of of system Integrity because of the boundary dissolution okay so then we arrive at uh the definition of the dysfunctional Mark of blankets and this this is a statistical boundary that um contrastingly has lost its adaptive capacity for Effective mediation between internal and external States and the key characteristics of um the dysfunctional Mark of blankets are that the system is persistent in High free energy despite adaptation attempts and what we observe is the breakdown in information processing and the loss of effective environment mental coupling I will unpack this further but let me just situate us um for a moment so uh we will go from the framework to practical tools to cross scale applications that's where we would like to go uh so to do that we need to First Define quantifiable measures then um identify a dysfunction index and finally the recovery metrics then as to practical tools we need adaptive perturbations in order to kick um the system out of the valley and Recovery strategies and this on a more therapeutic kind of site and hopefully we'll be able to apply these cross across scales so now looking at the framework itself the first thing that we need to do is to um find that dysfunction index and what that is going to help us do is to measure and identify and delineate the dysfunction it will measure the information flowing through the wrong channels like a diagnostic kind of tool that um spots the system bypass and the higher the value the more dysfunction for example like a cell with a compromised membrane then uh we will need also a recovery metric and this will be a pro a progress kind of indication indicator for us so it shows a this function reduction and the thought here is that um we ask ourselves are we fixing the bypass problem so then uh we will need uh to uh in order to advance with that we need the landscape uh reshaping kind of like bringing out a new geography of points uh that are afforded to the system making sure that we don't get stuck again and hopefully we'll attain the systems adaptability where we find the question can it be explore can it explore and respond flexibly to uh the environment so why are these two measures relevant well they are they will U serve the role of being Universal tools across scales we will be able to apply them uh to uh all scales from to ecosystems and they will help us diagnose and track recovery and guide our next stage which is the intervention design okay so now moving to practical tools adaptive perturbations and Recovery strategies uh we have adaptive perturbations here playing a crucial role and um an Adaptive perturbation is going to be a controlled intervention that temporarily disrupts a system state to facilitate adaptive change and what we mean by these or our goal is uh not simply to push a system out of being stuck but it is to help it to become more resilient precisely to make sure that it doesn't come back to the stuck State as you can see an illustration here on the graph below so uh then we Define the Adaptive pertubation design as in the first term as traditional intervention approach where we would like to nudge the system towards uh these better states such that it adapts based on the system's response then with the middle term we can reshape the energy landscape have a different sort of more um flexible uh geography that makes it harder to get stuck again like smoothing out the deep valleys and then finally uh we can promote exploration and that means help the system uh discover new affordances new possibilities hence enhance adaptability um there are kind of like two examples of adaptive perturbations that um I just want to mention and uh one is by the parameter changes where for example you can think of that as a temperature modification in cellular environments and the other one is the external force or input an example of that is transcranial uh magnetic uh stimulation so why is this adaptive perturbation relevant well it combines three intervention strategies self adjusting all parameter parameters will adapt over time um and it uh includes a balance between immediate Improvement long-term pation sorry long-term prevention and future adaptability the Practical impact of um an Adaptive peration is that it will help us guide for Designing interventions and work across different scales and prevent the kind of uh so to speak Band-Aid Solutions right so then why do perations matter in the end well for the stuck States they're going to be very relevant to help us overcome energy barriers um break self-reinforcing cycles and enable the system to have more States and the capacity to navigate um and explore uh the the environment and select the states that are going to be much more preferable to the continuation and maintenance and self-maintenance and self organization of the system itself in the case of the blanket breakdown which is this is the case of for example the immune system that has lost uh the capacity to differentiate between what is self and what is no self and attacks um its own tissue then utilizing an Adaptive perturbation uh will be uh useful to restore boundary function to stabilize information flow and to reset System Dynamics and therefore uh repair uh the capacity of the system ultimately to recognize what's self and what is no self what is internal States what is external States okay so uh I will recap uh the journey now uh from the dysfunction to the recovery um so first we need to measure the dysfunction then we need to track the recovery then we will need to design interventions and do that through the Adaptive uh perturbation to implement the recovery finally we need to monitor that kind of recovery such that we now have um been able to uh get the system to be adapted to the environment uh rather than uh just um in a very short term so key relationships that come up within this journey include going from higher dysfunction to Stronger perturbation that is required going to better recovery to reduced intervention strength and going from more selectability to Greater adaptation capacity future directions uh where we need to go is to extend uh these mathematical formalism to non-erotic systems to multiple time scale Dynamics and coupled systems interactions in order to do that we also need to uh develop measur ment tools such as realtime dysfunction metrics early warning indicators uh that can predict a Tipping Point or a point of no return and then we can apply a preventive measure and cross scale validity tests some of the implemental implementation challenges that um we can see um include of course measurement intervention validation and scaling up and in order to address them we will need uh better tools more precise tools standard tests and infrastructure the potential impact of uh this framework um is like I said across scales uh in healthcare for example we can look into uh Therapeutics for uh any sort of disease kind of Cycles uh it includes uh mental health and aging patterns in climate action uh we can apply um this uh framework to tipping points to as a preventive measure and to Restoration in II we can apply to uh develop more robust AI we can also apply it to uh make more smoother ethical and governance uh measures for safety in AI as well as um can help us design bio inspired AI such that it's a system that is uh more adapted uh and hence more effective finally in Social systems we can also apply this framework for uh policy design for economical stability and social cohesion and um coming to the end uh call for Action um so uh research priorities include developing standard standard design standardized metrics and build shared data sets create open-source tools and establish cross disciplinary teams so please join us in uh theory development tool creation application testing and knowledge sharing conclusion and takeaway so we went in this journey from uh this functional systems that we observe that are stucked in a local minimum and we want to develop a perturbation in order to help the system become more resilient and adapt to the environment out of the stuck State and hence a full recovery in the long term so then key takeaways uh are that we hope that we um have developed the foundations for a formal theory of system dysfunction quantifiable measures for the stuckness phen on and a practical recovery strategy that can be applied that can be further in the future applied to different scales thank you so much for your time please do not hesitate to contact myself or any of the authors uh if you'd like to uh engage or learn more about uh these work all right keeping with the co-authorship theme next talk is a 25 minute recorded talk by Alex kefir resilience and active inference 2024 this is Alex kefir this is Alex kefir I'm here to present some ideas on resilience and active inference um so this is largely just a unpacking in a careful way of some Concepts from this paper from 2022 that I was involved in um but also will explore some some novel directions um that this uh this thinking has taken since then so the concept of resilience really if you look at the emology it's just it means like the capacity to recoil essentially or to or to to bounce back or just to survive through hardship and change um so obviously this concept is sort of a fundamental one if you're looking at uh life life forms um dynamical systems generally um in the context of like um ecosystems and sustainability it's an important concept um so that's why we're interested in this um it's actually also sort of a fundamental Concept in Material Science so so since this paper has come out I've been approached a couple of times by by journals of Material Science for um for follow-ups and such um so as I said the the fundamental concept has to do with recoiling or bouncing back um but there's sort of there are some peripheral sort of different facets of this of this meaning and we explored all of these in this paper um so roughly speaking you can think of resilience in sort of three ways um so first there's inertia um so inertia I mean I guess literally just means really uh sort of not changing um so persistence uh but you can also think of it as resistance in the sense that right unless you're unless you're a exists in a vacuum uh then you're a system that's embedded in some broader system and inertia is basically continuing on your way despite changes uh that might that might occur or interactions with that broader system um so if you think of the red arrow as representing the path of a of a subsystem or an agent through its uh State space then and The Black Arrow is some kind of potential disturbance um then the top diagram here illustrates inertia um the central meaning again of resilience arguably is elasticity or the ability to bounce back so in that case you undergo some perturbation or change um and you end up recovering your initial doesn't have to be that you recover your exact initial position but the point is you make your way back to a reasonable state in terms of the kind of system that you are um then there's also this sort of third thing called we call plasticity or adaptation um so in this case you begin in one configuration uh the red arrow at the beginning here and after the perturb uh the perturbation or disturbance you end up in a slightly different configuration so that should be read not as just that you change your state like you update your um your beliefs in the time scale of infers or something but that you actually you actually wind up somewhat adjusting your model in response to this change um so this is more like something like Al estasis um whereas elasticity is returning to the same state um so I'd say there are some close relationships just to to get into some philosophical conceptual analysis for a second here um the first thing uh inertia probably doesn't really exist in its pure form right in nature in a way or it it if it does it it reduces to sort of elasticity so you know I mean presumably if you uh if you perturb a system um then you have to cause some change in the system system however slight um otherwise there hasn't been a physical interaction right uh and inertia is going to look in practice just like the capacity to sort of instantly recover um from that Tiny from that tiny change um there there is in the limit case there is sort of a different meaning here because again there might just not be a change but I think um I think in the context of realistic systems there will always be some some interaction um but arguably these two these two concepts are very close um plus this it seems like um on the face of it like a different thing because um oh first of all uh the ability to be to be plastic in this sense sort of explains how you could come to have this property of elasticity um so if you look at the example of neuroplasticity for example um it's the ability to adapt to change and to basically update your your synaptic weights to arrive at a new model that allows you to be uh in part allows you to be resilient in these first two senses um but there's another sense in which if you just assume that there's some property of the system that doesn't change through this um through this process which I think is a reasonable assumption in the case of uh you know biological systems um then there's going to be some kernel of the system that survives through change and then you're back to your sort of original definition um okay so if we look at this concept through the lens of the free energy principle um I mean first of all it turns out to be a very fundamental at you know as is often the case when you try to unpack um sort of uh fundamental properties of of systems in nature in terms of the FP and active inference uh you find that there's a fairly direct mapping from the properties that the FP attributes to systems or um the types of systems that are well described by the FP and this uh and this concept of resilience um so I mean Carl fron has said in discussions of this that essentially uh systems well described by the fdp are resilient um uh full stop so uh but we can look at at the three uh facets of the meaning of resilience here so um certainly systems described by the FP are they have this property of inertia in the sense that um by definition right they're things that have managed to persist so they limit their exposure to uh to unlikely States unlikely observations um but they're also sort of by definition elastic systems um because because they have this attracting set so um having an attracting set of States means that um you know of course there might be changes or perturbations that are sort of catastrophic that destroy the system but as long as the system is not destroyed then by definition it's going to make it make its way back to this attracting set of States um so resilience in the core sense is really a um almost a defining feature of systems that are described by the FP uh and then finally plasticity you can think of as something like Basi and model selection um on some time scale and so that that might seem like something extra but it's only extra if the multiscale aspect of the FP is extra and I would argue that it really isn't because at its heart the FP is a is a scale free description of things and this multiscale and scale free are sort of uh sort of um connected Notions um so that's plasticity um um okay but so most of the interest I'd say of this lies in the the mapping of resilience onto this this formalism um of active inference so a key concept uh in this analysis is the notion of degeneracy and this is a uh somewhat technical term um here we're follow we mainly follow on the definition of and and in fact the analysis of degeneracy in terms of active inference in uh SJ all 2020 this is a great paper on this top topic um so here we have just the expression for variational free energy um in the top row um is a expression in terms of basically a Helm Holtz free energy so it's an energy term minus an entropy term uh uh so if you just look at that for a second um the we've identified uh again following this paper from 2020 we've identified degeneracy with the entropy term um so we'll get into why that is in a second um but basically uh if the concept of degeneracy is useful redundancy um so this you can think of this as like complexity that that earns its keep or that's worth having um so complexity in this this bottom breakdown of variational free energy uh you uh you analyze it in terms of complexity which is a the the difference the K Divergence between the prior belief uh overstates the the generative density Over States and the approximate posterior so this is kind of like how how difficult is it or how much change has to uh does the system have to undergo in order to represent the states that are um inferred from some observations um what's what's what's the departure between that posterior distribution and the prior distribution Over States uh that's complexity um minus accuracy um the the the expected uh log probability of observations given States um so what's the connection between complexity and this idea of useful redundancy uh sorry of just redundancy um there's there's a assumption here that complexity and redundancy are closely related um I think the basic idea is that so redundancy uh in this context is quite literally just duplication of States um so the I'd say the classic example is like an error correcting code so if you're trying to transmit some uh a bit of information over a channel uh you might you might send 10 copies of a bit in case in case some of those bits get flipped or corrupted uh and you want to make sure that the that the message gets transmitted um so that's redundancy it increases the complexity because um uh well one way of looking at it is that it uh it it increases the description length uh right this this kale Divergence is like how how how much how much more do you have to say in order to communicate that an event drawn from this Q distribution has occurred curred um G given that you assume this P distribution um so uh so you can think of it as well what these these extra bits are are the Redundant bits um so there there's a sort of conceptual connection between complexity and redundancy um and then degeneracy on this first definition is just the complexity that serves some purpose and we'll get into that in a moment um it's just also important to note that there's this definition of um degeneracy as partial redundancy in the in the literature uh and this is partial overlap and function so um the idea is that um redundancy exists in a system uh sorry degeneracy resists uh exists to the extent that multiple states in the system serve some function in common but then those States also have um otherwise uh have distinct or disjoint sets of of functions um and in practice I think these two almost always line up so there might be sort of uh limit edge cases in which um in which they're different but in but in practice um right if you have multiple States they'll always have some different properties and therefore um perform some different functions okay so so this is one of the core things that we pointed out in this paper um of ours so um this this actually sorry this breakdown is from the sjad all 2020 paper uh but we sort of have a different gloss on the Central Center term here um so this is the complexity term in the variational free energy you can break this down in terms of um further break this down in terms of this uh expectation under Q of the negative log probability of States under the generative model um so this is called a cost term in the 2020 paper um we describ this in epistemic terms as as as an uncertainty term so this is uh this is basically uh you can you can read this as the Shannon uh minimum description length of the of the states given the generative density um but it's sort of the expected surprise um how surprising are are these states that I that I infer uh using the variational posterior under my generative model um so that's an uncertainty term uh and then we subtract this degeneracy or the entropy over um over the variation of posterior Over States um so I mean first of all sorry I realized that I didn't quite make the the link here between um degeneracy and entropy so why is this the case uh why do we identify these uh it's it's because um this means if if the if if this Q distribution Over States is higher in entropy it means that there's a um degenerate mapping in the likelihood from states to observations right so meaning for multiple States uh you can get the same observation um and so then when you invert that likelihood uh mapping in order to recover this posterior um this means that for one observation you might have several um several different states and the the spread of the of the distribution Over States of course is just the entropy so that's that's the link to to genery there uh so so this complexity term is uncertainty minus degeneracy um one reason that's interesting is that when this complexity is zero um it's not as if the system is completely certain of what's going on um the uncertainty term actually remains it's just balanced by this degeneracy in other words all of the uncertainty in this ideal case um is is useful um and so I think this really this really captures what what degeneracy in this context amounts to um it's uncertainty that's useful and is it's useful precisely because there's some amount of uncertainty that's inherent in the source of the information you're trying to model right so um here p is the the in this context is the generative model that the system has of um it's the sort of internal generative model of of the external States um and so there will be some right there'll be some entropy some Source entropy associated with that system that that you're trying to model and um so long as you match your variational posterior matches that degree of uncertainty um then you're in sort of an optimal um an optimal model so in other words for maximum model goodness try to ensure that um P equals Q um so uh this ties nicely into the idea of plasticity resilience is plasticity so your goal here is to adjust the posterior to match the generative model essentially um so you can do that um you know in in like perceptual inference uh you can try to infer a state uh a q distribution this is again an approximation to the posterior the true posterior over over those States under the generative model um but equally you can adjust the generative model in in a in a multiscale setting you can learn learn or adjust update the parameters of the generative density in order to match what's in the external World um so right this this complexity term um is affected by changes to t as well as changes in Q and so I think the fundamental idea that I had here was that um in an ideal uh sort of optimal system under the free energy principle you should expect um first of all P will be adjusted so as to model the environment for sort of um basic you know um free energy principle SLG good regulator theorem types of reasons uh and then Q will also be adjusted to match to match P um so this involves um this cybernetic principle called The Law of requisite variety which is sort of a um um it's closely related to the good regul good regulator theorem um but basically it just says that you know if you're trying to regulate or control some states then you need uh at least as much uh variety or uh in probabilistic terms that that translates to variance um or or entropy um you need as much Variety in the controller as you have in the in the environment and so it's a fairly intuitive thing because you basically need sufficiently many degrees of freedom in the controller uh to be able to M to to model what's in the environment um so you can see this as this degeneracy um property of resilient systems or uh any system under the FP as as an instance of this law of requisite variety it's also clearly by the way a an obvious case of constraint maximum entropy so there's work um showing that many many lines of thought really showing that the active inference uh sorry the free energy principle is um is equivalent to a principle of constrained maximum entropy uh so here the constraint is this uh you can you can think of this as constrained by accuracy um but you you basically maximize this entropy term subject to that constraint uh subject to the generative model okay so um looking just a bit more closely at this notion of degeneracy um so I've said that it's Central um one pointed question you might ask here is well are all do all resilient systems uh have this degeneracy property uh and so I want to say the answer is yes um so the type of degeneracy that we've already talked about is this degeneracy in the likelihood mapping uh so this is multiple States mapping to the same observation um and again that's uh that can be cast as you'd expect to see that given um that you're performing maximum entropy inference um so that that is um crucial but equally important I'd say is degeneracy in the Dynamics uh so this characterizes just systems that again that have an attracting set of States so if these STS are states at a given time and then st+ one is the state or set of States at the next time step um so this in order for for a system to be resilient um it really has to be the case that this transition um uh that these transition probabilities or the Dynamics of the system are such that um that that mapping itself is degenerate um so you know this means that basically you can perturb the system in several different ways and you'll still arrive uh you'll still end up at the same at the same uh state or or set of of attracting States um obviously if the if the perturbation is too large then that might not be the case the system will diverge and you'll destroy you'll destroy it but by definition the systems that survive through change are the ones that have this property um so there is a very deep link