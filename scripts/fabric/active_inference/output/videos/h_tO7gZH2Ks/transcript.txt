all right welcome to active infrance stream number 8.1 it's December 15 2024 I'm going to start with a GitHub push to the GitHub repo active inference Institute slstart while we have meanwhile cursor current version 0.43 translating curricula in the background visualizing learning paths having our agenda up so thanks for joining if you're watching live definitely write some interesting things in the chat and I'll look to reans it into the stream all right here we are symbols greetings good evening Jeff again this is in GitHub repository active inference Institute start the idea here was to build on some prior perplexity and llm based methods that we've worked on in different projects earlier in the year and kind of continue with this all by all theme across domains and backgrounds intersecting with onboarding and curricula for active inference this is more than just a translation tool it's a framework for understanding and optimizing how knowledge flows through different perspectives modular treatment of topics and perspectives enables flexible application okay so first where are we headed to in terms of the output then let's look at the scripts that are using perplexity even now to go through it so the written curricula are in English and then they're being translated in chunks to other language so here's a synthetic bi ologist background introduction curriculum to active inference translated into Japanese and this is more than 1,400 lines long that's what it looks like in cursor let's reload the repo it's still writing this very large initial push we'll look at what the markdowns look like in a few minutes that's the translated curriculum um the written curricula there's two sets of audiences that we'll Trace back to that it's drawing upon the First with just their names are individual specific people or different audience profile we'll get to how that's specified like a high school student and a middle school student so these are different audiences or different synthetic knowledge domains that are constructed we'll again look into them like cooking so this is a complete curriculum implicitly in English for somebody in the cooking industry conduct workshops where participants develop new recipes using active inference algorithms predicting food safety risks those are the different domains so this uh pipeline that we we're about to go through uses perplexity doai large language model llm interface API to do research on entities specific or groups of entities and different synthetic domains we'll look at how they're defined those researches those are done in step one step two write introductions again going to call perplexity to write out those curricula in chunks in English three does visualization of a few different kinds on those generated curricula four translates into languages right now these languages are uncommented out there's more languages you can just comment uh or uncomment I will show my perplexity balance at the beginning of the stream and then we'll see how much it changes on during the stream okay $67 68 in the perplexity Bank using the small here on their suggest uh model card page I'm using the Llama 3.1 sonar small 128 online this is the same context length as these more larger parameter models just from short testing though it's faster and it's cheaper so 67 68 okay all right so where we're going is this V now let's look at the visualizations on those curricula from step three then look at the script okay first metric so this is four each of those it's just translating chunks in the background that could be multi-threaded but it can also cause more API trouble than it's worth okay so this is for each is a comma separated file open it in Libre office this is some word count information on in the sections and the paragraphs descriptive statistics on curriculum metric this file is some summary statistics on those 37 written curricula in terms of the mean and the average and all of that of those descriptive statistics metrics distribution.png has those descriptive document statistics for those 37 curricula okay content analysis this is principal component analysis PCA analysis on 13 cumulative percent of the variance in the term frequency so it's interesting that the entities are all clustered more closely clustered over here or at least some of them key Concepts top 20 Concepts word cloud similarity matrices with hierarchical clustering so here that this um dendrogram does sort of visually and at the hierarchy monopile uh topological clustering method confirming this cluster for example some or there's this relationship Logistics with whatever it is spatial we topic distribution this looks like the uh rows are the loading in the different 37 curricula the topics are top top seven topics and their presentation across interesting that Shaka Works in Fashion and so there's a connection with the fabric we can see if that pops up again different topic okay and topic summaries topic one so this is the one that has this loading across these different fields okay then there's concept maps and learning paths here's the concept Maps these are just starting points all of these are just starting point draft outputs but they can be updated from the underlying information so here's for Chris fields for the curricula generated for Chris Fields this is a igen vector centrality Network topic graph in different communities high school student concept map let's see this one free energy principle looks like it's in this one Orange Community variational minimize inference Active Energy there's the blue neural biology decision organismal tool making may maybe these have used maybe not Middle School student but cool nonetheless that's concept map and here are learning paths these are just okay we'll see how this one's looks like a very example applications are Central connected this Hubb and spoke M way to the these high school student path H somebody with a background in ATM transactions AI powered ATMs case study 2 personalized user interface biology graph Bucky Fuller ephemeralization bi mimicry cooking data on flavor profiles and ingredient interaction actions food safety training predict the flavor profile for a given combination of ingredients entomology insect behavior and biology so at least in the ballpark those are the metaanalysis visualizations let's just look at one of those written curricula files then into the scripts so let's look at video games let's see if the GitHub push is done okay yeah write any questions or write a topic in the chat I'll try to reenter it let's look at high school students and video game high school students in 2025 Source high school student worldview H key knowledge gaps cultural linguistic considerations conceptual Bridges so this top part okay now getting into some topics predictive coding further reading learning pathway learning environment okay and this is for now let's see the video game game design game mechanics narrative design user experience agile [Music] programming game integrating active inference into game development Dynamic difficulty adjustment mental health same example used in the high school case here's the curriculum content so it's about it's two and it's it's it's separate in two there's the domain analysis and then there's the curriculum content and then they're just concatenated to get that marked down so here's the curriculum itself so let's see that has welcome message relevance value proposition connection to existing domain knowledge overview of Learning Journey success stories and ex examples conceptual foundations core active inference Concepts using domain analogies mathematical principles with domain relevant examples practical applications and domain context integration with existing domain Frameworks case studies in the domain Dynamic difficulty adjustment narrative generation for gaming interactive examples and exercises some of these interactive ones are TBD but at least they're scoped there technical framework mathematical formalization using domain notation that's kind of interesting maybe you could use the letter set or domain of some given uh config computational aspects with domain tools utilize game engines such as Unreal Engine or Unity for integrating AI driven mechanics implementation considerations practical applications domain specific use case implementation Advanced topics br computer interface virtual reality augmented reality BCI VR AR research Direction collaboration possibility further readings books practical implementation steps addressing potential questions conclusion references okay let's just briefly look at that for Blake and then look at the scripts all right districts let's look at how some of the translations look translated curriculum Russian synthetic biology okay these sections are empty repeating repeating English back to Russian English code Russian back to English okay Hindi language somebody with the RSA cryptographic background okay repeating not filling out sections this could just be a prompt to error doing the Json or um nuanced approaches to documents blocks of Dos these kinds of approaches that'll add a lot of value this is the Big Blob method the all byall folder script method okay here are the scripts okay there's two scripts with a beginning prefix onecore research domain and research entity so there's domain and entity folders each of these script script Ones Will iterate over the markdown files in the synthetic domains and within each entity subfolder just the py so the idea here is whether it's a purely synthetic python I just say generate statements looking like a high school student from chrisf fields. py or just fields. py format and then those B version so that's the pure synthetic uh llm based approach there's also generating transcripts from videos so these were like transcripts from Chris Fields giving different lectures different papers different podcast appearances could have one to many could be all the literature of a researcher it could be their they could bring in their private nodes what could be using a local llm or none generate these py that are used so the py are the pseudo code uh quote this is not a py file representations of key attributes quotes perspectives it can be written in structured list so basically just a bullet point list all the way on through having here for Carl first I think there's some numerical estimates or some pan psychism 7 so then it can have also these other expressivity with with the pseudo code at the semantic layer or these could interface with they could have some sort of schema to be used in a more structured way as well and and these are just transcripts and other things that are dropped into a folder markdowns and then just within here given the markdowns in this folder add comprehensive accurate statements to this py that's that's what is um like the entity specific or the audience specific payload or context is the single py in each of these subfolders whereas for the domains these domains um there's for for the context here there's basically two kinds there's free energy principle and active inference MD which is just examples facts explanations definitions facts questions about active inference and about other domains so here's the synthetic hopter so a b is wasps and softes and then to to add on to this just I'll use control I to use the agent mode in the current cursor so this is only available to at this time I believe the anthropic models but it it uses specific specific code functions and it can do a few different interesting things so early days for this but pretty cool so here add questions accurate facts examples information on the I'm off here so I can be done from a blank slide rank blank text file uh or it can be versioned like more and this could also be in adjacent format or it could be the export from a database but just having these semantic little nuggets and examples so let's see how it edits synthetic hop okay so now it's adding adding sections keep questions about hop scientific facts Advanced behaviors in hop evolutionary adaptations in hopa conservation environmental impact so human interactions with hopa so that would be included the next time that um script one is run and when script one is run it outputs into these domain and audience research files the output is as both a Jason and a markdown this request let's look at the prompt in one says generate gen Analyze This domain content and identify aspects of Their audience characteristics Analyze This domain content to understand the typical industry professionals background knowledge and perspective according to these structured responses for that domain so this is the research domain the research entity question says generate a comprehensive analysis for this audience and also it can inject in the FP factual information itself to or you know I'm saying factual just in the sense of empirical from what can can be version right here so both of those um output these research reports so here's the high school student research report this is the background analysis so it's just researching the the different characteristics and situation for communicating with the average professional from that domain or that audience about active inference okay that's one two so here also go to this code viz cool tool so this is the code viz uh code analysis let's do it on two um I hope it's let's get redo it for here yeah help us fully analyze code visualization tool meanwhile okay it loads in the API key um okay so let's look at the graphical flow API key is is loaded with this function file is loaded with this this function load file extract sections extracts subsections from these different splitting signals generate section prompt takes that chunk its name and its content and the Fe information and responds to that section's need given the FP content with these constraints create a thorough version so this concatenates this is the Mad Lib The prepr Prompt that concatenates the information and gives a format again there's other ways to do this get get perplexity response calls the perplexity API again using the Llama 3.1 small sonar small save section saves one section concatenate section puts them back together save complete curriculum saves the concatenated output process research file um iterates through the sections here's the main so this is what is going to be now visualized in the graph logging setup occurs script directory and the llm keys are pointed to base directory for future file references is named audience in the domain those two different research directories the location of the FP o in information and the output Direction the directories are made that need to be made so set up logging Main initializes set up loging let's see if these other methods so these are just definitions complexity API is called perplexity API is loaded that's the next function that's called process the research file with these methods extract the section parse markdown content into discrete curriculum section for processing for each section extracted generate the section prompt make the concatenated uh query to be sent to perplexity that is the um Mad Lib component that's being iterated over and the ACM FP factual part empirical part and the format output component and the system prompt but that's the same for all here that prepr prompt or whatever we call it concatenated final payload assembled is sent through the get perplexity response that passes then to the save file operations and here are the outputs so kind of cool I'm sure there's other ways to use this really interestingly we can cop it in copy it in different formats there mermaid format there okay so that's two it it goes through the sections of the personalized curriculum overview and then seeks to generate that material let's analyze the next number three uh well four is another perplexity script this one as alluded to earlier there's a Target languages at the Top If it's uncommented it'll be iterated over so that's what happening right now is synthetic astronomy German language here on the synthetic astronomy run then it'll do Russian Portuguese sili toolik and then uncommon or comment the ones that you want iterated over this is some font mapping this has analogous mad living curriculum content in English maybe for some languages it would be better to just go direct to the language two but here there there can be a repeat it could be a curated of one and shifted to another um format content and language and the chunking into 4,000 characters okay let's look at the code viz on three okay collects the curricula pre-processes them analyzes their content generates the descriptive statistic metrics on curricula over curriculum here's a dependency check for the visualization components here's a decorator for plotting safely plot safely decorator at plot safely this is a decorator being applied to plot content analysis plot safely applied to plot learning paths plot concept Maps those are the three folders that are shown in the beginning curriculum visualizations there's the metrics initial output then there's these three plot safely sections concept ma content analysis learning paths okay looks like it's still processing the three let's do another GitHub push add a few more languages push to GitHub 6768 reloading perplexity seeing how much just this one thread of continuous chugging with the so that was a 65 cents USD from having one thread continually run for however long that was so couple dollars an hour for one thread and each let's just do it the llm way copy in some logs chat how long does each of these Claude 3.5 Sonic 2024 1022 coming through so between 9 and 23 seconds and there are probably better translation models okay this one's still processing maybe it'll get there if you're watching live though ask a question otherwise I'll just sort of review everything okay so to review where this is at what can it do well we can learn a lot by putting in specific or general entities or domains it could be the people at this conference at this exact time MD it could be entity in organization it could be a specific person at an organization or a group at an organization you you you can add your own using whatever background supporting files projected onto a Pui synthetic domains entities research them the research prompt could be extended or or the research output could be used even by itself it's in the domain research in the Json and the markdown two right introduction splits the personalized domain or audience research and writes that curriculum for them resulting in the written curricula so again here's let's go to the high school student written curricula high school student versionable curricula and research on bridging to their background or situation people can continue to to update this and add you know sub high schools that's analyzed in three visually and translated in four so hopefully these are useful starting points by themselves they're just files you can download uh and send to somebody you can use the GitHub repo slstart to request collaboration access make a merge and PLL all that fun stuff become a collaborator um this was kind of it's all in the languages folder so we could have some other top level folders of course this is just a new repo so lots of ways we can go with it um I'll look now to the to-do but if anyone's watching live wants to make a comment or a request otherwise I'll go to the to-do organize a few of those add a few more things that have come up see what anyone writes in the chat um okay okay researcher integration Implement orci ID API integration or CID and other identity API Integrations get research profile information on researchers in open- Source databases have data interfaces visual interfaces for people adding information publication scraping of course we're legal research domain analysis this could be using perplexity with internet to to look for other context on the fields of somebody's research as as scraped and submitted could could be some other research design researcher profile structure so this is maybe where it could connect with something like a within or across organizational platform different user profiles validations okay so this is developing researcher entity domains researchers so that's the first improve are handling and structuring and Ingress and accounting for of entity audience information and synthetic domain information database implementation got clarify design and clarify SL documents database schema entity relationships works at of co-authored with metadata structure version control system so versioning information uh how knowing which person's information was used for which input to which model what parameterization at what time Etc Implement database connectors uh not sure what scale some of this query optimization and caching becomes relevant and economical probably eventually d validation definitely important okay so this has to do with having a better structured instead of the the all by all blob by blob folder by folder inch by inch approach here having some better structured database and and I guess it goes into it here in phase three which let's just look at that one use Fabric or maybe another maybe another agent orchestration or kind of computing or task framework but one of the previous streams used Fabric and and there there there may be others too research here but again use that database with more of an expressive pattern language templated procedural operational nuanced approach like if we only want to do one language and one background right now the fastest way to do it is to comment out all the languages except for that one in the file and then remove temporarily uh temporarily everything from the domains and the entities except for the one that you wanted but but that might work to get one done but if we wanted to have each of them done in a custom way then having some more expressive reproducible composable uh pre-rendering layer for these operational traces that would be important back to phce two here as it laid out application extensions okay organizational support organizational configuration system so different templates maybe having a single config folder with its own versioning where the The Branding or the logo or the Styles and the document formats for a given organization could all be handled there so it sort of could be plug-and playay and separate out the entity background for example from the uh organization config and this is more like an interactive interface Direction but to build out better some of these analytics and visualizations within an across curricula and then have a static output dashboards or interactive dashboards using some kind of business intelligence tool use catechism framework templated structure question answer answer format grant writing system so this gets more into the um document genre modular writing composition writing with human and llm and all that uh happening okay phase three that was the uh pattern languages for more expressive task execution model Improvement more expressive model so that could be a better um it could be using the perplexity API with a different call it could be replacing that with some other analogous um llm uh a colleague has provided me some information on uh using it but if somebody wants to help update these methods so that they call some other longer context window uh llm or something then just make make the change multimodal support that could be really interesting making and receiving images yeah parallelization memory management wasn't an issue on this old laptop with cursor because the llms were via perplexity but maybe at some scale uh interactive live demo interactive tutorials Community features monitoring and maintenance resource usage reporting dashboard documentation okay prioritization and some notes yeah pretty interesting what it comes up with let's improve the documentation a little bit and then see if anyone writes something interesting in the chat and then maybe do some one other random feature and then call it for this one develop and Improvement and add concise new accurate information directly in best sections to this R Me Oh I thought we were talking about synthetic common opter let's see where goes with it though and then we'll return to the reading that was with control I composer while that one's going chat improve at read me adding new accurate sections so even though it's like in the same window they're different composer is a little better okay accept it now make thoughtful contributive accurate additions to read me for how far we've gotten and only that far the chat little bit of back and forth in and out of the code sections composer I found just doesn't get into those sorts of issues bug finder this looks funny I think it's it uses a custom cost so it one time it estimated it at like $30 to do this it's like there's bugs for free outside and the code okay now updating the read me okay focusing a little bit on the hopter but okay current domain so this is like very hopter related not very helpful for the read me reject it start a new composer Trace reindex cursor sometimes it it it does it on its own other times just to double check okay and then let's also do read me finish hello Aubrey Walters okay this python virtualization is probably not needed so that's a little bit invented m not useful information I like the built with heart really just always shows it's just such a fractal ride sometimes it's so good sometimes it it really can be Insidious over different time scales these parts were accurate yeah I hope that people find the primary artifacts to be useful like if you have a friend uh who is of a certain expertise and a certain language then it's not if it's not already here uh add it to one of the scripts or make the request so that you can send uh translated curriculum just sends okay here's a bunch of different backgrounds in Portuguese for somebody who wants to use that so I'll continue to let this one run for now um probably doing a multi-threaded or some other way would be better or for some of the languages it seems to be leaving a significant uh number of blanks so maybe I won't um do all of it let's just do one sort of semi manual llm so here translate read me into Spanish output in me Spanish okay the documents folder move in the Stream agenda and do one baseball Yep looks pretty cool o okay so I was just translating it into one language manually push GitHub all right now in baseball for edification and more please help write intricate imbricated deep lore for Baseball American 1920s memorable moments describing the at read me and all it entails for this given where it is all comma all at let's see what format it does and then we'll say um continue continue to make more crystallin and preent and accurate deep lore specifics specifications Min particulars for more baseball deep lore and specific reference with the read me and FP synthetic quote all by all folder script kitty style terminal interface synthetic intelligence in the format of long quote who's on first F okay let's see but it has some knowledge graph analysis of the 1920s in baseball okay that will be useful metric analysis baseball's Quantum Leap batting average contextualization home run frequency analysis pitching Effectiveness metrics Fielding position Evolution hm okay getting there distill the entire dialogue retaining all the Galt and specifics only entirely being in the quote who's on first style of dialogue all details must be retained and with more linguistic cultural baseball esoterica and actuality this dialogue serves as a meta commentary on how baseball's complexity mirrors the fun FAL principles of both okay remove anything that is not dialogue incorporate everything into The Who on first dialectical style retaining alls and reference all right once we get to this all or I'll just trim this first part we'll push it on GitHub read it see if anyone has a comment and it all distill everything into pure avit and Costello style dialogue incorporating all historical Technical and theoretical content through their exchange thank you Claud okay that looks good documents baseball all right who's minimizing free energy on first a complete baseball metaphysics let me tell you about the 1920s Yankees Castello most complex predictive processing system in baseball history fine but first tell me who's on first base now you're talking who's been minimizing free energy on first since the dead ball era the dead what era before they put cork in the balls when who was still optimizing his predictive model what are you talking about who's on first exactly been there since 1920 right after Ray Chapman's tragic prediction error with Carl Ma I don't know anything about Ray Chapman no no I don't know I don't knows our shortstop working on coupled predictive models with what on second What's On Second naturally part of Murderers Row hierarchical basian Network beautiful statistical anomaly that 1927 infield look all I want to know is the F's name on first base who been there since Miller Huggins optimized the team's variational free energy State Miller who no Huggins man who changed the whole generative model of the game revolutionized how tomorrow predicts today's pitching patterns what about today no what's on second today's AR basian belief coordinator in the bullpen with tomorrow tomorrow best predictive pitcher in the league learned everything from Walter Johnson's Precision weighted curveball processing Johnson he's pitching no he's retired but his generative model still influencing tomorrow's pitch selection Paradigm then who's pitching today no who's maintaining his marov blanket on first today is updating his beliefs in the bullpen and tomorrow's starting starting what the hierarchical processing of batter prediction errors of course just like how Ruth transformed from pitcher to Slugger through active inference Ruth now we're getting somewhere he's on first no who's On First Ruth's busy minimizing League wide surprise with his home run variational distributions home run what statistical Revolution my boy changed the whole predictive processing of the sport why before Ruth optimized his generative model never mind Ruth who's playing first base now you're getting it who's been optimizing first base defense since they standardized the cork centered baseball's predictive parameters I don't care about cork what's the guy's name on first now what's calculating joint probability distributions with second with I don't know you don't know no I don't know is integrating radio broadcast feedback with what to minimize spectator surprise radio sure whole new layer of cultural free energy minimization why when Miller Huggins first implemented his platooning strategy clutch his head just who's on first base exactly who's maintaining optimal spatial predictions while G's generative model emerges through pattern recognition garri he's on first no no he's still learning from whose free energy minimization techniques part of the farm system's hierarchical knowledge structure the farm beautiful example of organizational active inference whole network of coupled predictive models just like our start systems curriculum generation start what no what's busy with defensive shifts on second start our knowledge Network analyzer just like how tomorrow predicts today's relief appearances the dialogue continues through Extra Innings recursively embedding the entire history of 1920s baseball active inference Theory and knowledge network analysis into an Ever more complex web of who's on first styled misunderstandings okay great great dialogue I'll just look over the to-dos one more time see if anyone has any last comments end it for e okay right contributing code of conduct so contrl K inline the script contrl L chat contrl I composer but you can be using all three at once just deleting everything except for the structure here uh uh to-do section for completing the all by all for all audience the names and languages then if if somebody wants to plug in their own perplexity API and do it get it working in their own hands push the updates um we could also um add in to do for better methods for recognizing which combinations are already done or being able to override that all right fine all right hope this is uh useful I'll push the final get Hub push all right good luck symbols greetings have a good one bye