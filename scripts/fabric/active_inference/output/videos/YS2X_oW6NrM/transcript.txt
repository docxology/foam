hello and welcome to the active inference lab this is active inference guest stream number 3.1 and we're really excited to be here today with sarah haskis we are going to be talking about as the title slide suggests active inference vr and psychedelics so sarah i'll leave it to you to just introduce the topic yourself and the ideas however you want and it sounds like we're going to be having a first section of the presentation then a question and answer and then a second section of the presentation and then we're going to have a second overall discussion so with that kind of structure in mind just uh introduce yourself and please take it away hi everyone so yeah we're going to be talking first about my research in vr inactive in france and then my research in psychedelics and how it relates to a few issues about active inference and we'll have a short break in the middle of our questions so uh yeah my name is sarah hashquist i have a master's degree in cognitive neuroscience i opened the first vr lab in the motor control department at radboud um where i was researching vr's effects on our brain body connection within the predictive coding framework i am now ceo of radix motion we're a startup that combines neuroscience and immersive technology and uh we're focusing on empowering the psychedelic ecosystem with embodied tech so yeah let's get right to it a lot of this is uh basically all of this lectures based on my research from academia it was pretty nice to go back and and reread all the things uh from 2017 but uh before i actually get into it i'd like to activate our active and friends and a very short interactive experience so wherever you are especially if you've been sitting a lot i actually suggest you stand up um and hopefully you can see my hand on the screen and i'm gonna ask you to put your hand in front of my screen and just follow it for a few minutes moments how about do it with your mouse move your mouse yes okay we can do that too if you can see the oh you oh i see you're not seeing my face in this okay yeah so follow the mouse uh for a few seconds uh and see if you can track this so what just happened here so okay i gave you a auditory um instruction that hit your brain and created a goal a top-down prediction um that you were supposed to well you didn't see my hand using my mouse but uh basically that prediction was that the position of your hand to the mouth should be the same position the difference should be zero um now uh your eyes were then giving you bottom up sensory input um that were telling you oh there's this difference between the mouse and the hand and due to active inference wanting to minimize this prediction error your hand was moved towards the mouse so this is a very simple example of active inference and this follow me um instruction and activity is really quite basic for how we learn motor skills as children and you can see this in the way kids play games if it's simon says or you know patty cake and the way children's look to their adult figures to actually copy them and learn movement patterns and just follow me example was part of the research i was doing so we'll get into that so what exactly was i trying to do well i was really hoping that if i could receive visual cues from inside of a body of a professional mover i would be able to activate this active inference ability of my brain and actually improve my movements i have a background in martial arts and dance but i'm actually from childhood a very clumsy human that took me really really i guess a lot longer to learn motor skills and i was looking for a way to hack this and i kept dreaming oh if i could just be inside the body of my martial arts teacher would i be able to punch like him would my body learn to adapt and move faster and more accurately because i wouldn't be receiving this prediction that i am actually him so um this is sort of where i started uh um exploring how i could research this now why would i even think it's possible using vr so part of the things about vr is that it doesn't just activate your visual sensation uh and sensory input it's actually a multimodal integration between your visual perception and your proprioceptive auditory and even haptic if you're currently using uh like haptic controllers um in my experiment i this was like we were using the sdk one of oculus no haptic controllers i was connecting them to lab equipment to get positions of the hands um but we were still connecting visual and proprioceptive sensations meaning basically when you move your neck around the world is still moving the way you're expecting it to move so you're getting the same uh upper body and neck activations of muscles of where you are in space along with your visual perception and that activates these higher brain areas and really uh gives you this what we call the sense of immerse immersiveness right immersion you're in that place your brain believes it's real even though you know parts of you know that it's not now vr really gives us a very unique ability to influence the brain because we can start controlling the relationship between the different senses in ways that either fit these previous top-down biases or don't so we can uh this will really modulate the amount of prediction error that we're receiving and activating in a person's brain um and there's uh really a lot of experiments out there that's that have been done for instance with anorexia patients and people uh with pain issues that really prove that we can give a new sense of self um to a person so when i talk about a sense of self what am i talking about from the predictive coding perspective it's the really really uh um great uh paper by absentes curious um that really i go about explaining this concept of minimal self being the top-down prediction that just explains the sensory correlations between the different senses and a great example is the rubber hand experiment so um if you don't know this a hand is your hand is actually hidden from you like in the photo but it's receiving sensory input uh at the same frequency that you're seeing a rubber hand being simul stimulated now uh researchers are a bit cruel with this experiment they at some point take a hammer and smash the rubber hand and you can really see um and register and record the physiological measurements from the person and how that person gets stressed now neural research is actually showing that this isn't um the same for everybody there's some people that seem to be more susceptible to this versus others that don't there's some amount of suggestibility and i guess maybe strength of what is your own self versus how mutable this top-down prediction is um in society might be actually quite different and we'll talk about that later but generally this there is a very strong ability for us to change our sense of minimal self and we do this all the time when we were closing our predictive abilities uh become able to just predict away the sensation um unless you know you're somewhere on their autistic spectrum and then every little itch and and sock can drive you crazy um but also using tools uh these abilities really extend what we call our minimal self so i was hoping that uh with vr i'd be able to transfer myself from a clumsy person uh into somebody that can do movements that are much much more difficult and a much faster time um so um what did i actually do i needed to find a task that was known to be a hard task that i could measure right what is the difference here so i went with a path that's called bimanual interference so bimanual interference uh is basically our inability to create different movements with different hands um and get an actual interference pattern between them um a very simple game which is what i coded in my experiment is tray to make a circle in one hand and a square with the other right and just to try for a second and what basically happens uh is that you're you go between phases of making circular movement and phases of making square movement so uh i set out to look at this task and see can i improve this task in people with various vr interventions so um oops i don't know why this is finding here but okay so let's look at a little bit of the experimental design exactly of all the different things that i was doing and i'm not going to go into exactly all the analysis you can read the paper because i want to talk more about like um the actual discussions and you know what are the ideas that come from this that we can implement in our daily lives um but there were basically uh a few different conditions one condition was a follow me condition similar to what we did at the beginning of this lecture where you had an avatar that was able to do these movements and your instructor to follow the avatar the other instruction was complete the movements yourself so there's a self um experiment and there's a follow experiment now on the follow experiment we were actually uh discovered that we needed to hide the square and circle because the the minute people were uh already predicting that we're doing a square and circle they were not following as much so when you were following you are doing random follow movements uh along with periodic small parts of circles and squares which i then analyze specifically those parts um so that was one just like i'm going to say sort of base like baseline condition to see what is the difference between following someone versus doing uh these movements yourself and um the second line that you can see on this design experiment um was a bit of making things um stranger with what we can do as we are so the second uh condition when you moved your right hand you actually saw your left hand move now there's no way i can recreate this for you without putting you inside of vr it's a very trippy experience everybody's like wait what you're like moving your right hand but your left hand um is is moving uh and that really like messes up uh your predictions now the third experiment i put you inside of the avatar that was able to do these movements and then told you follow the movements or just become this avatar and follow the movements from within this avatar so we actually manipulated uh two things one is the high level task goal right follow versus move yourself now we assume that the self movement has a very strong hyper prior of allowing only one task this is something that um you know we're sort of um meant to be doing naturalistically one task at every given time we also have this basal ganglia that's really blocking more than one activation of um a physiological uh motor activity so that was one thing we manipulated follow versus um but there's do yourself and the second manipulation was of the visual feedback right do you see your own hands moving do you see your weird cross hands moving or do you see an avatar's hands moving from the perspective of your own hands um now um there's again this the difference here when i crossed your hands really giving you lots of prediction error this is a prediction error that doesn't fit the normal minimal self activation and predictions that you have versus when i'm putting you inside of an avatar that's uh moving in the same way that you are trying to move i'm actually not doing something that's against this minimal self um prediction and we'll see how these things um gave different results so let's go with the uh start with the first thing that i was you know most hoping to get oh i'll just plop myself into an avatar that can do these movements and i'll be able to do them well that didn't work in fact we got uh the exact opposite so when i say this increase the interference it means that the people were less able to perform this difficult task um they were having even more uh periods of both hands doing the same thing either circle or square and not able to differentiate uh each of them so um what actually likely happened here right and and this is sort of a theory because um i was not looking at neural correlates i was just looking at behavioral data but um there is a general consensus in literature that the visual modality at least for most of us is much stronger than proprioception data so it seems that we were getting this mismatch between the sensory input right of i'm seeing my hands doing the thing but i'm feeling my hand in a different place right now i was hoping that we that would cause active inference pulling your hand to where they're supposed to be but it seems that the actual actually the opposite happened and because when i i asked people they were all actually believing that they were doing the task better some of them actually thought they were the avatar they didn't even realize that this was a pre-programmed avatar they thought it was them um and all of them reported it to be easier um so but when i looked at the data all of them were actually doing uh much worse so it seemed that the visual modality caused the proprioception modality to actually uh lower and become you know less accurate and your brain decided when it has this this a higher prediction or there's a few tactics it can do one is active inference the other is modulate uh one of the sense modalities and it seems that that's what it did just lowered the proprioception stuck what stuck with the sensation from the eyes which it could predict and that's the way it sort of minimized the prediction error so this uh and we can talk a bit more in the discussion about the meaning of this and and how um what it means for training but this was again um you know it was uh um against what i was hoping and against you know my my initial prediction but thinking about this a bit deeper it does make sense because accurate feedback of what you're actually doing seems to be incredibly incredibly important for learning any type of skill so giving this false feedback um of somebody doing the movements seems to not be the way to go uh around actually teaching movements um an experiment i would love to conduct in the future is having both of these as an overlay like have um the avatar that's able to do the movements but they'll give you a physical um ability to actually know where your hands are and this is something that um if people have vr they can actually download um my my vr app that's called miu which lets you play with these things yourself it's um it basically records 3d movements and then you can move around them and also get haptic feedback and see where your hands are inside so i played around with that a bit with like professional jugglers um creating movements and then me stepping inside and needing to follow their hands but also receiving the feedback of where my hands are through haptic feedback and visual perception um but i did not do an academic research on that yet and that i think could be really interesting uh because i think there might be something there because obviously the prediction error was increased meaning your brain needs to deal with this prediction error in some ways so there is a potential of still creating uh better updates and maybe activating this active inference ability but we need to not only trust the visual perception uh visual versus proprioception the the visual just wins and you don't feel where you are anymore at that same level of accuracy so that was um interesting um now let's look at the other two conditions of what i found so um in the other two conditions i found that when i actually gave people the totally incorrect things that were against their minimal self i crossed their hands they were moving their right hand seeing the left hand some things that they've never ever experienced before they were able to follow the avatar better but the self-movement movements were worse right and that actually does uh make sense because again we're interfering with this self-level prediction and having to maintain this goal of doing a self-movement while your whole minimal self is just like totally wonky and doesn't know what's going on and has a much higher prediction error um is the causing interference and um making you not able to do this task as well but a follow task you don't actually need the goal is really different you don't actually uh need to have a goal high-level goal-oriented self um structure that's creating these predictions you just need to really minimize this location and space versus that the other person um they were actually doing better um and we'll talk a little bit about uh why why that might have happened um yeah um so just one more finding before we go into a bit more of the discussion is that there was actually um um two very different strategies might have been employed here as we see between the two different tasks right when you're in the self task um this higher prediction error that you need to deal with is you can't just make yourself disappear you need to keep maintaining this goal so maybe your brain created a weaker prediction right and then it would just like lower that prediction error by having a wider sort of um i guess and there's a lot of between like stronger weaker and wider um my specific professor was very um in the in the prediction predictive coding scheme looking at things from actually not a continuous gaussian but more of a discrete level and a lot of this is about like detail the level of detail of your prediction so if you have a less detailed prediction you're going to decrease your prediction error right if you say and we'll talk a little bit more about that when we talk about psychedelics because it's very relevant there but if you're just saying okay my hand needs to be around here you know versus your hand needs to be exactly at a specific point that's another way to decrease prediction error so on the self task that might be what was happening your brain was just decreasing the detail of the prediction in order to decrease a prediction error which would result in worse behavior versus the follow-up task your brain might have reduced the prediction uh error by actually reducing the visual perception and getting on the relative right between proprioception to visual actually getting more of this proprioception creating these closer feedback loops with where your body is in space and getting this more independence between your hands because you don't need to have this single task anymore you're just following um and uh an interesting finding i don't have the quote of the paper here but um there's a paper that did the exact same task but with the eyes closed and just touch so people had their eyes closed and were just touching uh something that was moving in a bi-manual independent manner and they were all able to do it and suddenly from becoming a very difficult task became a very easy task so um this really shows that sometimes and i think anybody that you know practices movement is it's sort of a common trope but there's a thing of like thinking too much so goal oriented movement can actually um if we are taking this goal oriented movement and putting it in a way that's too detailed we're telling our brains create these movement paths and create two separate movement paths that's a really not naturalistic state for our brain to be in um versus when we're just like okay we're following things that's a very naturalistic the low level feedback between our motor cortex and our proprioception and our sensation can just work um so yeah that that that's all sort of hopefully understood um now another finding that that happened was that um there was a much higher variance like i was expecting this to be a very hard task for everyone that's what the literature said um but i found that uh and again this is a small experiment we're talking about like 13 psych students right um uh all healthy subjects um young but uh two of them were very able to do the baseline task and that was a surprise some of them the task was so difficult that i could barely use the data because they were literally just moving one hand the other hand was just freezing in space and the other hand would move so they weren't even able to maintain these two goals of move two hands um in separate ways uh at all um so uh that really goes to show like just how different our brains are one of the interesting things to compare this experiment to is a binocular rivalry um which has been written a lot about in the predictive coding literature um so for whoever doesn't know this binocular rivalry is when we project two different images onto each eye so one eye is seeing one image you can see in the picture the right eye is getting a red image with one angle of direction and the left eye is getting a different one now what happens to your brain is it actually switches your perception switches between seeing one of the images and the other images with some short phases usually of like mixed perception but really just bouncing mostly between one to the next now here too there's a lot of variants in the population and one of the interesting things is that people on this uh autistic spectrum which you know i i have endless criticism about the dsm and how we're classifying uh autism and generally mental health and different types of um brain brain abnormalities um but some people definitely have much more um mixed dates than others and different drugs can also affect the ability to have mixed states and it's sort of interesting to compare these two tasks because both of them are sort of creating a bi-stable system coming from an unnaturalistic task or a naturalistic um um input that you're not usually supposed to be doing because the way our brain was really meant to be doing motor actions is around high level goals completing high level goals right give me the border um turn on the thing grab bang there's always a high level goal that's oriented towards um one of our high higher level needs or lower level needs but like a specific need that you're trying to complete um and with this task of telling people oh move your hand in this motor path's way and your other hand in this motor pathway it's very unnaturalistic for our brain very similar to giving someone uh these two different visuals which we never see in real life so um again this is really correlated to this having a very strong hyper prior in in a binocular rivalry situation your brain has a very strong hyper prior no i can just be seeing one thing this is this is how the world works there can only be one thing in one place and then your brain activates in order to minimize this prediction error it goes between okay i'm i guess i'm seeing this one now it keeps getting prediction error oh i guess i'm seeing the other one so it keeps jumping between these two states in order to minimize the prediction error now the same thing it quite likely is happening with this uh bimanual interference with having a very strong hype of prior of oh i'm only supposed to be doing one task at any given time you're just giving me two different tasks to complete at the same time i'm going to be switching between them now possibly when you have this different sense of minimal stealth this can actually create different activation patterns and let you be more in a mixed state more in an independent state have basically weaker hyper priors around either what you're seeing or what your task abilities are so this might be an interesting way of generally like understanding um a lot of what our hyper priors are doing to us and just the variance in the population might be around these hyper priors so let's let's now let's get into like more like what are these possible implications um and and how can we use this data to like you know do cool things with rehabilitation and even just training and getting me to be less clumsy so one of the things that comes up from this is really naturalistic design of movement goals instead of telling people you know move the head your hand at this angle reach over here um and we see this a lot actually in in traditional martial arts they give movements names of activities like in kung fu you have stroking the horse the the host of maine and karate we have a move that's called breaking breaking the wall or breaking the gate so having these um goals that are really oriented towards task completion versus goals that are too low level are much more likely to activate this active inference in ways that we want them to when we put the goals into two low-level um patterns um we're then creating this sort of unnaturalistic um activity and maybe even like slowing things down like the the fast parts of our brain that deal with bringing together these sense modalities of visual sensation and motor actions they need to stay down there they need to not be part of this high level self um you know network uh part of our hierarchy called network and i think that's really interesting because it really is like what i was saying every martial arts teacher my whole life has told me stop thinking and i like i don't know what to do about this you know how can you stop thinking um but part of it is uh letting these low level parts of your body do the movements while maintaining high level goals that are not oriented to exactly oh where's my elbow is it here or here um where am i holding my fist like all these little details uh work much better if you give them high level metaphors somehow that your brain that the lower levels can then take those predictions and minimize prediction error so um another interesting part of this is that it might actually explain uh some of why mirrorbox therapy works um and and why mirror rocket therapy is now something that's also being used in vr and so for whoever doesn't know mirrorbox therapy is used on amputees and people that have pain in one hand um like chronic pain or even a stroke people that have suffered through a stroke so what you do is you take the body part that is actually okay that doesn't have pain that is working and you use a mirror to then get your brain to believe that your other body part is also moving and is healthy or even there if it's an amputee and this was actually uh figured out by i think the first person was ramachandran uh working with amputees that were suffering from just phantom pain and how do you get rid of pain from a limb that doesn't exist anymore right this was like a top-down prediction from the injury that was just not getting updated um and you see this a lot in chronic pain really you see the neural circuitry even change as the predictions for pain just cement themselves and become biases in your brain so um by using this type of mirror box therapy um it helps relieve the pain so why or how is this connected to what we just saw because the visual modality is stronger than the proprioception modality it's not too far fetched to believe that the visual modality is also stronger than the gnostic scepters the pain receptors that we're getting so if we can actually cause your brain to believe and and just by seeing cause that visual perception of a healthy hand it might be enough to mute the same way your proprioception was muting to mute your nasty scepters um and then reduce the pain so that's that's really interesting and it's interesting to see they are also starting to use this with stroke victims to actually help them rehabilitate their hand um and that's a really interesting because right that's a little bit not in accordance with what i saw um from my experiment but it might be different because they might not be getting um current data about the hand i think it might be really dependent on you know what the stroke has actually damaged in your brain if the stroke part has damaged your ability to receive any type of proprioceptive data um then maybe getting that visual perception uh might help reprogram that there might be less of an interference going on i'm not sure it's sort of interesting to think about um so let's see what are we left with oh yeah so now there's basically that third uh condition why i gave this crossed hands condition and i think that's really an underutilized mechanism generally in any type of rehabilitation and training um messing your brain up even more in some ways and i think that that's a very strong intervention that we could play around with a lot more uh because it's you know 100 percent we're creating more prediction error that your brain needs to deal with now how your brain will deal with that prediction error will it actually uh remodel whether create active inference will it reduce some other sensation might really depend on the task on your brain um on how we do this but being able to increase prediction error in your brain is a very powerful ability and i think that's something that really is worth looking into more when we're talking about rehabilitation tools um and we see these types of things starting to emerge in here more with like total body replacements like i was talking about about like um anorexia patients uh putting them in a whole new avatar and creating a full body um rubber hand experiment so they're getting sensations on their body at the same time that they're seeing uh their avatar getting sensations and it's been proven to help them remodel this top level uh belief and accurate belief that they have about their bodies being overweight even though um they're not so that's very interesting but i think there's really a lot that we can do with um increase massively increasing prediction error in the brain and you see this in a lot of crazy experiments right like the upside down glasses experiment that people just within a short time were able to totally adapt and live in a world where everything is upside down um or with like training people on bikes that work in a totally different direction so um sometimes by making things harder we're making things easier um for our brain or we're forcing it out of this um minimal potential well that it might fall into it's like okay now there's too much prediction error it's time to actually change uh because you know as we've been talking about like um this minimal uh uh prediction error if if you're in a stable state where you've minimized prediction error that's not always a healthy thing for brains um and side note this is also a model of depression basically not having enough prediction error in your brain falling into a too stable state a low entropy state of your brain so increasing prediction error in many ways can be very important for our physical health our mental health and it's something that i personally try to hack on myself all the time i'll do like dark showers so i can't see anything uh i'll try to use my left hand to to to use toothbrushes just like very basic daily activities that you get very used to doing just put another constraint on yourself um and make it like a fun game it's also a great way to train in any type of physical task uh okay so uh yeah we talked a lot about uh the visual dominating their proprioceptive but why why is that so there's a paper from 2009 i'm not gonna i don't know how to pronounce that you um but uh their theory was that it's around using tools that in order for us to be able to use tools we need to be able to uh not trust the proprioceptive sensation as much as the visual perception because we don't have data from any proprioceptive data of any tool that we're using um so that's a very interesting theory um but for my paper i think there might be something actually deeper than that going on i also think and this might be interesting to figure out a way to test this on animals that don't really use tools um but i would expect it to uh possibly and this might might be very different because some animals you know actually smells their smell world is much more dominant their their hearing um but i think that for this goal-oriented movement where we need to coordinate many body parts you know you're driving a car uh you're just even getting out of bed and um getting dressed or walking all these things require um lots of body movements working together so in order for that to happen we really need to reduce the proprioceptive sensation so we can get this high level goal to dictate the lower level hierarchies of the brain to actually pull towards the active inference if we had very strong perceptions of where we are right now we wouldn't be able to move into the next state of where we want to be so we're taking our brain is taking this high level goal and breaking it in to movement predictions and for those movement predictions to be able to move your body into where we want it to be using active inference we need to weaken where you are currently um and have a a weaker now in order to get to the future um of this goal-oriented movement that's that's sort of how i see it at least um a sort of interesting thing that i'd like to talk about and we can also open this to discussions and questions at the end is how this uh connects to flow state and improv and dance and play and all these things that i'm you know totally infatuated with and want to spend most of my time doing um so flow states uh for if people haven't heard this term before it's um a term that's been coined when uh athletes or really anybody that's good at a specific task is at their peak uh it's been sort of defined as being freed from self-consciousness now a way to look at it through this lens of the these experiments and these hierarchical levels and predictive coding and active inference is um that it's basically inhibiting actually these higher level abstract goals and letting us get hyper accuracy within this proprioception modality so really what we call being in the now right getting incredibly fast feedback loops that aren't about the end goal it's not about if you i think talk to any of these athletes it's not about winning it's not about getting to the end it's just like this pure joy about being in the moment um and and letting your body be right um and a lot of this uh really might have to do with inhibiting this higher level of our neural hierarchy so we can get this hyper accuracy of proprioception so we can create these fast feedback loops between our body and the environment to get to this like higher level of performance um and this takes us to more of like improv dance and play that are really non-goal oriented right this is sort of the definition and how i see you know there's a big difference between games and play games there's always like you're winning you're going towards a thing or you're losing um but in play there's none of that there's just exploration um the same goes really with improv dance compared to i don't know ballet or choreographed dance you're just exploring um the possibility space of what you can do um and that at least for me and i think well not only for me this is quite widespread anybody that is into these things i think finds them the most joyous um and you know kids find them the most joyous and it's a very very healthy part for our brain to not be in a goal-oriented um movement pattern and to just let go and explore without having um a high level active inference happening active inference is it's still happening but it's being sort of driven more by the bottom up signals and really the moment to movement possibilities without having this like okay let's gonna do let's do like a pretty move let's do a pirouette let's do something um yeah so i'm i don't think i actually want to show this video right now but it's more about focused on vr but if you want to look at it it's a cool video that's called this is your brain on vr that uh we made um just predicting the sound and stuff won't work uh but it sums up a bit about these what we've been talking about uh how different modalities get expressed uh activated in your brain on we are reprogramming your predictions and how we can play with these things okay so yep this this one this was it that's awesome wow i wrote down um a ton of stuff so shall i ask uh some questions and we'll head into another part many places to jump in i really just caught on to this weaker now like a weaker current moment in order to get further so that the way you connected that to accessible experiences like the play and the martial arts really awesome so when you talked about active inference kicking in what what is happening there and what what did you mean by active inference as far as um the way that an individual person might relate to it uh i'm in what like domain all right give me more you talked about how you talked about predictive coding and how active inference would kick in um or would go into a different mode depending on whether a task is being selected at a high level or a low level or lower level yeah exactly like we're always gonna be i mean there's always prediction error in our brain right uh depending on what's happening now are we gonna minimize it with with what there's again these three different tactics that your brain can implore one um reducing the sensations two updating your prediction three doing an action um so when you're just in this free play there's still a need to create a prediction of your movement pattern for it to actually happen right but it's happening at a much lower level of your neural hierarchy it's not happening because there's a high level part that's saying oh i want to look pretty i want to get my teacher to like me so i'll do something um it's happening at a level of the sensation uh at the level of what feels good from the proprioception the haptic um the the pure like your body knows what's joyous to your body or these lower level um predictions and biases that your body has about what is joyous healthy for your body the knowledge is there there are these lower level biases so we're allowing our system to collapse into a state that's being led by these lower level biases um and this goes i think like in both ways this can go very much into like your body knows and has biases around what's healthy and fun and feels good but there's also that's where trauma is stored too there's an amazing book i think it's called the your your the body remembers and both emotional and physical trauma really are stored in our low-level biases of our prediction around our movements and our body um that too can be you know a big issue for many many people that need to deal with this and and being able to go from a state where your brain's triggered right and in the mental health spaces it's called like when you're when you've been triggered um and these by these negative biases appear that are predicting oh pains how gonna happen even though there's no objective reason for that except your history something's triggered that prediction so being able to switch into that place in your brain that knows what feels good for your body and what's healthy for your body is super important and we don't really know how to do that in a very reliable way yet you know there's mindfulness practices breathing and stretching and massage um but i think like my biggest fear is that we're with technology we're actually being pulled away from our body and that's generally what i'm trying to close that gap um wow that makes any sense thanks for this answer and it's actually related to this next question you talked about how when the avatar was getting almost like false perfect feedback it didn't improve the actual performance so it really highlighted how the technology can't just um just simply mechanically demonstrate like a scaffold it has to give somehow accurate feedback yet it's often fantastical scenarios or abstract scenarios so i wondered what were your thoughts for technology design how could technology be used to actually enable and enable these kinds of states that you're getting towards um yeah without constraining us in that way yeah so i think this was a very big learning for me right you need to give a person accurate feedback of where they are to and what they are actually doing um and there again there might be an ability to uh impose two such uh uh predict things like you might be able to still be in a first person perspective of somebody doing the movements but also either feel with vibrations like if you try new that's what happens i just use the haptic feedback to tell you if you're are in the right place you'll get a vibration so i'm strengthening your ability to know where you actually are so there's sort of ways to hack our other sense modalities with you know music uh really it's a bit of like gamification when you're on the path um of of doing the things that were you're trying to learn uh activate the this the even like the dopamine um you know dopamine at least within the predictive coding framework it's really like a precision signal for a for prediction error so like uh give some more bling bling bling with a little bit randomness when you are doing the things that you're trying to learn correctly but you need to get that feedback of when you're doing things correctly um yeah and i think that that's what was missing from sorry from this experiment was uh giving and the faster and tighter these feedback loops are the more your brain will perceive them and actually activate them so there's some really great you know meditation tools that you'll meditate and afterwards you'll see your heart rate um there's some more professional like biofeedback rhythms that you can see what's happening either with uh eeg or um heart rate in real time um and that's also actually i talked to you before we started like i'm build i'm starting my first hardware project um to do that and give a real-time bio feedback of heart rate with leds if you want to go it's called the wisdom truffle go to wisdomtruffle.com uh you'll see a trippy truffle and basically that's the idea of helping people reach some of these um tighter feedback loops on a lower level by hacking their their visual modalities like let's use it we figured out okay the visual modality is super strong can we connect it to other sensations that we don't have as much insight to like could i connect the visual modality to heart rate yeah pretty easy could i connect the visual mentality to perception yeah if you go to mu when you dance and move your actual your avatar changes color and the sparkles change color so um by hacking uh the the visual modality that really is like our brains graphic computer it's like super strong it's dominant can we use it in different ways to connect sensations that we don't have as strong and are not as dominant in us that's that's one of the things awesome and that point you made really um about the high level priors like both my eyes are looking at the same thing and you can kind of play with that by pushing on one lightly and you know it just does distort the image and it's sort of it just demonstrates uh it's one of those examples like the high acuity in the periphery or color vision the periphery or the blind spot one of those examples that help bridge between for example input output information processing understandings of cognitive sciences versus this really extended and predictive way that you're talking about things so it's really awesome and thanks for making it really accessible with this presentation so you want to go to the second part and then if anyone has questions live otherwise i'll just keep on writing cool things down thank you so okay context there's a counting switch um but we're still staying within the predictive coding framework and active inference but um yeah i'm gonna just give you know i really like this idea of giving visual metaphors based on all the things that i talked about i think it's a great way to learn and and have this intuitive understanding uh both of math i don't know if you follow the three blue one brown videos but they're just phenomenal and and mathematical understanding because it's all visual coded math but i'm going to give a visual metaphor you can either look at the images or even close your eyes and just imagine that when your brain is creating reality it's very similar to playing in a sand with some buckets and making sand castles and the sand is just this constant incoming bottom-up signal there's just so much of it constant sand is just flooding into your room into your space and you have these buckets and you're trying to take all the sand and like fit them into the bucket constantly to create this um reality really and this is pretty much what your brain is doing with with uh at least in the predictive coding framework with the buckets being these top top-down priors and the sand being the bottom-up and incoming signals so there's a constant buckets trying to predict and put the incoming signals into a specific bucket that then turns off the sand basically now let's talk a bit about the classical psychedelics psychedelics they're starting to expand to be you know like basically everything but um classical psychedelics are whatever activates the 5-ht2a receptor agonist just means activating um so um what really happens when we take a classical psychedelic is that these your brains buckets are getting broken down into smaller and differently shaped buckets so your priors are getting broken down they're getting diffused into uh sort of weaker but more accurate um or more detailed um uh buckets that let your brain create an actual different reality for itself now uh where is this coming from uh when we look at the actual micro circuit of the brain um and this is of the cortical uh you have you know seven layers of your cortex and how the neurons are actually laid out there this is a very amazing paper by pasta we can see exactly the feedback loops that are correlated to uh predictive coding where are the um where's the data going upwards of which we see in these forward connections in purple it's coming in from layer four it's going up to layer two and then looping to layer five and then where are the backwards connections that are coming from these higher level brain areas that are correlated to our predictions um and we see that in green um coming from layer five going down to layer four doing these these feedback loops um so when we look at where the 5ht2a receptor is we see that they they're situated very specifically on these layer five um on the backwards connections basically so we're seeing that they're activating uh hyperactivating and making the sensitivity of these neurons uh much more sensitive so that's that's why um the theory is that it's causing these predictions to fire at a much lower threshold if beforehand let's imagine a whole neuronal population you'd need to get you know x amount of neurons resonating together um to create a prediction now you need much less of them just a few neurons are like throwing out their own prediction and a few other neurons are throwing out their prediction and you're getting many more competing predictions so very simple uh metaphor if you're walking down a forest in a usual state you might have a probability of 0.4 percent oh i'm gonna see some animals and a 0.6 percent prediction of i'm gonna see some plants but after you've taken psychedelics and these predictions have decomposed um your you might get a much more detailed uh uh prediction possibility space you know maybe you're gonna see a dog a cat hey maybe you're gonna see an elf that's you know a low probability but it's suddenly there so things that were uh below the threshold of probability are now becoming a higher threshold uh and part of your predictive probability space now what really happens here right um and and what's really uh sort of important is how this correlates with the rest of your bottom up sensations um if you are in a very uh getting very clean bottom up signal which almost never happens but let's imagine a very clear auditory signal um at least for a moment or very clear visual perception that you're getting suddenly you can have much higher accurate predictions uh and this is uh you know people on psychedelics including myself will report this quite often like oh wow i can see footprints in the forest the way i've never seen them before or this perception of uh hearing music and like the audio is just so crisp and so detailed in ways that i don't perceive them when i'm not on psychedelics um but if you are uh in a bottom-up environment where you're getting noisy signals it's dark things are constantly changing there's various sounds um suddenly the the possibilities and the predictions that you might be getting are might not be as correlated to you know quote-unquote objective reality suddenly your predictions can become much stronger uh um and just or much less you know accurate but still a type of overfitting might happen that's i think a good word to use here where you know oh i'm in this forest it's dark and gloomy there's something moving oh it's an elf right because that was part of the probability space of your prediction and with the noisy bottom up incoming sensation that was enough to reduce prediction error um but what's important to note and you can see sort of in in this very uh simple image that the decomposed predictions are all uh less strong so they're not going to turn off as much prediction error so your your brain is going to constantly be while you're on psychedelics in a state that has higher prediction error which as we saw before and we've been talking about when you have a higher prediction area your brain has a few different tactics that it can use to deal with it one modulating the senses two updating predictions three active inference right so this is why uh psychedelics offer a very very unique opportunity of updating your model because there's a much higher level of prediction error and then um under some situations your brain might decide to actually update the model and if people are suffering um from trauma and depression we're seeing absolutely amazing results in research uh around using psychedelics to update people's biases around these things um again very much depending on the setting don't expect to take psychedelics and go to a place where you don't feel safe in and you don't have a support structure and to get these benefits in order to update your model you need to be getting bottom up sensory inputs that are very strong and accurate around what's happening to you right now but if you do that and you are able to feel safe in your body like we've talked before be in place situations or improv or flow that might allow your brain um to really really heal deep traumas to really reprogram deep biases from childhood when your brain was more uh had this extra neural plasticity so um now let's get to active inference so the interesting part about these five ht2a receptors it is that we don't have so many of them in our motor cortex there's a lot less of them so we can actually uh say that active infra inference modulates psychedelic experiences and makes them uh weaker and we see this both in the instructions that people are given when they're doing psychedelic therapy uh right now in um at maps for instance or other um universities and hospitals that are researching this they really tell people lie down don't move and when i talk to therapists about this they could really say we know when something's bubbling up with the person we see this in their body when it's becoming too much they start really moving and we try to encourage them to actually stay still because they they when we start moving we're actually weakening the prediction error right our brain is creating a prediction that's still pretty stable because we don't have as many 5-hc2a receptors that have been activated hyper-activated um because we took psychedelics in the motor cortex so our motor predictions are able to modulate this psychedelic experience and you know that the benefit of this is that oh wow if you're are doing psychedelics and you're feeling really overwhelmed move do a bunch of jumping jacks push-ups move around space um that's gonna really help lower uh the prediction error in your brain and uh help you be more uh back to i guess the reality you're used to so yeah this this was a uh a short uh connection between psychedelics and and active in france um that i just wanted to bring to the space too thank you yeah there was so much um even in that that i could uh yeah you can unsure then i will just um yep that's perfect i'll just then i'll just re-crop it um i guess i'll crop it while i ask this first question which is um you pointed out that a lot of even the clinical work on psychedelics is done in a way where they actually prevent movement and also like education stay at the desk but don't encourage it yeah yeah okay totally yeah like putting people in not restraint but it's it's it's only you know continue with a um authority as a restraint so just um yeah what is maybe the role of the body in uh remote work or in now that we're all so in um again not physically restrained to our chairs yet but it feels like that some days it's a disaster it's it's um yeah i think we're uh and i'm a total motor chauvinist when it comes to our brain i don't know if you've heard that term before but no i'm going to explain that yeah so basically we have a brain in cognitive functions because we move you know when you look at the animal at the designation between tree to algae to creatures and the crazy ocean to animals that move when do you start getting brains is when they move and there's a great example of some really insane sea creature that moves around and has a brain then it finds a place to plop itself down and digest its own brain so once um yeah it doesn't need it our our brain is needed to create predictions with the environment right this is the whole uh predictive coding and andy clark has a beautiful book and papers about about the feedback groups that we create and within with the environment and if our environment is relatively highly predictable because we're staying in the same place then guess what we don't we we don't need this uh and we and us as humans and andy clark ursula goes into that quite a lot we've sort of because we want to minimize prediction error that's that's part of this whole free energy thing in our brain um but we've done it a bit too well you know we've created these uh large scale active inferences with our environment shaping uh our supermarkets looking the same as the roads the our house um and uh i think that's a really a big cause of generally the mental health crisis that we're seeing and especially when it comes to then not even moving your body being in the same state uh physiological state that's just true low entropy um and that's uh really really i think i'm gonna even go to the extent of saying like my main goal and my in my work um is that one metric improve the amount of uh states your body is in for humans if i can do that then um for mo you know modern uh citizens that have a total lack of this i am absolutely certain that we're gonna see improvement on many metrics um that we're lacking so when it comes uh there and there's as much as i can to like uh not fall into this like i've been working remotely for uh even before this crazy covert thing happened but um you know do calls while you're walking outside or stretching outside anything that doesn't require you to be in front of a 2d screen yeah bring toys yeah create like we do have the ability to uh gamify and bring toys into our environment like i have rings outside and punching bags and um any anything that will create constraints around your movements you'll move your desk change your table don't work from the same space if you possibly can just keep bringing in that prediction or goes into your body and um and i think that's a very like incorrect way that ergonomics is looked at you know ergonomics is all about like let's find the perfect position for you to be in for many hours that doesn't exist we're not supposed to be in a perfect position just going like this um well it's it's really it's like a modern mental ergonomics and it's the um ergo the work the dynamics of a dynamical system that's us and so the ergonomics is now the back angle for the chair but then there's actually a ratio of the chair and the exercise ball or whatever that's not my specialty but it's all about changing those ratios um because whatever there is no such thing as the the perfect sitting state or not moving state for your body you know lying down too much gives us pressure problems and people in hospital need to be turned around to not get these things we are a dynamic physiological system and i think there's a very big gap between people that are studying these systems a lot of the times and like math people and computer science people that are just really not spending enough time connecting to their own bodies um so i think like i highly encourage everyone to like just find the time to figure out what you are as a full human being as like blood blood pressure bones tendons um and be in that space and let that those prediction take precedence at least a little bit of your day yeah one thing you mentioned is this like top down almost standardization and and synchronization and stabilization of the lower level states and then the the two affordances we have are kind of like our physical movement and our communication and we've seen how those are subject to top down as well as bottom up constraints but that is kind of what's being remapped is the constraints on our movement variously and our communications networks variously and so to have a multi-scale framework like active inference where we can talk about the remote team having input and output and then the body we can be like okay here's an algorithm that's awesome for this sector for this team but i want to be not reinforcement learning driven or not reward driven i want to have a framework that allows me to express these kinds of playful states or even have a rationale to construct these states and design for them yeah so i think there's a you that was a lot a lot of good points you brought up so maybe first the communication thing right um we're doing this this 2d sometimes only voice sometimes 2d screens that none of this is naturalistic for our brains right we're 3d creatures we pick up social cues from being able to gaze into each other's eyes predict where you're looking at um and they and it gets worse the number of people you bring in it's just i can't like group zooms for me are torture there's so much awkward silence so much inefficiency who's next who what is the level of even when we're in a normal social group you know i usually put little numbers around people how much i think they understood me okay this person understood seventy percent thirty percent zero and i do the same for myself and i try to be very honest with people i like i understood around thirty percent of what you just said um and that's how we are like we have different uh just data structures internally and managing to like minimize the differences in these data structures is a very difficult task um uh even when you're not working remote and remo remote work has definitely made that a lot more difficult um i've tried to use things like uh vr um and found it uh sometimes okay but also it's and i'm a superhero you know enthusiastic person and still spending more than 15 minutes in there at a period of time is just too much it's heavy it's clunky it's um a close point of view um so that's that's a difficult thing but we do try what we do try to do a lot of the times is create um mock-ups directly in vr and share this the mock-ups in vr so we understand what we're trying to build together um from this 3d perspective um generally i'm mew we have like an ar app where i'm trying to create interactive holograms so more on the social connection instead of just uh getting you on like a 2d uh screen or the facebook and instagram but getting something that moves in my space and actually interacts with me like you can blow kisses i don't know if you've ever seen the roger rabbit movie but i realized i think i got it like i started when i was a kid and then i built this thing i started the movie again and i was like oh wow i just built the roger rabbit thing kisses through the screen or something you you blow kisses and it goes into your 3d space in ar space so you can actually move around your space and catch the person's kisses so i'm really excited about the possibility of creating what's called joint action uh in cognitive neuroscience remotely and joint action is a physiological thing more than anything like if i even shake your hand or just passing you that pen is when i start predicting you and starting to predict you is starting to build empathy with you um so these empathy things really uh are naturalistically based on very low level of physiological data and without this because we're currently constantly on zoom and you know i i click here to send you a button to send you a chat or i raise my hand by clicking a thing and you see a thing over there it's much much weaker and not really creating these joint actions so i think there's a lot to explore with technology with just joint actions and um team building activities that are in vr even if for a short period of time and just simple things like let's build blocks together or things that you would do in in physical space if you could be together to just understand a bit more on this low-level prediction of who a person is how they move now vr is very strange because you can like mess things up i don't know if you've ever been in vr chat but you can interact with someone as like oh i'm now a three meter uh giant with fire wings and i'm a tiny pixie and you're just interacting this way um so i think that's really interesting and and i'm actually it can be a very interesting way of also de-biasing things like oh what happens if we conduct interviews that are not like we know we know there's a huge bias in um in in the whole hiring system and building teams to begin with that is not solved there's like okay you do uh um you know you don't even write sometimes hr doesn't look at the names and there's all these things to try to hide who the person is but the moment there's interviews you know who the person is and these racial biases come in these gender biases come in so there's a lot of really interesting ways that we could be de-biasing ourselves by just again this excessive prediction error and and the ability to really detach ourselves from the normal um perception that we have um that's really interesting about the physical components of remote whether it's a physical object that's going to be moving and i don't know like a you know little jellyfish that does a movement or just synchronizing and it made me wonder if there was like an exercise like could people just say okay beginning of this video meeting yeah maybe we all just switched over a context like let's do you know three claps and take a deep breath and just do something think of a you know purple cow and then just coordinate that way and then prepare or here's our narrative here's what we're here to do because that kind of stuff the mission which comes back to your earlier idea about like giving the information in the way that's naturalistic and what is the natural unit it's kind of the the goal the end and so having the group assemble with the synchronization and also a remembrance of the end is like really an interesting implication i think of active inference but not some of these other um sub component theories per se yeah and uh so that's also what i'm trying to do is this hardware is let people synchronize their heartbeats let me you're over there i'm over here let me send you my heartbeat as beautiful glowing pulses and you can send me your heartbeat and get this uh biophysical data that honestly if we were even in the same room together i wouldn't necessarily get unless i was like touching you right so i think we are and this is the exciting part of like immersive and embody technology like we are getting to the stage where we can start uh moving beyond 2d video into biophysical data that lets us get a deeper understanding a deeper connection and deeper synchronization with each other um well i'm happy to talk for a little bit longer if you want but i guess my closing thought or at least direction because maybe we'll see or hear from you again would be like what are the next steps for applied active inference or for um an individual who's curious about these kinds of topics is there like a website or is there a research direction where is active inference coming into play in the next months to years yeah so honestly before i heard about your podcast i've never heard about any uh such likes focus within the predictive coding framework i would be very excited about um connecting and this is something i've i've been trying to do is bring movers into the space it seems that there's these two different worlds there's movers that hate technology because they feel it's taking them away from their bodies um and there's there's technology and science and there's a very unfortunately very small overlap between them and that's what you know i was very lucky i found a co-founder who's in both these worlds um but if there was a way to bridge this gap and like you're saying these you know movement exercises it's a thing that they we do constantly and improv classes uh building physiological synchronization and the things you'll see there sometimes are just like this is one organism you know like people moving together not pre-decided um and there's a a beautiful research paper i think noi it's link to it afterwards um but what they did there is they tried to figure out what's going on uh in the brains of improvisers um and they have a beautiful like mathematical uh paper about the double feedback loop that's created so it's not and they tested two conditions leader versus follower that was more what i was testing versus um there is no leader and follower you're doing things together and they found a much higher degree of synchronization and a very they built a very cool paradigm of just like moving your hand on one up and down that's it very easy to measure but it was there the data was there they managed to find um a much higher degree of synchronization when there was no leader and follower between um improvisers that were either musical improvisers or dancers and trained their brain to tune in to the other person because when you're doing this and this is a very um you know when you're talking about teams and synchronizing and and play i think we really need a shift in the hierarchical structure of teams like i don't see and this is something that i'm you know really exploring as a ceo of a startup how do i get these ideas that i want out there but how do i bring in people's ideas so they feel that they're part of this and how do we then get a cohesive um product at the end of it not none of these are like easy questions uh but having um this ability to double feedback so it's not just a top-down manager right that's saying oh listen this and this you guys understand this and this and this this is my vision don't expect people to be excited about your vision right that's not how these things work if you want them to be excited and attuned you need as a you know manager in this hierarchical structure be attuned to their visions what they're seeing their perspective why did they join this project and try to get that coordinated um into a global vision that is um you know doesn't break apart because there's too many voices so none of this is easy but i think that a lot of the times yes going to the low-level physiology first and building this low-level physical trust with each other and understanding with each other um is really unique like me and my co-founder we dance together we climb on trees together before he moved to hawaii now we try to hang out in vr and do things together but it's really not the same um so um figuring out how to do these joint physical actions together i think will really get us into the future somehow or a future i want to be part of yeah nice or a likely future or one that i expect myself in um yeah but on that um tech and movers and sort of the transdisciplinary nature and the potentially productive conversation uh we we kind of think about these guest streams like it's too directional because maybe for those with a physics background or a philosophy background you've inspired them to think about how they're doing their physical activities or wanting to go deeper into a knowledge or movement tradition they were already a part of and then there's people who that's their daily life and then maybe this is the first time they're hearing about active inference so we can always do like event series or discussions or working groups or events like that or projects like that to help make that two-lane freeway like accessible for everybody and respect both directions of the interchange and it's a really important part like you said with the moving away from the hierarchy like of disciplines you know is physics the top dog discipline physics is awesome but it's actually a relationship i survived in physics it was dramatic it's great and then different people will be drawn to different parts and that will be sort of like the colony phenotype and um when we uh when we constrain what the lower level looks like we increasingly infringe on what the higher level can be as well so it's about that balance and so that's a common theme with these active inference discussions well yeah this is awesome i really appreciate this conversation and it was uh i'm sure something that a lot of us will we listen to and think about so we hope to maybe interact with you or other communities in the future but this was really fun so thanks a lot sarah peace bye