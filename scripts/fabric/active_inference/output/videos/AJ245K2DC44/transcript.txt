all right welcome back cohort 6 it's 219 and we're in our first discussion of chapter 2 so Oli thank you for facilitating and kick it off and then let's see what we go okay thank you uh all right so in chapter two uh we delve into I mean uh the main uh compon one of the main components of active inference Theory uh namely uh how to formulate action and perception uh based on variational approaches so for perception uh the optimization parameter that needs to be minimized is the variational free energy and for the uh action counterpart the similar parameter would be uh the expected free energy uh so basically uh in this chapter we encounter two of the central equations of active inference Theory uh which are equations 2.5 and 2.6 uh one for the variation of free energy which formulates how uh the agent or the cognitive agent optimizes uh the variation free energy as its minimization parameter and similarly in equation uh 2.6 uh we see the same uh same mechanism as applied to a minimization of the expected free energy in the case of action uh but before uh going into the details of uh what those equations show and how how they work uh in the context of active inference it might be helpful to just uh go through the whole chapter um I mean uh in case uh there's any there's I mean any Gap in the understanding of what the whole picture here is so uh first uh as the title of the section 2.2 suggest perception as inference is one of the motors of active inference because uh as we know sometimes uh in the computational or the traditional cognitive science uh we treat perception as uh simple information processing um mechanism but here in active inference perception uh is treated as a kind of inference and not just symbol processing or information processing approach and the way to do that is uh to formulate it in terms of uh basian theor uh base Theory so uh in box 2.1 uh there is this fundamental rule of probability about I mean sumon product rules which is which are used uh in base Theory and then in equation 2 .1 uh The Familiar uh base theorem uh uh is introduced and then uh through an example in figure 2.1 it shows how exactly priors and beliefs are formulated and expressed in base theorem and later by using uh by by using those summon product rules uh we can write the posteriors uh and specifically the posteriors in the denominator as a kind of marginalization over uh over the probabilities of observations so that's basically what's been done in equation 2.2 and then uh here on page 20 there's this a useful distinction between the notion of surprise as applied in Psychology or at least folk psychology and the notion of surprise an active inference or namely the basian surprise sometimes it's called surprisal surprisal function uh so uh basically the difference between those two concepts are uh they are obviously related but they're not actually identical so when we talk about psychological surprise it connotes something about the effective uh notion of that surprise I mean uh we need to be uh we need to be affected by uh By An Unexpected observation in order for us to be surprised but here when we talk about basium surprise it's much more formal and it only describes uh the degree or the extent of the unexpected event regardless of how effectively it influences us as human observers or cognitive observers so it's just basically um uh something very formal and statistical so um one other key movements of active inference formulation is the use of callback lier Divergence uh for the comparison uh uh I mean uh for the comparison between the prediction and The observed uh data so uh the the justification to use callback lier Divergence comes from uh neon Pearson Lemma uh which I don't think is stated explicitly in the text because uh in uh non Pearson Lema uh it states that the most efficient statistic to compare two distributions is the logarithm of their likelihood ratios so basically that's what callback lier Divergence is it's basically the uh um it's just um uh the comparison between the logarithms of the of the likelihood ratio it's the ratio between two uh likelihoods so uh here on page 20 we see the definition of callback lier Divergence in equation 2.3 and then it's applied to uh the example from figure 2.1 in order to calculate uh the difference between uh the posteriors and the priors uh from that example uh and then okay so here we comes we come to an interesting discussion about the difference between biological I mean about biological inference and optimality so uh in this case uh optimality uh can be I mean in active inference literature optimality is defined in two distinct way ways so here in this book optimality is used um as a as a as a cuss function uh that needs to be optimized or minimized so but there's another U that's basically called basian optimality but there's another notion of optimality uh which is called Janes optimality uh which is used in uh basian mechanics in order to calculate the dual aspect of active inference namely cmap or constraint maximum entropy principle uh which is the direct mirror uh of or uh the symmetrical Counterpoint of fe uh but without going into much details about how that works and how that relates to basian inference here when the word optimality is used in this book uh it's almost always uh it almost always means basian optimality and not Janes optimality uh okay so uh the view of active inference uh here we treat biological organisms as a kind of machines uh that do uh that undertake these um basian optimality in order to optimize uh the surprisal or minimize the variation of free energy or expected free energy and the way they do that is through a tripartite um model namely uh the generative process and generative model and markup blanket that kind of acts as an interface between the generative model and the generative process so very briefly generative process is what happens in the environment uh I mean external to the agent and the generative model is uh what the organism uh and perceive about the environment or at least uh anticipate the environment the data or the latent uh hidden states of the or latent states of the environment and uh it constitutes uh its generative model based on those processes uh and the way it interacts with uh the generative process is through Mark of blanket uh which will come to uh much more detailed in chapter 3 so here on page 23 we can see this tripartite um model in figure 2.2 and uh then as again as the symmetrical counterpart for perception as action we can formulate that mechanism similarly for the action namely action as inference and uh the only thing that needs to be changed in equation 2.5 is the substitution of um is a substitution of um I mean variation free energy with the expected free energy so basically uh those two equations are uh related to each other and they're somehow actually the mirror image of each other or counterparts to each other so um yeah that's basically uh I mean the rest of the the rest of the chapters from 2.5 unto the end of the chapter is devoted to the derivation of those Central equations uh which we can talk about a little bit more detail uh this week or the next week if you will thank you Oli awesome summary does anyone want to like what's a part that they like to to or a question or a thought one that was written in the questions that we can go to or a new one I have a question in terms of uh self assessment so does it in terms of like creating almost like an atlas in terms of like say if you utilizing like a a flow mechanic in terms of active inference so if you're in a state where you know you're working on a problem for example um using a tool and you've arrived at a certain point how to then map back without consciously interrupting um your Flow State um what's the best way to kind of map backwards so you can extrapolate any information forwards so people have questions you know on how you arrived at a certain conclusion how you might be able to uh clarify if that makes sense there's a lot of that question like um know how and know that so is it about knowing where you've gotten to or knowing how to navigate forwards or backwards those might be all the same answer or it could be all different answers and going you know swimming up the river isn't going to help you swim back down the river or there's a path on the side of the river um you might be able to use just passive recording like recording the screen grab or something like that so that certain parts could be checkpointed and and just kind of locked in I considered like OBX in that sense um but I was just thinking in terms of like storage um you know if there's a way around that too in terms of like can you do a direct correlational map kind of like a neural net to go backwards through that you could build out and then apply forwards as well well on the kind of um incremental Improvement you could uh use a time-lapse video in OB yes like you don't need to take a HD 30 frames per second you could do like a time lapse and then to but to kind of bring it to the generative model side you could come to develop your own model of perception and action in the interface with that tool and then by building out your model of perception and action with that tool you might arrive at a very distilled very reduced parameter description yeah I mean that's how I'm kind of considering it like how can I build nodes backwards that might give me inference forwards with you know less energy input in the future or in the present like can you build uh naral nodes that would you know reproduce the same kind of U predictive result by examining um you know those priors and how they changed or branched from the origin well I'll try to give one more active thought on on that like so we're looking to be building some generative models something like figure 4.3 so a huge amount of the actual action selection dependent causal unfolding of the world is basically all loaded into B Because D is just a prior a is just a mapping between observations and hidden States so basically everything about like how one's actions in the world change it going forward is the expected free energy equation 2.6 B Matrix here so let's just say that you were um just doing a a mouse movement tool um one way to kind of engineer that would be try to um you know de memorize or try to reconstruct long sequences of planned Mouse movements another one might be just hyper locally understanding like the gradients around the mouse location or around the attention very locally and then trying to determine that um attention landscape or that affordance landscape and then try to um either find yourself within or create new can iions of like canalized attention where to go from point A to point B it it may be just as simple as just sliding downhill rather than needing to like invoke um symbolic cues okay it makes sense I'm just trying to think I guess it's a little bit more granular but I was considering more um I mean depending so personally for myself I would like to build something I guess that's more eye tracking because I do that more I think than Mouse tracking um but that's a different conversation for a different day so I guess it would still be you could do XY parameterization in that same sense but sorry go and if I understood you correctly uh there is another uh related uh sub theory of active imprints uh called branching time active imprint uh which kind of an attempt to improve the efficiency of active inference computation uh by using basian filtering uh and um so instead of the iteration of the update equations uh they uh alternate between integration of the evidence and the prediction of future States so it significantly improves improves the and U also reduces the complexity class of the of the algorithms and uh it improves the benchmarks uh significantly so uh yeah branching time active inference is a direct attempt to uh somehow do those kinds of computations with much less energy and with much more efficiency as well okay great thanks I'll review that here's kind of a core image with the two ways that the discrepancy is addressed or resolved with like change your mind or change the world like it's too warm in the room you know there's a plus it's plus three degrees too warm either that can come to be explained Away by updating belief that that's the temperature or that can be changed materially or you know the tension could be held there but these are kind of the options when there is a Divergence and then how agents and the environment interact um Thomas oh no I have a question so it's um oh yeah go for it oh yeah just in general I didn't want to interrupt I wasn't meant to inter I'm not reason my hand to intervene on what's being proposed talk about right now um just's a question my question was related to in the policy section later part of chapter 2 the C parameter related to uh gpie so the future expect 10 expectation part I was just curious like because that seems like a very important that I was curious I saw there was a that element is there and it seems like kind a surprising surprise uh that there's a uh preference such an important part to play in the I guess I assume the pragmatic value part of the equation uh yeah it is it's part of that section um so I I'd love to hear I expecting that probably talk more about in later chapters but that was just something I caught in my mind just more about that if there's some just clarification about where that's coming from and why that's there yeah Oli or anyone what is the preference variable how does it come into play and how does active ence's treatment of expect Ed and preferred observations have similarities and differences with for example reinforcement learning yeah uh so the preference variable I mean uh it's again one of the key moves um in active inference to encode preferences uh so uh for expected free energy I mean if we use the exact counterpart of equation 2.5 uh then uh I mean we would uh we wouldn't need uh to incorporate any other additional parameters uh I mean to incode preferences or anything like that but uh one of the uh I mean the dependence on policies uh can be dropped from the pragmatic value term uh that's that's the idea behind uh the the encoding of the preferences because um although in uh some of the earlier papers of active inference uh it it's not incorporated into into the equations uh I mean but uh in recent papers uh they tend to clearly distinguish uh this form of the free energy namely the expected free energy from the variation free energy by incor incorporating that pragmatic uh value explicitly as conditioned on a preference variable so uh basically uh the way they do it is to try to again uh embed the whole equation um under um or let let me put it this way uh they U by encoding the preference the whole equation can be uh somehow uh uh written down as a kind of lower bound of uh another optimization parameter which is which has incorporated this C parameter or the preference parameter so uh basically what it means is that uh by replacing the true posterior with an approximate posterior uh this uh pragmatic value can be explicitly conditioned uh pragmatic value can be explicitly conditioned on um the the preference variable C so uh by flipping the the terms inside that expectation value uh it can be uh it can be treated as the exact opposite of uh the variational free energy without compromising uh I mean the symmetrical relationship between those two uh so but why the epistemic value term here is important well because obviously uh so take for instance if we're in a dark room uh there's this pous problem dark room problem the mapping between the hidden States and the and the observations would be entirely ambiguous so uh the best way to reduce this ambiguity is to uh uh somehow uh seek out the observations that uh reduce this uncertainty by U maximizing the change from prior to posterior belief after a new observation or uh similarly uh by um uh I mean reducing the uncertainty about those hidden states by reducing the the variation of free energy so uh by applying the epistemic value here it uh provid Ides a way to navigate that space of possibilities when the agent tries to uh maximize its certainty about the posterior belief uh so yeah that's one of the key differences between the earlier active inference literature from about up until uh 2021 and the later ones from 20 uh 21 uh up to recent papers thanks a little more on C so this is the PDP partially observable Markov decision process and every letter here is like a statistical distribution or a variable in the program so defining the active inference agent is defining the a b c and and d and so on so this is like defining the whole agent in its unfolding and so we can look at each of the nodes as statistical distributions or parameterized and then also when there's an edge which is like a causal relationship it's an influence so we can talk about that as a conditional value like conditioned upon this being the case what does this and so that's the vertical line here so how does active inference deal with goal oriented behavior so it's a big question but at this kind of minimal essential level the key move is to define a preference distribution explicitly over observations so in reward or reinforcement learning you'd have observations coming in like let's just say we're talking about body temperature and different body temperatures would be associated with how rewarding they are and then movement would be performed to move to more rewarding States and utility discounting and all these other kind of ad hoc um issues arising in contrast what happens in the kind of a preference orientation in active inference which doesn't need to exist at all you could remove have a flat preference distribution and just have purely epistemic Behavior or or you could have purely preference driven Behavior but what happens here is that the preferences which is hence the the origin of the joke preference slash expectation like what do you expect and prefer is going to happen how does it become a self-fulfilling prophecy because there's a coincidence of the preferred and the expected in self-fulfilling agent so then the preference is explicitly the distribution of body temperatures so when I'm at 37 degrees it's not like I'm in the most rewarding State and then if I drop down to 36 I need to increase my reward back by going to 37 it's actually I'm more surprised by being at 36 than 37 and the ball rolls back downhill to the bottom of the bowl resting in the low surprise State the most preferred the most expected the bottom of the bowl rather than trying to Hill Climb a secondarily proposed reward function or utility function that's scaffolded on an observation distribution so observations are not secondarily evaluated there's just the definition of a preference over observations and that kind of grounds the model it's kind of like having it touch grass because otherwise how would the actions come to um influence in this way that is control like um the the fundamental piece of the model and then just last piece here um here's the G equation 2.6 expected free energy and then this is just showing how when you remove or zero out certain components of this equation how you get other well-known forms like if you remove epistemic value all of 2 three four five so all you retain is this pragmatic value of future observations conditioned upon preferences you get expected utility Theory whereas if you drop pragmatic value and only have epistemic value you have novelty maximization Infomax Etc so this is kind of like a generalization on a bigger space of decision-making possibilities that run the gamut from arbitrarily pragmatic to arbitrarily epistemic and all of their mixtures David yeah so I just had a question related to um you said rolling back down um the bow so in terms of I'm just trying to think of it analogously um wouldn't that be kind of return to priors but that in that sense would you bring your new experience back down into the bowl therefore expanding the priors as well or would it be combinational in a certain way or bringing that back out of that lower state to that reward State and then back down would that transfer any difference between the two states it's a great question there's been some work on C learning preference learning so the easy setting is if preferences are a priori fixed then um yes the pragmatic component of the agent will just just like a moth for a flame just like it will just keep trying to get to see and so maybe for like thermostat or something like a fixed C makes sense um then you could have a higher order control policy on C that's one option or Andor you could have C learning now C learning is a bit of a challenge because if you just updated your preferences to whatever was happening you might as well not have preferences like if you just kept on reup dating what temperature you preferred to just whatever temperature was in the room you might as well just be going with whatever is happening in the room okay so it's more more of a recomposition in that sense or Reformation um can I add something uh in in answer to your question sorry uh so okay uh if I mean uh the uh probability of observation conditioned on C uh is totally different from the prior over stat because um prior Over States only uh encode beliefs about the true states of the word uh irrespective of the preference or what is preferred but uh in this formulation uh we uh by using uh the condition I mean by using uh the preference value see the observations with higher probabilities are treated as more rewarding so it's a kind of reward function uh if you will right but uh again it's totally different from the prior Over States uh and um also another Point uh this kind of formulation enables u a kind of planning as inference which is distinct from both perception as inference and action as inference right because here uh the use of these probability distributions to incode preferences and policy values uh is a kind of um it brings the elements of action selection uh within the domain of basian belief updating uh and the key paper uh on that is uh Kaplan and friston's 2018 paper namely um let me yeah find the name of the paper it's something about yeah it's planning and navigation as active inference so this seemingly simple trick of using uh the preference value here uh again enables another form of inference which is completely different from the previous ones as well okay great thank you does anyone want to um ask a question or we can look at upvoted questions or we can look at any other question a lot happens in two this is kind of the first technical chapter and it's the low road with the how Peter hi there um so I did have one question about the um the kind of like triart type model that's shown on page 23 um so I understand the idea that um I'm trying to sorry just use the the correct terms here um so so there's there's two hidden states that are listed one that corresponds to the world and one that corresponds to uh like the agent's model I understand the the concept of why the hidden State associated with the process would be hidden because it's on the other side of like a marov blanket why would the hidden State associated with the model be hidden like what is that what's the sort of implication there go for it Ellie uh okay so the uh the implication of uh using the hidden state in the model um I mean by by using u x wait I sto hearing only uh okay can you hear me yeah yeah go ahead okay uh so by including X in the generative model uh it so the X asterisk is the true value of hidden State uh which is in the generative process right and in generative model the agent tries to model to or tries to perceive as accurately as possible the true value of hidden state in the generative process say but uh obviously it can't be done uh through direct perception or any kind of uh with any level of U complete certainty uh and it can only be done uh probabilistically so uh the reason on one side it's only X and on the other side it's X ASX is to uh distinguish between those two values of x one being the true one the other being the inferred one right uh so here the X as hidden state in the generator model is actually the inferred hidden state of the true value of hidden state in the generative process so I'm not sure if it's helpful or not but yeah I'll add a few more pieces um close synonyms first off there's many um kind of natural language assertions and descriptions that don't interfere with this essential particular partition um which one of which I'll explain but the hidden State also sometimes called an external state or a latent State um but it its external is hidden from the internal via the blanket and also there's a sense in which this is hidden from the point of view of the environment and that symmetry um here is a kind of very fun um kind of Light breaking post textbook Group which is here's what we were just looking at generative model generative process but then later we see um ramstead and others using generative model as system level description and then in this video and transcript um Ali basically asks that specifically so suffice to say that when you're working with multi-agent simulations or where there's agents communicating across an interface it isn't always the most useful to be talking about internal and external because internal is always just specifically from one reference point so if you're talking about ecosystems of shared intelligence then the way that they're addressing that with with no fundamental change in the actual topology of the model which is really what defines the variables um at least they are using more generative model to describe the total constructed model representation so not just not focusing on partitioning generative model from process but again these are different natural language assertions on top of the same exact formalities but it just can be a little bit unclear because sometimes people are talking about the environment as generative process but then it's like but if the environment is another agent then doesn't it have a generative model and what are we really talking about well again the variables in the math or the variables in the computer program when you build it it wouldn't matter if you described it one way or the other per se but this more recent is just a simpler more inclusive um way to describe it however it's also not the distinction between generative model and process that the textbook in insists on which is it's accurate it's not like it's going to mislead it's just that you'll also see people talking about generative model in a little bit of a broader way David then Thomas yeah so uh just in terms of concept so I'm a little behind on some of the uh terminology so I'm just trying to grasp it a little bit more so I was just thinking in terms of the Markoff blanket um as an opaque State versus a transparent St so we updating are we able to is it kind of like a transference through that field state to another higher state or is it you know more of a transparency of the field being able to glean context from you know what is behind that and in that sense you know going back to the model that you were just showing if we are let's say capable of generating something that includes the model the inferred State and the process wouldn't we then create another blanket encompassing the whole and then how would you approach that in terms of you know are you not reproducing at the same time a new state these are great questions yeah go ahead Ellie okay sorry okay so the whole idea behind markup blanket and the justification for its use uh lies behind the concept of sparse coupling so in a complex multi-dimensional a agents or systems uh there's a kind of sparse coupling between the agent and the environment in the sense that uh the agent doesn't have any direct access to the environment and uh the causal relationship between uh the disturbances of the environment or any kind of data of the environment and uh its causal effect uh on the internal state of of the agent is uh completely nonlinear or as friston himself would like to call it's caused vicariously it's not direct but uh it's kind of indirect in causal INF influence on the internal states of the agent so in that sense yes we can say the markup blanket is kind of opaque it's not transparent in the sense that there is a direct uh I mean transparent lens or direct access to um environment to external uh States and data but on the other hand uh there's this notion of basing lens which has developed uh recently uh through category the L yeah it is opaque they've explored that in the phenomenology setting like metser opaque states of mind and they've really mapped that closely to the blankie concept it's also opaque but but shines through that's why it's the holographic blanket and then you're totally right you can design or imagine special situations like where you know once you're dealing with metacognitive agents or counteract like you can end up with some very creative and and twisted settings and then it just like well you can make a map where San Francisco has you know an underground railroad and all these other things it's like okay and then those the map the map play can be infinite okay it's interesting thank you thank you Thomas yeah the build on what Peter was asking originally with the hidden States um yeah we give it more is this a Kind part of the distinction of AC and policy discussed in this chapter in that sense of what we just what you just said where the hypothetical counterfactual operations uh they seem to operate in in likelihoods as opposed to kind conf your probabilities uh so it seems like at least in my notes I had about that that the hidden States like basically the I guess expected X in the formulation is a segment or a trajectory of a hidden State over time and that's against or I guess uh conditionalized to Pi the condition the conditional trajectories on the policies so just like there's some clarification about that yes okay so let's think about Tilda is the real temperature in the room Through Time Tilda is just sequence through time we're prospectively talking about action now not all things prospectively plan action so vfe 2.5 might be sufficient if we were talking about like a rock but we're talking about something that's actually engaging in that counterfactual so the real temperature in the room X Tilda through time and then the observations Through Time on the thermometer um so one of the decompositions that we talk about the most with expected free energy and the one that aligns it the most with those other models shown earlier of like decision-making is this epistemic and pragmatic value um distinction but that decomposition is exactly equivalent to first off the physics based energy minus entropy form that's what makes it a free energy but then these middle two forms and these are linked um here is um how ambiguous both have the same first term which is how ambiguous what's the entropy the policy dependent entropy of the mapping between thermometer readings and the temperature in the room so both first terms are basically saying like um policies are better when they're less ambiguous in their sensory mapping and then this second term is basically um very similar looking the only difference is here there's a y Tilda versus an X Tilda um because whatever our our action selection dependence uncertainty about thermometer readings is if we had a perfectly accurate thermometer our risk over thermometer readings would be the same as the risk over temperature in the room and then to the extent that we have even a tiny tiny bit of noise in the thermometer we're always more unsure about the temperature in the room than we are about the thermometer readings because these are the the approximate Target of estimation of like preferences and then there's always further uncertainty Beyond observables so H how exactly complex like World unfoldings get wrapped into this that is the the challenge of making the relevant model and what's interesting is that even if you write the equation and you u in your program and it calculates it this way you're also Computing a quantity or scoring measure that has these equivalent interpretations but this is usually how it's written but then that's equivalent to this physics based one this is the control theory based one and then here's kind of like a Investment Portfolio type one just wanted to say uh thanks for that explanation I think my misunderstanding uh with regards to X was thinking that hidden there meant something like hidden from the agent's perspective or like metacognitively inaccessible um so this is very clarifying thank you yeah like which es especially when dealing with a complex cognitive entity that like could have access or not to its own State it's kind of like this simple case is just we're writing the software for a thermometer and air conditioner and it just has Total Access to the variables we're not going to worry about the phenomenology or like whether they something that's shielded from itself or not um and and then again in this minimal case like the aboutness of X is exactly the the this x star the real temperature in the room but you could have a situation where the temperature in the room causes the lights to change and then the agent has a has a model where the Laten State it's inferring is bitcoin price and it thinks that that's a causal relationship with the lights changing in the room and like it will do its best with the knobs that it has access to this is kind of what variational methods are given the the suite or the scope of the knobs that it can change it will do its best with a KL Divergence minimization to to make the best approximation but that doesn't even mean that the aboutness of its model is like remotely the same like one case that's often talked about well this could be a continuous variable outside and then the agent could just be saying too hot just right or too cold so like the type of variable and the aboutness of the variable can be very different David yeah so I'm still trying to grasp this and think I'm close but I'm not sure so I'm kind of looking at this in terms of analogously like unknown unknowns and how that would update our own priors um where you know we may not be conscious I don't want to use the word consciously but aware um that our priors are being updated and then we do externalize them later I guess at what point do we then re internalize them as active good good question so let's just say that we had a kind of a cognitively like structurally cognitively fixed robot um and uh then it doesn't even have the possibility to like uh like let's just say it's inferring the the location of one you know it's hunting one prey but actually there's two out there or there's none or something it's like it will keep on doing what it can without one with trying trying to find the position of one um prey item and then um higher and higher levels of flexibility could could start to address a greater diversity of real settings externally but then that unknown unknown is a really interesting question because surprise is kind of like a um an agnostic or or pronostic even um alarm signal because the an increase in Surprise due to the the temperature um reading that might be like kind of a a Known Unknown like I can deal with this kind of temperature increase we've done it before or it could be something structurally different happening in the world that basically no action is going to be able to address right and then that's where like the structure learning comes into play and it's like okay well now let's propose new latent causes like wow the one cause model of temperature in the room is is is surprising I'm going to propose a two cause model of temperature and then like that the abductive process of proposing new accounts but but even that can can still be beset with unknown unknowns right I guess I was approaching it in terms of and this I might go beyond the scope a little bit but in terms of sensory perception so like you know we have our five main senses but that's what's known to us right in terms of conceptualizing it there might be you know trending towards an infinite number of sensory inputs that you know the brain might process differently which we might find as you know our modeling gets better and we deal more with cybernetics and you know evolution in that sense aren't we kind of fixing the model in that sense and not accounting for those extra sensory unknowns that might be incorporated later and would it require then a complete reconstruction of the model as we are conceptualizing it um with the metaphor of LEGOs this is like the minimal Lego tetrahedra four pieces this is the minimal um you know pick it up and and juggle with it Four Legos that are put together that makes a total cognitive simulation now if you want to deal with where there's like higher order like volitional or adaptive structural changes to the model just by sketching it out you can see that the state space is going to expand like Mega exponentially very fast um so to do adaptive search or even persistence in like structurally volatile spaces is a very large question so it w it wouldn't um unseat the basian mechanics or make something simpler than this but certainly you could make things way larger than this graph that have a lot of those kinds of like reflexive structural modification and structure learning it's just that in practice first off not all of those um capacities have been brought from the code and the research or sorry from the research and the math into the code so sometimes that would need to be like implemented um ad hoc in in a software package but we hope to build that into the open source packages and so on and then even if the capacity has been translated from the math into the code like structure learning has been then there's often a really really large optimization or parameter sweep step because you're just increasing the the state space of the model so vastly okay so this is kind of your gen state right for your your seed growth basically so everything's going to grow out of this despite you know where it may grow too in that sense like through the expansion we can still go back to that base model yeah like the the this uh structure learning is like proposing another latent cause and then asking whether the the more um the higher model complexity has sufficiently improved accuracy and then there's basian model reduction which is you take your model and you ask like which of these parameters is like the worst and which one can I reduce out and so with this kind of like plus one and minus one parameter moves structure learning and basing model reduction that's kind of like that's like the AGI ACM kind of dream SL meme would be you'd start with a single state space and then there would be a process that autonomously or or in a scaffolded way that the agent would basically propose alternatives to its own model including like arbitrary or higher order levels of whatever cognitive State and then guiding that kind of neuromorphic evolution again it's an even larger State space yet it would have like perhaps some of these um qualities like parsimony or um like ability to reproducibly develop appropriately and so on okay great thank you well we'll return next time for more low road um up Vote or add more questions and then if like you want to make sure that it gets addressed because we didn't look at too many of these just tag or something and then at this time next week that will be discreet Time chapter 7 in the cohort 5 which is kind of like the models we were discussing today with the pdps hence why bouncing back and forth and everything with the chapters is all good so thank you all till next time bye thanks guys have a good one