all right hey everyone it's October 24th 23 and we're in our second discussion of chapter 8 in cohort 4 so we can do any number of things but does anyone want to begin with any thoughts on chapter 8 we can look over past notes we can look at the extent questions we can hear any new questions we can look at the text yeah we'll have a very simple question uh in in the continuous case is there some kind of standard code to use to try modeling I mean to test some very simple models uh as there is for the discret case with you know the SPM framework a very good question um I'll give my thought just it may not be fully accurate in the mat lab SPM based and python pmdp based code there's been an emphasis on the discret modeling in the RX and fur Julia package I believe that there's a lot more continuous time models okay so I'm not offand familiar with [Music] um any continuous time uh implementations we can like look at the implementations table that we have eventually I'd love to see um the these repositories of course expand and grow but also annotated with different features of the model so then we could filter and say I'm looking for this language continuous time hierarchical model but almost all of the Python that I've seen was discreet okay any other random thoughts or or question questions otherwise we can look at some some popular questions and look at the notes does anyone have any just even for outside of active inference thoughts on continuous and discreet time anything with digital and analog do do we see one of them as being the general case of the other or do they both derive from a point one oh yes yes Daniel um I'm just wondering what is fundamental difference between um continuous time model and discret time model is that is that the fundamental difference is in terms of the um the state transition uh like in in the discret in the continuous model is represented as a differential equation and in the dynamic model in in the discrete model is expressed as a as a kind of a difference uh question does anyone have a thought on this yeah here what does continuous mean continuous in time or continuous in the in the uh State space uh or refer to both I suppose it you know most means it means the continuous in time right not um does anyone want to give a thought on this yes it is referring to the treatment of time so here in figure 43 We have basically chapter seven and chapter eight laid out in the discret time setting then let's talk about what's the same first broadly their architecture is the same that's why they're being laid out this way what else is the same priors are set the same way similarly enough D and also the upstairs the policy selection apparatus is broadly the same we're still dealing with free energy based policy selection mechanics um also what's the same is the hidden state to observable mapping the partially observable setting with two so really what's different is kind of like the core of the Horizon here in the discrete time setting we have explicit hidden State um or external State um calculations at given time points T minus one t t+ one right those themselves as you pointed out could be continuous so that could be like a number between zero and one um but we're estimating that at 1 p.m 2 p.m 3 P.M 4 P.M right so and then um in the continuous time setting we only are keeping an explicit prediction of the Hidden State at the current moments X and then we're dealing with um the past and the future with a tailor series expansion also known as the generalized coordinates so here whereas B plays the role of a marov transition Matrix in the discrete time model and has a kind of straightforward interpretation where like you have the hidden State at a given time you multiply it by the B and then like you basically transition into the next state right the analogous role in The Continuous time setting is a temporal derivative right and so we're extending out a tailor series which technically goes forever uh usually the accuracy of a tailor series exper ion begins to drop off quite rapidly unless it's a repetitive signal um so and then in the continuous time setting I guess you could have a discreet State space like you know the light switch is either on or off and we're going to have a a model that's continuous in time of being on or off that doesn't mean it's the best way but it's definitely a good point that you're raising which is that the treatment of time has a um very strong bearing on the semantics of the generative model whereas whether a given variable like an observation or a hidden state is itself continuous or just street is a little bit of a smaller difference this this figure 4.3 okay yeah um um yeah question continue sorry one are you are you done no no no go ahead go ahead okay uh yeah so uh to go on on this uh in the discrete time um in the implementation uh within each step there is some kind of continuous updating of the neural activity associated with all the states um so uh you know how does that uh go in this picture this yeah this is also a very good question like the the this kind of interpretative yeah continuous dynamical component that seems to be extraneous or it's unclear if it's purely interpolative like if a calculation is being done here here here and here and then it just being like kind of like spline fit or rather if there is a continuous time differential equation or [Music] something but then if there was a um exact concordance of the discrete time and some differential equation solving underneath why wouldn't we just focus on that concordance I don't know if this is accessory code inside of SPM uh I mean that's linked to the to the message passing within each step So within each step you know if you have for example uh I mean usually they use 16 steps uh to uh implement the message passing uh and you will have this uh differential equation implemented has uh you know 16 different uh updates in a discretization of uh okay so then would we say that this is truly a continuous time interpolation or is it just that we're doing like 16 with we're we're treating each time step in the discret model as like in Epoch and then having 16 kind of adjustment steps within yeah the later is my the letter is my understanding yeah yeah so it could be so the there well neurobiologically they analogize this to the local field potentials which is like a EEG recording but just statistically the of change of log expectations so using message [Music] passing they can continue to run message passing um okay well let's let's assume this the trivial case which is the variable is already exactly update it's already at its stationary state so new irrelevant information comes in um then whether you message passed one time or a million times it wouldn't really matter right in contrast there might be situation which is going to be more prevalent where there is some amount of basian updating that needs to occur um the whole point of using free energy is that we're able to approach problems that we can't do one- step exact Bays on we have something that's incrementally optimizable through message passing with free energy minimization so then how many rounds of message passing do you have to do well one approach is when the um rate of change of free energy um or the rate of change of log expectations is is low um or sometimes I've seen when like the overall Delta is small like just when round after round we're getting small updates but but both of those H have a degree of freedom because maybe you could continue in that slow regime for a long time just like any uh model so they're doing more multiple message passing rounds have you run the code to to reproduce 7.13 uh not this one no okay yeah I think that would be helpful to see because other even still like does it message pass up to here and then it goes down to there and is that point two or is two well I would uh I would say that for each I mean each step is composed of three steps here uh and for each small step you have this uh you know 16 by default uh message passing did they say the 16 in the text or is that from the paper uh well that's uh the default implementation with SPM okay about the text here yeah sorry okay yes interesting I I think also this is a uh a place to look at the RX and fur um toolkit um as well as the reactive message passing um variant so in the SPM based message passing every every message EV basically one round is Advanced every single node is messages are propagated across the entire network um in the reactive message passing setting it opens the door to like adjust in time type um uh operation where different nodes can be receiving messages at different rates in fact later today um we have live stream 55.0 on message passing and then in the coming two weeks um with Magnus kudal on message passing um but it makes me wonder like is there still message passing in the continuous time model I'm tempted to say yes any Bas graph since any B Gra has a message passing duel then whatever the semantics of the base graph are whether it's describing Markov transition probabilities or whether it's describing a temporal derivative for Taylor series expansion you still could construct the message passing to do that [Music] um I I I I'm actually even going to bring in this one slide for message passing just because it uh okay three kinds of graphs I and interestingly all three of these kinds of graphs come into play in the textbook but I think in this 2017 paper they they lay it out most clearly basy and graphs nodes are variables edges are dependencies figure 4.3 base graph for Factor graph nodes are functions of a distribution over variables edges represent variables per se so here's 4.3 well here's figure 7.3 same thing 4.3 so these two are equivalent now this which is to say 4.3 is equivalent to this forny Factor graph and then the Third Kind of graph is the neural network nodes are sufficient statistics edges are exchanges of sufficient statistics so it's kind of interesting on one hand we shouldn't be super surprised because the graph is obviously an extremely General type of data so the idea that there would be like multiple kinds of graphs it's not too surprising can you give an example of the neuron Network gra which one is neur Network um to the one we just showed on page 26 there are on page six you'll have you'll have basian graph uh and the uh Vector graph right yeah so here on the top are the base graphs then okay so these two are literally identical this is from the textbook hash Tex group and this is from the graphical brain paper which I'll just put into the chat just in in case you want to see um then every base graph any bayy network can be expressed as a factor graph right so basically they contain the same NE same necessary and sufficient information however there are operations and procedures that can be done on a factor graph that cannot be done on the basian graph representation so under the hood which is I think something to investigate in SPM and Python and Julia when we specify our generative model as a base graph and then we use standardized routines for message passing it's very interesting to ask well what's happening under the hood that converts that base graph into a factor graph the advantage to the factor graph key Advantage is that as these numbers suggest you can create node local procedural Logistics so you can develop per node an order of operations that calculates that node which allows you to develop um a procedure to to accurately update the entire graph whereas in if this base graph is presented it's kind of like well you know should we should we calculate this part first or should we calculate that part first is it even going to matter um okay now the Third Kind of graph the neural network or circuit again this is this is I think worthwhile to go into because I think when they're talking about a neural network this could this could be wrong but um these are the kind of yeah let's let's look in graphical brain just to see if they give an example the neuron netor graph yeah yeah because I'm not sure if they're okay yeah okay we'll get okay here we go here's a neural Network graph so I don't know if this includes standard SL mainstream neural network architecture like conet type stuff I'm I'm not sure if it would include that um and then also we've seen neural cognitive architectural representations described as a base graph for example in let's say here chapter 5 well for so that's figure 52 predictive coding so here the these kinds of things now on one hand I see I see that in terms of um sufficient statistics of unknown variables and other auxilary variables like prediction errors that's literally What's Happening Here the mean is the summary statistic of the central tendency and then the um error residual but I'm not sure if this can be interpreted both as a base graph and a neural graph or if this is one or the other because in either case it's slightly different in its representation one key piece that I think is happening here is there's like multiple at each layer but I'm not sure if that's being if these threes or the two at each are being used to describe three time points kind of as traditional or whether it's alluding to the idea that there's a population of neurons and that the variable is a summary statistic on like a rate code should not be taken too seriously yeah I'm just wondering if if they are kind of have oneto one correspondence uh among the three uh graphs yeah let's that would be a great like that would be like the triple play with the three kinds of graphs whereas they only really show two in connection um figure 11 figure nine what but here's what figure 10 looks like so but this looks a lot like figure 5.5 right we have the yeah prefrontal um the sort of abstraction um goal selection then the descending prediction down into the dopaminergic policy selection we talked about the tradeoff between basically habit driven policy selection on the left MH and then expected free energy sharpened policy updating on the right that's like the dopaminergic tone and then the descending prediction down to the um spinal Arc with a kind of simple differential based um set point predictor Sentry motor actuator um this is like literally the same we have that prefrontal stack here we have a um policy and a g and then um ultimately a descent into the spinal Arc but what makes this question one what makes this different than just saying well it's a Bas network with a neural semantics I mean don't the nodes as unknown variables already have an interpretation as sufficient Statistics question one question two oh yeah go ahead yeah one thing is that's you know when when you have categor categorical distributions to get the sufficient statistic you need the full distribution uh where you in continuous time I think they always work with um Goan distribution you know in which you know having the mean and standard deviation is enough yeah they're they're definitely working calmly with the Gan family yeah although in the hierarchical gussian setting the hierarchical gaussian filter has high expressivity so it's not like they can only model Simple Things No No Yeah Yeah Yeah so how what what makes this a neural network are they using neural network like many machine learning researchers would use neural network today sigmoid activation function all that I I don't EX exactly expect so um so how is this different than a Basi Network part one and then are are these models in cont could these models be simply does not include continuous message passing so here they're dealing with categorical and continuous State spaces M oh see but here these generalized observations describe a trajectory in continuous time so here we see the X X Prime X Prime so this is a a lot like the derivatives over time like first order second order over time yeah okay and and and those are the generalized coordinates of motion you know the position of the car the the um velocity of the car acceleration of the car and so on and then you know the more derivatives that you have that's using the tailor series or the generalized coordinates to expand out um live stream 26 on the uh basian Mechanics for uh stationary processes or something like that that um that is very helpful like let's just say you had a pendulum just moving back and forth well its position is obviously never the same moment to moment it's changing its velocity is changing moment to moment but you're going to get to a derivative not that high up for a pendulum where that derivative is not changing and that is stationarity on the generalized coordinates or if you had something that was moving around in a circle again the position is changing the velocity would always be changing like the actual Vector heading would be different at each moment but the Accel oh it's accelerating a little bit forward and to the left it's going counterclockwise so that would be stationary so that's a huge advantage of the continuous time is like if you can find the articulation where something has a regular mechanics then the the generalized coordinates the Taylor series expansion like capture it really really really concisely whereas if you were doing a discret model of that something going around in a circle you you would I don't know maybe struggle to to pull out that Simplicity especially if huh yeah I'm thinking when you implement this uh continuous time model in Compu computer you have to discretize it the time anyway right yeah this is a great question well it then becomes a discret time if discret the time will it become a you know discret uh model or it's it's actually fundamentally different from the discret model you're you're right I would say that still would not be called a discrete time model because discrete time model the B Matrix would be you know the Markoff transition probabilities whereas here you're still trying to learn the derivative so then you take some approach like you have some delta T and then you shrink the delta T and if you find that as you're shrinking the delta T you're converging on your estimate of the derivative you know we're studying the line Y equals X and we we start with a with a Delta X of five and we find the slope is one and then we do Delta x equals four it's still slope of one Delta X so we're converged in our estimate of slope as we shrink the Delta so then we feel confident in our discretization but we're still in the continuous time setting but we're just needing to take this this digital approximation on the continuous on on on estimating the um um continuous time Dynamics Lance D Costa is is one of the researchers who has I think expanded on this some of the most because in in 26 basing Mechanics for stationary processes here's where there was the discussion of the non of non-stationary steady States stationary non-stationary steady states in the generalized coordinate settings also even with three layers like x x Prime X double Prime that corresponds to PID control in engineering the um something integral derivative um so this is a very common engineering um technique then um in 52 this question of the delta T came into play so like obviously we want the underlying free energy landscape to be smooth because we want to be like analytically well behaved we want to be able to take derivatives so we want the free energy landscape to be smooth but we're doing it on digital computers so there has to be some approximation step in in space and time even if we're dealing conceptually with a analytical space that is continuous in space continuous in time um so how does that work well in this this paper they talk about how do you do inference on that um on the delta T how do you decide how much that step should be and there's a very evocative um image so we're we're the ball is rolling to the bottom of the hill we always talk about this but we're on a computer simulating the ball rolling to the bottom of the hill so um we are going to make these discreditation now how much time should we simulate okay we have the equations that say that the ball is going to roll smoothly in time smoothly in space but we need to simulate that they talk about this um delta T question in terms of accelerated optimization like you could choose a delta T that was so ridiculously small that it basically you know it wasn't moving like it was just super super you were just modifying a parameter by like a billionth um conversely you could make delta T that would lead to these kind of like chaotic sampling because you'd be like you'd be like okay the ball's here and it's headed this way and then you just go okay now let's just put it over there and then now it's like a totally different space and then it's like whoa what is happening now it's going that way so if your jump is too big then you're you're like you're getting kind of like I don't know if You' say you're getting Alias on that space But if you're taking too large of jumps you're not actually going to detect the regularities landscape you'd be on the side of the bull and you'd overshoot and you'd find yourself way off the curve you'd have to recorrect back to the curve and then you'd find yourself going way down here and so then there's this kind of second order um inference question about what the delta T should be and then they talk about like um that in terms of the ball rolling to the bottom of the hill with different fluid viscosities so if you're like in honey you stay kind of move you never really reach a terminal acceleration velocity versus if you're like in water versus Air you like gain more speed and so that that leverages the position of the ball on the bull and the vector Direction but then you can you can knowing how far you should go is as a higher derivative question it's like if the car is going one mile an hour then there's going to be a delta T that's going to lead to Optimal sampling of that car like one second later but then if the car is going a thousand miles an hour you might want the delta T to be shorter do all of these like again I think this um again returns to the code question um do all do we just like swap out continuous time if we have a model with a given State space how hard is it to go from a continuous to a discrete time implementation like is it could we just flag it up at the top and just say we're going to do both then we're just going to see what they look like or you know is it is it a flag or or a setting that can just be called or does it require more of a structural Custom Creation for one or the other such that it's a little bit harder to repurpose I don't know in fact I've done very little with the continuous time models but the RX and fur the RX and fur it it certainly is is [Music] um it certainly is is fine with with continuous so so this is a package for continuous uh time models um it's actually a a package for um like message passing and basy graphs generally so you can make you don't this isn't even an active inference specific package and so I think some of the reasons why pmdp gets more use at least today first off more people know python than Julia second off pmdp like SPM has multiple methods that are specifically relevant for doing active inference generative models like there's a class to define the the PDP agents so that makes it a lot easier to Define whereas here here's you can do active inference but [Music] um you have to write it from scratch basically but so it's all there because if you if you can frame it as a basian graph and we know that we can then RX infer is a general Bas graph implementation approach which allows you to specify the base graph and then under the hood I believe flips it into the message passing format and actually does the inference with message passing but these are these are very so yeah saying I'm not a Julia expert either but like some features that are presented already in RX and fur are very promising let's see if there's continuous time specifically yeah good try out SPM the discret case I tried out it's it's quite nice using the SPM package oh yeah totally I haven't tried any other packages yeah I think this could be a good I mean Bert Dev rise and again we're having live stream in the coming weeks with the authors of of this package and so we can ask these kinds of questions [Music] um but just in I mean let's like this textbook the the 2022 textbook that were ostensibly discussing um is kind of the Matt lab SPM era and approach yes um pmdp represents a sort of partially first principles approach partially ported over from SPM approach and they focused their development on discreete time modeling what is the name of pmdp pmdp yeah okay infer actively the pmdp package um whereas RX and fur set out to solve a significantly more General problem of just doing basian inference but using pmdp to construct arbitrary basian graphs is probably not the best package for that but again it has all these helpful methods for making pdps um move movement is a classic case of continuous time modeling the analog realtime continuous nature of proprioception and actuation so sensory motor control this is an absolutely classic setting for continuous time modeling by thinking about what that con continuous time model is in the game of doing not as um maximizing utility a reward function per se but reducing Divergence to a set point and finding paths or trajectories in a space towards a set point already this foreshadows this kind of like sensory motor continuous layer and then a higher order discret now could be discrete time but at the very least discrete State space with like selection of different goals so you know to oversimplify the brain is send is is um setting discret preferred States for the body to be in and then the body uses its um basil intelligence to realize paths navigate paths to that set point there is a discussion I think we may have talked last week about this sum about the sensory attenuation about the Precision Dynamics and we talked about like getting up out of a chair we talked about moving with the eyes the icade and about how as during the execution of a movement you're still getting sensory feedback and so if you expect the the consequences of a movement like when I when I um move my eyes or move my head I'm expecting all of the visual input to change a certain way when I get out of a chair I'm expecting my um touch receptors to feel a certain way those expectations are calibrated out so that they do not rise to our attention and so that can be understood in terms of a transient suppression of attention which is attenuation and then once the body is fixed again or the Gaze is fixed again there's a re-engagement of precision so you know pay attention to when things outside of your control are happening but calibrate out the expected sensory consequences of action and the easiest way if you know that there's going to be like um you know if you know that you're going to drop something loud just turn off your hearing for a split second while you drop something loud instead of engaging in in some more sophisticated like noise cancellation method they then move to an ecological analogy with a generalized lock of voltera Dynamics this is also called winterless competition because like plants herbivores carnivores like none of them can like wa in they're all in this oscillatory relationship in this three-dimensional space and that's also been used to model winterless neural Dynamics like if you have two brain regions that have a mutual inhibitory relationship like the one that's on top inhibits the other one um until the rep the repression switch happens so that's explicitly brought into this uh winterless competition lock of voltera was some of the earlier models of sequential actions in active inference and this Thomas Parr in the bookst Stream 2.1 I think he talked about this so like there was sort of the continuous time this was like you know 2008 2012 The Continuous time models were great with like real time flow but doing like a um so it was all good for you know the arm close the arm okay now open the arm now close the arm but to do a multi-stage action how do you get that into continuous time so the first approach that was shown here was um like saying how do you get a multi-stage action where first I want the plant population to drop then I want this carnivore population to go up or something so there are ways to tune The Continuous relationship so that you can get um a kind of canonical unrolling in response to a certain stimuli or endogenously generated which is what they did in the handwriting case however as Thomas pointed to people wanted to account for explicit planning because even though the models tuned up to do this kind of repetitive scribble there was no explicit planning and so that motivated the development of the discreet time formalism and the PDP formalism which had been widely used for planning and now we're kind of merging back those streams with the continuous and discreet times especially in light of the recent advances in basing mechanics with path integrals G Theory these kinds of topics learning and continuous models generalized synchrony now generalized synchrony you could also think about this in a discreet time setting but it works really nicely in a continuous time setting too this was um this specifically coupled Loren's system explored more in live stream 34 on the stochastic chaos and marov blankets turns out that you can get generalized synchrony between um two chaotic systems between two non-chaotic systems and so on so you can get entrainment and generalized synchrony even when there's an analytical chaotic relationship is it hard to fit chaotic processes in real life yes probably but in the example of the two birds that are like singing where their turn taking is described by like it's like you're like spiraling on one side of the Larn attractor and then there's like a flip into the other side of the phase attractor that describes the turn taking and the generalized synchrony well and combining the generaliz synchrony plus learning with the two birds we can look at how like through when we have a learning if the models didn't have learning obviously they would just continue to act along wherever however they were in the beginning but with learning they can come to have higher Mutual information on each other closer to this like Y equals X line so that represents the the tightening of of the generalized synchrony which doesn't mean lock step doesn't mean that they're seeing at the same time it actually means that they're turn taking is sharper in its alternation then in the close missing line here in the close of the chapter they introduce the kind of folk psychological model that gets explored in live stream 46 that we discussed earlier where we have a continuous time sensory motor actuation as a leaf in a higher order discreete time model then there are these just [Music] um examples of icting with mixed or Hybrid models where we have a discrete time goal um selection and then the continuous time um Cate daian mixture model big uh topic here about iterative clustering kind of like K means clustering and then they review 10 or 12 continuous time settings so a very interesting one certainly one that we could benefit a lot from having like the same setting described with continuous and discreet time to get a better handle on whether we do we make the state space and then do we add the time layer and we can do it both is there any downside to making models that way or if we think back to chapter six and the recipe is the decision that we make about continuous or discreete time something that happens very early and in a more committal way I don't know and the way that the packages are today may not be how packages are forever so then I have one question regarding the the example giv in qu giv in chapter eight are this example like other figures are they are simulated by computers right they are not the data connect from from experiment or from uh from the nature right correct in the examples here the generative model was specified and then synthetic data were generated so that's kind of like the forward Direction and then chapter nine is where we get into the discussion of okay now what if we have empirical data and we want to parameterize the generative model but here they just set the generative model and then synthesized data however they obviously set up the generative model to anticipate the kind of data that they were collecting in the lab so this is the forward Direction and then chapter 9 is when we go into modelbased Data analysis with going from empirical data to parameterized generative models cool okay thank you Daniel thanks thank you thank youil thank you on Sean farewell bye thanks bye