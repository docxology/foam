when um man that's distracting distracting okay I just won't share screen it's what's causing it where I was getting at was okay so on the one hand we we have this discretization and this continuity and on the other hand uh we can we have this embedding of the discreetness within the continuity or the continuity uh within the discreetness um or there's like nestedness right where you like you have like discreetness continuity continuity within the continuity and so on and so forth but also uh while we have the prioritization of what's Happening there's also kind of like a tuning aspect right like a tune like there's tuning me mechanics happening right like when you think of stuff like hyperplanes and or hyper parameterization um I think a beautiful paper to look at um is biological cognition by uh I think is hubner and schulen and um they bring up this term called heterarchical processing right and heterarchical has to do with like selective tuning or you think of like a like you're an audio engineer and you're mixing you have a you have a mixing board and you're trying to get the right trying to mix the channels and all that together the right way like this tuning or this adaptive tuning rather but it's like there's like a mix of there's layers of the hierarchical and then n tuning happening right so so it's not even though I would argue it's mostly hierarchical you still have this tuning happening right you still have this sort of like this degrees of tuning happening but I'm not sure if that I like I'm I already am like about I'm on chapter five right now TR it's chapter four but I'm not sure if the book really addresses like this hyper parameterization issue but I guess you can kind of go over that if you can yeah yeah yeah that's um I think that's a really interesting point too so whenever I don't want to find the the figure I don't have it top of mine but it's I'm it's in the second half of the book and it might be in the section of learning but what are your what everything you're saying about like tuning and hyperparameters th those are absolutely like those are compon like directly components not just figuratively and in active inference models and so what what you can have is like like I mentioned Precision earlier that came up at the end of the the chapter this chapter um but you can actually like Precision can be modulated such that it impacts the rest of the model once you have a hierarchical model it's like it's not just you know every once in a while something happens here but most of the time it's over here it's like the whole model is integrated right and so you do have this really strong constant interplay um between the different layers um so so that that that hopefully at least figure figuratively gives you some sense of like the the way you describe like a continuous and the discret the discreet and the continuous it's like well as far as like coding a simulation you do have to more clearly Define one versus the other at least at each layer um but then that said they're so tightly integrated it's as if you know the discret is impacting the The Continuous in given moment and vice versa so that's there and then somewhere in the in the in the textbook there's um there's another figure showing the PDP we were looking at earlier but it also adds on hyperparameters and those very hyperparameters are what can be adjusted or tuned and they relate to Precision uh for example you could have agent learn its a matrix meaning the the likelihood Matrix which figures in this chapter and so they might learn to you know let's say we we have our Mouse in the t- Maze the agent right let's say they repeatedly go through the maze over and over and through trial and error um end up learning that the stimulus is always on the left let's say we're we're the experimenters we to choose that let's say the stimulus is is always on the left for an agent who's who's who who who undertakes or engages in learning um it's not only going to try and minimize its free energy uh it's variational free energy but it'll actually start to modify and change its beliefs within that a matrix to where it'll start to associate the cheese or the attractive stimulus with the location left in the Maze right it's belief about that um and so that's that's how that plays in but kind of tweaking to the models and so on so we have we have learning they H something like hyper parameters here in the case of precisions U are directly implicated so all that all that is here but that's a really interesting paper that you you brought up um I almost feel like other folks uh other members of The Institute might find some interest in that um found something heterarchical control mechanisms in biology does that sound right yeah um if there's like any resources you have to suggest for me to look into i' would love to take a look at it because I haven't really found something other than that one paper I mentioned it's biological bi cognition um it's I think it's published through Cambridge um yeah it's the one through uh when you look it up biological cognition it's the Cambridge article um it's paywalled but uh I was able to get a copy of it nice but uh yeah I I hate when when like really nice papers are pay walled it's like H it's such a crime you know it's anyway yeah I mean I'm always looking for more papers that talk about this hyper mization stuff um because I feel like there's just not enough out there explaining that there's so much stuff talking about hierarchy and like embedding things in hierarchies and even though I would say like the hyper parameterization requires hierarchical stuff in order to be effective it it's still not hierarchical in its nature but um yeah I I you know definitely throw it at me I'm I'm Catman do on Discord so if you want to if you DM me that those resources if you find them or even share it in one of the channels I'll definitely I'm I'm constantly eyeballing what's shared on server so yeah in uh for the relationship between discreet and continuous in the machine learning Paradigm what they actually use is a mechanism called clip uh which is simply going to take the energy relationship between this categorical and this discreet thing and then just throw it in whatever's easier which happens to be uh language in our case for uh large language models so what you do is you find the most straightforward abstraction layer uh this is going to be naturally hierarchical if we're talking about deep learning in this specific example uh you create that abstraction layer and then you just keep it consistent through the entire thing uh two other things that might be related to what you're talking about is anthropics most recent paper uh which I forgotten the name of I'll have to find it but they essentially talk about not hyperparameters but the expectation of the actual state that you're putting into it which uh they Define as a feature a feature is going to be deeply related to what we consider to be a concept like a dog or a cat or something like that and being able to take these Concepts which kind of create these little bubbles that you can create a context around there is some situation that it's talking about which has some set of features which are going to be activated how much you activate these certain features are going to be deeply related to what you're saying about these hyperparameters so deciding which hyper parameters to use is going to be found right now in machine learning through the creation of do we make this feature really big do we shut these features down to be really small and that's going to be how we control how these hierarchical mechanisms are actually going to consistently be able to produce the results that we're looking for one other slightly related thing is of the application of going through the network and having a kind of a meta Network decide what inside of the network to actually create this is going to be a very recent paper called uh it gets into a bit of the weeds but it's a mixture of a million experts which essentially takes the entire hierarchical network that we're going to be dealing with and all of the variables are going to be encoded into these on layer MLPs these one layer fully connected networks and from there we're going to find a way to create a meta Network which decides which ones to turn on and these ones that I'm talking about are going to be analogous to hyperparameters and it's taking a large Network which again talks about the context that you're speaking of and being able to decide which ones are important which facts are important inside of what we're talking about and only turning those on which I think is deeply analogous to the hyper thing but that does lead me to a question of whenever we're talking about hierarchies in the active inference domain how much does it matter you guys were talking about system one earlier how much does it matter with respect to continuous or discret variables once you get past the system one level of the direct input from the external environment that you're trying to understand do you guys think that once you enter into a hierarchy it's all just abstracted away or do you guys think that the discreetness or the continuousness of the actual input is going to have really deep ramifications as you go down the line I'm sorry if that was a lot great great comment we'll each just give a short closing thoughts in these last minutes I mean shortest answer it simply 100% dictates it it's like asking about a transistor how would it matter what the composition of the metal is or how would it matter what the size of the plate or how closely these elements are it's like it simply determines it so then there there might be some situations where you could discretize a continuous and get this or that performance retention or or the other way around but that's kind of the fun is the textbook is just showing us those fundamental motives and uh Toby SMI in the structured active inference live stream earlier today showed that in a in a far more comprehensive way than the Markov blanket formalism with the broader applied category theory for interfaces so it's like all about how it's specified like a physical transistor or Linux kernel deployment I also um this is this I don't know if this is a little too simple relative to to what we were all just discussing but I did drop a a link to one paper uh into the chat there and that is um deep active inference and scene construction and just given we have such little time left I just wanted to drop something there is U what with as a kind of combination of things we're talking about including hierarchical modeling um and here the idea of like looking at particular features that when combined maybe allows for let's say class ification or categorization of uh in this case the paper I shared like a visual stimuli it's rather interesting they put together so a two- layer model where um it's like a sod experiment so it's like uh someone is viewing a 2 by two grid and then they make inferences based on particular things that they see um in each of the um uh coordinates of that 2 by two grid and then they uh from there have to infer certain aspects of what's going on in each of those two grids and then that gets pushed up to a higher discrete level that allows them to uh to make an inference as to how they should categorize the entire uh scene the entire 2 by two grid like what is going on in there it's based on a previous experiment that's very similar but uh it would just be um it it's it's a way to kind of simplify uh a lot of the principles we're sort of talking about here uh but also being able to see something like trying trying to theorize like how does scene construction happen if you look at a broader uh expanse and you're trying to figure out like infer a belief about what is going on there well your eyes are limited and so they can only look at certain points so how do you start to construct represent a belief about what the overall scene is based on particular features that you look at and you have to make inferences about those individual features in the first place so you know for me to look at a you know an overview of like a street and I don't know exactly where you know that is in the city I have to look at different parts of the overall image I have to infer oh that's a particular sign that's particular building so on eventually to the higher order inference of where am I what what is this location so I just thought that one would be interesting um I'll try to add other things that that maybe come to mind uh in the Cota um and uh yeah really really interesting talk today so thanks everyone thank you fellows see you yeah thank you both so much bye guys bye