hello and welcome it is April 29th 2024 we're in active guest stream 82.1 with Robert Warden discussing basian modelbased cognition the requirement equation thank you Robert to you for introduction and jumping into a presentation and discussion thanks for joining hi I'm Robert Wen and I work theoretical neurobiology group at UCL London um I'm going to talk in this live stream about an equation called the requirement equation which has a lot to do with free energy principle and and active inference but it's not the same thing um and I hope this is the first of a series of live streams talking about how we apply these ideas to 3D spatial cognition particularly and eventually to conscious but to get on with this presentation here's a summary of the key ideas the main idea of this requirement equation is to ask not ask what brains have to do not ask how they do it and so this approach is different from the free energy principle and active inference but I believe it complements it in a way I'll try to explain so as a statement of what brains have to do there is this requirement equation which is a mathematical equation essentially brains need to compute it or to give the same answer as if they computed it to give the greatest possible Fitness to their owners so that's it's a statement of what brains need to do and it's not a statement of how they need to do it doesn't say anything about neurons or pre- energy minimalization or anything like that it simply says to get as fit as possible a brain has to do this and I'll describe what this is and it turns out this requirement equation is like bases theorem it's very similar to Bases theorem but it has some extra terms in it and you'll see that and because it's the fittest possible brain brains have to evolve towards the requirement equation towards doing that thing and the trouble is the requirement equation itself is too expensive to compute and brains don't compute it so um animals don't compute it but they have to produce very similar results to that but we can compute the requirement equation we can compute it by brute force and this makes no assumptions about how brains do it and so that gives the best possible Fitness that's a kind of yard stick against which you can calibrate any model of cognition and in particularly you can use it to calibrate and Test free Fe and active inference models and I'll will put the Viewpoint that they are really approxim to this equation and so we can test active inference models by comparing their results with the requirement equation results so the first part of this talk is to derive this equation and show you how it works the second part of the talk is to show how it can be used to test and calibrate active inference models and the third part in point six is just a few remarks about internal basian models Okay so to start how do we characterize what a brain does without making assumptions about how it does it and the first characterization is a black box that takes sense data as it inputs and produces choices of actions as its output so that's all a brain does takes a set of sense data D which can include Vision touch all sorts of stuff and then it chooses an action now we want to go a little bit beyond that Black Box model and we're going to what I call a a gray box model shown in the lower diagram and what happens is this the brain examines it has a set of possible actions AI I is 1 two 3 Etc now for each of those possible actions it calculates some real function f of d and a and that real function depends on both the sense data and the action it's going to choose so f is a kind of expected payoff for that action in these circumstances and then the brain Compares all the possible FS so here we are on the bottom left diagram it's Computing a bunch of different Fs in parallel and then it puts them all to comparison and it picks the biggest one it picks the best possible payoff from uh any action and so this is a gray box model it's it's assuming the brain looks at all the Alternatives and chooses the best one but it doesn't assume more than that it doesn't assume how it computes the fs or what the fs are it just assumes that it Compares them all and picks the best one so that's kind of minimal set of assumptions about how a brain works now we'll go on to put that set of assumptions inside a real life situation where we've got probabilities and this comes in the next diagram this is a kind of meta model of how brains work work life for an animal is divided number of different domains where it has different kinds of decision to make and it has Encounters in each domain and we show here a picture of one encounter in one domain so we start on the left there is s which is a state of affairs that's a state of the world and it has a certain probability a prior probability P of s and the animal doesn't know what that state is of course but the state gives rise to sense data in the animal so these two arrows here show sense data about the external world and sense data about the animal's internal state so the animal has some sense data D which it receives with a conditional probability for any given State s it gets sense data D with a conditional probability P of D given s then it has a decision function f that I talked about in the previous slide and it computes f of DNA for all the possible A's it might think of uh of doing and then it chooses one of the actions a and what happens when it chooses an action is the action a and the previous state s lead to a next state which are called S Prime and so that comes about with a certain conditional probability again P of S Prime given s and a and that next state has a certain value to the animal and that value relates in some way to its lifetime fitness and so that's in principle the computation and the assumption is the brains act as if they've computed F ofda to choose their actions and this as if will turn out to be very important so how do we use these probabilities in terms of an actual situation the mean payoff an animal has loads of different Encounters in its life and the mean payoff is just given by this expression and so W is the average over all the states of Affairs he going to encounter weed by their probabilities and then each state of affairs gives it some sense data D with a certain probability so it gets all the possible sense sta that it might have then it makes a decision chooses an action AF ofd and that produces a new state S Prime with a probability P of S Prime given s and a of D and there's a value for the new state value for the animal of a new state for instance if it's dead the value is zero and so on so this is just a straight application of probabilities the previous diagram and you get that formula and um then you just rearrange the formula slightly what you do is you shove this first term PS inside the second term and then you write it this way W is a sum of functions J of D and J of D is this thing and it's already beginning to look very like bases here you can see but it's got some extra stuff at the end it's got these values stuck in at the end so that is the first step towards the this requirement equation and to understand how it's used what I'm going to do is to choose a very simple example of an animal um a simple special case and the simple special case come out here is an animal that has only two actions and I think you can imagine something like a clam living in the sea it has two actions it can open up and try and eat consume food or it can shut down it can clam up and so those two actions are called action one and action two and this simple animal has very little sense St it only has one piece of sense data it can say detect the cinity of the water or the temperature of the water and that thing it can detect with it sense state has values between n and one and now we consider various kinds of this simple animal firstly we consider two no-brainer animals a no-brainer is an animal that doesn't use its sense data and just makes the same choice of actions the whole time and so the animal that is open all the time it gets value H1 of D and depending on D it gets a pay off positive less positive more positive and so on an animal that stays closed all the time gets a payoff H2 of G and so you can you can see that for each of these two no-brain animals the average payoff is just the integral of P of D * h of D over this interval not to one so that's the payoff for a no-brain animals either W1 or W2 but an animal can do do better than that if it uses it sense data to choose which action to take so a brainy animal can choose the correct H that gives a bigger payoff in in terms of any sense State D so a brain animal can ride the top of this curve it can go down here along the green curve along the red curve and finally along the green curve now that gives the greatest possible payoff the greatest possible average uh payoff in all situations so it's the the greatest possible Fitness so a brain animal that can compute these two functions H1 and H2 and can always choose the largest one is going to get the best possible Fitness and if it gets it wrong if it's got some inaccuracy in the way it commutes H1 or H2 so these overlap points move around a little it gets less average fitness uh quadratically away from that best point so that's what happens in a very simple case where you got two actions and one real piece of sense data now we're going to generalize back again and you can see what kind of decision function this simple animal's got to make the decision function it's got to calculate is basically the average fitness uh and and that gives it the greatest possible Fitness over all situations so if we generalize away from this to the more General case can you see all that slide I hope you can yes um so we generalize this case to when the anals got more choices of action say five actions or an infinite number of actions and more sense data different dimensions of sense data and you can see then you generalize away from the simple case to the general case that the best possible decision function is given by this equation the decision function f is the sum over all states of the probity of the State uh weighted by the posterior probability and the sum overall final states of the probability of the final State weighted by the value of that final State and we could simplify that by this equation but that equation is basically a definition which doesn't say anything about what happens inside a brain how it computes it it says if a brain chooses the same actions as if it had work at these equations it gets the best possible Fitness so the requirement equation which is is here says what brains are required to do and any brain which chooses actions as if it had done all this has the best possible Fitness so that means that brains will evolve to choose actions as if they computed this equation which you can see is very like baz's theorem and so that's the first result that comes out of this work is that the basian cognition the assumption that animals use basian cognition is not really a hypothesis I believe Carl friston has said has been quoted as saying that is a hypothesis this result implies it not really a hypothesis it follows from a piece of analysis that makes no assumptions about how brains do what they do so um I should say this derivation of the requirement equation has some similarities to partially observable markof decision processes and some differences you'll have to look at the paper to find out those so that if you like is the end of the first part of the talk which is deriving this equation and uh saying that is a way of analyzing what any brain is needed to do the second part of the talk is how you use this equation so all terms that you put in the requirement equation all those probabilities depend on things happening outside the brain they depend on states of Affairs in in the environment they depend on sense data they depend on what's the payoff what's what happens to the animal after it takes various actions so it's all external biology and you don't have to make assumptions about what's going on in the brain to compute it that I think is a big Advantage now we can write down the requirement equation in lots of different domains and to do that we have to model biology which we can observe directly rather than modeling brains which we can't observe very well the trouble is Computing this requirement equation is very expensive it's expensive in four different ways because to really compute it you have to sum over all possible States s you have to do the parallel computation over all possible actions a you have to sum over all possible subsequent States S Prime and in some domains you have to to compute the value you have to have branching look ahead to different future situations so it's very expensive and it's not tractable in animal brains they it would be too slow to try and too expensive to compute so animal brains don't compute this equation they do much cheaper as if calculations now having said that animals don't compute it we can compute it because we have digital computers and we can do the Monte Carlo Integrations and we do the optimizations and so on to do all of this by brute force and that's what I will call Brute Force basan computation and that we can compute it it acts as a kind of yard stick which we can use to compare actual animal behavior how well they compute their choices of action and any cognitive model including models built in active entrance so um that I I'll skip over this slide just to say you can apply this equation in all sorts of domains skip right so coming on to active inference Fe and AI models are not like the requirement equation in that they do make assumptions about what happens in brains they assume there's a generative model which is a simplified version of the actual generative process some simplification that the animal uses in his brain they use variational model fitting they minimize a variational free energy minimize a k k Divergence between that and minimizing a k Divergence is the process and they have a certain model of the fitness payoffs of actions and they are represented as bias prior probabilities and the animal has a goal to minimize its surprise and an active inference um it predicts how animals make the tradeoff between exploration trying to get sense data to uh help better future decisions and exploitation here and now so there's a whole load of assumptions in there which are different from the requirement equation and so for any Fe or active inference model we can test those assumptions against the requirement equation and this procedure to do that is is here you first compute the F AR model which you've been doing already anyway and you define the equivalent requirement equation and so you make biological assumptions about the states in the environment about the sense data about the consequences of actions and so on and then you compute the requirement equation and that as I said you can do by Brute Force integration and optimization and then you compare the two sets of results requirement equation and Fe results you compare them in terms of the Lifetime Fitness how many encounters of this kind does an animal have in his lifetime how much does the fitness vary between the two and you get a fitness gap between the requirement equation and the F uh result and the requirement equation is in some sense bound to come out fitter and so there'll be a gap and if that Gap is significant even a very small Gap 1% natural selection will have improved on the Fe model in other words if there's a 1% difference in Fitness between two animals in a 100 Generations the fitter animal come to dominate the population and based on that you can use you can refine the FP model in other words you can ask questions like what's the sense data which makes the largest Fitness Gap what can we do to get the FP model better for th those kinds of sense data so I'll illustrate this process here um some animal has a Lifetime Fitness which you measure percentages we compute what that is in the requirement equation so that's that point there we also compute what it is in the Fe and those are different um and we can also by the way look at Nob brainer animals we just make same super decision every time or real animals all four points on this C graph can be computed and the interesting comparison is here between the Fe active inference and the requirement equation so the question we're asking is how close is the F model to the optimum and what is the gap between the two and what difference is that going to make over say 100 generations of natural selection now I think this comparing with the requirement equation is a very useful thing to do for an Fe and AI models it's a win-win test if the model comes very close to the optimum then you've confirmed it can't do better than that but if it's not very close you can work out how to improve it you work out what sense data is not doing so well for how to make it do better for those sense data right um now we come on to the third part of the talk and the headline is that brains are not always basian I know uh it's it has been stated an assumption that brains are basian the requirement equation tells you exactly what are the circumstances under which they are basian and under what circumstances they don't don't need to be basian and it turns out that in complex domains where there are complicated choic of actions the brain needs to build an internal model and the two most complex domains in that respect are two-dimensional navigation finding a way around and 3D spatial cognition and there's always been a kind of debate between in modeling people and in activist people who say the brain just reacts to events you can prove this way that in complex domains an inactivist brain may do a job but it doesn't do as good a job as a brain that makes an internal model and so you can prove in certain domains main of the complicated ones that you need to build internal models you can't build that prove that in all domains in simple domains models aren't needed and these are domains in which there are very few choices of action or very simple States or simple sense data and in these simple domains the as if computation can be uh very simple it can just be a matter of um some very simple neural relay and no internal model is needed and an example of that is a special case that I use to motivate the requirement equation where you've only got two possible actions you don't have to build a complete internal model of what's going on you simply have to know the thresholds at which you switch from one action to the other and this can be seen in terms of information capacity what comes into the brain is some basium priers which have made it evolve and some sense data and it makes an internal model or may or may not make an internal model and then it chooses actions now if the choice of action is very small it's very small information content then it doesn't need to make a big complicated internal model in order to make that simple choice so the information in the internal model needs to be less than the information in the choice of actions and if the choice of actions is very simple small information content there is no point in constructing a very complicated internal model it's a waste of uh effort so if you regard the brain's internal models as an informational Channel between sense State and actions then you can see the information model is very necessary in complex domains but not so necessary in simple domains now now as a further comment on this um I say you should not trust the good regulator theorem now in Fe literature the good regulat theorem is often cited as support for the idea that brains need internal models I'll just very briefly say I don't believe it does give good support when you compare it with the requirement meta model and the good regulator theorem which was paper in 1970 by con Ashby that has a couple of different metamodels of what cognition is about and it's based on Ash's ideas of homeostasis but I think the argument they make Con Ashby make from the good regulator theorem I don't think it really holds up and I've just listed the reasons here um there's the good regulator result every good regulator of the system is a model of that system here's the requirement equation result in complex domains only the fittest brains canut a model of the domain now the regulator model I'll assert is not a realistic model it lacks some key ingredient and uh it's less realistic it assumes a homeostatic picture of a brain where the requirement equation analysis is much more flexible and realistic but mainly this fourth point the regulator concept of a model is not a graduate ated it's a yes no All or Nothing concept and what it says is a model how they Define a model you can have any function of the system variables can be a model of the system even a very trivial function and so the concept of model they use is a binary yes no concept but the real meaning of you of model in everyday life and in science is a graduated in other words the more complicated the more detail there is in a model the more it counts as a model so um this is just my personal uh beef you should not be using the good and regulator theorem to justify internal models a few remarks about how brains evolve because obviously this analysis depends totally on the idea that the requirement equation defines the fittest possible brain and brains evolve towards that Optimum and as I said even a small deficit in Lifetime Fitness even a 1% deficit in Lifetime Fitness over a 100 Generations that gives enough selection pressure to make something start dominating the population but if the probabilities in the habitat change rapidly brains can't catch up they only evolve very slowly by random mutations and therefore if the properties in the habitat are changing brains need to learn and they need to learn fast within a short animal Lifetime and you can actually apply this requirement equation analysis to learning and animal learning it turns out is very close to the kind of learning you get out of the requirement equation on the other hand if probabilities in the habitat stay constant for very long time brains can evolve to come very close to the optimum and the key place where this is the case is spatial cognition 3D spatial cognition where the states of the world have to obey some very strong constraints they have to obey ukian geometry and they obey constraints of physics and these constraints have been true for all EV evolutionary time so animal brains have evolved to embody those constraints very precisely uh a final point is that animals we know have very precise and sensitive sense organs they would not have evolved if the brain couldn't make the very best use of the data from them so here's a plug for the next live stream the next live stream is applying these ideas to 3D spatial cognition understanding three-dimensional local space is actually the most important thing a brain can do because it has very huge impact on Survival because it's a 3D model of local space is used to control physical movements at every moment in the day and if the model is as inaccurate mistakes in movement can cost the animal's life any time and they're also used for recognizing things and action so the selection pressure to get the internal model of 3D space good has been very strong and sustained over 500 million years since the camera in explosion When Animals first had Limbs and precise sense data even insects control their movements very well and Consciousness shows that our own internal human internal model of space is a good precise model so the 3D model of space is a very good model and what this next live stream is going to talk about is how you build that how you build neural models of 3D local space now people have there have been very few attempts to do this there are no working neural models of 3D spatial cognition that I know about one of the reasons is the architecture because how do you what's the neural architecture to represent three dimensions of space neural sheets can represent two-dimensional space but there's no equivalent Arch architecture for 3D and the key problem is neural spatial memory if you represent spatial coordinates by neuron firing rates it turns out that that memory is too imprecise and too slow insects for instance have a tiny fraction of a second to work out what their surroundings are like and in that space of time you can't get precise representation of 3D space using neuron firing so in the next live stream I will introduce the proposal that 3D space is not represented by neuron firing at all but it's represented by a wave in the brain and that wave is actually a projective transform of 3D space and that's the same Transformers used in in the projective Consciousness model PCM which is an Fe derived model of Consciousness so that's the commercial for the next uh live stream just to summarize on this live stream gosh I've been quick 30 minutes um there are two kinds of modeling you can do you can model by biology or you can model the brain and modeling biology is a great deal easier because we can observe it we can observe easily uh what's happening to animals in their habitat whereas modeling what happens inside the brain is more difficult there's much more hypothesis and guesswork about it and the requirement equation approach which I've talked about in this talk only requires modeling biology only need to model events outside the brain and you can compute this requirement equation in any domain you like and it defines the very best A Brain can do so it's a kind of yard stick against which you compare actual brains and models of brains so you can compare the requirement equation with any active inference model and that I say is a win-win for the model because if the model comes close to the optimum then you've confirmed it but if it doesn't come close you can use what you learn to work out how to improve it and I would offer to work with anybody who wants to make that kind of comparison with their active INF models okay so I finished rather ahead of time um questions thank you Robert all right thank you for the presentation there's a lot to discuss and also people can write question questions in live chat while I'm just like recro everything um do you want to give any like context how did you come to study this question and how did you get to this equation and why um I got to this equation about 25 years ago actually um and I came back to it when I joined Carl's group and got interested in requirements in Fe and so on and I realized there was a link between it and and Fe and active inference and I've just been working recently to work out what the link is and this is what I think the link is um but I think it's a a very basic foundation for studying the brain is to not make hypothesis about what is how brains do what they do but to just analyze what they have to do and I think this is quite a powerful approach in all sorts of domains hope to convert one to people to yeah okay while questions are kind of coming in I'll go over a few of my notes just to go over some of these core pieces because I think there's a lot lot to say and it's really exciting um in quantitative evolutionary population biology Fitness is conceived of as like a top down Force whereas the generative model is built up as like an agent level view from the inside like agent based modeling with kind of a top down pressure and and that's one uh way to look at that kind of setting and I got the sense that I I want to explore and learn more about that the requirements equation sits basically between those two and sets the high bar that gives a requirements for the agent-based model because you can always just build an agent based model than have a constraining force we know from simulations that that all does give um Improvement of algorithms uh in simulations but it's kind of an open-ended search and there isn't necessarily A calibration what do you think about this is a calibration yeah if if you are prepared to analyze an animal's lifetime in terms of number of different encounters like each time it goes out to get food or each time it meets a potential mate or each time it comes across a predator and say what's his probability of surviving all those encounters then you can start doing the requirement equation analysis and you can find in principle you can find the very best it can do and that is an endpoint if you if you got your FP model and you're improving it improving it this is a kind of end point Beyond which you don't expect to get but you know when you're getting near awesome and here's one other kind of angle that I was noting so the Standalone Baye equation Baye with no additional terms is like this unified imperative for sense making it can cover discrete or continuous State spaces single Dimension multiple Dimensions there's even considerations for like how many parameters should be Bay optimal but it's it's all framed in terms of this um unified sense making imperative the requirements equation and and so then one of the key steps in active inference is to expand that unified basian imperative to not just the sensemaking but include action like a variational autoencoder with action the requirements equation says well it's a a little bit more over the lifespan than just optimal sense making about about sense and action action but not sense making has Fitness or survival consequences so then there's another term that applies to only actions because there's a direct Fitness consequence of actions and phenotypes whereas the internal sense making is actually shielded from direct selection because it has no consequence except through the actions in the black or the gray box yeah I mean I I think active inference is a particular way of making the tradeoff between I mean the way I understand it it's a bit simple the trade-off between exploration and exploitation in other words do I fly higher so I can see more prey or do I go down and actually catch that bit of prey and the requirement equation can Encompass that because the value that an animal gets from an action can be direct exploitation that gets a piece of food or it can be exploration that it learns something that it can use Downstream so that kind of tradeoff is made in active inference there is also the the apparatus in the requirement equations we making the same tradeoff and see if you get the same answers Just One Last Framing and then I'll go to the live chat questions just at the purely empirical even just putting aside The evolutionary setting it's a common theme to make a model and then do a variance partitioning on the residual and find if it's loaded or associated with anything so when we're studying bird migration and empirically 80% survive some stage and then we're making an agent based model well we don't want a model where 100% of them survive because that we've kind of overshot how successful so then the empirical path could be like okay we just made a simple version and it 50% of the simulated birds make the migration how is that correlated with the actual and then how is the what's left in the residual oh It's associated with age okay now we include age as a factor in the agent based model that eats up some of that variance how is the remaining variance associated with known or unknown factors and so there's an iterative process where you can discover potentially Associated variables and have that in feedback with the agent based constructions like the agent responds to requirements as modeled and then it looks like they're doing something else that's required that you don't have in your requirements equation so then but it sets a kind of local calibrator again to what kind of performance envelope could even be expected or relevant so that you don't have in silico experiments where like bacteria are trying to read a book and a person is trying to do a metabolic thing that they can't do it doesn't make sense we all have a feeling like well that's not the niche it's not what they are being selected on for and so this again helps put an adapter in between the kind of bulk properties of the niche and then the specific performance that gets all the way down to thinking like you mentioned about like the survival through time as being like the joint distribution of many many many many many encounters yeah I mean I I agree with all that I I think the requirement equation can be used to do those kinds of analysis when when you when you formulate it you're really looking at Lifetime Fitness and so you should be looking at all the things that influence Lifetime Fitness including things that are nothing to do with brains at all as you say old age whatever but nevertheless the requirement equation it gives you I I think starting just from FP models and AI models you have the model on the one hand and practical observational animals on the on the other hand and you've got nothing in between but requirement equation is some in between them that you can say here's this requirement for what a brain needs to do and if a brain does it absolutely as well as that then here's the survival so you got a midpoint in the calibration between your brain model and your real life awesome okay I'm going to go to read some questions in the chat if whatever you want to say or answer or well just however you see okay B asks is an activism exclusive to internal representations I always think of it that way an activism is generally the idea that you don't need internal representations you can just sort of do something um and in simple cases you can in that simple clam case the clam case only has to know what are the three thresholds at which he has to switch over from open to closed that's an active IST I think an activist is usually hostile to internal representations and they are in opposition and what I'm saying is the requirement equation tells you which circumstances one is right and which circumstances the other is Right awesome one other free energy principle paper that this reminded me of and then I'll go back to the live chats was by yela brunberg in and others in 2018 the anticipating brain is not a scientist the free energy principle from an ecological inactive perspective which discusses it's kind of like the good scientist and the good engineer like it's not just the pure sense making imperative when action is considered it's not just about accuracy and action because actions have consequences in a way that the sense making component doesn't because the black or the gray box is set up so that the sensemaking doesn't have consequences except through action that's that partitioning if you explored a model with telepathy where there was direct consequence for internal States then it would be basically as if that was an external or an action State because it would just make those yeah representations external facing so it's like whichever States aren't external facing are the internal ones whichever ones are external facing and then those can be under quote no selective pressure if they're all all that's like a novelty type or just an open diffusion space or it can be under strong selection pressure which just means that there are higher consequences for a certain distribution but that's a distribution that you're actually having the agent model converge towards and maybe it NE maybe the population convert maybe the optimal height would be like 15 ft and it just there's so many other trade-offs that that actually the population just continues to push up against five but at least there was a way to say well it only got to a third of the value but that's the highest score is 30% yeah and that sort of approach sounds very congenial to what I'm doing I have to read the paper I'm afraid I haven't read it yet but certainly will do okay all right Benjamin asks how does the amalgamation of maravian Dynamics and basian Theory reflect the necessity for brains in complex domains to construct internal models for better adaptation and decis decision making how does the combination of marov Dynamics yeah how how does how you brought together I think this equation you're good sorry um this equation certainly builds in maravian Dynamics I mean the probabilistic model of the states is a maravian model if there are certain State transitions in the world then those are modeled so I um I believe that that's encompassing this approach I don't know the word maravian changes things particularly okay Benjamin asked another question as the brain has evolved do we know which of these definitions of brain functions are inherent versus emergent and how does this potentially impact their functioning as they evolve collaboratively or indep dependently um seem to be two questions there can you read it again yes oh we have many amazing um conjugate question askers um how do we know which of the definitions of brain functions are inherent versus emergence I think that's the first part inherent versus emergent right and then how does that impact their functioning as they evolve potentially collaboratively or independent L well the requirement I've tried to State the requirement um in know words what brains are required to do as in the abstract and making no mention of how they've actually evolved to do it and that applies to brains at the very beginning of their evolution when they were totally unfit um and they gradually got fitter by natural selection so inherent versus emergent I mean what what is emergent is brains getting closer and closer to an Optimum given by that equation the equation doesn't describe how brains are it describes where brains are evolving towards so that's the first part of the question now the second part of the question was a different contrast which was how does that potentially impact their functioning as they evolve collaboratively independently now collaboratively or independently there's a huge amount of question there I mean obviously um social insects for instance uh their brains work collaboratively is that the distinction we're talking about I mean again I think the requirement equation is applicable to social insects that don't have they're not so concerned about individual survival they're uh concerned about survival of the genotype of the hive if if you like but I think the equation still applies to that but collaboration and competition are very complicated areas to analyze well definitely especially next time I look forward to going very deep into the insect questions um I'll I'll give some thoughts though I mean to the first half in inherent versus emergent without just saying it depends on how you define it it can be emergent through development that certain functions arise so that that can be both inherent feature that was the sense of the question was it emergent in a lifetime I don't know what the sense of the question was maybe I didn't get that right certainly um emerging in a lifetime yeah um because it's optimizing Lifetime Fitness you have to consider different periods in the lifetime and whether the brain gets fit in adulthood by having leared along the way or having inherent um innate traits uh is a choice that brains have but um uh as I said the requirement equation can be applied to learning and that's a very interesting application I haven't done talked about it at all here but it can be and there's a result a book from Anderson in 1990 which shows that data on annual condition the sort of rats and pigeons in cages and so on um agrees very well with the basian model the requirement equation model of how what the best possible learning is so that's um another domain in which um the Basin assumptions are confirmed interesting this is making me wonder about abiotic constraints and biotic constraints for the requirements equation so maybe one setting is that the external environment of the organism is very harsh like it's a desert ants colony and so a lot of the selective pressure and the differences amongst colonies like in their persistence or their survival is related to their performance against the environment and there's another setting where maybe the environment is materially abund but then there's a lot of Dynamics like Game Theory and sexual selection and like these internal so how how does the requirements equation I mean if we have to change that as often as the weather changes then what are we really getting by just setting this local requirement which itself could be changing all the time or variable across settings that's a great question I mean in principle the requirement equation takes as its starting point natural selection not sexual selection because sexual selection as you know doesn't make animals doesn't always make animals fitter and sometimes makes them less fit as in the peacock's tail and I've got a whole separate interest in how sexual selection has acted on the human brain particularly and in terms of Social and sexual competition um and that I think the requirement equation is not not a result of that requirement equation is what you get after natural selection what do you mean after natural selection well natural selection uh you have to have the fittest possible the fittest possible animal is one that survives to adulter with the greatest probability and gets a mate with the greatest probability getting a mate with the greatest probability is not as simple as surviving with the greatest probability because it's a competitive activity and you get positive feedback effects for instance as in the classic Peacock case where female peacocks prefer males with long tails and uh and and so that gets baked into the genotype by a kind of conspiracy between fem female genes preferring longtails and male genes having long tails but it's not good for the peacock species in terms of Fitness male PE peacocks are less fit because they got big tails and you could have similar things like that happening inside the brain and I have all separate uh set of ideas which I would love to give a talk about sometime that natur that human language is actually a consequence of sexual selection um perhaps we could live stream that sometime but human language hasn't made us more fit in natural habitats there's some controversy for you wow to return to one other potentially interesting um aspect of of Benjamin's question there with the second half collaborative or independent uh challenges so this made me think about how the factorization of the problem to be solved like the sparsity of what depends on what in the problem may be very disjoint from what is connected to what in the organism so that's kind of widely been recognized by like evolutionary constraints or just evolutionary or tray architectures like yeah if there's a covariance in the niche um between two factors and then that is not the case in the organism then there can be like separated movement of those Ates or you can have the opposite setting where two things are I mean there's also pieces that are correlated just like having length and volume like having a longer body Le other trait to change I would guess that in terms of physical evolution of bodies uh you can get two traits that have to be correlated like long legs are inevitably correlated with something else but in terms of evolution of the the brain it's not so constrained and you can have different cognitive functions that evolve independent of each other much much more freedom there that's just a guess yeah that's very interesting and the kind of there's a lot to say about that and also with um very fruitful to consider the differences with the mammal nervous system and with the insect nervous system and the ways in which the insect nervous system helps clarify certain sensory mappings that get very lost in the large mamal Brain yeah I agree insects are very worth looking at um okay so anyone else can write questions in the live chat but I mean what other directions would you like to explore how how do you think this moves forward given where it is now well as I say I'd be very interested if anybody's interested in taking an active inference model and kind trying to calibrate it this way I'd be very happy to work with them too I would do the requirement equation side of things and uh that would be a very interesting comparison for me to make but say we're going to make the next live stream in in a month's time and that's be about spatial cognition that's the main Direction I want to take this yes moving things animals with spatialized nervous systems and then also nervous systems that enable movement but don't necessarily need to be spatialized like a worm may just have a topological or tactile mapping but it may not need to have the kinds of echolocation or Centerplace foraging components of a bat or a be I agree I agree um am I allowed to play with Robert's equation is that okay of course you are of course you are okay um all right so this is very cool it sounds like in around 1 month we will have a session on insect brain and forging spatial cognition and then there's probably many other things we can continue to talk about and also it would be awesome if um somebody has a active inference model or some other kind of model that they could reach out to you and try to get some more empirical and specific well has have you applied this to any specific like empirical or published cases yet so people could see like what the kind of anal is is um well the main analysis is coming in the second live stream C can I just uh move to the slide that um shows where you can get hold of the paper yes so there is the link to get hold of the papers yeah and there's an application which we'll look at next time as well but there's a very um interesting video game slash executable simulation Bas language you.org Bas language. org cool all right thank you very much Robert thanks everyone for the comments the questions in the chat all right okay great thanks farewell bye bye e