all right welcome back cohort 5 we're in our second discussion on chapter 7 so is there anything anybody wants to begin with can look at the text we can look at any question we can add a new question we could look at pmdp anything um anyone can write in the chat or raise their hand or just go for it yeah so I have a sort of more more open-ended question I've been sort of um thinking about you know doing a little little modeling project and um what I would like to se what what I'm thinking about is that I'm just wondering how I can use this active inference Machinery to model some kind of um kind of like a recognition task like how would you approach doing something like an amness classification with um um active inference but what I am interested in is is bringing this you know one key difference um from the active inference like um approach to um the problem that's you know basically completely solved in the using artificial neural networks but in a in a way that um you know when you if you feed amnest to um like a Transformer based classifier you patchy it so it turn it into small little chunks of image and you feed that into a Transformer and the Transformer kind of like sees the whole sequence and then finally learns how to classify an image I'm just sort of wondering um thinking about like how to find some sort of connection between between doing something like that except getting the neural network or some other kind of you know um system to choose where to look to to to sort of model the behavior to be almost like like a sakad like I'm seeing a part of an image and then I have to make a decision where to look next to um you know um resolve as much uncertainty as I can about what what am I seeing and it doesn't have to be something like an amist it could be like a really really simple images to start with you know like 8 by8 pixels some generated I don't know kns and Crosses or whatever but I I don't really even know where to start with that um it's a it's a very open-ended big big thing but if you have any any suggestions worth look I would love uh some pointers yeah great great questions well certainly what I'll show now is not the end of the story but I think it'll provide some motifs that you can pick up on here's a 2020 one paper with axle constant so in this setting it was an icade Visual and one of their Novelties was that they made a bounding box that was kind of like the zone that was visible in the Cade so the Cross Focus is the center of vision and then the affordance of the iate is to move to another cross um and then that would move the bounding box um and it's a cultural pattern recognition task like based upon identifying these higher order patterns of um Pottery so that's one visual way I think this is a super fascinating thing because in almost all visual recognition tasks um either the whole image is passed through if it's sufficiently small like mnist or it's ch tokenized or chunked or however um but here we bring the action into Vision with actual guided cognitive Vision moving to uncertainty so that might be um way more efficient in certain settings um for example recognizing a person like first there's like the movement just empirically like there's like movement to the body and then there's a movement to the face and then identifiable parts of the face that's the pyade model and then on the mnist the recent uh structure learning paper actually specifically they they do amnest um and they [Music] um so the labeled data in amnest are the true identity of the digit and then they use their structure learning technique to um develop the the Styles handwriting styles of the digit so they're not doing structure learning on how many digits there are as as far as my reading but they do look at different styles like these are different styles of the digits so then it is learning a it's doing it's saying how many styles of one are there and then it's finding it through um structure learning which is the proposal of a additional latent factor and then basian model reduction pruning the model back down for redundancy um they find strong performance on The Mist the code is in mat lab and I've looked into it with some language models you might be able to bring it into something out of Matlab or just isolate it into pseudo code but the mat lb code itself it's a little bit of a um it it it integrates a lot with the broader SPM package so it's not just like you can just pop out the the mat lab script and run it because if you follow the the the rabbit's Trail it goes into a few other SPM scripts that I don't think have been brought over into pmdp yet but from a pseudo code perspective it's very um informative awesome that looks super relevant thank you yeah but this one does not have Cade this one the amness digit is coming in um fully one interesting thing also on the on the cultural one this um generative model was actually this is before P DP this generative model is the one that we utilized for active INF for ants we just said well what if instead of it was an iade in a bounding box without leaving a trace what if it was a nestmate moving in a Zone leaving a trace so that kind of opened up the the like observing from afar moving your eyes but not leaving Trace versus skin in the game where the movement from here to here is interpreted as a physical movement cool great stuff thank you this this paper Al I mean has other benchmarks like the the recognition um Sprite data set maybe even a third one towers of Hanoi classic TI 83 game there some very interesting shapes cool any other thought or part on [Music] seven why mdp right now is only in discrete time so pretty much everything that you encounter in Pim DP today will be in the chapter 7 Spirit um sanj's python work as well as RX and fur and Julia have continuous time models okay can we go through the matrices other than a to d so you're talking about in here um Pros or in which figure do you um uh maybe yeah this this seems good sure okay okay okay so here's the progression of how they introduce the the matrices in seven o is the observation Vector at each time point so if you just had one binary sensor o would be a Zer or one if you had eight floats sensors it would be eight float numbers s is a vector carried Through Time Each time point of the latent States so let's say we're talking about temperature in the room thermometer observations latent temperature hidden State s agents estimate of hidden State S A is a matrix that has um rows and columns corresponding to O and S A is a mapping between o and s and so it can be used to start with s and then emit a distribution of expected observations or it could be taking in an observation conditioning upon an observation like Bas theorem does and then updating um a hidden State belief D has the same dimensionality as s being carried through time so D is just like a vector it's kind of like t0 it just starts the chain going and it gives you s at the first time point and then B is a transition Matrix that has edges and rows and columns edges of the dimensionality of s because it Maps s at tus1 to S at T so this is a standard hidden marov model ambiguity Matrix a between hidden States and observations transition Matrix B between Ates at times there's no action in this model yet um the next thing that they do is they take that exact downstairs Model E coming down and then this is where the action enters the picture which is pi which is a the policy space expressed as a probability distribution over actions so Pi sums up to one it could be sampled from or it could be just pick the highest number or it could be you know pick the second highest or all these different things that you can do regarding how to pick a policy from PI but Pi intervenes with a policy in B and so while previously B was just a single Matrix going from s of T minus one to S of T now B is going to be having a matrix slice for every single affordance that we have so each slice of B is like an index card that describes the world transition model conditioned upon action selection G is expected free energy and G is a distribution that essentially updates the policy prior habit with respect to epistemic and pragmatic value equation 2.6 so that gets us to this kind of figure 4.3 or figure 7.3 discreet time classic Model we have a downstairs hidden Markov model with no action and then actions select which B Matrix is going to be applied as s unrolls through time and then um in seven figure 710 we see a lowercase letter above each of these other letters that we've talked about that represents um a hyper prior or however you want to think about it but it's a distribution which that uppercase variable is drawn from so instead of just saying well um there's four states in the world so r d Vector could be 0. 2525 2525 that' be like a uniform prior across initial location or it could be one z00 Z just unambiguously starting in the first distribution or you could parameterize Little D which would be a distribution which D Big D is sampled from for example across individuals or across nestmates or across trials so then learning could happen on Little D across trials so you could begin with like a uniform prior of where I begin and then with learning by counting you could just say okay I'm going to um with the conjugate prior the de lay distribution I'm going to add one count to Little D every time I observe myself starting in a certain location and then trial after trial you'll end up drawing Big D from a prior distribution that does resemble the experienced history that's the learning by counting part and then the end of the chapter they just kind of present one more Motif which is now that exact 7.3 three or figure 4.3 exact Motif they're just demonstrating a little bit of the composability of the base graph by showing that like the observation being emitted from a top level model here level two that observation could be understood as for example a prior on a lower level model but figure 4.3 or figure 7.3 is kind of where in the descript every time we get the downstairs sense making part sense making from observations transition World model and then finally to the action dependent World transition model yeah thank you it's I think it's a very good overview and uh I'll refer it back again but I think it's I just have one question when you use the word motive all the time what does it exactly mean like I think I I just don't understand that word actually sorry Motif I use yeah what do you mean by Motif sorry I I use it to like a pattern like a pattern language or like an identifiable theme so um like let's just say we were looking at cars as generative models like one of them is red another one has convertible top another one has six wheels another one has a trailer like those are just kind of like motifs some motifs might be mutually incompatible but other motifs it might just be like well like um here we're looking at the motif of having a hyper prior here we're looking at the motif of nesting now the lower the lowercase letters are not shown but of course you could have lowercase letters so there's nothing contradictory about those two motifs it just and this is definitely not like the only or formal way to think about it it just like here in figure 7.3 we kind of have like the kernel nucleus um there isn't much you could remove here if you wanted to remove something you could remove like this part so it was just focus on the transition of two time steps but that's very minor um this kind of shows the basic initialization and then the step from the previous moment and then the step to the next moment of action as reweighted by expected free energy so here we have kind of an essential kernel and then there's all these motifs or there's other ways probably to say it but there's all these other strategies that we could start to see or use Sor right thank you and and um yeah I mean I don't know how far the uh Linux kernel metaphor will will go but like this model doesn't have just as written various of the more sophisticated cognitive phenomena so it's not the advanced cognitive phenomena or higher order human type thinking um is already in this image it's like this is the canvas that is a starting point for like learning a novelty so it's not not that all these different diverse cognitive phenomena are like intrinsically essentially within the active inference formalism per set it's that we have something that's General and modular enough to accept and even entertain multiple different Renditions of a given phenomena like there's not just learning a narrow definition of learning would be like each parameter being updated and then if somebody wanted to engage with a higher order like kind of conversationally what we might talk of as learning like chapter 6 is there for us we could go through it and we would find like a room of 10 people could come up with a thousand ways to implement learning at a higher level in active inference and a lot of emphasis on like what's the core and then what are some of those like decorators and modifications augmentations and so on because chapter 6 Plus The Core Essence plus the confidence to play with motifs gets very far um would people be up for maybe a little if we take a look at figure 7.10 would people may be up for trying to walk through it perhaps like and I don't know if this is applicable it's a long way from the teamas but like walking through it like with like a real life kind of decision idea like what I'm thinking is and try to map that process onto the model so like what I was thinking it's like I'm sitting in my room and I'm looking outside of my window and I'm trying to decide whether I'm going to go outside or not and like looking at okay what are the parameters of that decision what are the different um you know observable States or part like or hidden states that are going to influence you know that that policy is you think that's an appropriate exercise Daniel or something along those lines would people be into that I mean it it definitely is I'm just finding a table that I'll copy in okay I'll make this under chapter 7 okay this is just one little template so here we have the ab B CDE e and pi and O and S so we um have we don't have the the the lower case letters yet but we could add them and um each of those discrete time variables is linked up to an ontology term most of which which are all of which have been used um and then here is an example like for an ant calling estimate how those got mapped um but I mean let's hi hide these and then let's just go for it okay what's the setting what is the agents living room living room in New Mexico okay if that makes sense yeah aren't we all okay observing the the window out the outside through the window so I guess outdoor Outdoors through through the lens of window okay pixels coming from outside sure and then what's truly out there could be is it raining or not yeah the weather I guess you know is it is it that's the decision I'm trying to make yeah okay well and and and this just goes to show like first off how open it it like what actions are but um I mean we'll get there but can we control the rain so then there's already such differences between a model where like there's a hidden state that you can't control but you still could make a policy decision to like take an umbrella or Not Right But then there would be like another level of kind of interactivity of a model when the policy can also control the hidden State like when there like a like an air conditioner with a temperature versus just sort of like a put on a jacket if it's raining okay so now given that we already said SN o what is the dimensionality of a and what is the semantics of a let's just say just just to be clear that the the visual stimuli that we're getting is 10 pixels and then they have a continuous level of brightness like we could go into the RGB and all this other stuff but there's 10 pixels and then raining or not is a binary state so what is the dimensionality of a or what does a map I mean not the the site of raindrops are not right so like it yes it it it it Maps but that's two dimensional yeah Maps between the observations in the hidden state so its shape is 2 by1 Matrix okay I see yeah there there's other we're just going with just the first sketch pass okay so this is mapping the these pixels from the pixels down to beliefs about the rain or vice versa so if we said condition upon I know it's raining we could emit a profile of pixels that's the likeliest thing or we could take in an empirical pixel distribution and then ask what is our conditional probability of it being raining so that this is and then last piece of the downstairs sense making part what is the B Matrix shape given it's a transition Matrix should be 2 by two I guess yes these are just the transition probabilities of staying raining staying not raining or switching the the on the identity Matrix the diagonal on B is like stay because you're mapping from let's just say that the first um row is raining the second one is not raining the upper left corner on B is rain to rain and then the lower right corner on B is not rain to not rain and then the off diagonals are switching okay this is the downstairs part this is a partially observable marov decision or sorry this is just a partially observable marov model no decision has come yet right now we're going to come into the decision okay so what is the action space go outside or not okay now if we go outside then we we're going to get different observations but we'll just say yes go outside or not okay um what is what are the affordances basically not raining it's an affordance for us to go outside yeah um almost here the affordances are the same as the policy space because there's no planning okay so affordances are what you can do in a moment like up down left right and then the policy space is the forance space exponentiated to the time Horizon of Planet so if it's a non-planning model then affordances equals the policy space but if it was um a two-step model then you could plan I'm going to not go outside and then I'm going to go outside or I'm like I'm going to go outside and then I'm going to come back so then that is considering over multiple time steps but here single single time step no blame okay um oh we need d as part of our downstairs D is prior on it raining and so this is just this has the same shape as s so this basically is just this could be like one zero which is like we start sure it is raining or it could be 0. 5.5 we start unsure if it's raining okay and then um actions are selected different ways obviously that's what we're interested in so what are some different ways you could select actions well you could just draw from your habit distribution that's one approach in [Music] um okay now we're going into the the the New Mexico dopamine [Music] brain on the left side this is where we have actions being drawn from The Habit distribution so that's a totally totally um viable decision- making strategy that's kind of like thinking fast like type one is drawing from habit [Music] um another approach is to use expected free energy to re weight the policy prior into a policy posterior according to equation 2.6 which is to say reweighting policies according to their epistemic plus pragmatic value so in this setting the epistemic value um right now like everything's pretty much fixed so there really isn't much epistemic value to gain yet pragmatic value though is going to be about preference satisfaction so what would the preference be preferen is over observations all I'll just note like this recent one on intentional Behavior that they do explore like essentially preferences over whether it's raining or not but suffice to say in the textbook and in this kind of primary version the preferences are over observations so let's just say that um let's just say that it's um darker when raining lighter when not so if our kind of if our conversational preference is like I prefer it to rain our C preference variable because it's over observations would say I I prefer I expect and prefer it to be darker because the preferences are grounded in sensory observations that haven't come in yet [Music] um I mean uploading the textbook to a language model or not copying in this table and then saying write active inference python code given that this is my PDP you will get a script with just this much information because you've given all of the information needed there's there's no other there's no there's way more complex models to make but in terms of describing all of this I mean G is not really a variable it's a functional that gets applied to Pi conditioned upon c and e so by defining a b CDE e s o and then the generative process the true rain but that doesn't have to be active inference that could be um a random number that could be an API to a weather station that that it could just be anything else that plugs in and outputs observations and then intakes actions but here it's kind of a a null action intake because you're choice to go outside does not affect yeah that makes sense all right yeah thank thank you for that cool yeah I mean plug it in and see where it goes or um look at a pmdp um documentation and see which one it looks most like I think we looked at it a previous time but this one is a moving grid World agent in one and then two [Music] um Pros that this is a under chapter 7 yeah um and then chapter two here is is again similar and then the uh I mean make some kind of Fusion variants like there's a t- maze it's like your living room is the starting position in the Maze and then like you know you could check the weather that's the epistemic Q or you could choose to bring an umbrella or not and then it would already have almost isomorphic relationship with it with a tutorial and especially when when starting this is like why the iterating on having a model is so informative like ironically or or appropriately depending on how you see it's like it's epistemic chaining the modeler is in an epistemic chain with their own modeling it's super easy to wait on the sideline until what I really don't know but by putting something down then there's all these other things that open up so that's the epistemic chaining as a process [Music] um okay Francis let me just see what you wrote is it straightforward to download either the PMD the PDP notebook is the chapter 7 document also okay um about PDP yes at the top of each page like I'll just put it in the chat you can open it in collab super easy and then just make a copy so those are very nice notebooks to pull um is the chapter 7 document also a Jupiter notebook no um that would probably require going to appendix C and uh going to the mat lab code or um this the team is foraging and bringing this Matlab code into something like a notebook I'm not actually sure if mat lab even has notebook or realistically since it's the teamas like this pmdp Taz demo it's the same exact thing so this notebook is probably a good one to go with uh I don't have collab uh as a non-academic it's expensive uh or used to be expensive is it um sorry you said you don't have Koda or mat lab collab Co it's free you just click on the link and then just duplicate it okay right so okay that lab does have a costly license except there's the octave like variant that's open source but like rather than get into this whole like octave mat lab situation like the the python and Julia are the way to go and rust when the time is right this is interesting recommend breaking the demo to change generative model only by doing this will an intuitive sense mechanics of active inference develop pretty interesting that's exactly what I was exactly what I was planning to do thank you okay yeah I mean it's it's super fun and then also so like again seeing the kernel execute like just seeing a decision be made that helps open up like oh there's so there's like all these levels of Diagnostics or analytics about the trace of the model that you might be interested in now is that active inference well it's about an active inference model but then a Next Step might be like to do a parameter sweep across parameter combination and then summarize the outcomes of those swept parameters with another heat map and then what do you do from there you know then you could fold that back into the base model or maybe you make another active inference agent that looks at the heat map and then does something from there hence it being more like lines or journeys of inquiry rather than like developing an artifact that just kind of slams the door especially with these kind of more intuition pump examples um without wishing to derail things uh I have an interest in doing uh test automation for code and I was looking in um chapter 7 at the uh way that the agent uh explores little maze quite efficiently because it's seeking novelty um and I was wondering whether I could do something like that to um do exploratory testing of say an API um that that sounds very interesting I've never specifically seen it but something like adaptive fuzzing or testing and then that's where the epistemic value comes into play like let's say pragmatic value is I prefer the program to work so and then epistemic value is I'm going to find out about where and how it does or doesn't work so if you only um made the breaking changes that you knew were going to break it it would be very confirmatory but it wouldn't be epistemic because you would only have been confirming which which might be valid exercise but you'd only be confirming the consequences of the actions that you already suspected but maybe that's important and then another setting would be like with a kind of uniform prior or with an informed prior what kinds of changes would actually reveal more learning about the structure of the program or the API and then it's like but once we figured out that that's a breaking change like we've kind of mapped that part of the maze there's probably a lot of cool ways to go there Kristoff so I think it's it's an interesting problem what to do with the something like you know exploring an API um there's this paper um for planning uh no certain that planning navigation um uh active inference and int in intentional behavior and they have this um uh notion of um uh sort of like inductive planning where there is a certain goal that the agent wants to um uh reach and it works backwards to figure out what are the steps to get there uh for something like you know exploring API and I think a lot of for a lot of applications though I think what's going to be eventually necessary somehow is to have a hybrid model that you will be you will have to be able to attach almost like an llm to it to basically at some point say like well I would like to actually you know I decided on my course of action I want to write some code that actually invokes the API and here with this decision um the structure of decision- making we're looking kind of like at assembly level of intelligence and at some point just like we are attached to an to an llm that continuously pumps out words um and I I'm just you know sort of choosing over over the distribution of those words they're being pumped out um but I'm not actually generating them right like I'm not aware of how that's happening um in the same way you would have to kind of attach an llm to almost like an active inference Ang that does some kind of goal oriented exploration of the API so that would be very interesting if you could achieve something like that it's probably you know um quite a quite a lot of money to be made on on on this sort of intelligent exploration of apis given how how much um uh trouble bugs are causing cool thanks yeah like and then in it kind of like in in that um like we were talking about right before began with like the feedback and the training examples like you can have a data set of correct API calls and incorrect API calls or something like that um and then it's like plan to make it work and those are kind of the semantics of perception and action that like ecosystems of shared intelligence synthetic intelligence if we can construct a formalism that has the semantics that are how we think about it Andor co-evolve that with updating how we think about it then we would have kind of a high bandwidth or a low loss or some other good property of the edge between us and the constructed artifacts yeah I think this is where you know like lm's really kind of suckut that you can you can sort of ask it to do something like this but it will not really have any kind of notion of thinking about the problem deeply about how would you go about breaking uh something for an API because that requires a sophisticated model of what the API might be doing under the hood for instance like some kind of buffer overflow and something like that but if you could if you could come up with a minimal example that would Muer that would model something like you know assume there is a possibility of a buffer overflow in an API how would you go about making some kind of inference about um or planning to exploit that that would be super interesting as a minimum as as a kind of a minimum example even for just a simple you know very simple IPI that takes like a a car star in in a size or something some something like this um how would you go about you know inferring whether it's possible to to do the buffer overflow in this one one funny um piece there would be use check out the PDP API and then the what would be appropriate the appropriate um the latitude within that API is active inference that's kind of one that would be a plot twist and then also on the interpretability here's um a paper with MAO and others from last year um like largely there aren't technical advances in this paper the images are drawn from saned Smith's live stream 28 paper so it's not like this is breaking new theoretical ground but the whole point is when you ask a neural network or llm Etc you know how sure are you about that it can come up with a sequence of tokens but that sequence of tokens is only going to be coincidentally related to anything like an uncertainty evaluation whereas if uncertainty parameters are explicitly encoded then self access introspection is just like a parameter lookup it's not like some second it's like part of the real time function of the system is um uncertainty estimates of different kinds at different levels so then to ask about how certain it is about a certain thing at a certain level you just ask that question and then return that variable um you know maybe even that's on some kind of special chip or something like with um verified access or something so it opens up to all these different architectures rather than like well let's train like another adversarial neural network to like you know try to infer this about this one and then that gets into this whole like deception game versus having these introspect architectures yours to what extent does this differ from basan reinforcement learning great what what what about that or what what you you mean overall house active is inference different than reinforcement learning or yeah yeah maybe indeed even overall but uh yeah if if you want this kind of uncertainty measure with whatever you infer then I I guess both Frameworks will allow this or is there yeah uh is it's probably very unclear question but yeah okay there's there's there's a few a few works and a few approaches there so um models that aren't active inference still do deal in uncertainty so there still is an uncertainty parameterization in even a linear aggression however a neural network or reinforcement learner that's trained to generate language its outputs of languages don't necessarily represent a veritical introspection maybe there's a way to train it so that the output is a vertical representation but that would be a secondary kind of an U welding together whereas this architecture that we just looked at like it's it's a trally part that's kind of on the the uncertainty as part of the architecture part um the key the key difference between active inference and the reinforcement learning is in reinfor ment learning a reward distribution is proposed secondary or auxilary or cor corollary distribution is proposed that's like layered over the observations so in the homeostatic environment we'd say we're getting observations about temperature and we are rewarded by 37c and we seek out High reward in active inference we do away with this secondary auxilary function and we directly work on on the observation distribution by calling our preferences not a preference for higher score on this proposed distribution but rather we have an expectation and a preference that connects directly to the observations themselves [Music] um yes I I understand that but I have been uh vaguely looking at uh um things that Benjamin V Roy and uh his lab are doing and but I I don't really understand it yet but that's there is some kind of thing like beijan reinforcement learning uh and I uh I wonder if they have I I would expect it to have a um a measure of uncertainty that comes with with any prediction there but I might have uh yeah it might just be a too vague Concept in my mind it no you're right there it is there I mean this work on um demystified and compared and there's several other works that do directly tackle it it it's not out of the picture that I mean architectures can include arbitrary components so it is possible to be like well let's have an epistemic module well yes but then epistemic and pragmatic value at the end of the day in the RL learner or the Deep Q learner or whatever epistemic and pragmatic value are going to get Blended back together into the reward function right you have these like artificial curiosity and and other uh intrinsic motivations that you could add add on to a classic canonical reinforcement learner but and I see quite a lot of of research on that but then it it seems that from an active inference framework it it kind of emerges from the from from the the system but um absolutely that is the Hope like that that there's a variety of cognitive phenomena from planning to epistemic Value curiosity intentionality variety of phenomena that could be implemented in a in a range of architectures but will pursuing a first principles approach lead to simpler more interpretable more efficacious models possibly possibly um if if somebody needed uh uh you know there might there might be a better model for a situation today because there are such incredible gyms and toolkits for reinforcement learning um so but but yes hence the moment of the epistemic excitement that we have and share here plus the accelerating pragmatic conversion and understanding like yeah where is this useful because it's it's all good for something to be insightful but then um I mean there could be a situation where active inference just gives us an intuition pump and then we use a neural network or there are hybrid architectures that that do this or that and um but if utility is is what is desired or or some other property that another system already has then yeah definitely active inference has to work to to to be superior there one really interesting um moment from um this was a couple days ago with uh Ryan Smith and two of his PhD students so the empirical status of predictive coding um what was super interesting about this paper was like in this textbook group if someone said like what's the status like of of f or status of ACM people would often take it like in a metaphysical Direction however they um took it the other way and asked what's the empirical status of predictive coding and active inference in explaining neuroimaging and neurobiology um so the it's very interesting and these are these are the ongoing questions and curating the models and being able to compare them based upon how they do treat these phenomena that would be super informative and then if we can have a similar um Syntax for the variable similar API use agent I mean this um the demystified and compared it uses the open AI gy so like they presented some promising interesting results and that was several years ago and a lot of the work has probably not been published on the open AI octave inference side but it is right there like with frozen lake or with um the other reinforcement learning settings then look at where they're different find out where does the super high performance large scale RL model do well that this simple act imp one doesn't and then what would we need to do to make it different well fun yeah chapter 7 is pretty fun um next next weeks we'll head into chapter eight on the continuous time maybe we'll look at some RX and fur examples because there aren't going to be pmdp examples for chapter 8 um The Continuous time models there's there's a lot to say there on the math and on the philosophy and so on chapter nine integrating with data and then chapter 10 just a long wrap up with no figures or equations so thank you all see you next time thank you Daniel bye everybody