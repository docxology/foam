okay hello and welcome everyone it is january 28 2022. we're here in academ lab model stream number 5.1 with pietro mazzaglia and tim verbellen so this is going to be a model stream presentation and discussion on their recent work contrastive active inference we're going to have a presentation section and then a discussion so please feel free to ask any questions during the presentation that we can address in the discussion and tim and pietro thanks a ton we really appreciate you joining to share your work so please take it away and thanks again yeah thanks daniel for uh for inviting us as well so uh i'm timothy and together with the colleague pietro we'll talk on our work on contrastive active inference so maybe first to um set the scene why are we looking at active inference uh well basically our lab wants to build intelligent agents and so from that perspective we noticed early on if you want to build something intelligent it needs to be embodied it needs to be interacting with its environments and then it's a small step of course you know interactive inference where basically your your agent needs to understand the environment it's interacting with and and need to build the model basically so i'll first give an overview on on active inference and the way that that we approach this um a lot a lot of this material has also been covered uh in a previous model stream i think number number three uh so if you want more details you can pick up that one again um and so then afterwards um uh pieter will take over and he will um go into the details of the contrastive approach to active inference so let's get started so basically active inference it's a process theory of the brain and basically it says that your brain are the agents he builds or he builds generative model of the environment which is basically a joint probably distribution over observations so think that you can see or experience actions which we didn't know the scale and then um states or um yeah hidden states of the environment so basically you have your agent that is separate from the environment and it can do actions it can interact with the environment and this gives rise to new uh observations and so the idea of the generative model is biscuit agent figures out which are kind of the the hidden states that the change by my actions and that gave rise to my uh observations and if you can build such a model then basically uh this enables the agents to plan uh some actions to to bring the agent to some preferred observations or outcomes and support but so the crucial bits is basically how do you get this model um of what happens if i do my actions how does this influence the state and and how does this influence the outcomes that i see and so the crucial part connective inference is twofold so first of all it says okay this is what the agent does and it does so by optimizing so-called free energy which is an upper bound and surprise or prediction error so basically [Music] the generative model allows the agent to predict the outcomes that it will see that it will witness and um the better these match your actual observations the more happier you are as an agent and crucially you will also select the actions that will minimize the free energy you expect in the future and so we'll dig a bit into the mods um just to to set the scene uh on the one hand notation-wise so that we all know what folds and s's and a's are but also to then see the move that piet will make from the let's say valera active inference presentation towards a more contrastive formulation of of the active influence object reality objective so we start off with uh setting the c with a generative model so uh it's it's a bit laid out uh the the diagram of the agents and the environment that was on previous slides so basically this unfolds over time so you are in a certain state that gives rise to a certain observation and then given an action on your previous state you basically move ahead to the next state and this process unfolds over time and you can see um some of the um the circles are covered gray and these are basically the things that you can observe so you know the actions that you did up until now and you know the observations that you saw up until now and all the rest is basically for you to infer so you can only refer the hidden states until now but you can also try to infer the hidden states of the future the actions that you want to take or the observations that you will experience and so basically the um the so-called joint model or joint distribution over the sequence of observation states and action is then basically factorized as follows so you have a prior over actions uh you have a lightweight model so uh so yeah so you have prior over action so this basically determines yeah what is the probability you take certain actions certain time you have some transition probabilities so what is the probability that i i will transition to this next state given my previous state and the action i did and you have the likelihood model which basically says yeah given the state i am um which observation i will see and so this basically covers the so-called markov assumption that your observation that you see uh at this time step it only depends on your hidden state and it does not really directly depend on anything else uh because if you know your your your current in state then you know the observation you will see so that's basically what's but it's reflected here but of course as an agent having this generative model this allows you to to assess how likely is a sequence of observations for example and it allows you to predict given these actions what will happen but one crucial bit of course is still the inverse of this model like given that i saw these observations that i did these actions which is my current state um and and this is basically uh non-trivial so even if you have the exact strength model inverting this is typically intractable and so that's why why in active inference you uh you resort to variation inference and you just say okay i just assume that i can build a model the so-called bricks and posterior model and this is the thing that will uh tell me given certain observations what is my probability to be in a certain state so this is what's depicted here so yeah we introduce this q and q is basically a variational approximate here so can be any distribution you can choose it and you just say okay um given some observation i want to have the best estimate for the state i am in and the free energy principle genesis states if that's what you want then this is easy you just uh optimize the free energy which is the note f here and it's basically expectation over uh states uh generated by your personal posterior the expectation over the the difference between the local accuracy of your approximate posterior and look like of the gif model and if you can minimize that that basically means that you will have um the best explanation for the observations you see but at the same time you also have the best examples here for the true one and we're not going to go to the whole derivation of this flow but basically uh you can convert this to the the second equation line and this is the one that we we use most often in our models which is basically clear versions between this approximate posterior so basically what is the state i am given the observations i saw and um uh this briar that basically says this is my my guess that i am in the states given my previous state and action so i don't know the observation yet but i want to have uh the best guess uh without my observation and if i see the observation i don't want my beliefs to completely switch because then probably there's something wrong with my model and then you have the second term which is the uh the like return this is basically the accuracy of your model or how good are you at reconstructing the outcomes so um this is all for the past uh basically or up until your current time step so you know the observations you saw and you can evaluate the free energy and then you can basically update your model in order to uh to minimize this thing but of course you also need to know your actions you want to look into the future and so if you look into the future then we talk about the expected free energy and so here uh we didn't we we used by the shorthand for the sequence of actions into the future i also switched from d to daw just to denotes that we are talking about future time steps basically but other than that same but the important bit is now that in the expectation now you don't have expectations only over states but also over outcomes because you you couldn't sense uh your observations yet so you don't know them so you can just kind of do an expectation over anything that could happen and and then uh the move that is made in active inference is basically that uh you on the one hand form uh the term which they call or which we call the instrumental value or realizing preferences and it's basically stating that okay as an agent for future outcomes i have some prior that i think that i will regardless what happens that i think i will realize it's kind of your your preferred outcomes let's say you can also cause this more like homeostasis so i want my body temperature to be uh 37 degrees celsius so my expectation before knowing anything is that it will be 37 degrees and hence i will act in order to make it so so that's a bit reflected here like you you disregard the um the dependency on your future actions you just say okay my prior is that's this is what i expect uh so this becomes the instrumental value and then the second assumption is basically that your approximate posterior model is basically very very close to the true posterior so that you have a good approximation then you can rewrite it's in the second term which basically means that you have on the one hand and the third that says this is my belief or the state given the actions i will do and the other thing is also a belief about future states given the actions and some certain outcomes that i expect to see so basically it says what would be the information that i get from looking for certain outcomes and it's kind of an epistemic value or an information game which can drive you to to explore basically you want to have if if you don't know how to get to your preferred state at least you want to get to state to states that give you more information on on where you are go first often says it's like the owl that needs foods so what do you do do you eat first or do you search for prey first and so the epistemic value is basically searching for prey it's like where should i go to to to get more information on where the play is and then once you know where it is then you can realize your preferences and uh go towards the towers of free basically so how how does the action selection works then well basically you want to select the actions that minimize your expected free energy so at each time step first thing you do is you use your approximate model to uh to estimate your burn state it's like knowing in which state am i now given my latest observation then you can evaluate the expected free energy for uh for each pulse your plan so for any future sequence of actions you can evaluate and the expected free energy and then this basically results in a belief over policy so you basically take the the mind the negative expected free energy you multiply with this precision parameter which just states how yeah how how much confidence you have that your expected free energy is correct basically and then you use this the soft mix function so basically just it just says um the the policies that have low expected free energy are the ones that are most likely basically that's that's the only thing this formula says and then you refer the next action according to this you just select the next action for the sequence that you think will give you the the minimum the minimum expected 3mg and that's how it goes and then you you take this action you get a new observation and a process [Music] so one crucial point in in our work is that uh it all starts with this genetic model and it's approximate model and um typically um you can and you have a certain problem and you know how the problem uh looks like what what the observations are what the hidden state might be and then you can you can really pinpoint and write down the exact model and start optimizing but in our case this is often not true if you if you look at the robot that drives around and gets camera inputs for example yeah what is the state space that you need to um that you need to track how do you convert these pixels to the state space so all these things are yeah they're just not there yet and the goal in our work is that yeah can we can we completely start from scratch and learn this and for this we use deep neural networks to act as function approximators to actually um provide us with these these models and we optimize the parameters of these neural nets also by minimizing the free energy that's that's the core id that's it so how does it look like we call this an artificial word model so we start off with uh observations and actions so these can be pixels basically so uh an n by m matrix of numbers let's say and actions which could be any action factor it could be your velocity or whatever your agent can do and these numbers are put into a neural net which we call the encoder and this basically reflects this approximate posterior this is just saying given my previous state action and my current observation it outputs a probabilistic state representation which is basically the means and the variances of multivariate versions and then we have a second neural net which we call the transition model and this is then [Music] saying yeah what what will happen if i do a certain action how will my state evolve if i if i do a certain action now finally we also have the the decoder or the likelihood model that then outputs given states some observation so in case of an image for example this will generate you a new image and the goal is of course to have the best predictions possible so if you look at the free energy formula again in this case it's again you have this likelihood which basically just says and given the the output of the decoders or the generic image i just want to have this close to the actual image that you then uh see basically so it's just a reconstruction loss in terms of a neural net let's say and the second term sorry second term is a gal divergence between the distribution that you generate from the encoder and the distribution that you generate from the transition model basically so we apply this on a number of cases which also um were seen in the previous model stream so just to give you some intuition so first thing was uh the mounting core problems just which is a basic control problem so here the sensory input is the position you are with the card you basically have to infer not only the position you're in but also the momentum you have the velocity you have and so you can see that uh on the right you can see the model predicting all likely trajectories for going left or right and you can see how in the beginning it's not sure on the velocity so it's it's very spread out in in what it will predict but the more information it gets the more it kind of collapses to yeah i'm pretty sure that this is the behavior that will happen and then you can use this to to drive the agents towards a preferred state in this case the the flag the second one was using the car racer environment so here you get these uh observations are now just pixels from from this uh game and the preferred state of the car was to be in the center of the track and so you can see how it actually infers the actions that will bring it to uh to the center of the track uh and it might even cut corners in order to reach the preferred state a bit faster and finally we also did this on on the robots navigating our lab where we equipped it with a number of sensor modalities so you can see a camera but also a front facing lighter and also a radar range doppler so the radar range doppler basically gives you in the y-axis the range and in the x-axis the the velocity of the reflections basically and here you can see how in the beginning we feed it with a number of observations and then we basically let the model imagine what could happen so these are real observations and now it basically imagines what it will see if it turns around for example because it actually learns like basic uh dynamics basic behavior um of of all these sensor models so it's pretty cool okay so what are the limitations of this thing well there are two core limitations that we address uh in the work of pietro so the first one is we we use this this recon pixel wax reconstruction board to uh to learn the model but also to define your preferred state like this is the image that you want to see and try to make it happen but the problem is that means where error in pixel in terms of pixels is not really the best metric so for example if you have the the left image and you want to assess how good an image is similar to that one we have two examples here on the right and you can see that the the same image with some salt and pepper noise is actually scoring worse in terms of mean squared error than an image where the the two two joint arm is actually uh incorrect so although in terms of behavior the the left one is better in terms of means great error the the right one is better so that's of course problematic if you want to control the arm towards the goal and then the second limitation is that if you need to evaluate the expected free energy for a huge number of potential trajectories potential actions you can do then of course this becomes intractable as the number increases and so the the ways that we coped with this in in the contrastive work is on the one hand instead of using a pixel-wise reconstruction error is to use contrastive learning instead how exactly this works in the in the next few slides and then the second thing is instead of evaluating the expected free energy for all the policies we basically amortize the policy selection scheme so we also train neural net to output actions given during the stage and so with that uh we can now shift to ghetto who will talk about the contrastive formulation of the principle thank you tim alright so thank you daniel for having us and uh thank you team i hope you can hear me well okay so i'll try to share my screen now okay all right so now i will talk about our recent work contrastive active inference so this work was recently published that now rips 2021 so very recently it came out like last month and let's start delving into it so the the setting that we discuss in inactive inference is very similar to the reinforcement learning one with the difference that in reinforcement learning uh the all behavior learning is driven by rewards so the agent receive a reward function and positive rewards should reinforce positive behaviors while negative rewards should penalize the agent to avoid to the states and actions however one of the problems comes with reinforcement learning is that in order to actually learn from rewards you need a rework function and that's not always easy to have for instance a steam mentioned especially when the state is not known in advance so the agent doesn't exactly know it states it's difficult in that case to design a reward function because you're not sure of what the agent knows and how it can assess its performance compared to the environment so we instead focus on active inference in inactive inference the agent separates to the the principle of minimizing free energy as we we have just seen so the the principle of minimizing free energy actually enables two things the one thing is to learn uh a model of the word we we call this an artificial world model in our work a steam show and the other objective is to minimize the free energy in the future by trying to achieve uh some preferred outcomes of the agent so we assume that the agent does some preferred outcome distribution that he wants to achieve and this goal will be in the future to actually achieve these preferred outcomes so the the environment setting we discuss is that's one of our pndp so a partially observable marketplacing process so just to recap we have observation that the agent receives he has to infer the internal state of the environment which is not and then there's action which are actually known for the past but the agent should infer or somehow choose among a set of possible actions in the future so this is just a summary of what the the an artificial word model looks like so as as we've seen in the previous slides we have an encoder that encodes the information from an observation we focus on visual environments so here we have uh again an image which is basically n-by-n metrics so the encoder could be for instance a convolutional neural network in our case then we have uh the the hidden state model which which takes the previous state and the previous action and in particular in our case here we use some some form of recurrent neural network model in order to keep observing the history of the environment and then we have the decoder that computes reconstruction of the the observation of the current state so it tries to encode uh inside the hidden states of as much information as possible from what it comes from the from the observation so the problem with uh reconstruction is that computing them especially in visual environments is is quite complicated because you need big models that have a very good representational capacity and also the models cannot be a hundred percent accurate as in uh lowest low dimensional settings because for instance predicting an image pixel by pixel is practically very invisible so it very rarely happens so let's go on example here so a few a few weeks ago i was uh i was training uh bae like model so like uh a model similar to this one on the left so where we have the this encoder decoder architecture on uh on an atari game the breakout game and try to to learn an eden state to to learn action on top of the the interstate the problem is that the the reconstruction of the the vae so we're actually pretty bad in that they were losing very important information about the game so for instance it was kind of able so with some uncertainty to model where the puddle of the game is but it wasn't able uh to model where the ball is which is actually one of the two most important details in order to actually be able to play so even adding the reward function in this case so having the the the game score available uh the agent wasn't able to learn the task because of uh of the state which was lacking the most important information in order to to keep improving so this is one issue that we try to overcome in our work and the second part of active inference involves learning to pursue the preferred outcome so in order to pursue preferred outcomes active inference agent will do two things one try to minimize uh the distance with respect to these preferred outcomes but on the other way also minimize the ambiguity respect to the environment so normally this is done by trying to match uh these the two distributions or trying to to match as we saw with uh with a kill divergence try to match the distribution of the imagined outcome with the preferred outcomes distribution however again in a high-dimensional setting this can be quite complex because uh how do you define a distribution on a 9-dimensional image could it be for instance just a center gaussian around the pixel so with the with the mean being the pixel value and then some fixed standard deviation but in that case we we get into troubles because we have the same issue discussed before with for instance having this kind of call here and a noisy observation like this which actually adds an eye and higher mean square error compared to to an image that is very distant from the goal and this kind of situation especially when using reconstruction or in more realistic settings are very very likely because for instance you can you can think that the center image is actually just a reconstruction of the model which is not 100 accurate so that could be the case and indeed the agent will be confused and he will think that it's not achieving the goal compared to maybe for instance a past observation but if it seemed it was actually actually closer to the goal or again when there is some some noise in the environment in real world setup like also robotic we always add this noise into the observation so it's hard to uh to match a preferred outcome in a internet dimensional setting so we we also try to to overcome this issue here so what we do propose is to use contrastive learning uh contrastive learning is a is a mechanism popular in the unsupervised learning scene that we will discuss more in depth in a few moments so the the the objective that we want to to have with our method are to avoid reconstruction in learning the word model so we don't have any more the decoder here as we see on the right then we want to be able to match preferred outcomes in a lower dimensionality space because we have seen that uh net dimensionality that's problematic and also we we would like the this low dimensional state to be somehow representative of the task so that we when we match our goal in this low dimensional state we are actually doing something that actually brings us closer to the actual preferred outcome that we we want to achieve in the i dimensional setting so let's let's try to compare to see what are the difference in between using the likelihood active inference model and the contrastive model so the idea in the likelihood active inference model is that we want to maximize the accuracy of reconstruction so basically this this means that we have this decoder that maximizes this uh maximum likelihood of the the observation given the state so we want the state to to to maximize the information that it contains about the observation basically in contrasting learning we in contrasting active inference we do something different so instead of trying to reconstruct the current observation we try to compress with the encoder again this observation and compare it to all the other uh to not all the other as we'll see in a while because it's invisible but many many other samples that represent something different so that we in the in the latent space in this compressed space we want our state and the compressed image to be very close uh while uh this our state should be very distant from all the other images so we we are indeed maximizing the similarity with this with the corresponding sample this is here called the positive sample where we want to minimize the similarities to maximize the distance against all the other samples that are called negative samples in contrastive learning so as we'll see also in in a moment this this mechanism here maximizes the lower bound of the mutual information so we are basically trying to maximize the information in between corresponding observation and state while minimizing the information with respect to to all the other negative pairs so as we've seen uh before the the free energetic past can be summarized with this equation here so here i'm just uh talking about one time a one moment in discrete time steps so instead the team presented for all sequences by using the tilde notation well here we're just considering one time steps at a time so as we as we have seen the free energy is basically uh an upper bond on the surprisal information that we we want to to minimize so we minimize free energy in order to minimize the surprise of the agent and the ear is is actually evident that we have this evidence spawn so the the scale divergence is always greater or equal to zero we wanted to basically to to reach zero hypothetically in order to minimize this uh this evidence bound and uh this can also be rewritten uh in a way it is more practical to implement it so by having the the likelihood of the observation given the state which actually means the accuracy of our model and having again the complexity of the model which is the the kl divergence between our variational distribution q uh which we we we use q of f given o which is basically using the auto encoding uh variation of a variational uh posterior so this is uh as typical as is done in uh in variational encoders so when you when you try to infer the parameters of your posterior distribution by using the corresponding observation and then we we want to minimize the child divergence between this uh auto encoded posterior and the our prior about the about the the current or future state we can say given the the past state and actions and in our case in particular we generally learn this this prior so we don't just use a a uniform prior state but we will learn uh our priors to be predictive of what the state is given uh the past states so it can basically seem like for machine learning prediction as a as a conditional uh variation of the encoder with with contrastive learning what we try to do is as i said to maximize state similarity uh with the correct and corresponding observations of our positive sample while minimizing with the other this means again that we want to maximize the mutual information between the positive sample and the corresponding state and minimizing the information with the negative samples this can be written like this so the noise contrastive estimation so nch that's the abbreviation basically provides uh again a lower bound on the mutual uh information uh so where we where we see that we basically have like like a soft max so over the over all the the the observation state what does it mean so uh for each a pair of state and observation we want uh this value so the value of this critic this this critic function f to be as high as possible or we want it to be a very low respect to the other so that actually the exponential of this compared to the sum of all the other exponential is higher with the corresponding exhibition and very low with the rest so this is basically saying that uh that you'd want matching pairs to be to be very close and distant pairs to to a very very low value and the the this this lower bound is is an approximation normally when we take a number of samples key from a joint distribution that we define uh between x and y in particular in our case this x and y represent our observation and our hidden states so we define a priorities uh this joint distribution to actually represent the fact that this state corresponds to this observation and we want to maximize the information in between them and this x y again is a so-called critic function so what does it mean is a function that should approximate this log density ratio that we see here on the right i won't go into the the mathematical details of this but basically it is a mapping of the of two the two inputs so basically is a mapping of our observation and our state uh and we want this uh again these outputs to be i for corresponding pairs and and low for non-corresponding pairs so how do we transition from the free energy of the path that we have seen today to our contrasting formulation so what the the first step that we do is adding to the to the free energy functional uh a term that we we assume to be constant that is the the entropy of the observation so how can we assume that the entropy of the observation is constant so in machine learning we generally have a data set from which we samples our observation about the past so we assume when we train that our data set so that the distribution over the observation is fixed so the the entropy of this distribution will always be a constant because we cannot modify that as opposed for instance to the states which which we instead learn so the distribution of our outcomes cannot be modified and so its entropy is a constant if we add this term to the free energy functional we can rewrite it as a as the child divergence minus this information gain or mutual information term here between the states and the observation so given this we can now apply the fact that we did contrastive learning uh functional is a is a lower bound on the mutual information to actually derive the free energy of the past we're expliciting all the terms out according to the the previous slide math we basically have again this kl divergence term and then we have this uh the value of f between o and s to maximize and then to minimize all the uh again the the the value of the functional with respect to all the other negative pairs so this brings us to to again an upper bound on the surprisal term we see that the this upper bound is actually even higher than the normal uh free energy upper bound but as we'll see later these are some nice property that explicitly help us to get rid of the reconstruction and to learn a different representational space that has some advantages compared to the to the likelihood based uh representation so let's now talk about how can we learn uh to behave using contrasting vacuum inference so in the likelihood based active influence model what we were trying to maximize was again the likelihood of the future observation but under the preferred distribution so we want to we want the imaging outcomes to be as close as possible to the to the outcomes that we prefer uh so this for visual environment implies that we we reconstruct what the what we imagine will happen in the i dimensional uh space so the image basically and then we compare it to our uh preferred image for instance and uh as we see before we can for instance use a max mean square there or distance or we can just use like uh we can use the condition and compute the likelihood under the preferred distribution for contrastive active inference we again instead use a contrasting mechanism when we want now the the future state to be corresponding with our uh samples from the preferred distribution so we want the the outcomes that we prefer to actually be uh close to the to the state that we imagine so that now we don't need anymore to reconstruct what the what the outcome of our action will look like but we can just say is the the state that we imagine matching with uh the preferred outcome that they want to achieve and uh that's that's what we we maximize similarity with and again we also have some some form of ambiguity minimization or epistemic value in trying to minimize the similarity respect to other outcomes so in this case minimizing the similarity respect to outcomes that are not in the preferred distribution basically means that you either want to go far from something that you have already seen before in order to maybe get closer to the to the preferred outcomes or you either just want to minimize your ambiguity so you want to be as far as possible from other outcomes and as close as possible to the to the actual preferred outcome that you that you're hoping for so as we've seen before the expected free energy can be summarized like this i i'll first i like some difference in respect to to the equation the team presented so first of all here we we take action to be part of the octave active inference process so the inference process well uh before we have seen that you can you can have a distribution of our policies and then you can you can sample the action from the policy and and compute the free energy of the future uh given a posterior on your on a policy on a given policy instead here we we make the the actions part of the degenerative model for the future and we actually want the agent to to infer the reaction from the future and not just compute them as a as a posterior over over some distribution of the overhead policies so we we have now in the posterior is uh this 80 so we that we infer both the the the the future state and the future action and also the the prior other over the preferred outcomes that i indicate with tilde i hope that's not confusing before because before the tilde was used to to indicate sequences but in the paper i actually used it to to indicate the preferred outcomes so yeah notation issues but uh i hope that's not confusing so the till the year is basically to say this is the preferred uh distribution over observation state and action so this is basically our target distribution what we what we hope to achieve in the future and we can rewrite this as a as the the sum of three terms so we have this uh uh so first of all i'm assuming that the agent has no prior preference on action so that for him any action that will bring it to the preferred outcomes it's fine so yes a uniform prior over action so the action doesn't really matter what it is as long as it brings to the to the goal let's say and so this in this way uh we we obtain an action entropy term and then uh the rest the intrinsic value is the is the same epistemic value that we've seen before and uh so the one that uh should lead the agent to to explore the environment more or either to reduce its ambiguity about the environment and then we have the extrinsic value which is basically rewards or uh just uh just a way to get closer to the to the actual uh preferred outcomes so the the value to pursue in order to to minimize distance uh from the from the preferred outcomes in our contrastive expected free energy uh we we again do a similar move as we did from the past so here we assume that we are taking expectation over our preferred outcomes since we don't imagine outcomes in the future we just assume that the outcomes will will be according to the to the preferred outcome distribution so that we can again sum the entropy over the our fixed uh preferred outcome distribution and then the steps are the same so we we have again this mutual information term between the preferred outcomes and the the state that we imagine in the future and the the the action entropy term and uh this scale divergence between uh the posterior states and the prior of states so is uh a complex term i'll say because it basically should represent uh the difference between what the what agent believes it will happen and and what is supposed to happen in the environment so normally in active inference we assume for the future that the the model of the world is correct so that the agent does not control over over his world model so he cannot change how the the environment dynamics will transition from one state to another so i i'm assuming here that this is this scale divergent term is actually zero though i've seen that some work this could also be be left there are being greater than zero but then then it's basically uh adding the agent imagines that he can um it can violate the the environment dynamics hoping for a better dynamics that it will allow it to uh to be optimistic and think uh yeah the thing that i imagine it will happen it's actually gonna happen so here we we don't allow the agent to to modify all the environment move from one state to another and we just assume yeah the dynamics environment so our posterior over the the transition dynamics of the environment is correct and so the scale diverging in zero and then our objective uh again doing the applying the the contrast if the learning uh lower bound translates into this when we we have this uh contrastive mutual information between the preferred outcomes and these this action entropy term so if you write it out explicitly we again have this uh this two term which kind of reminds the two extrinsic value and intrinsic value for for the free energy so we have this uh the term that actually should minimize the similarity with the with the negative sample that is doing something similar uh to what the the intrinsic value in inactive inference should do so basically trying to to be distant from from previously seen uh outcomes is is kind of similar to explore the environment to minimize your ambiguity so try to find something that gives you more information not something that you have already seen and so the the the world model can be summarized in these three main components that we learn we have our prior network that as i said before is learned and should learn the the transition dynamics of the environment so trying to predict future states given past states and actions and then we have this gre self that is shared between the the prior and the posterior network and this is what allows us to to brings our history with us so just not stop to the previous state but also to include some information about previous states so that we have more more information available in order to infer what the current state is uh actually is then we have our posterior network which also has access to the observation and this posterior cnn as a as i mentioned is a compositional neural network and yeah here we have the actual layer description for our environment which are 64 by 64. but yeah that's that's less important important thing is that we have a convolutional model that compresses the information from the observation for us and this same convolutional network is also linked to the to the representation model that is uh this the the critic of the contrastive learning mechanism so the function that is indeed matching states and the observation in order to learn the a good contrastive learning representation so the the function that we minimize with respect to the past is uh is our contrasting free energy of the past uh summed over uh an arbitrary number of uh of discrete time steps in the over past sequences it is important to say that uh for the past the negative samples that we take are uh observation uh of the of the same sequence of the of the corresponding observations so let's say that we have an observation in the states the negative samples will be uh all the other observations within the same sequence that are not the same in time but also observation that come from from other sequences so so that we're basically contrasting uh the the current state with uh different time steps so uh what happened in different uh moment of the same sequence of actions and what happened in different situations or different sequences and that's how we we try to to foster our contrastive learning mechanism then we have the action model so for our action model we have two uh networks one is the so-called action network which basically infers the the action to to take a given state and then we have this expected utility network and this helps us pursuing what uh team anticipated so that the fact that we we are amortizing the action selection process for a very long-term sequences by using a network that should estimate what the value of a certain state is in the future so i'll try to be more more clear here so basically you have the action network to minimize this uh this g lambda t a functional that is basically an estimate uh that of uh of how much value is in a certain states and how do we get this this estimate so this estimate is is provided by this uh formula here so basically at every step uh we provide the the actual uh expected uh contrasting free energy for that state and then for the future uh we with some uh we compromise in between uh an estimate of what the what the network would predict uh it's gonna it's gonna be the value in the future and the the value itself that we are that we are computing with the with the functional so that every step we basically uh sum the value that we we expect in that step uh we bootstrap this that's the way we normally say reinforcement learning so we we apply some form of dynamic uh programming approach to to sum this value with uh what we expect will happen in the future and uh we use this as our uh target for learning the estimate so we basically have the estimate and the estimate of the future plus the current value and we compare the two and we want the the the actual estimates to be close to what is actually happening plus the the future uh estimate and this is this is actually what is normally done in reinforcement learning when you apply the so-called bellman equation in order to estimate what's going to happen in the future by using uh what you actually know so generally like the rewards and in our case the the explicit uh free energy value and what you you already can estimate for the future so in our experiments uh we compare to we compare four flavors that make for uh reinforcement learning uh using likelihood model this is dreamer the dreamer baseline so it does a likely based uh learn the word model and uh it uses rewards for learning action so the the river function is uh is already given to the agent and then we compare with contrasting dreamer it is a modification of trimmer using a contrastive learning for his world model instead of reconstructions and then we compare the two flavors of active inference uh the standard let's say one with the likelihood reconstruction model and our contrastive formulation so we use similar architecture and training routine for all the the four base lines and the the training routine can be summarized as we see here in pseudocode so for a certain uh for a certain amount of number of training steps that we fix intense we are going to train our world model on the previous experience so now on a replay buffer that basically represents our data sets of past experiences then we are going to use the trained world model to imagine some trajectories in the future by using our action model and the replay buffer as well which is used because we we need to take this the negative samples uh for the contrasting free energy functional uh and then on the imagine trajectories we are gonna train our our action model in order to actually try to pursue the the preferred outcomes uh better and then we are going to go back to the environment collect a new trajectory uh using our word model to infer where the with the hidden state of the of the environment is at every time step and using the action model to select the action according to this data that we inferred and add the just collected trajectory to the data set and we do this continuously so train the world model imagine uh some trajectories and uh train the action model and again keep collecting so that we we continuously improve both the data collection process because the the the word model and the action model gets better and also our model indeed so that we get closer to the goal so one important insight that uh that first introduced before that into an empirical evaluation of the method is the fact that using contrastive learning strongly reduces the the component requirements of the model so here i i'm comparing the number of multiply million multiplier combination operation for our models and the number of parameters that we as we see these are much lower when we use a contrastive mechanism compared to to using a likelihood model and this is also reflected in terms of workload time so our model is is quite faster compared to dreamer that trains the the likelihood based uh model for the world that uses uh just three words for learning action and is much much faster than the likelihood the active inference model because the likelihood active influence model other than having to to do the reconstruction during the world model training also as to imagine uh the the high dimensional outcomes in the future so in that case you have even more computation because for every imagine trajectory you have to imagine all the possible images in our context that you are you you will get pursuing a certain a certain policy so it's our model is quite faster than that so the first arcs that i will discuss is the a simple mini grid task so the agent represents the red arrow who will navigate a black read in order to reach a green square that is placed in one of the corner of the grids so the environment is partially observed because the agent doesn't know what's in every pixel of the creed so in order to find the goal first i should you should explore the old grid and and find the the green square or at least be in a position that allows him to to see the green square in front of him so for the for the reward model of course we have uh we have the highest reward and the in correspondence of the of the goal state so when the when the agent is actually on the on the goal square it could receive a reward of plus one and this is a sparse reward task so for all the other states the agent will just receive zero rewards so it will be just encouraged to to reach the goal well for active inference method uh the way that i chose to define the preferred outcome is to have an image of the agent that is that sees himself on the goal so basically the agent sees himself on the goal and says this is the position that they want to reach in the world and let's see what happens from a from a qualitative level so as i said before the rewards for for this task it's just a plus one in the right uh in the right square in the goal square what happens for the the active inference model so what will the the active inference uh model uh provides uh as a volume of a certain states of the agents in order to pursue the preferred outcome so we see that the likelihood active inference when uh that is imagining the outcome and comparing it to the preferred images is actually giving a very high value for the for the right square so this you know in a scale from zero one we can say that's that's a one oh sorry we can say that's a one but other than that the the function it is providing is a bit confusing because it is giving some higher rewards in the centers uh compared to the let's say the uh the the last uh row and column that are the one that at the least to the final goal uh the the the other corner are not even close to the the gold one so he is just uh he's just providing a perfect match and we have no no control over what the distance of the goal is because for the perfect match the goal that's that is indeed the the correct value that it is providing but other than that we it's difficult to understand what the what the the likelihood of the influence model is providing with the contrastive active inference we see that there is a different pattern so the agent is providing a very low volume for the center so it's understanding that the center is of course not what it wants to see but then it's providing i values to all the corners and in particular the highest values provided to the to the right corner so the one with the goal because it is of course the more the one that corresponds the most with uh we want what we want to achieve but then all the other corners also have a very high value and uh the fact is that from i i would say that the contrastive active inference is is verbally capturing uh more i would say semantic information about the environment so in order to distinguish a corner uh from from a center of tile is actually a modeling the fact that there is a corner in a certain state of the environment and uh when it when it looks at the preferred outcome image it actually says first of all this is a corner and that's the way it distinguishes and then there is the green uh the green tile in the in this in where the where the agent is so from from a value perspective we can say that a corner is closer in semantics uh respect to to a central tile to our goal and then of course when you also have the the green tile which represents the goal then you are the closest so this is of course a bit risky because it can also lead to to sub-optimal behavior in some cases but with with good expression of the environment it will uh for sure lead to the optimal behavior because still the the maximum value is still uh provided correctly so it's still the value for the right corner but if you didn't see the other corner the agent will will just go to another corner and say okay this looks similar to the goal so i'm trying to do something similar to to what i i would like to do i if i didn't see anything else disclosure this is my the best i can do so this these are lights how exploration is important in order to to achieve uh preferred outcomes and i think that's that also applies to likelihood active influence as well because in if you didn't see the goal you just have some some noise some noisy signal in the center so you wouldn't be able to reach the goal as well and then here we quantify the performance we see that with the with the likelihood active inference uh the agent struggles to to reach the goal uh consistently while our our methods uh leads to consistent performance that are in line with the reward based baselines of course the reward base baseline have an advantage because during training they always have a filtered objective so even if their model is not correct they always have this three-word function filtered function that tells them yes this this is where you need to go well our model uh can take a little bit more time in order to first have a good model and then being able to match but uh with the contrastive mechanism this process is actually happens uh fast and uh leads to consistent performance where we with likely the active inference we see that uh it it will probably takes more time to converge or it just leads to to suboptimal behavior so it's just inconsistent according to our evaluation and yeah these are two different grid environments but this one is smaller one is bigger but yeah the results are very similar in terms of performance obtained then the other task that we discuss is a continuous control task with uh with a 2d planar environment when we're a robotic arm through the penetrate uh sphere gold so the the red sphere and this sphere is bigger in the the so-called the richer easy environment which is the one for which we see the preferred outcome on the left and it is smaller for the for the so-called richer art environment that is the one that we we see in the on the right so for a reward based agent we have the reward function that provides when the agent penetrates the the sphere fully a reverse of one otherwise when it's uh off penetrating or just partially penetrating the sphere it provides a dense reward that that tells you yeah you're getting closer to the goal so in this case the reward function is actually helping the agent a bit more because it is telling him that he is getting closer to the goal and he should just try in the neighbor area while for active inference we we just provide the the preferred stating environment which is uh the agent penetrating the goal sphere and let's see let's see what happens so again we have a similar pattern uh actually yeah similar but different from the previous one so in this case where the the mean square there are distance from the goal is actually more confusing as we we've seen in previous example so the the likelihood the active inference agent totally fails to to reach the goal because it's probably all the all the states look alike in the environment because the background stays the same so the background is not moving as it was happening for instance for the mini grid environment uh the goal is always in the same position so the difference between two images is just provide given by the the few yellow pixel that moves around and if the model is not imagined perfectly where this pixel goes it's very difficult that it will provide some some informative objective for the stars instead our contrastive active inference agent is able to provide an informative goal and apparently the fact that he's providing some semantic information about the task is actually helping it to converge even faster than the then the reward based uh a baseline because the reward based uh agents uh have access to rewards just when they are close to the to the goal where the contractive active inference provider reward function everywhere in the environment so when the when we see that the arm is very far we have the the mutual information term that should basically take over and tells you yeah you don't want to stay there go elsewhere and until we we eventually find the the goal sphere and we converge to the correct behavior so the the agent is actually converging a bit faster than the other baselines and then yeah the the contrastive dreamer baseline is converging a bit faster than dreamer one because its model is faster to learn because it's contrastive so this is what actually happens these are a gif on the right at some point they show difficult reset so basically we just see here that the task is uh is correctly executed so that the agent is uh is able to match the the correct behavior so yeah it is taking a bit longer than expected but yeah you can see that for instance in the in the art task the agent oscillates around the current behavior but it keeps saying okay here we see it so basically the the in the environment is oscillating in a position that is uh very close to the goal so it tries to stay as much as possible to that uh that point and uh not uh not be driven far from the goal by the inertia of the of the of the arm then we we analyze qualitatively what's what's happening in terms of uh the value this provides the agent so what's what is the the objective that is given to the to the agent in order to learn here and as as i try to explain the reward is is somewhere in the middle between 0 and 1 when the agent is partially penetrating the goal and is totally one when the agent is fully penetrating the goal in all the other situations is zero for the active inference likelihood base model we see that the signal is very close for all the states so the agent basically thinks that there is very little difference between being very close to goal and being actually very far off from the goal and that's uh likely the reason why it's not converging to the to the optimal behavior for contrastive active inference instead we see that the agent is provided a an objective value is some somewhat in between zero and one when he's a when he's not close to the goal and something that is very close to one when it's in the goal and in particular when it when it's closer to the goal the the value is actually a bit higher than when it when it's far off so we see that there is some again something semantic information provided which is the the actual distance of the arm from the from the right goal or we can just see it as a okay the contrastive learning mechanism saying okay this this pose of the arm is actually very different from the one that i that they opt to obtain so let's try to move to a pose that is actually closer and uh we indeed obtain higher values when the when the pose is similar to the to the one that we we want to achieve so we exploit the fact that uh some semantic information is provided to the agent by using contrastive learning to to work on a more difficult setup is the richer distracting environment so in the research distracting task we have the same objective as before so we want the agent to to reach the goal by penetrating the red sphere but now we have varying backgrounds and we have this structure in the environment which could be just altered colors on the tilted camera and uh we still want to to achieve the agent tool to penetrate the sphere despite that so for the for the reward based agents the reward is same as before so being provided for the agent penetrating the goal sphere while for active inference the goal here is is actually a bit troublesome to define because given the fact that the the background is constantly boring across different episodes we cannot a priori define what's the what the the preferred outcome looks like so instead of doing that we attempted providing a more natural uh preferred outcome with the agencying itself achieving the goal but with the standard task background so we have this uh the blue uh just like uh background uh with the with the arm penetrating the goal and we we aim for this preferred outcome to transfer to the to the distracting setup this is of course pretty much impossible for for the likelihood the active inference model because of course he's trying to match this in a with a mean square there were like uh function so of course the the the signal provided will be very confusing uh very interestingly we see heels that also the dreamer method fails because it's based on uh on a likelihood based models and as we'll see uh reconstructing all the the variations in the environment it's very difficult so the the word the reconstruction based word model struggles uh to provide informative states of the environment while uh the contrastive learning based models succeed and in particular it's it's very interesting that our mother was able to to actually achieves the goal here we see less consistently than before so the there is an iron variance but still the agent is often able to reach the correct position despite all the difference in the background and all the structure present in the environment so we see that this actually the representation uh is actually learning what the the pose of the robot should be and trying to match it in the future and then here we see some some videos what's happening so we see here indeed the day the arm is oscillating a bit more so it's actually a bit more difficult for him to assess that he's doing the right thing but still the the behavior obtained it's still quite good i will say so it's pretty much achieving the goal and this shows why a likely based model will fail in this environment so here we compare the ground roof in the different uh varying backgrounds and what the dreamers or the the likelihood active inference model stays through the reconstruction so we see that either reconstructing from the from the presidio state or from the prior state the agent cannot perfectly model uh important information of the environment which in this case is the the arm pose so it sees where the the first link of the the robot term is but he is not able to see normally where the second part of the arm he is because he is very uncertain about that and then and that leads the agent to to not being able to actually assess where it is in the environment and to to provide the the right uh a value for uh for what's going on so uh using reconstruction in this environment leads to this kind of problem where the agent is not certain about the the internal states and so it's uncertain of what it should do next because the signal of the state is uncertain is confused and so it is also the the value provided by the to the agent by the model and that's it so i'll just briefly summarize what we have seen and uh so basically we use a contrastive model uh to reduce the computation uh of uh of active inference is also brought some advantages in reinforcement learning but yeah we focus on the on the contract on the active inference area where this model brought to a twofold advantage both in learning the the word model faster but also in imagining footer trajectories faster because you don't have the reconstruction then we saw that the contrast representation learned features that better capture relevant information for the environment and this was key in solving uh both the the richer tasks and especially in the ritual distracting task where without this this feature we wouldn't be able to solve the task and then we will show that we can we can use this method uh to provide performance that are similar to engineering rewards but in a much easier way so you can just say okay this is what i want to achieve in the environment provide the the observation to the agent and the agent will find itself a way to reach that that state without actually having to provide a reward function for every possible states of the environment which especially in realistic cases is is usually unfeasible [Music] and finally we have also seen that the acceleration is very key for our method to work because we don't want the agent to to convert to the sub optimal behavior that looks like the the right outcome the preferred outcome so it's it's very important to wisely explore the environment before actually delving into learning the our preferred policy and we aim to look more into this in the future so thank you very much that that was it i don't know if there's any question thank you both very interesting presentation so if anyone watching live wants to ask a question otherwise i have a few so you mentioned a critic model when you were describing the architecture and that reminded me of language learning like if someone says repeat after me and then they give a sound you might be accurate or you might not be but if someone said no it was you have a negative and a positive example so what does that speak to perhaps the biological basis of contrast of learning or how these contrast of learning settings active inference or not relate to the ways that organisms learn okay i i will say that the the contrasted learning mechanism though it's not completely equal i was added somehow remember resembles the abby and learning mechanism where you when when you have corresponding pairs of so the things that should correspond you actually want to to strengthen the link and when you have stuff that shouldn't uh be corresponding you actually want to to wake in the link so you know i think that biologically we could we could actually seeing this way so when you have something that you you want to be uh you you want to link further in in our case the thing that we want to to link further so to reinforce is the fact that this a certain observation correspond to a certain state uh then you you strengthen this connection well when you where you want you want to to be far and that's where where the contrast i think a bit differs maybe from this from this biological perspective we actually push it uh we push it further which is not always the case for regular learning because normally you don't have this uh pushing farther mechanism so i would say this could be one one possible links and as you said yeah the critic function is actually doing something very very similar to what you mentioned so you have a positive samples and you reinforce so the critic tells you yeah this is this is correct and it sure tells you that this is correct so it's trained to do that uh we we do it with machine learning but well if you have a good critic you could use that uh yeah it should tell you yeah this is the the right samples while yeah for for non-corresponding states and observation uh yeah this is not what we want in our representation we want this further yeah so maybe to add on daniel i think what you're hinting at at um providing like it should be like this is more like a way to uh to define preferred state so to speak so and if you translate it to do what we're doing it's basically saying um these observations are what you should like basically so so they come into place for um for trading the action model like how how do i get to these observations the contrastive learning part is more like um being able to distinguish different things basically um and and it's it's more broke than um than the different um well what you what you what you like to what you like to have so the contrastive learning just learns to distinguish all kinds of sounds even all the bad ones and uh and you just now say okay but now i really want to have this sound so try to get there i think that's the the difference here that kind of sounds like paying attention to the right details which we saw with multiple times like the breakout games like how could you miss the ball humans are watching that gif and we're watching the ball but we also have a sense of how to pay attention to the right details and then in terms of action to have curiosity about the right things so it definitely starts to bridge into some very interesting behavior another question was about the action entropy term in the free energy calculations so maybe could you restate what the action entropy term is since it's one of the major contributions and also what does that say about adding terms to the free energy calculation like the action entropy is always greater than zero kind of like a kl divergence and so that you mentioned gives some perhaps nice properties about the boundedness of f within a lower and an upper bound so maybe just what is the action entropy doing here can we just add other terms that are bounded at zero to free energy and use that in other ways okay so i'll start with the with the question about the uh the action entropy term and then i i'll also delve into the using different bounds for for uh the free energy term so uh here in this uh in the way we casted the active inference process for learning the actions uh the the key part is that we the actions are now part of the the future inference process so uh i i could i could also go back to the previous life that's necessary but the the normally the way that we see this objective is without this a term here and there but instead we have like uh a conditional on a policy on a certain policy so normally that means that you you have some set of policies already and you're just trying to decide which of them is better so this could be done like using a knock on browser for no cameras at first and then just assessing the the one that you think are best or just assessing all of them but it's that's impossible for instance in a continuous action setup where uh you cannot access all possible uh policies because uh yeah the actions are continuous a certain number is infinite for every dimension so let's make infinite by infinite and so on so it's it's a huge uh it's a huge dimensionality in space so instead here we we make the action part of the of the inference process so we want uh we want to have a separate model that tells you to tell us what's what's the action to to take at every step and uh i i what i said that we obtain an action entropy term uh and that's because we in in choosing the the best action so in in trying to to match our actions to the one that we we should actually prefer we we think it like we don't have a preference this over action so we so for instance i if i want to reach a certain state in environment so if i want to to go from this room to to the kitchen maybe i don't care what's the what's the sharpest word the shorter path is i just care about getting there at some point or i don't care about going left or going round right now when i when i get off of my chair i just want to to go where i need to go so we don't we don't place a prior over the action we just say uh whatever action is fine as long as it brings you the fastest as possible to the goal because the the the fastest thing is not given by the action itself but but by minimizing the free energy so we don't want a preference over the actions we wanted we want the free energy to this is the fastest spot and so the actions are we assume a uniform distribution over them and what remains is just an entropy so it will be a child divergence between the q over action given the states and the this p-80 which would basically becomes uh an entropy value if you if you assume this is a constant because it's just subtracting a constant to that and uh yeah switching to the to the other part of the question so what does it mean to us this constant term here so is having constant term useful so i don't know if there's any other useful constant term that we call that so mathematically speaking having a constant so having gone up an upper bound because of a constant will then be our mean because you're minimizing the same objective but then on top of that we apply the contrastive er the contrastive approximation and that leads to another upper bound and as i said there are some implications of these we get maybe a better representation but uh are we getting farther from there from the actual objective so from my point of view as long as we achieve something that is actually better it doesn't really matter how how far are we from the actual surprise i'll see now so in any case we will always uh get some kind of uh amortization or approximation and we will probably never get a 100 percent close to the surprising value so because we don't have a perfect model so a perfect model of the world doesn't exist it's it's impossible to imagine that we can model every details of the environment even with uh with a billion uh machine learning parameter and it's impossible uh to think that we will always act perfectly and always get in the in the perfect route to go uh using the the always uh optimal action especially because there's always some uncertainty in the environment and there's a a lot of things that we we normally want to ignore in our everyday life so a lot of things here is not actually important to to capture in the world model or action-wise not always important to be a hundred percent accurate in our uh movements so in the action we take the important things that we we get close to the goal so i i will say yeah if if there are any other term that we any other constant term or just any other modification that we could do to the to the free energy function that actually leads to better results without compromising the original goal of minimizing free energy i think that's that would be a good way to to address some of the issues that we that we currently have with uh with active inference and that that could also lead to to improving uh the performance of the the artificial implementation of active inference uh significantly so that's also why i think that uh taking advantage of some lessons that that we learned from reinforcement learning is actually a useful inactive influence as well because uh there has been a ton of research about uh way to amortize this thing or approximate this thing better or uh train a better deep learning model for some something some very specific aspect and i think which the the active inference research should benefit from this should take inspiration from this yeah maybe if i might ask peter can you go back to the slide with the action entry this one yeah so so maybe to um to make clear here or or maybe people that's that are less familiar with reinforcement learning but are coming from a more active influence background so in in terms of um active inference um as score christian would would look at it um this is basically something that you should not do uh so so basically um what happens here is that we we see action inference more like a habitual thing like i i know i'm in the states or i think i'm in the state so therefore i can just infer my action without even planning it's kind of you become habituated so i've planned this hundreds of times and it's always this outcome so i just stop planning and i just amortize this action into an amortized policy so that's basically the kind of the mechanism that that we apply here in order to avoid planning um all the time because that's that's the the the tricky part uh we have too much options to plan so we don't want to do this so we just say let's amortize it from start which basically means that this a this this or queue or approximate posterior is not only over states but also over actions and then so the action entropy just falls out by introducing the the action in the queue there so it's not that you that we magically add an action action entry return to the formulation it just comes out because of having the actions as part of our approximate posterior but so keep in mind that this also means that we have an approximate to steer over our action selection and this works um in in these three force flowing problems because there yeah your goal is always the same it does not shift uh it's it's not a it's also another uh uh um if you if you think back on biological agents it's not like a complex distribution to maintain from your state basically just yeah this is the reward this is where you get it it's always the same thing so this basically means that your environments where in which we test these agents and which also reinforcement learning solutions test their agents is exactly an environment institute for i can amortize whatever what i have to do because if i know my state i know what i have to do basically things will change i guess if you have another environment in which this is not the case where you could be in a certain state and you could still have multiple options to do and you and you can only you can only really know what to do by planning ahead or by first [Music] forage for information on on what's happening and in these kind of environments i i think that amortization trick won't uh won't help you a lot or you cannot do it just by amortizing so it's it's a trick we did to uh to allow it to work in these kind of environments because yeah we have to benchmark against uh some things and you have to be a bit on on on par there but so keep in mind it's not a silver bullet that will always work uh we do deviate from vanilla active inference here we cost action inference as like we just want to learn habits we don't want to plan but this also means that there might be situations where it will not work and then it's not due to active inference or free energy principle that's not working it's more like yeah we did a crude approximation here by which things might not work anymore that's interesting um about which training environments favor what kind of algorithms and then how that shapes the perception of different algorithms like the navigation task what if there was a fuel tank or there was a um larger space that was going to require like multiple foraging information foraging trips for example and um so then the sort of single-minded seeker is going to just die fast but then something that's able to actually engage in planning wouldn't so that was a little bit like to those who are familiar with active inference and then here's a variant on what we've seen before how about for those who are more familiar with the dreamer architecture or reinforcement learning what makes active inference active inference and how is it different well i think [Music] they are um they're largely uh similar let's say i think that would be the starting point because of often people think about what's the difference but but i think the the main point that we should rather stress more as an active entrance community is that there is a lot more similarities between reinforcement learning and active influence i would say that active entrance is a bit more general than reinforcement learning in the sense that on the one hand um we don't use a reward function per se but we we relax that a bit as in we just have a distribution over preferred outcomes which is a bit more general i would say and then the second thing is that instead of by starting off from the free energy principle as in this is the objective that we want to minimize you also get the the extrinsic value term here which is exactly the same thing as what the next reinforcement learning agent would optimize so if you only look at extrinsic value your free energy agents will also do this but the the the the added value i would say comes in the information gain terms and these will only give you an additional benefit in environments where there is information to gain uh and this is not your typical reinforcement learning environment but if you look at for example the the the the team mace mouse from call of wisdom these are typical environments where you can actually show that if you only go for extrinsic value yeah you you won't you will be acting super optimal so you can actually prove almost that in some environments uh only looking at its extrinsic value [Music] given the correct model of the environment active inference will win i think the crucial uh thing that we need to research on is how do you get the the of the the correct or the optimal model of your world in which by optimizing uh your expected free energy actually you do the sensible planning and this is still largely unresolved and with our models we are taking steps in that direction but as you can see there are lots of issues to just find the correct mole because in in if you just look at the mouth the likelihood based model should be perfectly fine but by by the way you optimize and practice then you see all kinds of problems like okay this this this little pixel is actually the most important pixel of the uh of the thing and that does not appear so in my loss function so that's why everything collapses so in theory every it should work but there are a lot of practical problems to to find the correct model that pays attention to the correct details or the correct aspects of your observations and this is still that this is something that is shared with model-based reinforcement learning as well as active inference uh and and i think there's a huge opportunity to find new uh techniques that can can put forward both both fields and we also show this like a contrastive dreamer in the distracting environment also improved performance on the normal dream so by by having a technique that lets you build a better model any model based algorithm will work and active inference has this special notion of also taking your accounts uh information gain in environments where you might be unsure unsure on on what your status so that's that's where it can prevail but uh i think in in most of the benchmark environments that we've seen nowadays especially in machine learning uh you probably don't need these terms you probably get away with uh just maximizing rewards uh which is in in in fact also an active inference uh agents and to some sense if of course if it's if it's i'm talking to about a model-based technique so like dreamer agent in this case of course the model 3 ones are these are a bit different as they they don't need a model at all but at least for model-based reinforcement learning agents i think it's pretty similar to what an active influence agents would do in these environments thank you tim pietro anything you'd add to that yeah i would like to to discuss one aspect of dreamer that we a little bit overlooked it is the fact that uh makes this amortization similar and it's it's also similar to what we have done so yeah the this this is we learn a policy basically we are an actual network that provides the correct state the correct action for every state but uh it is the key step that actually brings us closer to the active inference formulation is that we we imagine uh several time steps in the future so it is true that we don't uh evaluate uh long policies and over over time that that we have this uh this uh prior about action that is given by our action network but it is also true that given the fact that we we evaluate the state we we expect to see and then from there uh we restart uh doing the the action optimization process we actually get closer uh to to the optimization scheme of active inference in particular there are there is a paper called sophisticated inference that discuss this when you when you actually take an action and then you reimagine from that step what's gonna up what's gonna happen there are some implications of this but uh we are not completely drifting away from the from the original active influence theory because of this it's just a different way of doing the action selection process and in that indeed the dreamer is also very close to to active inference itself cool thank you i i wrote down if you don't know where you prefer to go you are lost drive fast if you know how to get there figure it out if you don't and then reassess continually and i hope that conveys some of the similarities and differences do you have any final comments this is a very interesting line of research and we really appreciate this model stream hope to see you in the future or should i say we expect and prefer it but thanks again pietro and tim this is really awesome thanks for having us thank you for having us have a good day everyone see