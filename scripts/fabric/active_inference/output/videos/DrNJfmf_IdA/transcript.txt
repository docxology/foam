hello greetings it's November 11th 2022 we're in cohort one of the textbook group and we're coming back to chapter 10. looking at the sections of 10 asking any questions about 10 thinking about any overview points before next week when it will be our final meeting and we'll talk about feedback fill out the form in future textbook groups talk about project ideas next steps talk about our plans for 2023 and so on um but where do we begin with 10. we covered many of the sections once last week there's a lot of sections the chapter kind of is like a symphonic presentation it has multiple movements and subsections that have their own like Rhythm and structure let's go to the end I think this is um really important note about learning and active engagements which is if you're not even before skin in the game and psychological ownership and so on if one is not making predictions not just about material like what's going to happen next in the TV show that I'm watching oh it surprised me but about the consequences of one's actions in the world they will not be able to update those models of action in the world so you could be surprised and end up being a good infer in a passive inference setting however doing is what actually allows you to learn in a totally different way because whether you realize it at the beginning of the journey or not or whether you even agree with this or not those actions that you take are chosen and exist on a trade-off Frontier of epistemic and pragmatic value and so simply choosing to act is a strategy that opens up a space for pragmatic as well as epistemic action it's kind of like the hyper prior on active inference is how active and one other um no just that anyone can add anything um this was a discussion with um with JF and Camden um some days ago in in our research meeting um about the scaling debate in machine learning all you need is scaling bigger natural language models and so on um and like how can active inference and how can we contextualize that and improve the state of things and um we talked about how importantly GPT does not choose its own inputs and so yes it can engage within a perception action Loop related to receiving inputs and sending outputs however in terms of its own training and development it's importantly constraints that's not necessarily A Bad Thing it's just that to actively select what to what stimuli to sample is very Central to active inference from the ocular motor to the tactile and so all of the Big Data training algorithms do not select their own inputs and then if they were to it could be done in two ways it could be done in an ad hoc way or principled way the ad hoc way would be um you do some kind of um different architecture of a model that you know predicts what inputs would be valuable to train or you could think of a unified imperative that would decide on both action and learning rules Eric and Brock yeah I suppose you could say that reinforcement learning the the machine is selecting its own um samples to get feedback on now reinforcement learning is not gpd3 it's not language learning language model learning but um I think an earlier Paradigm maybe I don't know I think they still use reinforcement learning in deep learning models although um you know it was also I think a lot of people are discouraged by that because it's it it you know as they say it's uh it's a very narrow uh very weak low bandwidth signal you're getting back but it is an example of learning models using generating their own samples thanks Brock um I was gonna I guess uh take tools biological place we left off in last session of cicading of like being a kind of data cleaning process almost or um stabilizing the you know the like data set coming in and um that we do have some very very crude like uh you know built-in labels for edges kind of in our visual cortex right uh that are presumably evolutionary um and some decent evidence that like maybe one level higher something gestalt-like of like edges and Heights you know um or uh yeah just other certain kinds of like really really uh generalized [Music] um images or views that you might take that have like very pronounced uh definition edges of like like a a prayer you know like a landscape of you know our natural evolutionary landscape being like somewhat comforting or pleasing or something like this right and walking over like a baby like eight months or whatever crawling over a glass but it looks like looks like they're gonna fall and they're they react to that right um like these sort of things uh where there's some sort of baked in like priors or again like labeling right um there's no I mean are those hyper parameters you know there's but then they're not controlling their hyper parameters doing these models like so um yeah I mean there's no affordance there for that sort of like goal directed or selecting data sets in any possible way or any of that so I'm thinking of a of a British English reading of how the world should be like we talk about expectations and preferences and if someone says like I should think so it's not actually a normative stance it's like saying I I expect and prefer myself I should be there at eight whereas often should is interpreted as purely normative like I wish that you did this [Music] um but should actually does kind of condense that expectation preference um scenario and then to kind of come back to the um inherited World models so yeah um sequential processing steps can be understood of within a hierarchical processing framework where higher levels have the interpretation as being hyper parameters on lower levels and I think we saw a little bit at the end of um nine which is basically for any and this also speaks to the composability of Bayes graphs which is that you can always have a variable as fixed or you can have a hyper prior on that and an uncertainty on the hyper prior or you could wrap it in another base graph but all of these can fall under the category of how things should be for that agents and a business that thinks that there should be um a lot of Interest next year and is wrong goes out of business the organism that thinks that it should be warm tomorrow night and doesn't plan for the cold dies and so the the should almost reflects the whole cognitive stock and the action selection that makes it that way one area that yeah I'll leave please uh yeah I was reading the other day about uh the importance of uh the statistical regularity of uh any particular environment and uh the way it constrains uh The evolutionary developments uh I mean uh for instance for terrestrial animals uh the majority of the objects are solid objects and of course that's not the case for sea animals but um uh this I was wondering that uh when an organism generates it or constructs it its generative model uh how exactly especially in the case of visual perception uh how exactly these kind of statistical regularities uh or let's say uh the statistical regularities as applied to their visual perception uh translates into their generative models because uh for the most part at least in this textbook what we saw in generative model is just exactly it is just an abstract conception of generative model and as you say how the how the world should be but I don't see how these kind of um evolutionary constraints or statistical regularities of the environment takes place when a generative model is constructed at each for each specific task because in my view that would make a lot of difference in constructing the other priors according to those regularities I'm not sure if I express my question clearly enough but yeah I was wondering how that's done in active inference framework yeah good question um just on the visual note and then the generative model on the visual note there are some statistical regularities of natural visual scenes like Pebbles on the ground trees and like there's some autocorrelation functions and some spectral density analyzes and some fractal similarity things that people have done so that like for quote natural scenes so not necessarily a skyscraper but again like the bodies of animals and um natural settings with trees and plants there are some like hyper parameters that make the scene natural which is interesting to think about in these like image generating algorithms and then um to the second point about the generative model um it reminds me of jf's approach towards Society of Minds where it's like spinning up generative models now in that case it's symbolic logical generative models but also we could imagine Society of statistical generative generative models um Eric yeah I would just um Echo Ali's question about how um or you know a a a a a a a a gap in you know what we've we've learned about active inference which is how do these models get made in the first place um you know for the examples in the book um well he walked in with the teammates and a model for how team maze works and that was probably the most sophisticated active inference model presented um but when you get to you know real world scenarios well how are models of the world learned and that's a big open question and you know are they even what sense are they generative models uh often predictive I think but um um you know are they probabilistic generative models in the sense that we think of them mathematically I I that's you know somehow people model it that way some people don't don't think so um you know certainly neural networks are discriminative or in many cases um so I just I guess agree that that's all there's a lot of a lot of uh work unsaid to be down there are a lot of questions open hmm I wonder if some people hope for perceptual control theory to kind of Save the Day which is like we don't need to make a generative models for the bewildering possibilities of external States but it might be enough first to have bodies that can engage in interception and homeostasis and allostasis and then with that Machinery as as enabling maybe all you need to know about the external world is like it's good or it's bad and just like run away from bad things and go towards good things and then you have to you know you can update your a matrix on whether you think good or bad States emit this or that kind of observation but [Music] um the the blanket facilitates the organism's hyper focus on its own States and might allow it to engage Less in this open-ended structure model fitting of the world not that you wouldn't do better with infinite resources in a better causal model of the world Brock um let's open the kind of discussed with JF actually too about um some of his generative models are going to be static and specified just like the robot is literally specified by him um it seems like if you wanted to you know have a more General explanation or process or whatever to get to whatever human level or something things that start exhibiting those sort of behaviors or or uh you know come insert with active inference type models of like you would need Devolution like like you would need to evolve the system to that point and then you could ask a question like if you did that would you gain anything compared to just build Optimus and it's not as good but it's a good like first order approximation first step of like towards something that would be constrained in such a way that it's more likely to produce generative models that are useful and whatever could be human-like or whatever you know um yeah conscious intelligent something whatever um but I didn't see how yeah I mean any possible explanations on the table for like how you get to something that's whatever ha that has some sort of central nervous system you know or or conscious or anything like that um did would all depend on something like evolution and and then that process is just necessarily like very long and laborious and if you're just if you're looking for something novel then maybe you have to do that but if you're just trying to get the first order approximations something similar to what already exists and why go through all that you know it's probably easier to just assume some priors which is what JF is doing so this reminds me of the problem in machine learning of one shot learning where like a single image is shown and the goal is to learn cat as a category from one picture of a cat now there are algorithms that do better or worse but there's also some fundamental limits of any type of One-Shot learning like to say nothing of over generalizing you know overfitting but of course so it's almost like this is um statistics slowly retreating or advancing into biology because it's like well if if we don't want to do just one shot learning on one data point or one data set which is one datum still it's one big datum we want to have learning model okay now we want to have a learning model with active selection of inputs now we want to have generators and basically like Landscapes of models portfolios of models Bayesian model reduction and structure learning in the Bayesian setting as being across distributions of models and then that's where Evolution gets us which is populations of models existing in different niches and maybe if our data set is our generative process like maybe there are some models that are doing like really well in this part of the data set and there's other models that are doing really well another part of the data set you don't just compromise them or um you know ligate them together their embodiments reflects the statistical regularities of the environment and I added this um citation this is from Professor Gordon my PhD advisor and it's like and there were several subsequent papers that continued on this theme but it was like trying to get without putting words in your mouth or anything like that like trying to make this point that you couldn't you can't just look at the collective Behavior or the end of you know everything's Collective all the way down so you can't just look at the behavior of an ants and just say like whether it's like good or bad or fit or not because there's Landscapes with uniform and passy distributions which is exactly the case for statistical distributions there's cases where the cost of stopping is high or low relative to the cost of operating and then there's situations where basically interruptions can be easily recreated or not among others um and it just like degenerative process is what the generative model is fit too so it's an incomplete sentence or an incomplete expression to be like speculating on just generative model architectures without a little bit of at least an acknowledgment of the statistical regularities of the generative process the generative process is a metronome that's alternating between states then your generative model you have some suggestions for how to design it if the generative process is another agent like you then that's another that's like the generative adversarial neural network case or the game theory case or the theory of Mind case or the communications case like those are different settings um returning to chapter six what are we modeling what is the appropriate form for the generative model how to set it up and then how to set up the generative process and I think we even discussed a little bit then like this is kind of a generative model first approach whereas one could imagine step four moving to step one and a half or you know however but but um it doesn't make sense you know nothing in biology makes sense except in the lay of evolution famous dubjansky quote so will we have similar memes you know nothing about the generative model is going to make sense except in light of a generative process the state transitions in the generative model they are parameterized given the weight of the leg that's not part of the parameterization of the state space but in fact that parameterization makes sense in light of gravity friction [Music] the solidity of objects and so the the disembodied General generalized uh General generative models we lose sight of it because it's like it feels like we can totally Define the whole setting therefore the parameters of the generative model should be like maximally interpretive but that might not even be true for the in silico case so especially when we're taking the metabasian perspective from chapter nine and we're thinking about doing inference on an inference Doer there's a lot of you know X only makes sense of Y which only makes sense in light of Z and I think it's actually that more complex predicate I'm not sure if that's the accurate way to say it but it's the more complex setup in the metabasian model that's one of the energy barriers to understanding not just active but just good science and the map territory and all these other related topics especially because this dashed line that's our own Gray Zone so our own parameterization is only going to make sense in light of things that we almost surely didn't specify even our own parameterization about it ourselves and so like understanding I mean then to kind of come all the way back foreign leads us to an existentialist like who are we what are we where are we going why are we doing that but of course that's one of the places that you'll get to in this road network with a high road in the low road um one area we didn't go too much into which is totally understandable given just that it's a textbook that we've read over the last several months and all that is like the citations and just okay you know structure learning Tenenbaum 2008 well we know Tenenbaum is still active so and it's always useful to cite earlier papers that are more positional however um there's a lot of threads that could be connected to like I don't know a 2019 review on structure learning techniques so that the framing of the question and the comparators might be a little bit more modern learning to learn in machine learning 1949 formation of learning sets the seriously brain damaged monkey developed sets which allowed him to behave more efficiently than the untrained non-brain damaged monkey it's brain damage is adaptive is that what this is the abstract that's not the conclusion but I just can't help it that's yeah you can see that the clickbait headlines already me um I don't know continuing this the title of the chapter and um another question about categories from the last uh session here like what what do what is is there a specific active inference definition of sentient how does that relate to you know attention salience epistemic Dynamics and not going down that existential rabbit hole we just left off on uh doctor sentient Ellie what's is there an active specific uh I think he dropped off in a second he's back but if you could talk Ali but I I just I joke because I know Ali was well um the upcoming Live guest stream that we'll have with the um authors okay so that they recently had this papers in vitro neurons learned exhibit sentience when embodied in a simulated Game World they played pong and and then like people were um yeah okay people were freaking out because it was just like a sentient as conscious but Ali what is the take well uh I'm I'm not exactly sure what uh I mean what what was the controversy about because uh it's it was merely about just uh alexical meaning uh of a term uh which is uh obviously something that that you can look up in a dictionary and get those lexical meanings because uh the word sentient uh is uh defined in Oxford English Dictionary as uh an organism that responds to stimuli that's it so one act at least one of the meanings of the word sentient so yeah I was not sure why that the use of this word uh raise the I mean so much controversy on Twitter and around because that's basically how this word was used even in uh 19 and 18th century so uh I mean before all these uh developments in artificial intelligence and philosophy of mind and so on so uh that's just minimal requirement of to for something to be described as sentient yeah I I guess um this framing distinct Centier you know in Spanish mesiento I feel a certain way it's situated in a not just a cognitivist but a phenomenological space and so I believe that's some of the baggage people brought but you're totally right going to another mainstream source it simply is the responsivity to Sensations is that like exhibiting agency I don't think exhibiting policy selection you know I don't even think it has to act something could be sentient and not take actions although one could then could you could say if it doesn't take internal actions or covert actions of some kind then was the impression made or was it just you know Teflon just sliding off of its perceptual apparatus so um but I mean it is what this chapter's title hinges on yeah it is I'm not sure I'm not sure that we I don't know uh yeah it's it's still a little confusing in the light in the context of active inference to me I think like it would have to be like the case that you like you're saying you know some sort of action was taken maybe it's a hidden state or something you know from the but uh I don't see how yeah like it could be responsive to stimuli and not act like um because I mean I don't know like a rock rolling down a hill is responding to the stimuli of the you know gravitational geodesic there it's like is that you know what I mean like that's not really um that's not response to simulai right so that's probably outside of the definition there I don't know maybe in Bayesian mechanics it's not but does sentient even add anything and are we only talking about organisms organisms in the kantian definition is the unit of biological organization that is the end in and of itself not that that necessarily helps whereas a more modern replicator evolutionary definition describes the organism as like The evolutionary unit or replicator um like the ant colony as an organism um and then olly I just saw you linked agential realism I mean could this have just been one of the benefits of active is it provides again we talked last week whether it's complete um provides a solution to the Adaptive problems that agents have to solve or inactive agents or active entities or active inferors have to solve but not not sure whether this brings in some lateral associations and over specifications that aren't necessarily entailed by simply active inference okay uh actually uh Karen Barat uh I mean uh the physicist to propose this um ontological stance of agential realism defines agency um as a kind of relationship between objects and not exactly as something that a subject or an organism has or capacity of an organism or a cognitive agent so to speak so in this sense if we look at um this definition of agency we can say that every kind of organism and every kind of even persistent objects in various time scales would necessarily have some kind of agencies what agencies and other polycamists were of course because it can refer to the agent-like behavior meaning something you'd specify more as like a mobile actor in a agent-based model but also it is then used to specifically also mean taking action and like in the Bayesian physics the textbook does not engage deeply with Bayesian physics and it will be interesting to see in the coming [Music] one to three years as um Dalton's at all's work becomes percolating through the space and potentially as reflected by a second textbook with friston and maybe others at MIT press um whether this the different specified kinds of particles help us step outside of some of the previous framings of agent and agency so if agent is just going to mean thing we have a way to talk about thankedness in terms of its repeated measurements Quantum things thermodynamic things cognitive things so we have a good thing definition first in 2019 which is cited in this textbook but it's not gone into too much um and then we could talk about mode matching mode tracking mode tracking and path tracking things and chaotic things and strange things and so um a piece of wood floating down the river is following a path of least action that doesn't entail any cognitive model it is a thing it's agentic but it's not taking agency if it were um a stick with um a different apparatus or increasingly different kinds of policy selection mechanisms that wouldn't change its thingness but it would be a different kind of thing also I I um hope it's okay to share that and anyone who's watching uh on Monday at um 6 30 A.M PT Maxwell ramstead is going to be giving um meeting at the theoretical neurobiology group so email active inference if you want more information but he's going to be talking about some of these areas basically as part of this banner year of 2022. um I believe there are some threads around Ollie you can totally say it differently if you if you see it differently but like um kind of standing with two feet in octave and fep as a framework in a filter as Dean might say rather than this procrustian journey of trying to stretch and distort what fep is into other ontologies that might have false positives negatives no quantitative definition in sight and so on and like just the way that so much ink and attention has been spent on framing fep Within past lineages of debate or trying to bring it to bear on one side or the other but I think that reflects a Time coming when a true fep interpretation actually is possible rather than seeing fep as interpreted by others but that will be kind of interesting um okay well we have five more minutes oh what a year it seems like going faster would have been challenging one chapter per week would have been a blitz three chapters per week I mean I mean three three weeks per chapter yeah exactly um three weeks per chapter would have taken us into the um you know two-thirds of a year range with some modifications and improvements that I hope everybody will engage in and and have so much to contribute to like in future textbook groups and and other affordances this is something like a semester-long course and that um like a four unit course or something where for two weeks reading a chapter with a few other selections so providing some other work and maybe starting slightly differently maybe but these are all things that we can explore just kind of like raising them I know that all of you are thinking about this too but just wanted to like say it um within the context of a semester course and initially more like a seminar course like we're working through it and some people like read and understand highly all the material other people like um fight through and get through some of it but it all just is what it is before becoming more of like a graded course not even saying that we're gonna grade I'm just saying like there is a format archetype where challenging material is addressed usually as a graduate seminar and you basically pass if you participate and it doesn't require tests at all before we get into other kinds of Assessments and everything um any thoughts in the last minutes here uh thank you for doing this and having the cognitive range to the answer between all these disparate gestures that we throw at you and uh yeah I really enjoyed it so thank you and also just one last night I maybe said it for some earlier but um we're in contact with the authors and with the publisher and um like I sent them some updates they're super excited um so we could schedule time to all talk potentially as like recorded but not live streamed um in early 23 with at least Thomas if not others and um we can return next week to some of the the future steps and things like that but it's just like it'll be cool all right next week all right thank you fellows farewell bye thank you everyone bye