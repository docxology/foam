[Music] [Music] great hello and welcome everyone it is just adjusting things it is may 4th may the 4th of 22 be with you and we're here in flab live stream number 43.1 blue i don't know how you do it with always joining during the theme song welcome to the actin flab we're a participatory online lab that is communicating learning and practicing applied active inference you can find us at links on this slide this is recorded in an archived live stream so please provide us with feedback so we can improve our work all backgrounds and perspectives are welcome and we'll be following video etiquette for live streams check out activeinference.org if you want to learn more about what's happening in the lab and get involved and participate in the live streams or any other activities all right well we're here in live stream number 43.1 and are continuing the discussion of the paper predictive coding a theoretical and experimental review by baron millage neil seth and christopher buckley in 43.0 with maria we went over some of the background and context and did like a first pass on various aspects of the paper but by no means exhausted it because it's a long and very intricate paper and also it touches on a ton of other areas of more general interest so there's a lot to discuss and in the 43.1 today we'll see where things go if you're watching live it would be awesome to have any questions in the live chat otherwise we will just start with some introductions and pick up with whatever people are finding salient and exciting so we'll just start by saying hello and going from there i'm daniel and i'm a researcher in california and thanks a lot blue for joining i'm blue i'm a researcher in new mexico and yeah i'm excited to be here today this paper was um really intricate and detailed and provided a lot of background um that i don't have because you know there are so many active papers that i want to read but i don't have time to get to them all um because i'm i like to read things very thoroughly and slowly and carefully and look up the things that i don't know and the back references travel down the scientific rabbit hole so to speak so yeah this was a good paper that kind of helped me you know shirk all those active responsibilities that i have and um came out with more like questions than than answers which is always a good sign of a good paper awesome well how about going into the paper what questions did you have and maybe we'll get to how our questions evolved but what were you expecting to find in the paper and what questions were motivating you to seek it so i i always have had a hard time like what is the difference between predictive coding variational base and active right and and like i was really hoping that the paper would kind of lay that out for me and like no so like i mean i kind of got a good historical overview of um how they all are interlocking and overlapping which i didn't know before um because you know all these things none of them are simple and there are intricate details in all of them like you know that i don't know all the details ins and outs but i think that this gave a very good like historical overview of how these things kind of layer into a nice little sandwich great definitely a good starting place let's also add bayesian brain there to the mix and start to look at maybe some similarities too because they're not exactly like addressing the same area and they're compatible often more compatible than not so getting at what distinguishes them is very important and like one way that that happened and it came up in the dot zero was so much of the paper was dealing with inference on the relationship between observations and unobserved latent states so think like trying to infer the temperature of the room given the thermometer readings and that was so much of the paper and then action comes into the picture in uh section four five it's like predictive coding the authors write can also be extended to include action allowing for predictive coding agents to undertake adaptive actions without any major change to their fundamental algorithms but that's 50 equations deep and so it's very interesting to see like if those 50 equations are kind of like the stem i know that the equations are not sequential and building on each other in in all of these cases but then action gets tucked into a bigger picture very seamlessly and i think that speaks to the the relevance of distinguishing how they're similar and different and why it matters i guess so how should we even begin to look at that so even action actually came up earlier in the paper before 4.5 so action came up in um section 2.3 with dynamical predictive coding and generalized coordinates and and this i thought was a really good um example like like i was craving and still i'm craving like a really simple example like like it's one thing you know we talk about the equations we go through all the letters we compare this and that but it would be really great to see a super simple example works through like from start to finish using all of these observable states latent states complexity accuracy like it would be really great to see that at some point um and uh if you're listening to this live stream and want to come on and give like a guest tutorial on like how do we actually work these equations into some very simple problem that we can solve in real life like i would be more than happy to facilitate that presentation um yeah because i can't even like i can't jump from like all these equations into like how does that apply um but even in in 2.3 this was kind of a good um a good a goodish example a a real life example about like inferring you know the velocity and the acceleration and the jerk um just given the the observed coordinates um so that was kind of and it talks here about also free action which um i've not heard before so instead of minimizing the variational free energy it says we must instead minimize the free action which is f bar um and i've not seen or heard it referred to in that way but it's modeling like time series of observations um so like you know you know the coordinates at time one time two time three of a of a swinging pendulum for example um if you know those coordinates then you can kind of guess you know how fast is the object moving and you can infer these other these other variables so i haven't seen that free action but it did even come up before four or five yes and it speaks to that expected free energy efe which is what do you expect the free energy to be and then there's the free energy of the expected future which one of the authors has worked on which is kind of a slightly different variant and then here's another way to look at the free energy of some future time steps and and it speaks to that difference between the variational free energy which is like what you're able to calculate right now with the information at hand and looking backwards and so the more perceptual aspects of inference can be calculated using the variational free energy approach but then any time the future is coming into play there's uncertainty about the observations as well as even uncertainty about what actions one will take let alone the consequences of the actions that one will take in the future so we do need a different way to calculate it and then as you're kind of getting at this is a different way than we've seen it before with a minimization of free action but let's return to this page and think about how we can distinguish with like a venn diagram or something that helps us understand how these core ideas are linked one couple that we can highlight is predictive coding and predictive processing do you have any thoughts or questions on that yeah so maybe not a venn diagram but like maybe we should build perhaps like a pyramid or like a multi-level building because i think um you know based on the the um what i read in the paper i think really predictive coding kind of starts it all off and also um it makes sense to make a layer cake like being that these all kind of start off with the layers of um cortical processing that happen quite literally in the brain so i i think um making a layer cake maybe makes the most sense okay so we're in the kitchen we're in um the active restaurant kitchen and what are we gonna build in or what is the bottom of this layer cake so i think the bottom at least um so so there might be two layers on the bottom um one from helmholtz so we can start with like the the perception as unconscious inference view of helmholtz um and then predictive coding um as described by rao and ballard um in their description of the visual system in the visual cortex okay and again kind of calling back to the dot zero the uh two qualitative philosophical ideas that the authors invoke and that maria helped us unpack were helmholtz's notion of perception as unconscious inference and kant's const notion that a priori or beforehand structure like prior structure is needed to make sense of sensory data so we're gonna have one um bottom layer of the cake with uh helmholtz and then also with uh the kantian so this is going to be our qualitative philosophical layer of the cake these two notions and then you mentioned another critical work which was round ballard's 1999 work building on a broader history of applying predictive and anticipatory approaches to different neural systems and in the dot zero the two neural systems that we looked at were first the retina system with a srinivasan 1982 and this is looking at retinal physiology and like the electrophysiology of photons hitting the retina and then another area where predictive approaches especially early on were getting built out around and towards was the cortical hierarchy and the different layers in the mammalian cortex so these two anatomical exemplars of some micro circuitry some histology that is compatible with a predictive approach okay so we have these two basal layers but we might even add more basal layers but we have biological systems that are doing predictive things and that includes the retinal as well as the cortex also of course if anyone has like questions we'll get to it in the right way so please ask any questions or doing any comments so we have the retina the cortex as examples but there's others okay and then if there's a third layer that we can add to the bottom i think it would just be bayesian statistics because so i think it comes maybe before that also or maybe like maybe now um maybe bayesian statistics is like a a layer two but like i think before the bayesian statistics um and even before biological systems doing prediction there's the information theory component that i think is important also okay so well we have um this is qualitative and philosophical then we have biological and actual and then we have a formal area and that'll be information and base information yeah the information goes under the bays i think just like um the helmholtz comes under the cont right um i i think it's the really the relationship between um information and probability which we started to explore in um or we explored a lot in the uh fep of generic quantum systems live stream number 40 that was 40 i think so many streams ago okay this is fun and i think we're we're going to start building this into a tasty structure so we have the philosophical coming in on the left and that's one on-ramp that just like we had discussed earlier like this is something that has been almost imminently available to thinkers for thousands of years these are the kinds of qualitative claims that anyone can experience whether by thinking about the blind spot or about how their sense making in response to some stimuli this is just the idea that the sensory observations are first off not the direct contact with the world like we're not seeing the lightning strike event we're seeing photons hit the retina and sound hit the eardrum and so we're receiving sensory input that's that kind of plato's cave angle and then kant and helmholtz elaborated on that to include this idea of unconscious inference that requires a priority structure or like what we would call like a bayesian prior then we have this biological area and these are just reflecting natural systems that are exhibiting some kind of behavior like we can think about the 1999 paper of round ballard themselves about the functional interpretation of extra classical receptive field effects and we talked about that in the dot zero like the classical effects are kind of the simple normative ones like the joke is it's classical because of the papers in the order they were published but that doesn't like make it even true let alone the best model the classical is just like it's like classic rock you might like other genres but then there's the classics or whatever so that's the biological systems that are exhibiting certain kinds of outcomes and then we have these two formal or quantitative areas and those are the bayesian statistics and probability as well as information theory and they're definitely linked as well but we'll just leave them as kind of two conjoined layers here okay now where do we go from here well isn't that the question so maybe um predictive coding and predictive processing um and like really what's the the difference between these two i think predictive coding came from neuroscience so maybe that layers on top of the biological part the biological piece of the cake and then predictive processing might go on to the quantitative piece awesome yeah and it really is what maria highlighted and and drew out in the dot zero which was using a a quote from an andy clark book and clark wrote that predictive processing is not simply the use of the data compression strategy known as predictive coding so at least in this take predictive coding is like mp4 it's like an encoding strategy which is also why we connected it to frame differencing and video encoding but predictive coding is going to be something that's data oriented and it's related to information theory compression information encapsulation and then clark is writing predictive processing is not simply that and so maria spoke to how another way to think about the difference between predictive coding and predictive processing is to use coding for the formalisms and implementations and processing for the philosophical understanding that prediction is the basis of signal interpretation as opposed to merely recognition or descriptive models so if anyone has like a thought on that then you know put it in the live chat or come get involved but let's work with that delineation going forward so predictive processing is going to be um somehow related to the formal or sorry predictive coding is going to be more close to the formal whether it's predictive coding in a biological context or whether it's going to be in a philosophical context or something it's going to be more on the formal side whereas predictive processing is going to be where a little bit closer to the philosophical yeah i think so and then maybe like does the bayesian brain like make up the the third part of our like predictive coding layers over the bayesian statistics and predictive processing layers over the philosophical then does the bayesian brain somehow make the in-between sandwich that goes over the biological okay okay so let's see we have predictive processing as being a philosophical approach and consideration of these philosophical memes and themes in the biological case which were certainly bound to then we can have predictive coding as kind of the formal way that these predictive anticipatory systems are implementing predictive processing and then what is the bayesian brain seems pretty fair to put it at like the intersection between bayesian and brain but what is the bayesian brain to you so to me the the bayesian brain is kind of um you know i i'm trying to look back through the paper to their uh description but um in my like recollection recollection of the bayesian brain which was like probably my first introduction to bayesian statistics i was like oh this is great um and it wasn't even the bayesian brain it was like the janesian interpretation of the bayesian brain paper but like that paper was like probably the my like turn on to bayesian statistics and and implementing this and um so so in my mind the bayesian brain is is just this idea that the brain uses bayesian reasoning or bayesian updating to process information and and go forward so but that's probably a very simplified interpretation i think there's going to be one more realist interpretation like the brain is doing bayesian stuff and then there's the more instrumentalist interpretation we can use bayesian statistics to model what it is that the brain is doing so is bayesian the territory or the map of the brain but we'll just leave it as an edge for now so i'll read a question from dean and then i think we're going to be exploring a lot where action and active inference comes into play so blue dean writes what do you think of this as coding you are doing something in coding as processing you determine by comparing processing and or not processing as you're doing so process entails processing types what i'm hearing there is encoding doesn't require two arguments it just takes one you take one file and you zip it you've encoded that in a way so predictive coding can encode and or implement some type of informational algorithm without a reference or a comparator whereas dean's saying as processing you determine not with this single argument um plug and chug but you determine by comparing as you're doing and so predictive processing it it does have that sort of top down and bottom up as it's often visually represented but processing is entailing that full stack and therefore the processing is requiring something like a minimum of two to have consideration if you're processing nuts then you're sorting them into two different kinds or you're removing something from something else about those like you're doing something that has compositionality or some type of multiplicity to it it's not just something you can take that one file and just zip it so that's a great point dean i think i'll add two little arrows um here yes okay yes blue let me share the slides thank you um i'm left in the dark here uh i definitely um yeah agree with dean's point about the um coding versus processing like i can write a whole lot of things right like i can take a whole lot of notes and that's coding that's like encoding the information from our conversation this live stream but like until i do something with those notes like turn them into elements of a paper that i want to write or or something like this like i have not processed what i have encoded until i take them forward to a forward step so i think like there is kind of this temporal implication with processing versus coding yeah also like the timeliness of processing like you want to process the food before it has become you know bad or something so okay we have predictive coding again these are just tentative slide play if anyone thinks differently they can join or write a comment but predictive coding might entail something of a unique directionality just like a dot zip and you say well somebody will unzip it later but we can encode it without that part of the loop being required and then we have this two directional relationship with predictive processing being like top down and bottom up um [Music] okay what do you see now or i can bring up something um yeah so i'm not really seeing anything so why don't you bring up what what is on your mind or what what you're thinking of where is action and then i think that'll start to walk us towards where active inference is so because of the the temporal nature of processing i think action might start in the processing um element like even in something like mental action like people do like thought correction or thought remediation um so like if you're uh you know you have this prediction and it like surfaces in your mind but then like you correct the prediction or like you um you know you think you see a snake on the floor and then you say oh that's a snake but then like you look let me see that snake like and then you have to zoom in look at it closer and it ends up being like a coiled up rope right and so it's not really a snake but like you direct your attention because of some prediction and then can continue to process that prediction so so possibly action starts in the processing um [Music] in the processing area or maybe it's there in the middle well i'm gonna start it in the middle we can see where it goes because i think it's going to have an important edge to each of these three areas so there's a few ways to represent it of course so how does action relate to biological systems or first let's let's um start with some philosophical frameworks that highlight the importance of action so this includes an activism embodied the four e's five e's seventies et cetera et cetera et cetera everything involving like this sort of embodied enacted and cultured extended etc approach to cognition and philosophy and those can be qualitative so i'm putting that edge here these are different qualitative and philosophical areas yes they can be formalized we'll get to that edge but these are qualitative memes that come from this philosophical area and they're going to draw us to action okay now how about biological systems and action what do you think about when you think of biology and action i don't know i think like those things are kind of extra inextricably linked in my mind um because like you can't have life without having some action i mean cells replicate yeah i think this is going to be just uh something that doesn't demand to be fed but is totally true biological systems are active there it's active matter and life is this multi-scale organization of activity so there's many ways biological systems are doing action and what are some areas that are formal whether or not they have to do with biology or any philosophy can you name a few areas of like formal theory and science that you think relate to action so i mean i think um just that's maybe more related to processing um but but as i said earlier i think processing is related to action but like bayesian inference and and variational inference i think um which also tie in a lot to active inference um i i think that they come in their uh in the processing slash processing through action or action through processing okay great so um bayesian planning as inference as well as areas like cybernetics control theory and different formal ways of modeling active systems and modeling decision making these could be related to a biological system or not and they can be drawing on a philosophical framework implicitly explicitly or not all right so now this is starting to get fun you added another term there and i think this will merit a detour but then i return to here which is the variational bayesian approach so what do you think variational is meaning or doing here so it's interesting and part of what uh makes this complicated um paper and also like a complicated detour um that like the authors cast predictive coding as variational inference like even though we've kind of made this distinction between predictive coding and predictive processing where coding does just is like a one-way street and processing is like a two-way street um the authors nevertheless use the term predictive coding as variational inference um and and this predictive coding um or this variational inference um i think the the key um construct here and that's probably i don't know it overlaps with with bayesian brain maybe two is the idea of having a model of of the data generating process and so i think um variational inference kind of throws the idea of a of a model into the loop okay so let's zoom in on this blue corner of formal and quantitative areas and so these can be taken in an action independent way they can be explicitly about action like planning as inference or they could just more implicitly rest on inference i'm sorry more implicitly rest upon action for example in the case of we're inferring the hidden state of the temperature of the room and we're observing the thermometer that doesn't have pi it doesn't have a it doesn't have action in that model it might just have those two parameters of like the room's temperature hidden state and the observations of the thermometer but then we can kind of take a step back and see all right well there's the person who's engaging in this experimental action the person's ocular motor zooming in on the thermometer maybe we can just abstract away but action is always baked into it because we're talking about active systems but let's just put some of that aside for a second and take a quick detour to talk about variational inference because it's a really important topic let's think about and we've also explored this in active livestream 26 and 37 and a lot of other times so let's look at three different ways of doing bays base ways bayes areas as they were so and maybe there are more but the three that we can list up here are exact base monte carlo and variational base so pick one and then what is like something important to know about that way of doing bayesian statistics um well so i'm by no means an expert in bayesian statistics actually like cracked out my bayesian data analysis book while i was reading this paper because it was uh one of those papers like i said i'm uh i'm anal retentive and and detailed and i love like what does that mean i have to look up every little thing i don't understand um so like okay so let me start with the exact ways um so exact base when i think about exact base it's like um the probability of a given b like the probability of rain given clouds or something like that um and so it's like a an alternate alternative to frequentest statistics where um [Music] the probability of rain [Music] and the probability of clouds or or like a coin flip is easier to talk about right like so each flip of the coin is half and so like you know the idea that what what are the chances that you're gonna get heads um you know five times in a row is half times half times half times half times half so um in bayesian statistics there's um or the probability that you're going to get rain here this is it in frequency statistics the probability that you're going to get rain and the probability that you're going to get clouds is um the probability of rain times the probability of clouds that's a great way to put it and then in bayesian statistics there's it evaluates the probability of rain with clouds like given what you already have observed before like if it's rained it it's usually cloudy this percent of the time um and it takes into consideration the probability of rain with clouds probability of rain probability of clouds all those things separately so it gives you a prior estimation i think so i don't know that's probably super confusing great no you mentioned a lot of really important points which is it's uh an alternative to frequentist statistics analysis so we're not getting a p-value out of this and we're able to explicitly say our priors and not to go down this rabbit hole but a lot of frequentism implicitly has a uniform prior and we'll just leave it at that for now so the exact base is using the bayes equation essentially as written and just as you said it there's the probability of something given something else and then you kind of find the probability of that other thing given the first thing multiplied by these other terms and then you fill out all of those variables and you literally do that division and so we explored like this in the case of axel constance paper with the bacteria and so there was the exact bayesian bacteria that was having its prior and then taking in new information and updating how it thought about what was out there based upon the new incoming information and it was like in a formula and it was like dividing exactly the numbers that you see here in this way okay so that's exact base now what are some issues with exact base it might be straightforward when you're talking about that coin flip but if you're talking about a high dimensional space or something that is just bigger than the ram or whatever of your computer when it comes to the implementation there's often challenges with exact base and so there's several techniques that have been developed to approximate what an exact bayesian approach would be in a more tractable way so there's two alternatives to exact bay's implementation and the spirit is going to be exactly the same of taking in new information and updating our priors as specified but it's going to be done in a few different ways so first what about monte carlo what is monte carlo um so monte carlo uh you probably know more about the history than i do but but um monte carlo is like a sampling technique so you know instead of um trying to calculate the probability of um rain and clouds using exact space you just sample like out of you know a thousand rainy days how many of them were cloudy and out of you know a thousand cloudy days how many of them were rainy and so instead of like getting an exact measurement of your distribution like what is where is the overlap between clouds and rain you just randomly pick out from all of the possible days you estimate your sample that way great so it's based upon sampling and so it's like we might not know one of these terms but we can at least draw a sample and so this does a few different things it connects us first to empirical and specific data as implemented like in a row of a program not just b sub i or some sort of analytical representation but like this is where we're getting into this specific sample was pulled by this algorithm at this time and one method that's a common thing is the markov chain monte carlo and this is using a local resampling approach like assessing local maneuvers of a given chain that's engaged in sampling and you kind of drop these chains into different parts of the probability distribution and then have them evaluate different aspects of that distribution so it's as easy or hard as dropping those paratroopers into different parts of the physical landscape and then having them accept or reject different proposed moves so that's sort of a two-dimensional landscape with the height being elevation and maybe that's what you want to map because you might want to know is there one central peak are there two peaks that are very similar and take that into the statistical case and this is especially helpful for when there's no um well i don't say no idea but there is not the desire to explicitly formalize what shape the distribution looks like so we might want to do like bayesian phylogenetics and so we need to talk somehow about the likelihood of a given dna sequence being that way but like where do you go from there and so that's where the monte carlo sampling based approach really comes into play and then because it's based upon sampling the advantages are you can run that sampling for three iterations or three million so it's very flexible with the amount of computing power that you bring to bear on this challenge but also there are the risks of over sampling which is that you have used some unneeded electrical power and computational power which is really non-trivial but also there's the risk of under sampling and you might even be like in a regime where you think that you've sampled because you're getting samples that are just confirming what you already know but you know there's a whole other island that you didn't do the paradrop to so one can be lulled into a false precision with bootstrapping and sampling based approaches because they can give ultra high precisions but that can be based upon um actually more like a biased sampling approach or all these other features and just the one anecdote that i have on that is in the bayesian phylogenetics case i remember this one professor in undergrad uh professor moore and he'd always say fuzzy caterpillar you want it to look like a fuzzy caterpillar and there's some technical details but it's like you want it to be exploring the full range of a parameter like if something could be zero to one you want to be exploring the full range but returning to the best estimate and that shows that you're sampling like the diversity of what that parameter can be but also spending more or most of your time in the most likely and in the best regions and so if the value were at point five and the range of what's possible was like zero to one that looks like a fuzzy caterpillar with more thickness in the middle and then a lot of fluctuation but it's not like it's spending a ton of time at one and then it drops suddenly to zero because that would suggest that you're not converged yet whereas when you see that fuzzy caterpillar it's like you're sampling the extremes and getting novelty and testing different combinations but you're also returning to something that is working so monte carlo sampling based approach brings in all these opportunities and challenges associated with sampling uh well so i i was under the impression like you talked about over sampling but i think um [Music] over sampling like as you as sampling goes to infinity like your um your accuracy increases like you're you're more closer able to get the true posterior um but i think it's it's uneven sampling that's the the danger in the monte carlo method like if you're sampling too far in one end or the other and perhaps like asymptotically if you exhausted the state space then you would know that you had the right answer but the whole reason why we're using any kind of heuristic approach rather than exact base is because like we don't have the full state space so yeah there's um relatively low cost to over sampling but we don't always know when that is and then under sampling can be just totally misleading if we take that as our only um representation of that distribution and so there's all kinds of cool techniques that involve like multiple chains happening in parallel and some of them are like higher temperature and less temperature so for one of those paratrooper teams it's very cold and so they're only making the best very local moves it's like the elevation is sharpened for them it's very hard to get out of a local rut and there might be another team that's very high temperature and for them the landscape is flat maybe even completely flat and then they're like a hot molecule that's able to move over something that might be a barrier for another team so that team is going to like fill in the details and locally explore quality solutions whereas there's another team that's like or another chain and so that's the multiple chain monte carlo approach and there's a lot to that in phylogenetics which is where i've seen it the most but also probably many other areas okay i've i've run it in phylogenetics too and like i mean the programs can take forever that's too fun because you know i mean you're too many parameters and and whatever they can really take a long time um the computational power is no joke like what goes into um the monte carlo sampling so we have exact base which is calculating the true posterior and then we have the monte carlo which is estimating the true posterior through sampling and then we come to the variational which is trying to minimize the divergence between the true posterior and your approximation of the posterior great minimizing divergence between the true posterior like what we would really have wanted to actually know what is the actual temperature in the room conditioned upon the noisy thermometer estimates that we're getting so are we going to sample our way out of that one the variational approach is going to be very different and and in 26 i think we had like the cat and then one approach was like the monte carlo was like pointillism it was like dots so we're sampling pixels from the cat and then it becomes like a pointless picture of a cat that if you do sample densely enough it looks like a cat or it's recognizable as a cat and the variational approach was like a clip art template and so let's just say that you had a template of different curves or of like you knew that that it wanted to be a cat you knew you were looking at a cat so then you have like a cat template that can get stretched or zoomed in or out and so that might be very straightforward to optimize because you're just trying to minimize the divergence between these very limited parametric changes that you can implement but then of course you run into a situation which will explore various aspects of variational inference what if you try to stretch that cat emoji onto a different animal or what if it's a totally different kind of data set so you will find some divergence minimizing solution that doesn't mean it's sufficient or even in the right ballpark just like doing l2 least squares minimizing linear regression will always find a best fitting linear regression but that best fitting linear regression can be like hilariously inaccurate like if the data are like a uh a parabola like a u the best fitting regression might be like a flat line through the middle or if eighty percent of the points are here and 20 are over here there might be a regression that's very misleading that's like the simpsons so-called paradox so in variational we're not going to use sampling we're going to use the minimization of a divergence which is a kl divergence but they mention in this paper it can also be a different kind of divergence and the rainy divergence has been explored in some recent work of sagitta at all so we're going to minimize the divergence between the true posterior and a distribution queue that we control and is tractable to optimize so it's like instead of that sampling based approach we were dropping the teams at different parts of the landscape and then they were going to report back on information here in the variational approach it's like we have a template and then we're gonna just do stretching and bending again this is kind of a torture metaphor but we're gonna stretch and bend and there's only a few parameters on the stretching and bending and we're going to find the best fit of those stretching and bending parameters like if it were a linear regression y equals mx plus b the two dials that you get to change are like the m the slope of x and b like how high or low the regression is and then you're finding m and b that are the best fitting about the data y and so you're kind of like minimizing the least squares error term here it's not a linear regression that we're fitting by minimizing the least squares it's a variational bayesian approximation that we're fitting by minimizing the divergence between the true posterior and the distribution queue that we control and is optimal or is able to be optimized what else would you add on that um i think that's it so uh the the true posterior and the the distribution that is tractable and we can optimize over yeah that's it so let's close this little okay computational intensity on variational is um like much less right than um because instead of calculating all of the parameters for every little dot every little point on the sample we are calculating for like a big blob it's like one big blob instead of several little points but we calculate for that big blob this is the density of that of that function um and these are the parameters of that so so now we're gonna back out of this formal cul-de-sac that we've been in and just remember to ourselves even if it's your first time hearing variational inference and or you're kind of like blue or eye where it's like we've read it in how many papers but still every time we kind of start on square zero so why do we use variational bayesian inference so the first thing is it allows us to use bayesian statistics and probability which we might prefer over for example frequentism okay variational bayes then allows us to use a heuristic bayesian implementation so if there's something that's too challenging or implausible for exact base now we can approach it like monte carlo or variational these are both heuristics unlike monte carlo it's not based upon sampling variational is based upon a family of equation fitting and so it can be implemented very efficiently which isn't always the case for monte carlo it rests upon an appropriate factorization of the problem and one other piece is this connects to analytical equations in physics so it's almost like monte carlo is like a computer scientist's approach it's like how much ram do we have how many processors do we have how big is this data set going to be and then there's no analytical equation or closed form for monte carlo it's like engineering it's kind of like you have to have some art and skill and science coming together for the monte carlo to be the best it can be and then it's more about like an adequacy and an efficacy question because you're not converging to the infinite asymptote again otherwise you could have done something else and then the variational is like the physicist's approach and it draws on the variational calculus of feynman and others and this is the part that's very amenable and connected to the equations of least action and all these other things like factorizing equations so it's funny that you bring up the computer scientist approach um in monte carlo and like so [Music] i am a horrible computer person i mean i will brute force wrangle my data like just give me more ram just give me more processing power like i i um i just want to make things work i don't have any patience for finesse right so in my mind i think about the monte carlo like the brute force just shove your data through the algorithm no matter how much ram it takes i think about monte carlo like that and i think about um the variational bays more like the people that can do like you know very simple things in code to make it run much faster that have that like very good skill with like oh well we can just you know run this little factorization or or um you know we can make the problem run so much better on the computer so i think about it about the variational um like that but you're right it does have that connection to statistical physics and to definement but i think about it as like a highly optimized way to run bayesian statistics but why daniel would someone want to run bayesian statistics in the first place so so maybe you want to give us an example why why would that happen well yeah we will pull back just the last point there is the variational methods we might even have a heuristic for those or an algorithm or approximation of those for example message passing and forney factor graphs those draw an equivalence between bayes graphs where nodes are random variables and edges are statistical relationships bayes graphs can undergo variational inference and there's a tractable way to use toolkits like that developed by the bias lab and use message passing and a 40 factor graph representation to have like a level of implementability for even the variational whereas it's not possible to imagine like a heuristic for monte carlo it is the heuristic sampling is sampling you can do better sampling but that's the game that you have chosen whereas in variational even this question of minimizing divergence does require kind of like a proximate or an operational approach because how can we minimize the divergence between the true posterior and something we control when we don't know what the true posterior is so there have to be a little bit more added but this is like very awesome discussion because it's helping us pull out like this is the sort of exact approach and then there's a more computer science like sampling approach and then a more analytical and physics-related approach okay so why do we want to do base let's pull back to our triangle here why do we want to do base and thinking about the concepts here why do we want to do base i asked you you're not allowed to turn the question around on me that was me asking you you're not allowed to just talk and then ask me [Music] well there's a few ways to take it i guess one would be what is the alternative policy selection if we're doing research or application what is our our alternative to picking some bayesian approach so if we take the bayesian road we know that there's going to be some sort of trivergence later exact base monte carlo variational base something else or we could go down a different statistical path like we could use frequent statistics and maybe that's totally adequate and effective for the situation and both are just maps so a well-fitting linear model of height and weight doesn't make height and weight a linear model a really good fitting action perception loop whether it rests upon a kernel of frequency statistics or bayesian statistics doesn't make those systems that way that you modeled it um so i would say bayesian statistics is useful when we want to do some kind of formal or quantitative inference where we want to be specific and explicit about our prior beliefs and how we want incoming information to update those beliefs so i i agree and um it's what i always try to tell people um you know when i'm explaining like what is bayesian statistics which a lot of people have never heard of um like in college at least when i graduated there's not there's no bayesian statistics course now many schools have them um and you know i'm outdated already but uh it's really like when you're doing frequented statistics you go out to the desert and count mice every year right every year you go and count how many mice you see in the in the desert or something like this and if you i mean you expect to see some mice in the desert otherwise you wouldn't be there counting them so um you expect a non-zero answer right so every year you go out you find 10 mice 20 mice 50 minus 30 mice 20 mice 10 mice and every year you go count mice so if you were to go like you have some expectation that you will find mice um you also have some expectation that there is going to be less than a thousand mice because every year you can sample this one place right and so um what bayesian statistics does is it takes actually that prior not like frequent statistics they don't care if you ever found mice there before like your probability of finding zero mice and 30 mice and 50 mice and 100 mice and a thousand mice is all the same it doesn't matter how many years you've been there before like you are starting with nothing in frequent statistics you don't get to have any guess that there's even going to be one mouse in the desert where with bayesian statistics like obviously you think that there are there are going to be some mice and you think it's going to be some number you can count reasonably so with whatever area and so what bayesian statistics statistics allows for is that prior belief or that expectation that you have as experimenter awesome and that's kind of what we were getting at with that idea of frequentism having this implicit uniform distribution you flip the coin ten times you get three heads your maximum likelihood estimate is it comes up 30 of the time on heads 0.3 however if you want to take like a bigger picture of you you might have some sort of precision about where it's likely to be like is it likely to be 0.5 and you're going to be surprised if it were 0.3 because you just pulled it out of a cash register and then in that case you still might say i want to have a uniform prior and if it is 0.3 then it's 0.3 or you might want to say i'm very confident it's something else but bayesian statistics opens up the space to be specific about how we want to use our priors how confident we want to be how much precision we want to have in those priors and then how we want to incorporate new information so any other thought on that or let's continue on that and connect it to some formalisms so i have one more thought on the coin flip with baby with the bayesian idea like if you know it's a fair coin and the probability is 50 and like if you're we're flipping coins and we're betting each time like you know a dollar a dollar a dollar if if we flipped that coin you flip the coin 10 times in a row in its heads every single time like on the 11th time like my bet is on tails because like it's not it's a fair point like it's not going to come up um every single time heads and so frequent to statistics like does never allow for that previous information and also like where is that information which is something that always baffles me like does the coin already know that it's been flipped ten times and it's come up heads all those times and that it then has to come up tails sooner or later i mean that the probability is so low that you're gonna get like a thousand coin flips of a fair coin in a row that is heads like i don't know if it's ever happened to anyone ever so like as we increase in the number of flips and if it's heads heads headsets heads like it becomes increasingly probable that the next flip is going to be tails in my view right which is maybe totally wrong but but where is that information stored and um the bayesian kind of prayer allows for that to be there and i wonder if this even connects to where is that information stored in the quantum reference frame of the observer and nowhere else but let's look at how that gets implemented in these formalisms so you just describe that setting where somebody has like a belief that the coin is fair maybe that's an empirically grounded belief like previously they flipped it a thousand times and they got 50 50. or they have just an a priori belief that is fair if that belief were generated by a real other data set we would call that parametric empirical base because it's the process where you set the priors as well as their confidence based upon some collected data set that's parametric empirical base or one could just take that a priori cons synthetic a priori and just say coins ought to be 50 50 and that's what i'm sticking with let's just say that then we play this game we flip it 10 times and likely or not 10 heads happen that's the trace of behavior in the niche it actually happened now that might be seen as a totally likely outcome by somebody who believes that it always comes up heads hence they're unsurprised or depending on what your priors were that could be variably surprising and then you talked about well when and how should you update your beliefs on that coin like maybe something changed while you weren't looking at it and so you want like your best estimate to very heavily reflect the recency and maybe that should be like a moving average of the most recent three flips or maybe the most recent 30. so let's look at two places in the paper where they do something like that the first one has to do with predictive coding and frame differencing algorithms in video compression and so in this area we can think about the way that the video looks people who are watching this video the way that the video looks is like the output and then we want to encode like how surprised are we what is happening when the frames are changing and so the simplest method is just count where it's different and convey that information but it might also be important to use as they write more advanced methods that predict each new frame using a number of past frames weighted by a coefficient in approach known as linear predictive coding so that's one case where you're determining how many frames back how many coin flips back should we use for that now casting and how should we weight them so that's one area and then a second area that they bring up and connect is the calm and filter so here's the common filter and this is their formalisms 33 and 34. and it has a lot to do with bayesian statistics so on this and bayesian column filter is very common we can see it in two different ways so on the left side of this slide the top image shows um a prior prediction that's the prediction and then two is the measurements and then there's the fusion so we have the prediction the measurement here with like gps and then there's the fusion so that is very similar to having a prior and then some updated sensory information and then the posterior so it's just labeled slightly differently but we can already see how this is like totally related to bayesian statistics what this also brings into the picture is like a pseudo code that unrolls through time so a bayesian statistics approach doesn't have to be about time it can be about a static data set and then you could be modeling like okay per extra sample of the population of their height and weight we're going to update our inference on that relationship but that's a timeless analysis it doesn't incorporate some sort of unfolding through time that can be incorporated into the kalman filter using this pseudo code down here on the left there's some prior knowledge of state and this is like two dimensions and then there's a cloud representing a distribution of precision or uncertainty so a more precise estimate would be like a tighter cloud there or a sharper peak or a sharper valley depending on how you want to think about it and then less precision would be like a broader basin or like a more diffuse ink drop and then there's a prediction then a measurement occurs which can be precise or can also have an associated measurement error term which could be fit with parametric empirical bays based upon testing or it could be determined on the fly with expectation maximization and then that prior gets juxtaposed in this update step and that outputs so this takes sort of this timeless bayesian update scheme and specifically adapts it to the case of something happening through time and that's shown on the right side here with this image of like time is happening and we're getting these noisy x measurements and the true temperatures the purple and let's just say it's unchanging in this case but that could also be changing one can just imagine that especially if it's very noisy it's hard to get increasingly complex dynamical patterns but that's all part of the game and then the prior starts high and then the prior is weighted it's kind of like a spline it's weighted through all of these x's and you can imagine one extreme case is like move to the last one that you saw and so that would be like basically recapitulating the measurement distribution through time the other extreme case would be wait all the time points evenly so that like we're kind of converging to a moving average so if we have a hundred hours of video that is like at a two and then all of a sudden it switches to an eight and then your moving average would very very slowly start moving up from a two and then one could imagine that there's some intermediate solution that doesn't use like the whole data set because that's too slow to adapt and maybe takes up too much memory but isn't just like a one step instantly switch to the nearest measurement and that parameter of how fast should you update your kalman filter is a parameter in the model and so that is exactly what is being statistically optimized is how much through time should we update our ongoing estimate and so in the static or the timeless bayesian case that's where we talk about precision how much should we update our inference as new information is added to our data set but that doesn't mean appended in a temporal way the common filter is making it explicit that these data points are being added sequentially and that is providing us with this pseudo code by which the latent state update the latent state is updated not just as a function of adding data but actually adding data in a sequential and unrolling way what do you think blue yeah great i think that that was a super good explanation and very clear and yeah um my first uh interaction with common filtering was in imaging and image processing and so i think that's maybe where people come across that a lot especially like doing laser scanning microscopy or a video i guess the video update also is like that so yes so let's just look at some of their um writing but these are like the smaller the formalism the less we've paid attention to it and the more that we would love to know about what it actually means but let's kind of pick up above their 35 they write coleman filtering proceeds in two steps first the state is projected forwards using the dynamics model or prior and that's the p of mu t plus one that's the mean at the next time step conditioned upon the mean at this time step so that's like what is going to happen next conditioned upon how it is now these estimates are corrected or sort of compromised coming to a compromise by new sensory data by inverting the likelihood mapping p of observations at the next time point conditioned upon our estimate of where the latent state will be so this is what temperature will the room be conditioned upon how it is now at the next time step what will it be conditioned upon what it is now this likelihood mapping which is like the a matrix in the pomdps is what is the probability of the outcome in the next time step being a certain way conditioned upon how we think the room's temperature is going to be and so these are the equations and some of the variables and then they kind of conclude or in this little section the derivation of the rules is relatively involved and that's in appendix a and some other places but also the common filtering can be interpreted as finding the optimum of a maximum a posteriori estimation problem so this is almost like we have kant with the exhortation of the a priori what information are we bringing to the table qualitatively can't or quantitatively bayes that's the a priori and then baze kind of bridges the a priori the prior with a posteriori not sure if that's the latin for that one but it's the posterior what happens after the sensory update and then that space between like the before and the after is the during and that's the during of the now that were doing action and inference within let's look at how they introduce oh anything otherwise we'll look at how they introduce action and then connect it back to active inference so i i think um the common filtering and the the updating so i think we we kind of skipped over um updating when we were talking uh when we were making our layer cake earlier um and just how um how messages in the nervous system biologically and in predictive coding are passed forward and backwards so it might be helpful to back up and talk about error propagation and propagation of back propagation and propagation of signal through time um just as like a very basic way because i think the common filter um is kind of an advanced way to do that okay can we detour we're back at the triangle and so now we've added variational as like a little badge to um the bayesian and also we're going to add coleman as a badge where have we visited on this journey and one can imagine that they could take a a monte carlo or a variational or an exact approach to common filtering and so on and then you mentioned a few more biologically inspired or compatible features like error propagation and so on so what is a area in the paper or an idea that's relevant here so i think the the signal propagation and and bottom-up prediction error like how does error propagate like through visual systems through like neural networks i think that there's applications here um and is it the error that's fed forward or is it the signal that's fed forward or where does the error go in the system so i think these are some some confusing things that that maybe they they touched on um in the beginning of the paper so let me see if i can find exactly um or there's some debate okay i'm while you're looking so i've i've pulled out this right edge of the triangle so we're going to just leave some of the baggage at home and take out to the table just this link between bayesian statistics and probability and biological systems and then you raise this really important question about like how do we think about error estimation and also some related terms there would be like precision ambiguity risk compression and that so that those are error estimates and then how are error estimates propagated and communicated how are they propagated and communicated in language i'm not sure if i had to guess in power error estimates communicated in systems and so we might be interested in mathematical systems so then the way that you communicate the error is just you multiply the two variables you have a precision matrix and then you have like some sort of hidden state matrix and then you basically just fuzz this kind of pristine matrix by some error matrix and so if your error on something is like zero this is not exactly how the multiplication would work because multiplying by zero but if the error were zero on something one would want their estimate to be passed through without being blurred if the estimate of the uncertainty were super high in the extreme approaching total noise and uncertainty and variance estimate then no matter where that pristine estimator were you'd want it to be like totally fuzzed over and then one could imagine that in a per variable ongoing way you'd want to be updating these variance estimators and that is exactly what happens in the common filter which is this unrolling and ongoing estimate of observations latent states and the the variance that links them and that's done like through matrix multiplication but how does it happen in biological systems is a different question so i i think um some things come into play here like um redundancy and signal compression and then error propagation forward and backward um and so there is this um in predictive coding slash predictive processing because i'm still like not 100 um on the on the difference there but but i do think this is a predictive um it seems like that it's a predictive processing uh thing but the authors do say predictive coding there is like a bottom up um construction of a model so the authors say that um but it's not only that pers it's the authors say that perception is not the result of an unbiased feed forward or bottom-up processing of sensory data but is instead a process of using sensory data to update predictions generated internally by the brain so there's the idea of a model comes in here yeah generative model where do you see generative model in these discussions i mean it's kind of threaded throughout so i think the the idea of um variational inference brings in a model because the model constructs the um estimate of the posterior like where we're consistently there's a consistent effort to update the estimate of the or to sorry to minimize the divergence between the actual posterior and the estimate of the posterior so the estimate of the posterior is like you know trying to make the model match reality so trying to make the picture of the cat match stretch bend fold into um you know the model of the cat yes awesome so generative model with no complications is being put into the bayesian statistics area because if you specify a model that's generating thermometer outputs given the temperature it's quite literally the generative model and we know that there's kind of two directions there's like the recognition model and the generative model that's the tail of two densities because a distribution can also be understood as a density and so in the realm of statistics generative model is quite literally the approach that is taking parameters of a model and using it to generate for example observations in this case no need to complicate that but then there's a few ways that we hear different people in different papers talking about the relationship between biological systems and generative models as well as recognition models but we're focused on the generative model here and sometimes these are even coherently or incoherently used differently in the same paper or the same sentence which is our favorite and will eagerly await the kind of automated detection systems that will enable us to do high throughput analysis but here's just a few of the kind of ways that people can talk about that we hear sometimes that the biological organism or cell is a generative model has a generative model enacts a generative model or the more instrumentalist we can model that system with a generative model and so this bottom one is kind of like saying i'm just planning to use bayesian statistics to model a biological system and perhaps we could say that this is the least bringing assumptions to the table of what that system is doing because by saying we can model it using a generative model we can use or fit derive a generative model for this behavioral cognitive system we're just remaining purely with both feet in scientific instrumentalism and empiricism we're making only claims about the map and not about the territory per se which might be throwing out the um whatever with the whatever however these ones are when people are making claims that are about the territory what is the brain is bayesian brain what the brain is doing well it's not passing around screenshots of bayes equation like we are so what is it that it is doing or again is it on the instrumental side it's just something that we can use to model the brain so yeah so um going back to predictive coding and and also linking to what is the brain um doing and really this is why i wanted to uh stop here um i found i found the spot in the paper um but but when we were talking about common filtering and um image uh processing and then error propagation in a system um they they talk early in the paper about predictive coding as a means to remove redundancy uh and applied it to signal processing to reduce transmission bandwidth for for video um and this they the authors say here um barlow apply applied this principle to signaling in neural circuits arguing that the brain faces considerable evolutionary pressure for information theoretic efficiency since neurons are energetically costly and redundant firing would be potentially wasteful and damaging to an organism's evolutionary fitness because of this we should expect the brain to utilize a highly optimized code which is minimally redundant and they say predictive coding minimizes this redundancy by only transmitting errors or residuals of sensory sensory input that cannot be explained by top-down predictions and so they remove redundancy at every point in the layer so it's like when you're watching a video and um or even like the flip book like so we see like you know the the flip book that likes of a little figure kicking a ball the point that changes is the point that's prioritized the point that's different in each frame not the point that stays consistent because that is where the action is here where the motion is happening so that that's what's prioritized here and that totally connects to information theory informative updates are the ones that update your prior to move it to a different posterior that's the bayesian information concept as opposed to for example the shannon entropy which is like based upon symbol entropy so they're totally compatible and if this seems like a technical detail it is but also this is the bayesian concept of surprise and information but that was a great section you pulled out so here we're we're really highlighting as following barlow applying this principle of redundancy removal but already we can caveat how much redundancy to remove because a more redundancy removed system is also potentially more fragile like if we only have one copy of each saved file now our threat of losing it is very high so redundancy is always in a trade-off with like resilience and other system features so this is one principle but it's not the only principle at play so redundancy doesn't mean we're going to pare it down to nothing it just means that for an organ like the brain which is using like 20 percent or however much of an organism's energy budget changes in its efficiency by several percent can be really important so that principle of redundancy removal is being applied and the argument is that the evolutionary pressure is for information theoretic efficiency so if you don't have anything to update you don't want to spend the energy to do something extraneous nor do you want to spend any more energy than you might have to on updating and that's again because of the wastefulness of potentially excess signaling and then with evolution there's always a twist like what if it is wasteful and that's just some second level reason why so it's not the whole story this is just sort of short sentences because of this we should expect the brain to utilize a highly optimized neural signaling neural firing code which is minimally redundant and then they connect it back from what the brain ought to be doing normatively at a first pass to the formalisms that were being described with predictive coding so in the frame differencing case it was like only tell me about the pixels that are changing and then if nothing's changing just send me like a little beep and i'll just know with just one piece of information just to keep it exactly the same even if it's 4k video and it's a gigabyte per second or whatever but we don't have to change anything for that video and then as pixels are changing update me on how to change them and so that would be like the maximum efficiency encoding and so you would give a full resolution of the first frame in the movie that's the prior that's d and then all one has to do is think about how to encode how the subsequent frames change and sometimes they introduce things like a keyframe so every second or every minute it's it starts with a fresh prior and so these are the kind of computational things that one can do when they're not bound by like the constraints of biological systems but this is an awesome connection um any other thoughts on that um theme or like where does error propagation come into play or we'll turn back to the triangle so i don't know the error of propagation is is interesting it'd be great to have like the input of somebody who um is more like versed in machine learning because i think the idea of like back propagation like this is an error and like sending that back to the previous like layer in the neural network is super interesting and part of what um like enhances the efficiency of neural networks like hey fix this and so i think um and i can't really find in the paper where they say that but but there's like a local correction for error within each layer of processing so um in the visual system and um in signal processing itself it's like each layer prioritizes just fixing their own error and then that contributes to the overall efficiency and um uh accuracy of the system at large yes back propagation is mentioned 45 times in the paper with a quick search so it's definitely an important concept and also something the authors have worked in a lot so good error propagation and back propagation and also that reminds me of um when professor levin was here in 40.2 with the back propaganda and the imperatives for like the different parts of the neural network and the kind of information that they want or expect or prefer and then how they want to escape being trained and engage in learning but not being trained but still updating so it's learning and being trained both involve like updating priors potentially appropriately but in the case of training there's a feeling like another entity or agents is imposing their will versus learning can also be associated with um behavioral and cognitive changes but has a different sense potentially than being trained anyways let's go back to the triangle as we're sort of not quite landing the dot one play you know the check has not arrived at our restaurant but um maybe we're working our way up to our first star um where do we sit on where action is and then we have a few terms that we want to put which is message passing and then active inference oh yeah so what is another term that we can add in or something that we're seeing differently in a new light or where do you see message passing or active inference um so i think yeah so we have action are we happy with where action is sitting i've used every feature of google slides sufficiently um i i like action as sort of a fulcrum in the center of the merry-go-round is action and yes more arrows could be drawn but we'll keep it a little sparse but go ahead so i think maybe message passing sits between predictive processing and action and then that's exactly where i was going to put active inference was right where you just typed in inference um because i think uh if we're or yeah it's hard to say um where active inference really goes maybe like not right where inference is but like somehow if action and the bayesian brain and biological systems could form a a triad maybe active inference would sit in the middle of those things um so i added message passing to bayesian statistics and probability because it's something that we can do on a bayesian graph it's a implementational approach but it's also leaning a little bit towards the biological because when people talk about like how does a given piece of information or signaling or stimulus in the toe end up having an effect on the other side of the body or on the brain like there's some kind of now it's being used in a technical bayesian sense with message passing whereas in a more just sort of like conversational way um with biological message passing but whether it's a synapse an immune synapse or a neural glial synapse or it's a mechanotransduction or a hormone broadly we can just consider these different biological mechanisms that convey information or updates or anything to be like past messages so message passing is going to be a formal technique but it does lean towards modeling the ways in which different biological nested entities are exchanging information yeah and so why i would put it with predictive processing is because when i think about the things that we were talking about like how are signals compressed how is redundancy eliminated how is error propagated through a system or back propagated so which is why like i liked it there in the processing because like you can't legitimately process that information without like knowing how the message is going to be passed like if you're only going to get every fourth letter of the message i passed well i'm going to encode my message in every fourth letter not in every single letter and that changes right okay thanks yes so we have message passing now as part of the philosophical concept of systems that engage in holistically considered predictive processing because it connects what biological systems are are doing with some idea about like information sharing without any formalism but then message passing via the forney factor graphs is gonna be in the bayesian side because that's like the implementation so maybe message passing here yeah it's like on the sort of predictive coding implementational side whereas we don't we're not saying biological systems are doing for any factor graphs and message passing in that technical sense but for sure we can think about it in this qualitative way okay so we have action in the sensor and then kind of bifurcated it to include action and inference aka active inference for a few reasons first off action is not just blind flailing action when we talk about planning as inference and inference on action and inference about future latent states and observations that aren't explicitly about action but we know an act of inference that they actually do condition on action so inference and action how we think and how we act how the entity's cognitive model is and how the entity's behavior is through time those two are like an inseparable duel and that's why active inference says it all in the title it's about action and inference and all of the ways that they are related to each other again whether it's like planning as inference about future action or inference on the consequences of action or it's inference about something that isn't explicitly action itself like what is the temperature but it requires or is conditioned upon policy selection in this framework not the only way to look at it but that's how they come together in active inference and like let's peep over to 4.5 where they introduce action formally the basic approach to including action within the predictive coding framework is to simply minimize the variational free energy with respect to action free energy is not explicitly a function of action up until equation 51 it can be made so implicitly by noticing the dependence of sensory observations on action so if your future sensory observations don't depend on action or don't depend on that kind of action this is like an extraneous calculation like if you have a coin flip and you're rolling a dice and they don't influence each other then they're conditionally independent and so there's no need to calculate the joint distribution if you're just interested in the coin flip because it's not having a causal relationship with the rolling the die however if we're going to have agency or model systems that appear to have agency then there's some kind of dependence on future sensory observations related to action in the visual case the visual input is conditioned upon the ocular motor decisions and those are actions of a musculoskeletal system and then the way that it gets tucked into the equations is the change in action with respect to time is going to be related to a gradient so like a partial differential equation but then it's amenable to gradient descent it's going to be like a partial derivative of free energy and how it changes with respect to action so like am i minimizing free energy by steering the wheel more to the left or by steering the wheel more to the right that's like d f over d a change in free energy over change in action and then free energy f here is a function not just of the joint distribution of o and mu not just a joint distribution of the observations on the thermometer and the mean estimate of temperature in the room that's kind of the pure inference take but again we're suggesting that sensory observations have a dependence could be partial could be complete on action and so now observations themselves are a function of action and then that can be unpacked a little bit more in 51 and they discuss it further of course in the paper and elsewhere but that's like how this inferential framework gets built in predictive coding and then action becomes introduced as something that inference can be about and that's what they say allows for predictive coding agents to undertake adaptive actions without any major change to their fundamental algorithms it's not like there was the temperature thermometer module and then now there's this totally ad hoc decision making module that's bringing in all this thing about what is the reward of different temperatures what is the reward of different thermometer observations rather within this same parsimonious and first principles inference framework we can model active inference which is inference including action let's kind of continue on this action theme and then we'll we'll end in a few minutes so under such a scheme the prediction error becomes the difference between the current observation and the target or set point that raises the question of where these set points and targets come from and that's where they just sort of have that road leading off into the distance saying well in the evolutionary case it's inherited in the computational case it's just simply there it could be just a priori speculated or it could have been a parametric empirical phase so that's one really interesting point is like when you do introduce action you have to discuss not just the target or the set point and how it's generated but also you have to make explicit in the forward model as they write the dependence of the observations upon action and that has to be provided or learnt because just that temperature thermometer model is not going to include a forward model of what happens when you turn on the heater or when you put on a jacket so that's something that has to be learned and then a second interesting thing that starts to come out of their framing here is like the costs of action and that closes the loop really well with the discussion that you raised of like barlow and the information efficiency argument for why the brains ought to be doing something something like predictive coding efficient signal transfer and hence why it's either what they ought to or are doing or and how we ought to or could model it so that's why the costs of this are important cost of action are important and then just one last piece in four or five was active inference in the pid control which is related to the generalized coordinates of motion and we unpack that a lot in live stream number 26 with the bayesian mechanics paper of decosta at all and this connects control in active inference systems to a very commonly used engineering framework for controlling processes which is pid control which is framing action through time as being related to these three terms which are described above so that's how they brought action into this paper was by spending many of the early sections providing like the single layer predictive coding model and connecting that to variational bays that was in like sections two and then in section three and also in section two they explored all these different interesting generalizations the spatial case the hierarchical case and then they also connected it to biological systems and reviewed some evidence for predictive coding of different kinds and then in section four when they connect to several other inference algorithms like predictive coding and the back propagation of error linear predictive coding and the kalman filter normalization and the normalizing flows which we didn't talk about predictive coding is biased competition also something we didn't talk about and then after all that action gets tucked into the picture and it's just like very clean in a very elegant way to think about it and helps us understand like maybe even how active inference is similar or different to some of these other frameworks so in the spirit of our layer cake i think um today like we covered if this paper is a sandwich we covered both slices of bread like the very beginning and the very end but we left out like all of the things that kind of come in between um the meat and potatoes have you had potatoes on a sandwich ever they do it with some french fries sometimes but i know what you mean go to subway i'm gonna go to subway and be like yeah can i get some potatoes on my on myself did i have a predicting coding special and then they go then they go what do you expect is gonna be on this sandwich it would be cool uh next week to really kind of dive in um more to uh error back back propagation predictions are sent you know up and errors sent down um or or is it the other way around um and really kind of discuss these these computational graphs that they start to talk about so i think next week would be really cool to kind of dive into what's inside of the of the sandwich oh awesome okay so we'll definitely go into some some so what's inside the sandwich um a back propagation of error biological evidence and examples so i think there's a ton of uh exciting stuff that we'll be able to talk about next week um like the different paradigms like we could cover over figure three the paradigms of predictive coding and um the normalizing flows so we definitely um we covered the bread today uh yes um um well anyone who is listening we really appreciate it look forward to your comments on the video or joining us next week if that's gonna work and blue this was like super interesting and very helpful so i think it's a great dot one and it's funny because like dot one it's it's the meat and potatoes of the zero one two sandwich so it's like our middle of the bottom of the bathtub phase the dot one where we're opening up all these ideas and just trying to like give a second coat of paint on a few things go into a few technicalities re-represent some knowledge it's like that was in a very paradoxical or delightful way about not the potatoes itself but rather about the beginning and the end well to be fair for those of you who haven't yet read the paper there's like 60 equations and like 55 pages plus an appendix so uh no surprise that we covered the bread um but it would be cool to tear apart a little bit of what's inside of the paper because i think that these layers are cool and important and the paradigms of predictive coding were super interesting to me and i love like the intersection between the brain and machine learning and i learned a lot um reading this paper especially like i was taking neuroscience classes i'm going to you know say how old i am right now but i was taking like neuro in college at like 12 at like in 2003 probably maybe 2004 um with no idea learning about you know the work of rao and ballard and hubel and weasel and and all of these um you know center surrounds like visual experiments uh with no idea like how new they were no idea like how um new of an idea it was like the processing through the visual cortex um really like i didn't know how cutting edge the work i was learning about was so it's cool to kind of have that framed for me um and put in perspective and yeah all the building upon that that's been done it's nice excellent thank you very much blue and everybody else who's participating and see you next week you