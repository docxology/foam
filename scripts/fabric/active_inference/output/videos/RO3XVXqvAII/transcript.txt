foreign live stream 52.2 on March 9th 2023. welcome to the octave Institute we're a participatory online Institute that is communicating learning and practicing applied active inference you can find more information on these links this is a recorded and archived live stream so please provide feedback so we can improve our work all backgrounds and perspectives are welcome and we'll be following video etiquette for live streams head over to activeinference.org to learn more about getting involved with projects and learning groups all right we are back in live stream 52.2 in our third discussion on the paper geometric methods for sampling optimization inference and adaptive agents we had a zero background and context video and last week Lance joined for 52.1 where we had a great overview discussion on the paper So today we're going to see where it goes see where our last week has taken us and how we're thinking about it or curious about it if you're watching live of course feel free to write questions in a live chat otherwise let us pick up on a mostly blank slide and uh just thanks again Lance for joining if you want to give any sort of introduction or recap opening here go for it well yeah I mean thanks a lot Daniel for organizing this I'm super happy to be here um I think well last time we went through most of the paper uh we discussed a lot about sampling I guess the idea of sampling we discussed a bit less about hamiltonian Monte Carlo which is one of the I guess state-of-the-art methods for sampling in continuous space um we discussed a lot about optimization there's a nice figure actually that we can discuss today which is which gives some nice intuition about the kind of optimization methods that we reviewed in the paper um and then there's another section on statistical inference so this is a bit of a different section that uh than people in the free energy principle literature and active influence are used to because here the goal of inference is about approximating expectations as opposed to just approximating distributions in and of themselves it turns out to be these two perspectives turn out to be dual but I guess here we want to develop Notions of divergences and discrepancies that are a bit more General than the KL Divergence and that can use to solve problems that the kale Divergence cannot um and I guess the overall picture for what we want to do that is the kale Divergence turns out to have a lot of really nice properties that we can discuss one of them is that if the Cal Divergence is reduced it means that that the two distributions in play um are more similar in terms of information so there is this idea of information well not monotonicity whether KL Divergence sort of um yeah gives an ordering as to what extent to this two distributions quantify I mean so if you had three distributions and you compute the KL Divergence between them um and now KL of Av is lower than KL of AC it means that b um is more closer in terms of information to a than C is to a so you have this really nice thing that is captured by local Divergence with which makes sense when we're dealing with information the kale Divergence also has some many other nice properties um it it's not a distance but it turns out to behave a bit like a square distance so you have this kind of like really nice Pythagorean theorem um and I won't get into the exact statement but it's like if you have uh a b and c distributions then klab plus klac equals klbc if you have like a rectangle triangle if a b c Define a rectangle triangle in information space um you have other properties regular KL Deliverance gives and so on um civical evidence is in general I would say the distribution the Divergence of choice but it turns out that in many cases you just can't use it for example when you have samples and you want to approximate some samples with a distribution then the Cal leverage just is not going to vary back so you need to derive some other things so this is to say that we're considering statistical inference a bit more generally than what we do in active inference in general and so this speaks to why the distraction here is a bit different from the standard inference literature that that we usually consider and then I think that the well then the section 5 which is about active inference and I think we should discuss that a little bit more because the formation of active inference that's presented there is to my mind the most General and also the simplest concept conceptually that's been out there I mean we sort of like recap the derivation of active inference and also like the properties of the expected free energy properties of active inference and also how to scale active inference and so on and we do that in just seven pages or five or five pages I mean it's just very short um so it stands sure bits like a really concise memory and actually from there you can rederive a lot of you know technical papers on active inference it's like to me it's the most General and if if you as a reader can understand that section then you can sort of really understand what active reference is about um so yeah this is kind of the overview for today um I would really like to be discussing questions because last time we rediscussed about most of the papers so but yeah whatever comes up awesome all right great well let us uh talk about some of the more foundational pieces hamiltonian Monte Carlo and then on through section four with the points that you raised about the KL which generalized inference Beyond how it may have been brought up in other active and then we can spend most of the time in section five and looking at some of those figures connecting some of the intuitions about the ball rolling down the bowl to the person running so sounds good on hamiltonian Monte Carlo where do you want to pick up sure I mean so hamiltonian so last time we discussed a lot about the problem of sampling and why that's a difficult problem um and we arrived at the conclusion or or it presented like Monte Carlo methods how they work so you're basically running a stochastic process like a random random motion and sort of the distribution defining the process is going to convert their target distribution which means that when you run the process long enough then every point that it's gonna be in is going to be like a sample of the distribution that you want to sample from um now there's a lot of issues with that um I mean conceptually it's not so difficult but actually when you want to implement this in practice it turns out to be really hard because if you just implement this simplest stochastic process to assemble your target distribution um it's going to be extremely slow and I think that's the main bottleneck When developing Monte Carlo methods Monte Carlo something and in general is slow and that's also one of the reason one of the reasons why one might want to do variational inference instead of sampling um so something is slower but it's also more accurate it can approximate our distributions that are completely arbitrary so if you care about accuracy then and you have time and computational resources then for sure go for sampling if you care about speed about doing things online and you don't care about accuracy so much then the right annoying prints is the way to go at least that's my understanding right now so let's say you wanted to sample a distribution in a continuous space so it could be um just as last time let's imagine the state space is the desk where where I'm at and you sort of had this this distribution which could be like multimodo very weird and you just want to take samples from there um Monte Carlo is probably the state of the art methods method to do that there's a lot of other methods out there but San Antonio Monte Carlo typically it just works very well and in a wide variety of situations until the area of hamiltonian Monte Carlo is you're gonna augment the state space with um so let let's say that the original state space you start with are the positions and so you're gonna augment that with a velocity State space so you kind of if your original state space was euclidean space of n Dimensions you just end up with a new cleanest place of two n Dimensions so you double the size of the state space and then you say okay well um the distribution that I want to sample from actually defines an energy landscape um so it technically technically it's like if you take if you have distribution which is p then minus log p is an energy landscape so um points where minus log p is low are points where p is high and so these are points that you want to sample a lot from and control wise um if minus log T is very high then p is low and you don't really want to go there so much because these are points of low probability so I think this minus log P actually there's many reasons why minus log p is is Meaningful in physics but just note for now that minus log p is just like a function and the Minima of the functions of that function have the high probability point and is there high these are high energy points where you basically don't want to go to go there so much um so let's pull this minus log P potential energy and so this is what this really is in physics if the technically in physics P would be give us a measure what people call an equilibrium distribution um last time we saw like soft Max of minus something the something is the is the potential energy um anyway and so minus log P would be the potential energy and then if you if you add kinetic energy on your velocities um then you get what's called a hamiltonian which is the sum of the potential and kinetic energies so what is exactly the kinetic energy the kinetic energy is like velocity squared pretty much uh so if you had remember that your state space is position and velocity if you add and you take a point in state space the uh so you would have um kinetic energy which is velocity squared and the potential energy which would be minus log Peak now if you add the two together what you get is a hamiltonian which is the sum of the kinetic energy and potential energy and this is just standard physics hamiltonian is a sum of kinetic and potential energy and it gives us a total energy of the system um so why we started with the problem of sampling and here I just told you something very complicated where we get a hamiltonian there's actually a good reason why we want to do that and it comes back to the idea of geometric integration that we talked about last time which is that um typically maybe I have a process that I know would will give very efficient sampling but actually when I implement it on a computer in discrete time I just lose all the all the properties that make that something efficient so actually it turns out that most people working in Monte Carlo sampling they're working on efficient discretizations of continuous processes as opposed to on the continuous processes themselves really the bottleneck and the difficulty here is is the implementation part implementing process on on a computer like you and I had in such a way that we retain all the good sampling properties so one very powerful idea is that of geometric integration which is of preserving geometric properties of a system I mean geometric integration is the field that develops computational methods of numerical integration and numerical discretization in such a way that they preserve important geometric properties of the system here in the gym the geometry in play is the hamiltonian and so you might think well that's pretty weird I mean a hamiltonian is an energy and here we're talking about preserving the geometry and preserving The hamiltonian Howdy these two things fit together and so it turns out that the presence of a hamiltonian and the fact that we have a state space that has positions and velocity um technically in mathematics what what we then get is what people called emblematic geometry so symbolic geometry is just arises when we had those State space that comprise positions and velocity and when where you have hamiltonians so this is just like a way of explaining why the hamiltonian why the hamiltonian is like closely and intimately related to Geometry so geometric integration enables you to um to yeah to disco type processes in such a way that the hamiltonian is preserved now when you think about the hamiltonian the hamiltonian gives you the energy of a particular point in state space and so if you simulate trajectories that preserve the hamiltonian what you're effectively doing is sampling from The Contours of the probability distribution Contours that have the same probability so you're basically going around let's say for example in circles in a region that has the same probability when we want to sample we we want to go everywhere I mean we want to sample regions of high probability low probability and want to be able to go from one to the other so what geometric integration allows us to do is to simulate dynamics that are going to preserve the Contours of the probability distribution and are going to do so very well um the advantage here is that when we use geometric integration to simulate hamiltonian Dynamics which are conservative and again stay in the Contours what geometric integration allows us to do is to take time steps that are very long so it enables us to travel very far in the landscape while still preserving the hamiltonian so you don't need to take very small time steps to remain accurate and present the hamiltonian but actually geometric integration allows you to you know take very long time steps and still preserve the hamiltonian so now we have a dynamic that simultaneous preserving that's just very good because you can take very long time steps and that enables you to go around the Contours of the probability distribution so what you want to do next is you know to change Contours you want to go to Contours that are of higher probability Contours of lower probability and you want to do so in a way that the Contours of high probability are sampled much more and much more often than the Contours of lower probability um so what you do is that you augment your hamiltonian Dynamic with what people call a velocity refreshment or a momentum refreshment and so this is how I work how it works um so you're gonna yeah simulate your hamiltonian Dynamics for for some time and then after after a while you're gonna be like okay well now I'm gonna I'm gonna change the velocity at random by by just drawing a new velocity from a gaussian distribution and so by drawing a new velocity from a gaussian distribution you're just going to change Contour completely and then this what's called um and and then there's the constraint I just talked about which is you want to change Contours of the probability distribution in such a way that Contours of high probability are visited much more often than Contours of low probability so you want to preserve that you want to do that um proportionately in a way so the third ingredient of hamiltonian Monte Carlo comes in which is a metropolis Hastings and some Metropolis Hastings is this accept rejects that that we discussed briefly last time and so Metropolis testing is just super clever and it enables you to to say um to it just tells you whether this um this momentum refreshing step is actually a good one or a bad one and should be redacted a mountain refreshment is good if overall your sampling is gonna remain um is gonna remain faithful to the Target distribution but it is bad if it remains Unfaithful so actually running this Metropolis state things except rejects that allows you to change Contours of the probability distribution in a way that remains faithful to the Target distribution and so there you go that that's basically hamiltonian Monte Carlo so to recap you augment the state Space by adding velocities this allows you to build the hamiltonian by declaring that the original probability distribution gives you a potential energy and you add a kinetic energy on the on the velocities um which is just velocity squared because you have a hamiltonian you can you can use geometric integration to simulate hamiltonian Dynamics very accurately and with very long time steps the random time steps is a crucial thing because it means avoid very low computational cost you can sample very far so it means that you don't get stuck in a region of the probability distribution where you can actually visit it much more fast and efficiently so that's the first part um the problem with those Dynamics again is that they remain on the Contour of the probability distribution and so you want to sample the whole probability distribution so what you do is every once in a while and I think that's a hyper parameter in your in your sampling algorithm let's say every 10 iterations of the hamiltonian dynamic every 10 time steps of the hamiltonian dynamic you're going to sample um you're just gonna randomly take a new velocity So you you're gonna change Contour in the probability distribution and The crucial point there is you want to change Contour in a way that's faithful of the probability distribution and that's faithful the sampling problem that you want to do there comes the last thing which is Metropolis Hastings which ensures that the momentum refreshment will be uh good for sampling I mean we'll preserve your target probability distribution so with that you just get a very efficient um sampling algorithm and so it seems a bit convoluted right the um one bottleneck of course is that you need to double the dimension of the state space and if your database is already extremely large that could be a bottleneck it could be the case that I actually doubling the dimension to add velocities it could be a computational bottleneck so that's a problem um but still in most cases that not a problem and um so again hamiltonian Monte Carlo is convoluted but the overall take-home message I would want you to to take from this is that it is a method that remains faithful to the probability distribution they want to sample and this is crucial and it is also a method that through geometric integration it enables you to take time steps that are very long and still get accurate sampling so this is the the really cool thing um if you you could come up with a whole bunch of other methods that did not require to you know double the dimension of the state space or that didn't use geometric integration so the problem that you would probably run into is that the over the the thing that you came up with when you implement it in the computer it doesn't exactly preserve the the probability distribution that you want to sample and so the problem with that is that this would lead to biased sampling and by assembling by assembling is like you're gonna sample a different probability distribution which might be just a bit different but still um a different probability distribution than what you really want to sample and this would bias your your predictions so the really cool thing of hamiltonian Monte Carlo is that you're actually able to have unbiased sampling through the Metropolis Hastings step um and this is a crucial thing so it's computationally implementable in general there's no computational bottlenecks apart from this doubling of State space and it leads to unbiased sampling and it's also relatively simple so this is why this is really used all over the place there's um there's another few perspectives that might explain why it works so well so in in the paper in in section in the last subsection of of uh of section two which is about optimization um so the the section on optimization is about how to accelerate optimization how to derive an optimization algorithm um yeah yeah 2.6 yeah that's the one so the whole thing about section two is about deriving an optimization algorithm that accelerated in a physical way and by immune accelerate what I mean by accelerated it's not about just going faster but it's about having acceleration um and which means that so if you go slightly up in the paper on the figure I think this figure is great and by the way I wasn't the one who who came up with this figure but I think I think this figure is great it really gives a lot of intuition for what acceleration really is so if you if you look at the at the green cup I don't know how you call this uh the green well on the left uh this would be like um a ball rolling down a little while in like uh and the well is filled up honey to to like a certain level and so there you get a lot of friction so your ball would just roll down very very slowly and the speed of the bow will be proportionate to the slope of the well um so this is not accelerated because there's a lot of friction and so the speed is just proportional to the slope of the well and this so This is actually what grade in the sun does but now if you go on the right you get what people call in physics on understand system which is also an accelerated system a system that's meaningfully accelerated so if you replace the honey uh in the world by some water then there's going to be way less friction and your bow is actually going to accelerate and overshoot the minimum of the well and then sort of stabilize but the point here is that this um this bow is just gonna get so much faster to the minimum now um so let's say that we just um I mean so this optimization this idea for optimization is is just extremely powerful and um and by the way on the right hand side in the graph you can see the improvements that you get when you when you implement these sort of ideas so this was on I'll come back to to sampling in a bit I promise but this is actually very well very related um you you can see on the graph on the right what kind of improvements you get when you go from a under the first order system grade in the send system to a well good when you go from an overdamped first order creating the Sun System to an underdamp second order accelerated system and so you can see the curve right um so the Black slash orange curve is overdone there's no acceleration it's very slow um the the last curve the the blue one it's accelerated and it's just super fast so you can see it sort of gives you like a quantitative data as to how many improvements you can get by implementing this acceleration in a physically meaningful way could you just describe the axes of the graph and also what the lead group is here um so the Lee group is um so in in the figure legend in the fourth line you see the E group is son so that's this um technically that the special or orthogonal group of Dimensions n and so if I if I remember correctly these are all the matrices all the square matrices of size n by n that have a determinant equal to one so the determinant is just like um a function where you give it a matrix and it gives you a number the fact that um the fact that the determinant is equal to one means that here we're just taking matrices which are um which produce Trend not translations but changes of coordinates that preserve the geometry in a system so so just to to elaborate on this a little bit because for people who are not intimately familiar with Matrix is it might be a little opaque um so when you have a matrix a matrix can multiply vectors so a square Matrix takes in a vectors of size of Dimension n and it outputs vectors of Dimension and by multiplication so if you have a square Matrix it just induces a transformation of your state space because each each Vector each point which is the vector gets converted into another point which is another vector now um the so there's a lot of I guess properties there right you could have all sorts of translations of State space or rotations in state space or transformations of database if the determinant of the Matrix is equal to one it means that the transformation of State space that the Matrix induces uh will preserve the geometry of the six facial preserve distances if you and it will preserve crucially orientation as well because if the if you only asked um the determinant to be equal to one or minus one then you would the The Matrix transformation would preserve the geometry but it would not preserve the orientation it could like mirror things um so if you only asked the determinant to be equal to one or minus one you would get a Lee group that's called the orthogonal group which is usually denoted o of N and here we we require the determinant to be just equal to one and so you get the special or orthogonal group and so it's this group of matrices that have this property now this is not um really I mean this is not really important for the example here because you could have taken any other lead group and got something similar and got a similar difference in performance but but um yeah I guess it's just interesting for its own sake um and so the graph though the K is the number of iterations so the number of time steps so that's the x-axis and the um the y-axis is how close you are from the minimum and and it's in lock scale so as you can see the the blue curve is gonna get like in 100 iterations it's going to get basically at 10 to the minus 4 10 to the minus 5 distance from the minimum so it's like super super close it's basically gets there in 100 iterations uh when you take the other normal gradient descent methods where you see that it's going to take so much longer um so yeah this is really the advantage of using second order method and what I mean by second order method is that by second order means that you actually doubled the state space to introduce velocities so that you not only have a dynamical system over positions where you have a dynamical system of velocities and so it means that you have a physically um a physical notion of acceleration because acceleration is a movement of velocities um and so if you double this database by saying this is position this is velocities and you say I have a motion in this like a state space that's twice as big then you have a meaningfully a meaningful notion of acceleration uh and this is really powerful and so you can see already a parallel here which is that hamiltonian Monte Carlo also has this notion of acceleration in some way uh at least just intuitively because we we also uh doubled the size of the state space to introduce velocities and so it turns out that this situation is actually uh you can make it into a formal correspondence um and this is I think this is something that quite interests me to be honest um so if you if you remember if we come back to the intuition for sampling the intuition for sampling through Monte Carlo methods is we have a target distribution that we want to sample from so we're gonna run a dynamic random Dynamic towards that towards that distribution now what makes sampling efficient is that the distribution that characterizes the process because again the process is random so that each time it is at the random location that is characterized by a distribution what makes sampling efficient is that the distribution characterizing in the process just converges as fast as possible to think that you want to sample from so there is this is to say that you can think of sampling as an optimization on the space of probability distributions you want to move your probability distribution of the process as fast as possible to your target and so you can see from there that sampling is actually not so different from variational inference um it's just the same idea only variational inference you're typically gonna like um yeah take a parameterized family of distributions and approximate the target while here you have a non-parametrized family that's given by your Dynamic that's going to perfectly match the target um so so again so so with that I want you to have the take home message that sampling you can think about it as an optimization on the space operability distributions in the sense that you have a target distribution and you want to get there as fast as you can with a process now let's suppose and so here is the crucial connection between sampling and optimization in in the way that we've described it here let's suppose that you run this accelerated optimization scheme on the space of probability distributions then the the process so you would get the dynamic on the space of the probability distributions that has a meaningful notion of acceleration through through the accelerated method that that was shown here so if you look at what that what kind of dynamic this really gives you it gives you a process which is given by a stochastic differential equation that is known as under that logical Dynamics um so there's an equation for it in I think a section subsection 2.8 but the point is under larger Dynamics is a stochastic differential equation whose density or probability distribution those this accelerated optimization problem on the space of permitted distributions so just from there you know that underground Dynamics is going to be a very efficient sampler because it meaningfully accelerates and gets the target I mean quite fast the problem is Dynamics you cannot simulate it accurately or you can simulate that accurately but you cannot simulate it exactly on your computer in practice so this comes back to the numerical integration problem that we discussed just before um and so you have to to find a way to to discretize or or in other words Implement underground Dynamics on your computer in such a way that you retain that the thing that you implemented on the computer retains this meaningful notion of acceleration and so it turns out that you can see a hamiltonian Monte Carlo as a faithful numerical discretization or numerical implementation of under that logical Dynamics that's going to preserve these acceleration properties and therefore these this efficient sampling so this is all like um yeah a lot of interesting connections but basically what I want to get at is you really have this notion of acceleration that permeates where I guess physics but here it permeates sampling and it permeates optimization and so the method that was shown here about optimization and hamiltonian Monte Carlo they're just the same in a way only one is applied to optimization the other is applied to optimization on probability distributions of AKA sampling great I guess last question on this part what is the Shadow hamiltonian and why does it sound so cool I know right yeah it's uh it's really cool um and it's a recall name so the shadow hamiltonian is um okay so so let's go back to the hamiltonian so we have a hamiltonian and we want to simulate the dynamic that preserved the hamiltonian we want to implement that on the computer we're going to do that through geometric integration geometric grid integration gives you a bunch of algorithms to preserve the hamiltonian I mean a bunch of algorithms they can Implement on the computer and they're going to preserve the hamiltonian do they actually really preserve the hamiltonian it turns out that no so what uh what I said before is a bit of a shortcut like um because the numerical integration or that you get through any any numerical method including geometric integration is not going to exactly preserve the hamiltonian but it's gonna preserve what people call a shadow Hamilton which is almost the same as the hamiltonian but with extra terms that sort of vanish if the if the time step is very is very short um so this is to say that your numerical Dynamic uh implemented through geometric integration is gonna exactly preserve the Shadow hamiltonian and approximately preserve the true hamiltonian and so there is um in the paper we show um I don't know if it's well it's probably algorithm dependent but basically depending on the algorithm that you choose you want to show to what extent the shadow hamiltonian truly approximates the true hamiltonian the the virtue of geometric integration methods is that actually the shadow hamiltonian turns out to be extremely close to True hamiltonian which means that you can take a very long time steps and still be very good at approx at preserving the true hamiltonian so here you don't when you implement these methods you don't exactly preserve the true hamiltonian but you do you still do it pretty well and so in hamiltonian Monte Carlo um the momentum refreshment and Metropolis brutally the Metropolis Hastings accept rejects them it is gonna correct for those failures of of truly uh preserving the hamiltonian so even though you have some you don't you don't exactly preserve the hamiltonian in your numerical integration in hamiltonian Monte Carlo the Metropolis Hastings accept reject step is gonna correct for this inaccuracy and so this is like really the true beauty of hamiltonian Monte Carlo is that even though you get a lot of things that are not exactly preserved when you implement things on a computer thanks to Metropolis Hastings overall your sampling is gonna be perfect in the sense that you're going to truly preserve your target distribution so this is really the key um now a follow-up question to that is okay well if Metropolis Hastings it's just so crucial and it just gives your Dynamic um the property that it's gonna preserve whatever it does even if it samples really badly issue and Metropolis Hastings it's still gonna um preserve your target distribution then why can't you just come up with the average process like whatever kind of process another Metropolis Hastings at the end and so you can do that you can take any kind of process and after and so let's say you take a random process it could be the worst in the world let's say you wanted to sample from a gaussian and you actually take a Brownian motion now this is clearly now going to work because brand new motion just goes all over the place it could go infinitely far ground emotion and it's just like random motion right completely random motion no there is structure to it but there's no the structure in brown emotion means that is touch that Browning motion is just gonna just spread really really far so Browning motion is not going to preserve your target distribution so it's gonna and it's also going to be very slow by the way so it's not gonna be a good sampler if you add Metropolis Hastings on Brown emotion which is you integrate by emotion so you just simulate a step of running motion on the computer and then you you do Metropolis Hastings to to know whether you should accept or reject that step which obvious Hastings will tell you whether you should do it or not you either accept and you stay and you keep going from that new position or you reject and you start from where you started from and take a new sample accept reject if you accept you keep going if you reject you go back and you sort of go like that so if you do Metropolis testing there your standard is going to be exact so it is going to preserve the Target and so you're gonna have accurate sampling um and so this is crucial this really highlights the importance of metropolis Hastings but and there is a big but your your sampler here is going to be extremely slow and it's going to be extremely slow first of all because brand emotion in and of itself it doesn't at all preserve the target distribution I mean just goes all over the place and you just wanna sample a lot around the mood of the gaussian for example so it means that Metropolis Hasting will reject a lot of your steps because a little of your steps will try to go further away by you while you want to stay around the mode typically of the gaussian so you're gonna do a lot of steps for nothing and also run in motion just does not have the qualities that make a sampler efficient and so the the reason why hamiltonian Monte Carlo is so good is because it's able to integrate Metropolis Hastings and still preserve a lot of other properties that make um that that make the sampler efficient so if we remember Tony Monte Carlo has this geometric integration that does not exactly preserve the hamiltonian but it does still does it pretty well approximately by preserving a shadow hamiltonian so this means already that in that integration step is going to be really good so there's going to be a lot of accept acceptance in the Metropolis Hastings um so so and and also the way the way the scheme is set up there's going to be a lot of acceptance step in the Metropolis Hastings so it means that most of the samples that you would draw will actually be used instead of being rejected and you have to start over again so that's one advantage the other the other advantage of Hamilton in Monte Carlo and this is a crucial one and we kind of discussed this last week is that one crucial thing to be a good sampler is this idea of time irreversibility and so I'll emphasize it again because it's really crucial and there's a lot of literature on this and it's something that we reviewed also in the paper so a sampler that is time reversible even if that is time irreversible is always going to be better or it's at least not going to be worse than than a sampler that is time reversible so what do we mean by this a sample is time irreversible um if and only if uh were you to play the Dynamics forward or backward there will be um yeah so yeah so so going on your bullet point here a sample is time reversible if and on the ETF if you were to play the Dynamics forward or backward in time so forward in time and then maybe you would play them by reversing the movie um by playing the movie backwards uh if you were to do that then um then the the two movies that you would see would be qualitatively the same and statistically the same so the process is time reversible if and only if the process if you're running forward or backward it's basically statistically the same now what does this mean in practice well if your process is time reversible then it's going to backtrack very often actually um so it means that you you would be somewhere uh in the probability distribution just sampling there and you will go forward and then you probably go backward and so on and you kind of get stuck in a region until you go somewhere else but it's just going to be very slow to move around um and so it's going to be very slow to get uh you know good sampling because it's going to take you a long time to visit all the regions of the probability distribution in other words the distribution characterizing the process it's not it's going to move very slowly to the Target distribution that's another way to look at it um if the process is time irreversible on the other hand then it's a lot less likely to go backward during the sampling process so it means that it's going to visit the target distribution a lot more I mean it's just going to go around imagine if you're not allowed to go backward just as a human being and you're walking around you're gonna just gonna end up in many more places than if you were allowed to go backward and if you were and if you were just going backward all the time to where you started then you wouldn't be able to to do a lot of visiting so it would be a bad temper so so I guess there's some like a straightforward in intuition there um so one idea is in sampling is we want to optimize the extent to which um a sampler is timely reversible we want to increase time and reversibility as much as we can to force December to just move around as much as possible so that's an idea and and it is explored currently I guess it's a bit of an open problem of how how do you really do that and how do you implement that on a computer in a way that you know works and preserves all the properties um but the point I want to I wanted to get to is that um Metropolis Hastings is a blessing and it also a curse it is a blessing because when you add it to any kind of process you will make your sampling unbiased so it means that you will sample the right distribution even though you're implementing this on a computer and there can be a lot of so many issues with the numerically numerical integration numerical approximation and so on but it's also a curse because whenever you add Metropolis Hastings um on on the process it's going to make it time reversible [Music] so actually yeah if you if you just took a random process and added metropoly six things you would get something that's inevitably going to be quite low and so now you get to the you know the problem or the conundrum where what do I do um do I go for unbiased sampling do I care about accuracy um and then I should add a metropolis Hastings or do I not care about accuracy so much um and then I should not have Metropolis space Hastings and it's going to go faster generally um and so hamiltonian Monte Carlo actually has the blessing and does not have the curse and this is why hamiltonian Monte Carlo is just so good and again it's also complementary to all of the things that we've been discussing and the reason why hamiltonian Monte Carlo is blessed and not cursed is because the momentum the how do you call this the Metropolis Hastings is just done on the momentum refreshment step as opposed to being done on the overall dynamic so because if you remember you do geometric integration to simulate hamiltonian Dynamics and then every once in a while you will take a momentum refreshment and then do Metropolis Hastings on that momentum refreshment as opposed to doing Metropolis Hastings on both the hamiltonian dynamic and the momentum refreshment so it's it's not um it's not something that I can really explain how how that works but it turns out that by just doing the Metropolis I mean Monte Carlo is a way of having Metropolis Hastings in a way that you preserve the unbiasedness that's so that your sampling is accurate but you don't sacrifice the time irreversibility and so and so you get both um less and this is because the Metropolis hitting is just one of the components and not on both to kind of connect that to an example as we move into active inference here we have some probability ISO Contours some we have the high probability carpool lane and then we have some lower probability Lanes and uh we can go different speeds in these lanes and we want to have a full accelerated model here it's almost like the MH is is refreshing our velocity just asking when we want to change the lanes but it's not our full self-driving car Metropolis Hastings algorithm but we're able to use our position and acceleration when we're in our Lane to take full advantage of the acceleration following the shadow Road not necessarily the true road but the shadow road is close enough or the Shadows on the road and then Lane changes are these computationally costly and reversible but still super useful propositions exactly and they actually they're actually not computational completely costly they would be if you were to reject many samples but here you in Hamilton Monte Carlo typically you don't get to reject many samples so yeah you that's such a I mean that's such a great picture see the card follows the shadow Road by just doing geometric integration and following the shadow hamiltonian every once in a while you get a momentum refreshment a little bit of Steel refreshment which is oh um now I moved to the first lane or the second main third lane and so on and you get a metropolis Hastings correction step that tells that says do I accept this proposition proposition proposal of Link change or do I reject it and so on it goes so how does this connect to active inference well I mean great question uh so inactive imprints in active interest proper you don't have any sampling inactive imprints uh whenever you want to scale active inference to implement it to solve any kind of problem in the world you want to do sampling because there's a lot of computations that are not going to be trackable otherwise and so one um one cool example is when it's in the figure after this actually so when you want to vote for any kind of decision making inactive inference you need to approximate the expected free energy and system uh no the one just before here uh yeah perfect um so if you look at the second equation there you get this minus log P of of an action sequence equals expectation of something this minus log p on the action sequence is our notation in this paper for the expected free energy um so the expected I mean you probably all know this the expected humanity is given by the expectation of something now um typically and when you have a very high dimensional model which happens in in most applications I would say you get a very high dimensional expectation what is an expectation an expectation is an integral with respect to a probability distribution how do you compute expectations well you do that through sampling or at least sampling is the way to compute High dimensional expectations that's the most efficient in statistics and this is the reason why something is studied in statistics is because that's how you solve these problems I mean explore expectation just come up everywhere um just like so many things about machine learning boil down to Computing expectations and so here in particular the expectation that is very dear to to us very close to our hearts is the one that gives you the expected for energy the expected frequency expectation of something so how do you compute that in a high dimensional model where you have to use something and so there comes uh the usefulness of hamiltonian Monte Carlo if you have a discrete State space model you're now going to be able to use simultaneous Monte Carlo because hamiltonian Monte Carlo is a continuous state-based models um it's on continuous State space right we talked about continuous positions continuous velocity I talked about sampling of probability distribution on my desk which is a continuous State space so you're now going to be able to use hamiltonian Monte Carlo if you have a particularly observed Mark of decision process when you're doing active reference there where you have for example I've been talking recently to Ryan Smith um who who does a lot of active inference and and he's really leading a lot of the modeling work with active inference and real data of patients of all sorts many medical data and also using active inference to model psychological experiments and um one of the things he told me about well is okay well I have a bunch of data and a partially observed marker decision process is just not going to do it uh why is it not gonna do it because the data that he has is lives on a continuous space I don't remember exactly what it was but let's just say for the sake of example that uh that data was you know the temperature in the room and and people had to the the subjects had to infer the temperature in the room based on their Sensations so that's a continuous database problem now imagine that you asked you asked someone to infer the temperature in a room then maybe you would change the temperature in the room or not and you would ask them again change the temperature or not and ask them again and so on in when you have this sort of setup you have a discrete you have a phenomenon that unfolds in discrete time because you just repeatedly ask some question and sometimes elapses between the questions um but you also have a state space that's continuous when you have this sort of data then the kind of model that you want to use is a partially observed markup decision process that like we know and love but the state space is going to be continuous instead of discrete the time is still going to be this week um now when you have this sort of model and I just want to say it as an or as a yeah as an aside that for the moment a lot of the modeling work and and simulation work in active influence uses the sweet State space on the piece partially observed Mark of the same process just because it's been sufficient and when you simulate agent is often like in green worlds and when you maybe have a state spaces in experiments they often buy design discrete because just easier to handle but that's not something that that's not an assumption a simplifying assumption that we're gonna have that we're going to be able to keep or you know that much longer there's just so many things that you cannot account with these sort of models so one other model that is interesting to have is partially observed Mark of decision process but with A continuous State space so with those you get the problem of you know estimating the expected free energy which would be then an expectation or in other words an integration an integral in a continuous State space and so how do you do that efficiently if you are in a high dimensional State space where you use I'm not talking about the cardo I think this would be really the the best method so this is how you join the dots um in this paper we didn't talk about um sampling in discrete space because the methods are quite different so we had we really had to choose what to focus on there's a of course a lot more methods in the literature than they are in this paper we just wanted to sort of review what were the main ones and what people really used in practice um but you know when you have the expected free energy in a discrete stated space Computing the expectation that defines the expected free energy can often be a little bit easier because it comes down computationally to a bunch of Matrix multiplications The Matrix Vector multiplications which generally is doable unless I mean the state phase is enormously high and then we'd have to think of sampling methods in this switch space um but as soon as you move to a partially observed markup decision process in with A continuous space which are iru and Ryan Smith argues and I'm sure a lot of people have run into this um as soon as you have this kind of model well yeah you just have an integration problem in continuous space and so Hamilton Monte Carlo is the way to go there awesome and your 2020 paper was the synthesis on some of the discrete State space formalisms of active so it's very interesting to see how you're now talking about where continuous time continuous State spaces can come into play and how interesting that active inference has uh the capacity to deal with discrete and continuous State spaces and sometimes we lean on one leg or the other leg more but it spans the Gap in a way that's actually like a value adding not like there's some sort of missing piece from one side or the other yeah yeah definitely I think active information is great because it's so flexible in the sort of models that you can consider um but but yeah I mean historically the first models to be developed were continuous space and continuous time and then it works quite well because you can take gradients of the free energy and just minimize them over time so you could you could get around basically everything by doing a gradient descent on free energy and then after that around like so that was like 2010 and then around 2015. uh people started thinking okay well how do I model um discrete time decision making with active inference the and typically the decision-making task at least the simple one that is studied in New York Times in behavioral Neuroscience just to make things simple um the the number of actions that you have is discrete so you have a family number of actions you have also and which is pretty small you would also have a final number of dates which would be pretty small and then and then people started to think okay well how do we use active reference to actually account for that um and so yeah and and so that's how the whole expected free energy and partially observed Mark of decision processes uh came into play and and now the community has grown a lot and there's more and more data that we want to account for there's more and more projects that are going on and where and you know people are realizing and I think I mean it's an obvious realization it's just not um it's not surprising at all that these two models developed in 2010 2015 they're just not going to account for everything you need to think about other kinds of models and it really depends on what kind of data you have at hand and then one important and obvious type of model would be partially observed Mark of decision process with continuous space and actually this is not a new thing I mean people have been using these kind of models um in reinforcement learning and control for a long time I don't know to what extent they've arrived at practically I mean big Fair arrived at practically implementable algorithms I don't know to what extent they're used to what extend their state of the art um but the point is they it's something that it's a kind of model that you know has existed for a long time um and so this sampling is would be a way to actually practically implement this and scale it when you want to put that within active inference so I think yeah it's an important thing is just think about awesome yeah Thomas Parr also recently in a discussion on the textbook was sharing some timelines and it's just so interesting how these things have been developing and from continuous through characterization of the discrete in a sense not culminating but being synthesized in your 2020 paper and now there's with an increased emphasis on empirical data the desire to bring in a lot of these methods that actually help us implement it rather than just think about it really parsimoniously so what is your active inference representation in the figure of the running person and then how do the variables and the processes described in this figure relate to all of this you know five hours of talking about Shadow hamiltonians and all of this really um yeah that's a broad question we might need another 10 hours you know to 10 so then detail but uh just very short um well so the active inference section was meant to be as complete as possible even though it was very short and by complete by completeness I mean that we started by deriving active inference and deriving active inference for first principles um so this is really what the free energy principle does even though we weren't able of course to you know review the whole free energy principle in like one or two pages but we what we focused on was deriving the expected reality from first principles and then from there you get the full active inference algorithm so proactive Infamous algorithm I think you just went past it is just below um if you go slightly below I think maybe patient nine or page 30 yeah uh still I think next yeah realizing an active agents yeah so this is the difference algorithm like 0.1 2.3 or something so just from a duration of the expected free energy um you actually get as a corollary from there the active inference algorithm that we know and love this is actually a more General version than what people use and I'd be excited and I'm actually talking to people to actually Implement that um but this is actually the most General version that has been established in the literature and it is more generally the meaningful way in the sense that all the beliefs all the probability distributions they are over trajectories or sequences of events so it's not only so so it means that all the computations they not only consider events at a particular time in the future for example but they consider trajectories of sequences of events and so this is equivalent to considering different events at different points in time and their kind of independencies and dependencies between them um so this is just to say that if you take if you would take then this algorithm and you wouldn't perform a mean-shield approximation over time which is uh saying that all the things that you see in the future are sort of independent of each other at different points in time then you would recover what people typically use in in the literature which is easily implementable the the point I want to make here is that you can actually not have that limitation and have things that are a bit more um yeah a bit more complex that are able to capture more complex relationships in the time series and input that they might be receiving like the kind of sensory data that they might be receiving and the kind of genitive models that they would have uh so anyway this is the active inference algorithm the most General one and we derived that from first principles by deriving the expected Community from first principles and so this relates us to to that figure that you just showed so that that figure that you showed um with s o a is the starting point of the free energy Principle as it's described in this paper and so the point is that the world the point that we want to describe decision making uh went to describe actions as the function of Sensations and so and we want to come up with the most General description of actions as a function of Sensations to be able to account for everything that's kind of the idea of the pre-enter principle um so what do we do is we consider a world a world in which there is an environment and an agent and so the environment here is denoted by S and S is a stochastic process what is stochastic process um because a stoopathic process is the most General type of dynamic that exists at least as far as I know um it's really a random Dynamic and it could be random in all sorts of ways it could also be done random we sort of take the the stochastic the random aspect as the extra ingredient just to include a lot more types of scenarios and things you can be confronted with so if you partition the world into the agent which is O and A and the environment which is s and the agent then you subdivided into two more components which are o and a o are the what we call the observable state so these are like the sensations of the observations that you get at any point in time also a stochastic practice and finally you have the autonomous States or you could think about it Loosely as the active States they turn out to be the active states in the implementation but so the active dates they're really um you know like the muscles the things you can actually activate and make it and then activate to you know influence the world um so we just partition the world into these three sets of states that interact and and evolve in some way they're both really stochastic processes and in that interact and then the goal of the community principle at least when applied to decision making is that you want to describe a as a function of o because what so what happens as an organism you can you have control over a you have access to O but you don't have Direct Control of o or your Sensations and you don't have access to S because that's the environment and beyond beyond the markup like it Beyond view beyond your envelope so you don't have access to as you know o and you can control a a is what you can choose from and so the 300 principle just answers the question okay well um if I take this very general description of the world how what is the equation of a as a function of O how can I describe a as a function of o and so it turns out um there's some mild assumptions in this and these assumptions uh they're they're Guided by physical considerations about how humans are and how humans interact with the world but they're very meld but whenever you take these assumptions you get that the active States or the autonomous States minimize expected free energy so so this is like a very succinct um derivation of the expected free energy from first principles we start with the partition of the world we describe active States as a function of observations and so it turns out that the expected free energy is what describes the active States as a function of observations um and so and and so the basic active inference algorithm which everybody uses and which we have proposed in the paper is about it's all about Computing the expected free energy and then selecting actions that minimizes expected free energy so the expected for the energy as we saw I think it's in the next figure so the one with all the panels well within it a couple times already it's um it's an expectation with posterior distributions in it um what I mean by posterior distributions here is that these are distributions conditioned on the history of the agent so in the observation that he has already seen and on the actions that he has already taken in any case in any case we have the expected free energy is an expectation with posterior distributions within so it means that to compute the expected free energy you have two problems you have Computing posterior distributions through Bayesian inference base Rule and you then once you have them you can plug them in into this equation and then you you need to complete the expectation and then you get the expected view energy so the first thing is about Computing the posterior distributions now what has been proposed in the literature so far is well how do you do approximate inference how do you actually approximate distributions you do that through variational imprints or approximate inference by minimizing fluidity then the treatment here the derivation and we our aim was to do the provided derivation that was as conceptually simple as possible actually it highlights that the free energy is not the most important thing here um the important thing is really the expected free energy the free energy is just a tool to approximate the posterior distributions to then get the expected energy but you could actually use any other type of Divergence the community is just like a tail Divergent plus some term that makes the whole thing tractable and so you can minimize the Cal Divergence to approximate the targeted distribution but actually what what this view highlights and by the way I'm not at all against the free energy I think it's really cool and whenever and there's so many methods to minimize free energy and if you can do that then fine and sure go for it like way to go but imagine you could not do that it would not at all be a problem because you could use any other kind of Divergence to solve those inference problems um so so that would be the first step you're going to approximate those posterior distributions by doing some kind of approximate inference which would be through minimization of the energy or the minimization of something else and then you have the Second Step which is Computing the expectation now either you're in a low number of states discrete States probably P like thing and it's all a matter of vector Matrix multiplications that are attractable either you have a very very high number of discrete States and then you need to think about sampling um in discrete states which we didn't discuss in this paper either you are in a continuous database and then you have an expectation in continuous State space and then you want to think about an alternate Monte Carlo for example so now with these two steps you have now an estimate of the expected free energy which gives you the quality of any action sequence and so it's not only the quality rates actually the negative log probability of any action sequence regarding in this formalism because if you remember the you know the premise of all this was to describe actions as functional Sensations in a physical system in not even a physical system but in interacting stochastic processes [Music] um and so the answer to that was well the expected energy gives us you know gives us how actions relate to Sensations um the expected humanity and this is why we use the letter we usually we used minus log P of the a as opposed to G to emphasize that the expected free energy is not just like a function of oxygen sequences but it is really the negative log probability of an action sequence given some Sensations um and so once you have computed the expected free energy you had this minus log probability of an action sequence if you take the exponential negative of that the exponential negative of the expected free energy you get um you get the probability distribution over action sequences um and so by the way this exponential negative of expected frequentity is what we use all the time you might recognize this as the sub Max of negative expected free energy this is just all the time um kind of fundamental thing in active influence models so really talking about the same thing here um so once you take the soft Max of negative expected free energy you get P of a which is the distribution of reaction sequences and now you have two possibilities either you want to stimulate the most likely action sequence in which case you want to simulate the the actual sequence that maximizes the probability distribution um so in effect you have an optimization problem you out of all um yeah you need to find the action sequence that is going to maximize this probability distribution that's given by the expected free energy or um or you want to simulate a typical action so what do I mean a typical action a typical action or a typical action sequence is just a sample from the distribution and so if you want to sample from I mean if you want to do that then you have a sampling problem or again you need to sample from from a distribution over action sequences given by the expected free energy so this is really how how all these methodologies connect um yeah I think I think that's kind of it yeah one last thing is um typically when we stimulate active inference we never do the sampling at the end we always take the action sequence that minimizes expected reality in other words in other words the action sequence that maximizes that probability distribution um and this is because when we simulate things we it turns out I mean this is how people have done it in the literature people are more interested in the most likely action sequence that um that an organism or an agent would produce but if you want to model data if you want to use active inference to model data then you actually want to not just simulate the most likely action sequence but simulate any kind of action sequence that would fall out of this and so this is really the case where you need to sample from that final distribution as opposed to the optimization so depending on the use case you either have a sampling or an optimization problem after you've um you've computed the expected free energy so this is how these whole Things Fall together and and so you might be asking okay well you talked about sampling and optimization there's also a section about inference where does that fit in and so it fits in to uh it fits in on on the remark I gave that you know we have this posterior distributions within the expected fragility how do we compute them how do we approximate them one way is by minimizing free energy another way is by minimizing any other kind of Divergence and in that section over there we just reviewed some kind of divergences that are very popular in the in this school in Prince literature mainly because they have desirable properties and and so if you weren't able um to use the variation of the energy for some reason or maybe something to be explored and something to be explored is just to use another type other types of divergences and and just see what happens um so the the open problem to be explored is whether we can get better performance by using different kinds of divergences and and see what we get uh maybe there's particle algorithms that are out there with these other types of diverges then we can make use of to do better performance to get better performance can we actually describe and quantify the improvements in performance that we get by using these other types of algorithms when would these be appropriate and useful and these are all open questions um another good question that I think is interesting is can we model maladaptive Behavior by using different kinds of divergences that would not work as well as the free energy and this um there's an interesting paper on that by uh newer Sajid I think and colleagues I think it's called um Bayesian brain Basin brains and the Rainy Divergence it's been published in neural computation I think last year or the year before and answering that paper instead of using the variation of free energy to approximate all these posterior distributions um she used the Rainy Divergence which is which generalizes the kale Divergence in some way and show that for different rainy divergences you got different types of approximate posteriors and she basically looked into yeah what kind of differences you get from there in terms of perception in terms of decision making and she showed that for that particular diver gence I think the conclusion was that you basically get different phenomenology you get different Behavior suggest something to be explored I think the upshot was that maybe I I don't remember how far the paper went in this but I think kind of the the goal was to model maladaptive Behavior using some kinds of rainy divergences that did not work as well as the free energy so this is to say so this is an interesting work um one could examine other kinds of divergences and see whether you actually get better or worse performance than with the free energy uh one thing one word of caution though is the ex the free energy is it's just so good in the sense that it uses the KL Divergence and as I mentioned at the beginning the care Librarians has so many properties and it's just so fundamental um it's not straightforward to see that when you first come up with it but I guess the more I read in different disciplines and the more the kale the evidence comes up and the more the more I see properties of Cal Divergence in different disciplines that just make it so interesting like for example the kale emergency in statistical physics is the relative entropy so quantifies the amount of entropy that a distribution has with respect to another entropy as we know is just a very fundamental thing in information Theory the KL Divergence quantifies the difference in information between two distributions the amount of bits that you will need to I mean um if you take two distributions A and B the KL between the two quantifies the amount of information that it takes to go from one to the other so now you might object and say okay but klanb is different from KL BNA and so how can it be that you know it quantifies the amount of information um they need to go from A to B or B to a because it's not symmetric um so the answer is it's either KL of A and B of A and B that has this meaning or KL of DNA that has this meaning and I just can never remember which direction it's one of them that has this interpretation in terms of the difference in information anyway um so kale is just like um something it just comes up everywhere and it's just um so useful and it has all of these nice properties which makes these free energy really a construct of choice but that said there's other divergences one of them which we describe a bit in the paper called um the maximum mean discrepancy which I think also has nice properties it's not necessarily well it's a bit different it's very different in terms of how you construct it from the kale Divergence but I think my current understanding right now and um my current understanding right now is that if you cannot use KL then maximum mean discrepancy is just like be nice but but yeah so the word of question was that probably most divergences out there are not gonna do as good of a job as a free energy uh but some clever ones might and it's an open problem of you know determining which and and yeah so this is how you know we get the link between all these different sections so to to do perception we need the inference to do decision making that is Computing the expected free energy we need sampling typically and then to do action selection we either need sampling or optimization and also inference is an optimization of beliefs and sampling of that we as we've discussed is also an optimization of probability distributions you can see that as an optimization of beliefs as well so it's all very tightly interconnected wow thank you Lance that's very informative brings up a lot of ways to go and it it really shines a different light on even what learning active inference or learning free energy principle it was a roller coaster just listening when you describe the figure of the running person and the new generalized representation here which is so sparse it looks like it's pseudocode but it's actually basically necessary and sufficient to describe yeah it is pseudocode and it is also exact like this is really what active influence boils down to and that was kind of like where we got it we're like okay well we have this mass of papers of active inference and that they're actually a mess but you know there's so much information and we're you know really thinking reading all this the latest free energy principle literature that I've also worked on a lot and also the active in French literature and really thinking okay well how do we strip all that of the Neuroscience how do we strip all that of the cognitive science how do we just retain the math and present it in the simplest way possible that would be appealing for a mathematician and also hopefully for a computer scientist it turns out that we got a way simpler perspective than anything that's out there I think um and really yeah so this is the active inference algorithm in studio code and in detail also in in a way so I totally agree again thanks for the work and for sharing it this way because it is the fewest pixels for the highest resolution picture so then to describe the fundamental cybernetic challenge as a partition an axiomatic partition which one can then say also has grounding in the spatial temporal boundaries of the world or in geometric boundaries in informational spaces but the particular partition is used to separate um in the map not necessarily making claims about the territory and the actual nature of the objects and their articulations or Anatomy but on the map which we get to construct we make it in a way that's amenable to the particular partition which is not too much more than the separation of figure from ground or agent-based modeling it's just a separation of some autonomous entity from some external process then the task of the free energy principle applied to decision making as you said was to describe action as a function of observation and everything in between is broadly considered cognitive but for any given system it's going to play out in this immensely nuanced way with a lot of bespoke mechanisms and why do we take that particular partition step well it's kind of like read write access in a computer system there's things we don't have access to Hidden States and also the reverse of that which is we can't access hidden States nor them influence or affect us so I think of that as like no telekinesis no telepathy you can't go directly across the blanket you have to intermediate through the blanket of observation action and then with respect to our particular States our blanket and internal States we have access to observations but not Direct Control nor necessarily would we want to because if we had the lever to change what we directly received then algorithms might learn strategies that basically self-deceive so that the observations look good look good look good until the whole system crashes so we want access to observations but not Direct Control and then the autonomous states are what we in the optimal ideal situation have total control of which is like our mind and our body what we think and what we do it turns out through the pragmatic turn and the inactivist insights in cognitive science that a lot of action sequences have to do with changing What observations are sought after like epistemic affordances so there is an enmeshment of action but it's also really important that we have access but not control of observations we don't want to have like our control on the thermometer but we want the best possible thermometer and then we want to control the best possible interpretation that's like signal processing and the best possible action sequence which is like decision making and control theory and then free energy principle is addressing that question what is the equation of action as a function of observation but then it was quite a roller coaster when you said that the free energy wasn't even necessarily the only way to do it but it's absolutely true it's yeah the properties of the free energy are inherited from one of the terms being the KL Divergence any other having the ability to basically ignore in certain relative expected free energy calculation contexts so then in that situation differences in free energy do come down to differences in the KL which does have all these properties but that doesn't mean that the free energy is itself axiomatically posited it's actually Downstream of the particular partition to use free energy or anything like that at all and other discrepancies may have other properties in different ways exactly and and so there's a bit of a Nuance in the sense that here we presented this version of the advantage principle just describing decision making as you as you summarize the describing a as a function of O um but if we and and by the way about that so we're we didn't even talk about Marco blanket in this paper um I would I would say that the mark of blanket is under the hood because we're saying okay well these are the states that you have access to and these are the states that you do not the states that you have access to are a and o the states of the agent and it states that you do not have access to your ID environment so in some sense there is a markup blanket but we didn't even mention we didn't even have to mention that in the paper it's just you partition the world into three sets of States as o a um that are by definition a um and you just yeah just from this tripartition you want to describe one as a function of another ignoring the Third um and so that yeah that how you derive expected Community you see that action sequences are described by the expected for energy and these this is a function of Sensations um if we weren't very there in the and so this is just what you need crucially this is just what you need to derive the active inference algorithm if you if we went further in reviewing the free energy principle then we would see the fundamental role that the free energy plays um and so actually when you know reading the latest papers on the mathematical theory of the community principle the role of the variation of the energy is pretty clear but here the point is that if we just care about decision making if we just care about the normal active inference algorithm then we don't even need the variation of free energy so sure the community should be preferred why not if it is available but if it is not for some reason then there's no reason why not to use another kind of Divergence so very interesting it's making me think about linear regression and the sum of squares the L2 Norm is one approach that's often used to fit a regression line because it has good optimization properties there's good software packages there's good education there's good communication around it and so on but one can select other norms and choose to fit a linear regression with an L1 Norm or with an L3 Norm and so that entire question of fitting the linear regression is a degree of Freedom how the regression is fit that's Downstream of a commitment to for example Model A system in a generalized linear modeling framework and so analogously the Upstream commitment or the first principles which yes can be understood as axiomatic and also have some empirical status in terms of this partitioning a figure from ground the particular partition can simply be accepted axiomatically which is to say without appeal to evidence or somebody might have another Upstream Axiom and choose to model things according to a particular partition from there just like we could have chosen the l123 norm there are different discrepancy criteria or measures that we might want to use and some of them apply better or worse or not at all depending on what software Hardware data set and generative model we have and so it makes sense to pull pull back into the the Upstream understanding of active inference as a process Theory for particular partitioned systems and then for those who want to engage in the modeling to have that uh discussion about the garden of the branching paths well you could use a discrete timer you could use a continuous time and then from here you could do this sampling or you could do that one and if we have this computer access we can do that but if we have to do it this way we'll do it like that and that's all operational and logistical but it's actually all under the umbrella or under the auspice of the theoretical or conceptual commitments that are actually not being questioned once one is in that modeling discussion just like you could have the l123 norm conversation and maybe a reviewer asks you why you chose the two Norm versus the three but it's a broader level of questioning why one took on the linear modeling framework at all and our analogous Upstream bottleneck not in terms of rate limiting but just in terms of like eye of the needle is the particular partition yeah definitely and I just want to add something actually about the choice of divergences or the choice of discrepancy that you might want to use to solve the inference problem I think the analogy with linear regression is a really good one when you delay in your regression yeah you can use all types of norms and it's really a design choice and here in the algorithm you you also have that design Choice are you going to use KL so free energy to do these inferences by the way the inferences are really equation 43 and 44 which are approximate um actually you had them in a slide already OH 43 and 44 oh oh here got it yeah yeah um which are approximate um you know some posterior distribution with an approximate per serial distribution so you can do that with the free energy which is the same as the KL Divergence or you could do that with a whole bunch of other divergences one that I mentioned and that I think is particularly interesting is the maximum mean discrepancy and so um here's the difference between KL and maximum heat discrepancy at least like I guess a important conceptual difference so the KL when you look at distributions that are very close to each other it's going to measure it's basically going to become symmetric when when the distributions are very close and it's going to measure the amount of information uh that differs between them the maximum mean discrepancy when you take two distributions that are very close it's gonna it it reduces to what people call the Earth movers distance also called like the Vassar time distance so here is the intuition if you take two distributions that are very close just imagine the solution a as a backup dirt and distribution b as a pack of dirt or or a stand with some shape the maximum mean discrepancy is gonna tell you the amount of work that you need to put all that dirt from distribution a and pile it in the shape of distribution B so of course there's so many different ways in in which you could take all that dirt from distribution a and remodel it in distribution B but you're interested in the minimal um the minimal energetic cost that we take so like the optimal way of doing that movement people call that optimal transport um now if you're if you're familiar with optimal transport you know that the faster time distance um is the distance between probability distributions that regardless of how far they are is going to measure um like the it is going to give you this um cost of optimal transport so the energetic cost of moving all that um you know a pack of dirt from place a to place B in the optimal way um so the investors time distance is something that we could use here but maximum mean discrepancy it has a lot of very nice properties and basically reduces to the Watson distance when we consider very close distributions so uh this is not something that just a mathematical curiosity but it says that when one build when one builds a distance out of a Divergence what one does is one takes you know very close distributions measures the Divergence between them at which point the Divergence is pretty much symmetrical and then you basically keep adding divergences along a trajectory until you get to the final one and this way by aggregating divergences between very close distributions I'm doing that and you know adding that along a trajectory you get a distance a meaningful distance between you know distributions that could be very far if you do that with the KL Divergence you get What's called the Fischer information distance which measures really the um really um yeah the the amount of information that took you from gold to go from one distribution to another distribution regardless of how far they are um if you do that with advanced search time distance you will get the optimal transport cost so the amount of energy that you need to put all that dirt please say to place me in the optimal way if you do that with the maximum discrepancy you would also get that you will also get the optimal transport thing so um I just so the yeah I just want to say that the maximum discrepancy mean discrepancy between two distributions that are far away or not coincide with the vast search time distance which gives you this optimal transport cost but when you actually derive a distance from these divergences the distance derived from the maximum discrepancy and the Western distance will end up being the same and um this distance that you actually get or the topology that is derived from the distance so by topology the topology is really a notion of closeness and so to understand closeness you need to understand you know intestinal distances um so the infinitesimal distances derived from vassarstein or MMD they're the same and the topology that results is the topology of what people call weak convergence which is the standard topology that's considered in a probability Theory um so MMD and investors time they are very natural in that sense in the sense that they they uh they people say that they match rise weak conversions like they give give rise to the standard topology between probability distributions um so coming back to the choice of divergences if you're interested in approximating distributions in the sense of approximating their information content then KL is very natural but if it was for some reason and maybe would not be in this kind of application but another kind of application if for some reason you're interested in approximating distributions for the sake of how close they are when you look at them then then you would use MMD or Vassar shine wow great great information and I'm just thinking about we're switching lanes on the highway we're trying to get from here to there yes we want to know about the informational closeness of this Tale of Two densities but we also want to know about the transport closeness because we have a schedule and a budget and decisions to make and there are trade-offs so being able to move the Earth optimally while we're switching lanes and accelerating and slowing down I know this is mixing many angles informally we want to have a lot of options for how to think about that challenge of moving dirt between the tale of two densities and make sure everybody is driving in the right lane at the right time definitely and yeah it's a it's a design choice and it's an important one because it depends on you know what kind of properties we want to preserve and yes so all these all these divergences some some are not so useful I guess but um some of them they're they're just very natural and the Western and mme they're very natural in that sense um so yeah very important to keep in mind and not only for active inference but just you know in general uh for any kind of interest from them well let us each have a closing round of Reflections any thoughts or any next steps or any suggestions or other information you'd like to provide it's hard to say I feel we've you know we've covered so much already um there's still I think to me what's most exciting about is a scaling active inference right now um because you know you get this active interest algorithm in the paper uh that's derived from first principles and just from the derivation you see okay well there's actually so many things I mean the assumptions are so small that there are so many things that you can model with this active reference algorithm and basically you oh and all the heavy lifting is done by the genital model so if you use one relative model you will get one Behavior if you use another jointed model you will get another Behavior but all the equations will remain the same um so so it speaks the generality now in in having something that's very general you get something that's also very non-specific and I guess for for neuroscientists and people who are interested in intelligence you just um generality comes at a cost of being you know non-specific and non-specific about the brain in general um so really the big question to me uh long term is what kind of genitive models do we need to simulate brain-like Behavior because this is really the interesting Behavior or the most interesting Behavior um so it speaks to a big research program that a lot of people are carrying and have been carrying for a long time but you know um it's about what kind of representations do we have of the world what kind of priors do we have what kind of uh can we identify the priors that we are born with there's a lot of research on computational or just plain concrete science like normal cognitive science just studying babies and seeing what kind of priors what kind of you know basic information they have when they come out of the womb there's a lot of things that they can already do there our genetic code is preconditioning us to operate efficiently in this world and be able to flexibly adapt to any kind of situations that might arise in the natural world and so it's a huge research program to you know understand and model these priors and um yeah and not only the prayers but also the likelihood all the representation all the state spaces uh so I think that's the way forward the community principle is very elegant because it gives you you know a very succinct description in terms of a giant and model that enables you to simulate pretty much everything but we're not interested in simulating anything we're interested in simulating brain-like behaviors so now we need to you know drill down even more onto the kind of genetic models that that would be amenable to that great my closing reflection I feel like I know less about fep more about something else for what we've discussed Earth was moved Bayesian mechanics were called in decisions were made and it's been a really great Series so I'm appreciative and thankful that you suggested this paper in our correspondence as one to discuss you were absolutely right that it is relevant to bring to the attention of the active community and I hope that everybody who reads or listens this far has shifted Lanes so they can be where they want to be too well that yeah I mean uh thank you so much I also really enjoyed this session and I think we had a really cool discussion um so thanks again and hope to do this again soon excellent anytime you'd like farewell sealance there's thank you